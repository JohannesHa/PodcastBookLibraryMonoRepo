WEBVTT

00:00.000 --> 00:03.480
 The following is a conversation with Demis Hassabis,

00:03.480 --> 00:06.720
 CEO and co founder of DeepMind,

00:06.720 --> 00:08.600
 a company that has published and built

00:08.600 --> 00:12.200
 some of the most incredible artificial intelligence systems

00:12.200 --> 00:14.120
 in the history of computing,

00:14.120 --> 00:18.040
 including AlphaZero that learned all by itself

00:18.040 --> 00:21.000
 to play the game of go better than any human in the world

00:21.000 --> 00:25.760
 and AlphaFold2 that solved protein folding.

00:25.760 --> 00:28.800
 Both tasks considered nearly impossible

00:28.800 --> 00:30.360
 for a very long time.

00:31.240 --> 00:33.040
 Demis is widely considered to be

00:33.040 --> 00:35.720
 one of the most brilliant and impactful humans

00:35.720 --> 00:38.160
 in the history of artificial intelligence

00:38.160 --> 00:41.280
 and science and engineering in general.

00:41.280 --> 00:44.640
 This was truly an honor and a pleasure for me

00:44.640 --> 00:47.320
 to finally sit down with him for this conversation.

00:47.320 --> 00:50.580
 And I'm sure we will talk many times again in the future.

00:51.520 --> 00:53.320
 This is the Lux Readman podcast.

00:53.320 --> 00:55.500
 To support it, please check out our sponsors

00:55.500 --> 00:56.780
 in the description.

00:56.780 --> 01:00.560
 And now, dear friends, here's Demis Hassabis.

01:01.580 --> 01:04.040
 Let's start with a bit of a personal question.

01:04.040 --> 01:07.840
 Am I an AI program you wrote to interview people

01:07.840 --> 01:10.100
 until I get good enough to interview you?

01:11.100 --> 01:13.120
 Well, I'd be impressed if you were.

01:13.120 --> 01:14.880
 I'd be impressed by myself if you were.

01:14.880 --> 01:16.520
 I don't think we're quite up to that yet,

01:16.520 --> 01:18.800
 but maybe you're from the future, Lex.

01:18.800 --> 01:20.400
 If you did, would you tell me?

01:20.400 --> 01:23.080
 Is that a good thing to tell a language model

01:23.080 --> 01:25.080
 that's tasked with interviewing

01:25.080 --> 01:27.440
 that it is, in fact, AI?

01:27.440 --> 01:29.680
 Maybe we're in a kind of meta Turing test.

01:29.680 --> 01:32.440
 Probably it would be a good idea not to tell you,

01:32.440 --> 01:33.920
 so it doesn't change your behavior, right?

01:33.920 --> 01:35.080
 This is a kind of link.

01:35.080 --> 01:37.100
 Heisenberg uncertainty principle situation.

01:37.100 --> 01:39.080
 If I told you, you'd behave differently.

01:39.080 --> 01:40.980
 Maybe that's what's happening with us, of course.

01:40.980 --> 01:42.800
 This is a benchmark from the future

01:42.800 --> 01:46.560
 where they replay 2022 as a year

01:46.560 --> 01:49.440
 before AIs were good enough yet,

01:49.440 --> 01:52.080
 and now we want to see, is it gonna pass?

01:52.080 --> 01:52.920
 Exactly.

01:52.920 --> 01:56.000
 If I was such a program,

01:56.000 --> 01:57.960
 would you be able to tell, do you think?

01:57.960 --> 01:59.960
 So to the Turing test question,

01:59.960 --> 02:04.960
 you've talked about the benchmark for solving intelligence.

02:05.840 --> 02:07.320
 What would be the impressive thing?

02:07.320 --> 02:09.120
 You've talked about winning a Nobel Prize

02:09.120 --> 02:11.440
 and AIS system winning a Nobel Prize,

02:11.440 --> 02:14.400
 but I still return to the Turing test as a compelling test,

02:14.400 --> 02:17.280
 the spirit of the Turing test as a compelling test.

02:17.280 --> 02:18.520
 Yeah, the Turing test, of course,

02:18.520 --> 02:20.200
 it's been unbelievably influential,

02:20.200 --> 02:22.120
 and Turing's one of my all time heroes,

02:22.120 --> 02:24.920
 but I think if you look back at the 1950 paper,

02:24.920 --> 02:27.000
 his original paper and read the original,

02:27.000 --> 02:28.840
 you'll see, I don't think he meant it

02:28.840 --> 02:30.880
 to be a rigorous formal test.

02:30.880 --> 02:32.880
 I think it was more like a thought experiment,

02:32.880 --> 02:34.640
 almost a bit of philosophy he was writing

02:34.640 --> 02:36.440
 if you look at the style of the paper,

02:36.440 --> 02:38.640
 and you can see he didn't specify it very rigorously.

02:38.640 --> 02:41.840
 So for example, he didn't specify the knowledge

02:41.840 --> 02:44.040
 that the expert or judge would have.

02:45.440 --> 02:48.320
 How much time would they have to investigate this?

02:48.320 --> 02:49.500
 So these are important parameters

02:49.500 --> 02:53.220
 if you were gonna make it a true sort of formal test.

02:54.320 --> 02:58.400
 And by some measures, people claim the Turing test passed

02:58.400 --> 03:00.920
 several, a decade ago, I remember someone claiming that

03:00.920 --> 03:05.920
 with a kind of very bog standard, normal logic model,

03:06.000 --> 03:08.440
 because they pretended it was a kid.

03:08.440 --> 03:13.280
 So the judges thought that the machine was a child.

03:13.280 --> 03:15.360
 So that would be very different

03:15.360 --> 03:18.680
 from an expert AI person interrogating a machine

03:18.680 --> 03:20.720
 and knowing how it was built and so on.

03:20.720 --> 03:24.600
 So I think we should probably move away from that

03:24.600 --> 03:28.800
 as a formal test and move more towards a general test

03:28.800 --> 03:32.040
 where we test the AI capabilities on a range of tasks

03:32.040 --> 03:35.360
 and see if it reaches human level or above performance

03:35.360 --> 03:38.400
 on maybe thousands, perhaps even millions of tasks

03:38.400 --> 03:41.940
 eventually and cover the entire sort of cognitive space.

03:41.940 --> 03:44.140
 So I think for its time,

03:44.140 --> 03:45.520
 it was an amazing thought experiment.

03:45.520 --> 03:48.240
 And also 1950s, obviously there's barely the dawn

03:48.240 --> 03:49.440
 of the computer age.

03:49.440 --> 03:51.480
 So of course he only thought about text

03:51.480 --> 03:54.580
 and now we have a lot more different inputs.

03:54.580 --> 03:57.080
 So yeah, maybe the better thing to test

03:57.080 --> 03:59.660
 is the generalizability, so across multiple tasks.

03:59.660 --> 04:04.540
 But I think it's also possible as systems like Gato show

04:04.540 --> 04:08.320
 that eventually that might map right back to language.

04:08.320 --> 04:10.800
 So you might be able to demonstrate your ability

04:10.800 --> 04:14.760
 to generalize across tasks by then communicating

04:14.760 --> 04:17.080
 your ability to generalize across tasks,

04:17.080 --> 04:19.200
 which is kind of what we do through conversation anyway

04:19.200 --> 04:20.800
 when we jump around.

04:20.800 --> 04:23.720
 Ultimately what's in there in that conversation

04:23.720 --> 04:27.000
 is not just you moving around knowledge,

04:27.000 --> 04:30.360
 it's you moving around like these entirely different

04:30.360 --> 04:34.920
 modalities of understanding that ultimately map

04:34.920 --> 04:38.920
 to your ability to operate successfully

04:38.920 --> 04:42.600
 in all of these domains, which you can think of as tasks.

04:42.600 --> 04:45.600
 Yeah, I think certainly we as humans use language

04:45.600 --> 04:48.440
 as our main generalization communication tool.

04:48.440 --> 04:51.280
 So I think we end up thinking in language

04:51.280 --> 04:54.440
 and expressing our solutions in language.

04:54.440 --> 04:58.320
 So it's going to be a very powerful mode in which

04:58.320 --> 05:03.080
 to explain the system, to explain what it's doing.

05:03.080 --> 05:07.600
 But I don't think it's the only modality that matters.

05:07.600 --> 05:10.920
 So I think there's going to be a lot of different ways

05:10.920 --> 05:15.600
 to express capabilities other than just language.

05:15.600 --> 05:19.000
 Yeah, visual, robotics, body language,

05:21.120 --> 05:23.760
 yeah, actions, the interactive aspect of all that.

05:23.760 --> 05:24.760
 That's all part of it.

05:24.760 --> 05:27.600
 But what's interesting with Gato is that

05:27.600 --> 05:30.200
 it's sort of pushing prediction to the maximum

05:30.200 --> 05:33.400
 in terms of like mapping arbitrary sequences

05:33.400 --> 05:35.480
 to other sequences and sort of just predicting

05:35.480 --> 05:36.400
 what's going to happen next.

05:36.400 --> 05:41.040
 So prediction seems to be fundamental to intelligence.

05:41.040 --> 05:44.160
 And what you're predicting doesn't so much matter.

05:44.160 --> 05:46.840
 Yeah, it seems like you can generalize that quite well.

05:46.840 --> 05:49.640
 So obviously language models predict the next word,

05:49.640 --> 05:53.880
 Gato predicts potentially any action or any token.

05:53.880 --> 05:55.320
 And it's just the beginning really.

05:55.320 --> 05:58.120
 It's our most general agent one could call it so far,

05:58.120 --> 06:01.240
 but that itself can be scaled up massively more

06:01.240 --> 06:02.120
 than we've done so far.

06:02.120 --> 06:04.280
 And obviously we're in the middle of doing that.

06:04.280 --> 06:08.240
 But the big part of solving AGI is creating benchmarks

06:08.240 --> 06:11.080
 that help us get closer and closer,

06:11.080 --> 06:14.920
 sort of creating benchmarks that test the generalizability.

06:14.920 --> 06:17.440
 And it's just still interesting that this fella,

06:17.440 --> 06:20.520
 Alan Turing, was one of the first

06:20.520 --> 06:22.600
 and probably still one of the only people

06:22.600 --> 06:25.040
 that was trying, maybe philosophically,

06:25.040 --> 06:26.840
 but was trying to formulate a benchmark

06:26.840 --> 06:27.880
 that could be followed.

06:27.880 --> 06:30.960
 It is, even though it's fuzzy,

06:30.960 --> 06:32.520
 it's still sufficiently rigorous

06:32.520 --> 06:33.960
 to where you can run that test.

06:33.960 --> 06:36.640
 And I still think something like the Turing test

06:36.640 --> 06:38.720
 will, at the end of the day,

06:38.720 --> 06:42.560
 be the thing that truly impresses other humans

06:42.560 --> 06:46.400
 so that you can have a close friend who's an AI system.

06:46.400 --> 06:48.320
 And for that friend to be a good friend,

06:48.320 --> 06:53.160
 they're going to have to be able to play StarCraft

06:53.160 --> 06:56.600
 and they're gonna have to do all of these tasks,

06:56.600 --> 06:59.560
 get you a beer, so the robotics tasks,

07:00.440 --> 07:03.160
 play games with you, use language,

07:03.160 --> 07:04.800
 humor, all of those kinds of things.

07:04.800 --> 07:08.000
 But that ultimately can boil down to language.

07:08.000 --> 07:11.200
 It feels like, not in terms of the AI community,

07:11.200 --> 07:13.120
 but in terms of the actual impact

07:13.120 --> 07:14.760
 of general intelligence on the world,

07:14.760 --> 07:16.640
 it feels like language will be the place

07:16.640 --> 07:18.480
 where it truly shines.

07:18.480 --> 07:20.640
 I think so, because it's such an important

07:20.640 --> 07:22.480
 kind of input output for us.

07:22.480 --> 07:23.320
 I think you're right.

07:23.320 --> 07:24.680
 I think the Turing test,

07:24.680 --> 07:27.440
 the kind of the philosophy behind it,

07:27.440 --> 07:31.120
 which is the idea of can a machine mimic

07:31.120 --> 07:34.960
 the behaviors of a human?

07:34.960 --> 07:38.680
 And I would say wider than just language and text,

07:38.680 --> 07:41.320
 then in terms of actions and everything else,

07:41.320 --> 07:42.680
 creativity, all these things,

07:42.680 --> 07:45.320
 then if it can sort of match or exceed

07:45.320 --> 07:47.320
 human cognitive capabilities,

07:47.320 --> 07:49.880
 then I think we have a true intelligence.

07:49.880 --> 07:51.640
 So from that perspective, you're right.

07:51.640 --> 07:55.960
 I think he did formulate the right kind of setup.

07:55.960 --> 07:59.280
 I just, I think there'll be a kind of humor

07:59.280 --> 08:01.440
 in the AI systems of the future looking back

08:01.440 --> 08:06.040
 to this conversation and thinking about the Turing test

08:06.040 --> 08:09.360
 and also thinking about by that time,

08:09.360 --> 08:11.880
 they would know which year they were finally able

08:11.880 --> 08:15.560
 to sort of cross the threshold of human level intelligence

08:15.560 --> 08:18.800
 and think how funny it is that we humans

08:18.800 --> 08:22.400
 were still confused about this whole problem

08:22.400 --> 08:24.720
 that has been long ago been solved.

08:24.720 --> 08:27.240
 Well, hello to the AI agents of the future.

08:27.240 --> 08:30.320
 Anyway, so going back to your journey,

08:30.320 --> 08:33.800
 when did you fall in love with programming first?

08:33.800 --> 08:35.960
 Well, I was pretty young age actually.

08:35.960 --> 08:40.840
 So, I started off, actually games was my first love.

08:40.840 --> 08:43.680
 So starting to play chess when I was around four years old

08:43.680 --> 08:46.160
 and then it was actually with winnings

08:46.160 --> 08:48.400
 from a chess competition that I managed

08:48.400 --> 08:49.800
 to buy my first chess computer

08:49.800 --> 08:50.840
 when I was about eight years old.

08:50.840 --> 08:53.160
 It was a ZX Spectrum, which was hugely popular

08:53.160 --> 08:54.720
 in the UK at the time.

08:54.720 --> 08:58.520
 And it was amazing machine because I think it trained

08:58.520 --> 09:00.440
 a whole generation of programmers in the UK

09:00.440 --> 09:02.520
 because it was so accessible.

09:02.520 --> 09:03.800
 You know, you literally switched it on

09:03.800 --> 09:05.120
 and there was the basic prompt

09:05.120 --> 09:06.680
 and you could just get going.

09:06.680 --> 09:09.920
 And my parents didn't really know anything about computers.

09:09.920 --> 09:12.600
 So, but because it was my money from a chess competition,

09:12.600 --> 09:15.760
 I could say I wanted to buy it.

09:15.760 --> 09:17.960
 And then, you know, I just went to bookstores,

09:17.960 --> 09:22.040
 got books on programming and started typing in,

09:22.040 --> 09:23.520
 you know, the programming code.

09:23.520 --> 09:26.480
 And then of course, once you start doing that,

09:26.480 --> 09:29.120
 you start adjusting it and then making your own games.

09:29.120 --> 09:30.840
 And that's when I fell in love with computers

09:30.840 --> 09:33.880
 and realized that they were a very magical device.

09:34.840 --> 09:36.440
 In a way, I kind of, I wouldn't have been able

09:36.440 --> 09:37.440
 to explain this at the time,

09:37.440 --> 09:38.960
 but I felt that they were sort of almost

09:38.960 --> 09:40.920
 a magical extension of your mind.

09:40.920 --> 09:43.080
 I always had this feeling and I've always loved this

09:43.080 --> 09:46.160
 about computers that you can set them off doing something,

09:46.160 --> 09:48.520
 some task for you, you can go to sleep,

09:48.520 --> 09:51.240
 come back the next day and it's solved.

09:51.240 --> 09:53.080
 You know, that feels magical to me.

09:53.080 --> 09:55.280
 So, I mean, all machines do that to some extent.

09:55.280 --> 09:57.640
 They all enhance our natural capabilities.

09:57.640 --> 10:00.080
 Obviously cars make us, allow us to move faster

10:00.080 --> 10:04.520
 than we can run, but this was a machine to extend the mind.

10:04.520 --> 10:08.480
 And then of course, AI is the ultimate expression

10:08.480 --> 10:11.360
 of what a machine may be able to do or learn.

10:11.360 --> 10:14.440
 So very naturally for me, that thought extended

10:14.440 --> 10:16.040
 into AI quite quickly.

10:16.040 --> 10:18.560
 Do you remember the programming language

10:18.560 --> 10:22.080
 that was first started and was it special to the machine?

10:22.080 --> 10:25.880
 No, I think it was just basic on the ZX Spectrum.

10:25.880 --> 10:27.480
 I don't know what specific form it was.

10:27.480 --> 10:29.600
 And then later on I got a Commodore Amiga,

10:29.600 --> 10:32.760
 which was a fantastic machine.

10:32.760 --> 10:33.800
 Now you're just showing off.

10:33.800 --> 10:36.520
 So yeah, well, lots of my friends had Atari STs

10:36.520 --> 10:38.880
 and I managed to get Amigas, it was a bit more powerful

10:38.880 --> 10:42.800
 and that was incredible and used to do programming

10:42.800 --> 10:46.240
 in assembler and also Amos basic,

10:46.240 --> 10:49.280
 this specific form of basic, it was incredible actually.

10:49.280 --> 10:51.000
 So I learned all my coding skills.

10:51.000 --> 10:53.040
 And when did you fall in love with AI?

10:53.040 --> 10:56.880
 So when did you first start to gain an understanding

10:56.880 --> 10:58.880
 that you can not just write programs

10:58.880 --> 11:01.640
 that do some mathematical operations for you

11:01.640 --> 11:05.400
 while you sleep, but something that's akin

11:05.400 --> 11:08.800
 to bringing an entity to life,

11:08.800 --> 11:11.840
 sort of a thing that can figure out something

11:11.840 --> 11:15.920
 more complicated than a simple mathematical operation.

11:15.920 --> 11:17.600
 Yeah, so there was a few stages for me

11:17.600 --> 11:18.920
 all while I was very young.

11:18.920 --> 11:21.680
 So first of all, as I was trying to improve

11:21.680 --> 11:23.160
 at playing chess, I was captaining

11:23.160 --> 11:24.680
 various England junior chess teams.

11:24.680 --> 11:27.480
 And at the time when I was about maybe 10, 11 years old,

11:27.480 --> 11:29.360
 I was gonna become a professional chess player.

11:29.360 --> 11:32.000
 That was my first thought.

11:32.000 --> 11:34.680
 So that dream was there to try to get

11:34.680 --> 11:35.520
 to the highest levels of chess.

11:35.520 --> 11:39.160
 Yeah, so when I was about 12 years old,

11:39.160 --> 11:41.320
 I got to master standard and I was second highest rated

11:41.320 --> 11:42.680
 player in the world to Judith Polgar,

11:42.680 --> 11:45.760
 who obviously ended up being an amazing chess player

11:45.760 --> 11:48.560
 and a world women's champion.

11:48.560 --> 11:50.760
 And when I was trying to improve at chess,

11:50.760 --> 11:52.760
 where what you do is you obviously, first of all,

11:52.760 --> 11:55.120
 you're trying to improve your own thinking processes.

11:55.120 --> 11:58.080
 So that leads you to thinking about thinking,

11:58.080 --> 12:00.400
 how is your brain coming up with these ideas?

12:00.400 --> 12:01.880
 Why is it making mistakes?

12:01.880 --> 12:04.520
 How can you improve that thought process?

12:04.520 --> 12:06.360
 But the second thing is that you,

12:06.360 --> 12:09.640
 it was just the beginning, this was like in the early 80s,

12:09.640 --> 12:11.240
 mid 80s of chess computers.

12:11.240 --> 12:12.760
 If you remember, they were physical balls

12:12.760 --> 12:14.000
 like the one we have in front of us.

12:14.000 --> 12:17.000
 And you press down the squares.

12:17.000 --> 12:19.600
 And I think Kasparov had a branded version of it

12:19.600 --> 12:21.000
 that I got.

12:21.000 --> 12:24.640
 And you used to, they're not as strong as they are today,

12:24.640 --> 12:27.320
 but they were pretty strong and you used to practice

12:27.320 --> 12:30.560
 against them to try and improve your openings

12:30.560 --> 12:31.480
 and other things.

12:31.480 --> 12:33.440
 And so I remember, I think I probably got my first one,

12:33.440 --> 12:34.920
 I was around 11 or 12.

12:34.920 --> 12:37.760
 And I remember thinking, this is amazing,

12:37.760 --> 12:42.760
 how has someone programmed this chess board to play chess?

12:42.920 --> 12:45.600
 And it was very formative book I bought,

12:45.600 --> 12:47.760
 which was called The Chess Computer Handbook

12:47.760 --> 12:49.000
 by David Levy.

12:49.000 --> 12:50.680
 This thing came out in 1984 or something.

12:50.680 --> 12:52.360
 So I must've got it when I was about 11, 12.

12:52.360 --> 12:56.120
 And it explained fully how these chess programs were made.

12:56.120 --> 12:57.680
 And I remember my first AI program

12:57.680 --> 13:00.440
 being programming my Amiga.

13:00.440 --> 13:02.920
 It couldn't, it wasn't powerful enough to play chess.

13:02.920 --> 13:04.200
 I couldn't write a whole chess program,

13:04.200 --> 13:07.480
 but I wrote a program for it to play Othello or reverse it,

13:07.480 --> 13:09.320
 sometimes called I think in the US.

13:09.320 --> 13:11.760
 And so a slightly simpler game than chess,

13:11.760 --> 13:14.360
 but I used all of the principles that chess programs had,

13:14.360 --> 13:16.000
 alpha, beta, search, all of that.

13:16.000 --> 13:17.440
 And that was my first AI program.

13:17.440 --> 13:19.440
 I remember that very well, I was around 12 years old.

13:19.440 --> 13:21.680
 So that brought me into AI.

13:21.680 --> 13:24.120
 And then the second part was later on,

13:24.120 --> 13:25.560
 when I was around 16, 17,

13:25.560 --> 13:28.840
 and I was writing games professionally, designing games,

13:28.840 --> 13:30.640
 writing a game called Theme Park,

13:30.640 --> 13:34.040
 which had AI as a core gameplay component

13:34.040 --> 13:35.680
 as part of the simulation.

13:35.680 --> 13:38.480
 And it sold millions of copies around the world.

13:38.480 --> 13:41.000
 And people loved the way that the AI,

13:41.000 --> 13:42.280
 even though it was relatively simple

13:42.280 --> 13:44.440
 by today's AI standards,

13:44.440 --> 13:47.720
 was reacting to the way you as the player played it.

13:47.720 --> 13:49.240
 So it was called a sandbox game.

13:49.240 --> 13:51.320
 So it was one of the first types of games like that,

13:51.320 --> 13:52.680
 along with SimCity.

13:52.680 --> 13:55.720
 And it meant that every game you played was unique.

13:55.720 --> 13:58.840
 Is there something you could say just on a small tangent

13:58.840 --> 14:02.160
 about really impressive AI

14:02.160 --> 14:06.560
 from a game design, human enjoyment perspective,

14:06.560 --> 14:09.680
 really impressive AI that you've seen in games

14:09.680 --> 14:12.480
 and maybe what does it take to create an AI system?

14:12.480 --> 14:14.240
 And how hard of a problem is that?

14:14.240 --> 14:18.360
 So a million questions just as a brief tangent.

14:18.360 --> 14:23.000
 Well, look, I think games have been significant in my life

14:23.000 --> 14:23.840
 for three reasons.

14:23.840 --> 14:26.080
 So first of all, I was playing them

14:26.080 --> 14:28.800
 and training myself on games when I was a kid.

14:28.800 --> 14:31.480
 Then I went through a phase of designing games

14:31.480 --> 14:33.000
 and writing AI for games.

14:33.000 --> 14:35.960
 So all the games I professionally wrote

14:35.960 --> 14:37.680
 had AI as a core component.

14:37.680 --> 14:40.040
 And that was mostly in the 90s.

14:40.040 --> 14:42.960
 And the reason I was doing that in games industry

14:42.960 --> 14:45.080
 was at the time the games industry,

14:45.080 --> 14:47.160
 I think was the cutting edge of technology.

14:47.160 --> 14:49.800
 So whether it was graphics with people like John Carmack

14:49.800 --> 14:53.040
 and Quake and those kinds of things or AI,

14:53.040 --> 14:56.160
 I think actually all the action was going on in games.

14:56.160 --> 14:58.440
 And we're still reaping the benefits of that

14:58.440 --> 15:01.480
 even with things like GPUs, which I find ironic

15:01.480 --> 15:03.680
 was obviously invented for graphics, computer graphics,

15:03.680 --> 15:06.320
 but then turns out to be amazingly useful for AI.

15:06.320 --> 15:08.760
 It just turns out everything's a matrix multiplication

15:08.760 --> 15:11.080
 it appears in the whole world.

15:11.080 --> 15:15.800
 So I think games at the time had the most cutting edge AI.

15:15.800 --> 15:19.800
 And a lot of the games, I was involved in writing.

15:19.800 --> 15:21.280
 So there was a game called Black and White,

15:21.280 --> 15:22.760
 which was one game I was involved with

15:22.760 --> 15:24.000
 in the early stages of,

15:24.000 --> 15:28.400
 which I still think is the most impressive example

15:28.400 --> 15:30.560
 of reinforcement learning in a computer game.

15:30.560 --> 15:34.600
 So in that game, you trained a little pet animal and...

15:34.600 --> 15:35.440
 It's a brilliant game.

15:35.440 --> 15:37.640
 And it sort of learned from how you were treating it.

15:37.640 --> 15:40.680
 So if you treated it badly, then it became mean.

15:40.680 --> 15:42.920
 And then it would be mean to your villagers

15:42.920 --> 15:45.760
 and your population, the sort of the little tribe

15:45.760 --> 15:47.240
 that you were running.

15:47.240 --> 15:49.400
 But if you were kind to it, then it would be kind.

15:49.400 --> 15:51.080
 And people were fascinated by how that works.

15:51.080 --> 15:54.160
 And so was I to be honest with the way it kind of developed.

15:54.160 --> 15:55.120
 And...

15:55.120 --> 15:57.240
 Especially the mapping to good and evil.

15:57.240 --> 15:58.080
 Yeah.

15:58.080 --> 16:01.640
 Made you realize, made me realize that you can sort of

16:01.640 --> 16:06.640
 in the choices you make can define where you end up.

16:07.440 --> 16:12.440
 And that means all of us are capable of the good, evil.

16:12.640 --> 16:15.240
 It all matters in the different choices

16:15.240 --> 16:18.240
 along the trajectory to those places that you make.

16:18.240 --> 16:19.080
 It's fascinating.

16:19.080 --> 16:21.360
 I mean, games can do that philosophically to you.

16:21.360 --> 16:22.200
 And it's rare.

16:22.200 --> 16:23.040
 It seems rare.

16:23.040 --> 16:23.880
 Yeah.

16:23.880 --> 16:24.720
 Well, games are, I think, a unique medium

16:24.720 --> 16:26.600
 because you as the player,

16:26.600 --> 16:30.080
 you're not just passively consuming the entertainment,

16:30.080 --> 16:30.920
 right?

16:30.920 --> 16:34.280
 You're actually actively involved as an agent.

16:34.280 --> 16:36.160
 So I think that's what makes it in some ways

16:36.160 --> 16:38.400
 can be more visceral than other mediums

16:38.400 --> 16:40.000
 like films and books.

16:40.000 --> 16:42.640
 So the second, so that was designing AI in games.

16:42.640 --> 16:46.440
 And then the third use we've used of AI

16:46.440 --> 16:48.440
 is in DeepMind from the beginning,

16:48.440 --> 16:50.920
 which is using games as a testing ground

16:50.920 --> 16:55.000
 for proving out AI algorithms and developing AI algorithms.

16:55.000 --> 16:58.480
 And that was a sort of a core component

16:58.480 --> 17:00.320
 of our vision at the start of DeepMind

17:00.320 --> 17:03.200
 was that we would use games very heavily

17:03.200 --> 17:06.360
 as our main testing ground, certainly to begin with,

17:06.360 --> 17:08.560
 because it's super efficient to use games.

17:08.560 --> 17:11.480
 And also, it's very easy to have metrics

17:11.480 --> 17:14.080
 to see how well your systems are improving

17:14.080 --> 17:15.840
 and what direction your ideas are going in

17:15.840 --> 17:18.400
 and whether you're making incremental improvements.

17:18.400 --> 17:20.400
 And because those games are often rooted

17:20.400 --> 17:23.360
 in something that humans did for a long time beforehand,

17:23.360 --> 17:26.520
 there's already a strong set of rules.

17:26.520 --> 17:28.240
 Like it's already a damn good benchmark.

17:28.240 --> 17:30.200
 Yes, it's really good for so many reasons

17:30.200 --> 17:32.800
 because you've got clear measures

17:32.800 --> 17:35.520
 of how good humans can be at these things.

17:35.520 --> 17:36.840
 And in some cases like Go,

17:36.840 --> 17:39.720
 we've been playing it for thousands of years

17:39.720 --> 17:43.280
 and often they have scores or at least win conditions.

17:43.280 --> 17:45.640
 So it's very easy for reward learning systems

17:45.640 --> 17:46.480
 to get a reward.

17:46.480 --> 17:49.320
 It's very easy to specify what that reward is.

17:49.320 --> 17:54.320
 And also at the end, it's easy to test externally

17:54.320 --> 17:56.920
 at how strong is your system by of course,

17:56.920 --> 18:00.240
 playing against the world's strongest players at those games.

18:00.240 --> 18:02.680
 So it's so good for so many reasons

18:02.680 --> 18:05.520
 and it's also very efficient to run potentially millions

18:05.520 --> 18:08.280
 of simulations in parallel on the cloud.

18:08.280 --> 18:12.800
 So I think there's a huge reason why we were so successful

18:12.800 --> 18:14.760
 back in starting out 2010,

18:14.760 --> 18:16.680
 how come we were able to progress so quickly

18:16.680 --> 18:18.880
 because we've utilized games.

18:18.880 --> 18:21.320
 And at the beginning of DeepMind,

18:21.320 --> 18:24.600
 we also hired some amazing game engineers

18:24.600 --> 18:28.000
 who I knew from my previous lives in the games industry.

18:28.000 --> 18:30.920
 And that helped to bootstrap us very quickly.

18:30.920 --> 18:33.880
 And plus it's somehow super compelling

18:33.880 --> 18:38.080
 almost at a philosophical level of man versus machine

18:38.080 --> 18:41.200
 over a chess board or a Go board.

18:41.200 --> 18:43.600
 And especially given that the entire history of AI

18:43.600 --> 18:45.960
 is defined by people saying it's gonna be impossible

18:45.960 --> 18:50.960
 to make a machine that beats a human being in chess.

18:50.960 --> 18:53.200
 And then once that happened,

18:53.200 --> 18:55.880
 people were certain when I was coming up in AI

18:55.880 --> 18:58.760
 that Go is not a game that can be solved

18:58.760 --> 19:02.000
 because of the combinatorial complexity is just too,

19:02.000 --> 19:06.640
 it's no matter how much Moore's law you have,

19:06.640 --> 19:08.560
 compute is just never going to be able

19:08.560 --> 19:10.200
 to crack the game of Go.

19:10.200 --> 19:14.920
 And so then there's something compelling about facing,

19:14.920 --> 19:18.160
 sort of taking on the impossibility of that task

19:18.160 --> 19:22.480
 from the AI researcher perspective,

19:22.480 --> 19:24.520
 engineer perspective, and then as a human being,

19:24.520 --> 19:26.120
 just observing this whole thing.

19:27.040 --> 19:31.560
 Your beliefs about what you thought was impossible

19:32.520 --> 19:34.120
 being broken apart,

19:35.920 --> 19:40.480
 it's humbling to realize we're not as smart as we thought.

19:40.480 --> 19:43.160
 It's humbling to realize that the things we think

19:43.160 --> 19:47.000
 are impossible now perhaps will be done in the future.

19:47.000 --> 19:50.800
 There's something really powerful about a game,

19:50.800 --> 19:52.880
 AI system beating human being in a game

19:52.880 --> 19:55.680
 that drives that message home

19:55.680 --> 19:58.000
 for like millions, billions of people,

19:58.000 --> 19:59.320
 especially in the case of Go.

19:59.320 --> 20:00.520
 Sure.

20:00.520 --> 20:01.640
 Well, look, I think it's,

20:01.640 --> 20:03.720
 I mean, it has been a fascinating journey

20:03.720 --> 20:06.880
 and especially as I think about it from,

20:06.880 --> 20:08.760
 I can understand it from both sides,

20:08.760 --> 20:13.080
 both as the AI, creators of the AI,

20:13.080 --> 20:15.640
 but also as a games player originally.

20:15.640 --> 20:17.960
 So, it was a really interesting,

20:17.960 --> 20:21.160
 I mean, it was a fantastic, but also somewhat

20:21.160 --> 20:23.520
 bittersweet moment, the AlphaGo match for me,

20:24.680 --> 20:28.520
 seeing that and being obviously heavily involved in that.

20:29.440 --> 20:32.480
 But as you say, chess has been the,

20:32.480 --> 20:34.360
 I mean, Kasparov, I think rightly called it

20:34.360 --> 20:37.280
 the Drosophila of intelligence, right?

20:37.280 --> 20:39.520
 So, it's sort of, I love that phrase

20:39.520 --> 20:42.960
 and I think he's right because chess has been

20:42.960 --> 20:45.360
 hand in hand with AI from the beginning

20:45.360 --> 20:47.440
 of the whole field, right?

20:47.440 --> 20:49.640
 So, I think every AI practitioner,

20:49.640 --> 20:52.480
 starting with Turing and Claude Shannon and all those,

20:52.480 --> 20:55.400
 the sort of forefathers of the field,

20:56.320 --> 20:58.840
 tried their hand at writing a chess program.

20:58.840 --> 21:01.160
 I've got original edition of Claude Shannon's

21:01.160 --> 21:04.000
 first chess program, I think it was 1949,

21:04.000 --> 21:06.760
 the original sort of paper.

21:06.760 --> 21:09.960
 And they all did that and Turing famously wrote

21:09.960 --> 21:12.480
 a chess program, but all the computers around them

21:12.480 --> 21:13.760
 were obviously too slow to run it.

21:13.760 --> 21:16.040
 So, he had to run, he had to be the computer, right?

21:16.040 --> 21:18.920
 So, he literally, I think spent two or three days

21:18.920 --> 21:21.360
 running his own program by hand with pencil and paper

21:21.360 --> 21:24.960
 and playing a friend of his with his chess program.

21:24.960 --> 21:28.560
 So, of course, Deep Blue was a huge moment,

21:28.560 --> 21:31.880
 beating Kasparov, but actually when that happened,

21:31.880 --> 21:34.120
 I remember that very vividly, of course,

21:34.120 --> 21:36.640
 because it was chess and computers and AI,

21:36.640 --> 21:39.240
 all the things I loved and I was at college at the time.

21:39.240 --> 21:40.800
 But I remember coming away from that,

21:40.800 --> 21:43.080
 being more impressed by Kasparov's mind

21:43.080 --> 21:44.480
 than I was by Deep Blue.

21:44.480 --> 21:47.680
 Because here was Kasparov with his human mind,

21:47.680 --> 21:49.400
 not only could he play chess more or less

21:49.400 --> 21:53.160
 to the same level as this brute of a calculation machine,

21:53.160 --> 21:55.160
 but of course, Kasparov can do everything else

21:55.160 --> 21:57.480
 humans can do, ride a bike, talk many languages,

21:57.480 --> 21:59.400
 do politics, all the rest of the amazing things

21:59.400 --> 22:00.880
 that Kasparov does.

22:00.880 --> 22:03.160
 And so, with the same brain.

22:03.160 --> 22:07.040
 And yet Deep Blue, brilliant as it was at chess,

22:07.040 --> 22:12.040
 it'd been hand coded for chess and actually had distilled

22:12.040 --> 22:16.400
 the knowledge of chess grandmasters into a cool program,

22:16.400 --> 22:18.000
 but it couldn't do anything else.

22:18.000 --> 22:20.080
 It couldn't even play a strictly simpler game

22:20.080 --> 22:21.280
 like tic tac toe.

22:21.280 --> 22:25.880
 So, something to me was missing from intelligence

22:25.880 --> 22:28.480
 from that system that we would regard as intelligence.

22:28.480 --> 22:30.880
 And I think it was this idea of generality

22:30.880 --> 22:32.160
 and also learning.

22:33.000 --> 22:36.120
 So, and that's obviously what we tried to do with AlphaGo.

22:36.120 --> 22:38.600
 Yeah, with AlphaGo and AlphaZero, MuZero,

22:38.600 --> 22:42.040
 and then God and all the things that we'll get into

22:42.040 --> 22:45.640
 some parts of, there's just a fascinating trajectory here.

22:45.640 --> 22:48.520
 But let's just stick on chess briefly.

22:48.520 --> 22:51.960
 On the human side of chess, you've proposed that

22:51.960 --> 22:53.400
 from a game design perspective,

22:53.400 --> 22:56.440
 the thing that makes chess compelling as a game

22:57.760 --> 23:01.000
 is that there's a creative tension between a bishop

23:01.000 --> 23:02.960
 and the knight.

23:02.960 --> 23:04.040
 Can you explain this?

23:04.040 --> 23:06.440
 First of all, it's really interesting to think about

23:06.440 --> 23:08.640
 what makes a game compelling,

23:08.640 --> 23:11.000
 makes it stick across centuries.

23:12.000 --> 23:13.480
 Yeah, I was sort of thinking about this,

23:13.480 --> 23:15.440
 and actually a lot of even amazing chess players

23:15.440 --> 23:16.840
 don't think about it necessarily

23:16.840 --> 23:18.280
 from a game's designer point of view.

23:18.280 --> 23:20.240
 So, it's with my game design hat on

23:20.240 --> 23:23.080
 that I was thinking about this, why is chess so compelling?

23:23.080 --> 23:27.560
 And I think a critical reason is the dynamicness

23:27.560 --> 23:30.000
 of the different kind of chess positions you can have,

23:30.000 --> 23:32.200
 whether they're closed or open and other things,

23:32.200 --> 23:33.520
 comes from the bishop and the knight.

23:33.520 --> 23:36.480
 So, if you think about how different

23:36.480 --> 23:39.240
 the capabilities of the bishop and knight are

23:39.240 --> 23:40.880
 in terms of the way they move,

23:40.880 --> 23:43.080
 and then somehow chess has evolved

23:43.080 --> 23:46.080
 to balance those two capabilities more or less equally.

23:46.080 --> 23:48.720
 So, they're both roughly worth three points each.

23:48.720 --> 23:50.560
 So, you think that dynamics is always there

23:50.560 --> 23:51.640
 and then the rest of the rules

23:51.640 --> 23:53.760
 are kind of trying to stabilize the game.

23:53.760 --> 23:55.080
 Well, maybe, I mean, it's sort of,

23:55.080 --> 23:56.560
 I don't know, it's chicken and egg situation,

23:56.560 --> 23:57.680
 probably both came together.

23:57.680 --> 24:00.520
 But the fact that it's got to this beautiful equilibrium

24:00.520 --> 24:02.360
 where you can have the bishop and knight

24:02.360 --> 24:04.400
 that are so different in power,

24:04.400 --> 24:06.920
 but so equal in value across the set

24:06.920 --> 24:09.480
 of the universe of all positions, right?

24:09.480 --> 24:11.560
 Somehow they've been balanced by humanity

24:11.560 --> 24:13.480
 over hundreds of years,

24:13.480 --> 24:16.880
 I think gives the game the creative tension

24:16.880 --> 24:19.000
 that you can swap the bishop and knights

24:19.000 --> 24:20.160
 for a bishop for a knight,

24:20.160 --> 24:22.080
 and they're more or less worth the same,

24:22.080 --> 24:24.040
 but now you aim for a different type of position.

24:24.040 --> 24:26.040
 If you have the knight, you want a closed position.

24:26.040 --> 24:28.160
 If you have the bishop, you want an open position.

24:28.160 --> 24:29.000
 So, I think that creates

24:29.000 --> 24:30.920
 a lot of the creative tension in chess.

24:30.920 --> 24:34.040
 So, some kind of controlled creative tension.

24:34.040 --> 24:35.960
 From an AI perspective,

24:35.960 --> 24:38.840
 do you think AI systems could eventually design games

24:38.840 --> 24:41.640
 that are optimally compelling to humans?

24:41.640 --> 24:42.920
 Well, that's an interesting question.

24:42.920 --> 24:46.000
 Sometimes I get asked about AI and creativity,

24:46.000 --> 24:48.880
 and the way I answered that is relevant to that question,

24:48.880 --> 24:51.240
 which is that I think there are different levels

24:51.240 --> 24:52.920
 of creativity, one could say.

24:52.920 --> 24:55.320
 So, I think if we define creativity

24:55.320 --> 24:57.280
 as coming up with something original, right,

24:57.280 --> 24:59.320
 that's useful for a purpose,

24:59.320 --> 25:02.240
 then I think the kind of lowest level of creativity

25:02.240 --> 25:03.720
 is like an interpolation.

25:03.720 --> 25:06.280
 So, an averaging of all the examples you see.

25:06.280 --> 25:08.320
 So, maybe a very basic AI system could say

25:08.320 --> 25:09.160
 you could have that.

25:09.160 --> 25:11.400
 So, you show it millions of pictures of cats,

25:11.400 --> 25:13.920
 and then you say, give me an average looking cat, right?

25:13.920 --> 25:15.480
 Generate me an average looking cat.

25:15.480 --> 25:17.200
 I would call that interpolation.

25:17.200 --> 25:18.720
 Then there's extrapolation,

25:18.720 --> 25:20.440
 which something like AlphaGo showed.

25:20.440 --> 25:24.320
 So, AlphaGo played millions of games of Go against itself,

25:24.320 --> 25:26.600
 and then it came up with brilliant new ideas

25:26.600 --> 25:30.760
 like Move 37 in game two, brilliant motif strategies in Go

25:30.760 --> 25:32.840
 that no humans had ever thought of,

25:32.840 --> 25:34.800
 even though we've played it for thousands of years

25:34.800 --> 25:36.600
 and professionally for hundreds of years.

25:36.600 --> 25:38.840
 So, that I call that extrapolation,

25:38.840 --> 25:41.080
 but then there's still a level above that,

25:41.080 --> 25:44.000
 which is, you could call out of the box thinking

25:44.000 --> 25:47.520
 or true innovation, which is, could you invent Go, right?

25:47.520 --> 25:49.200
 Could you invent chess and not just come up

25:49.200 --> 25:51.320
 with a brilliant chess move or brilliant Go move,

25:51.320 --> 25:53.680
 but can you actually invent chess

25:53.680 --> 25:55.880
 or something as good as chess or Go?

25:55.880 --> 26:00.080
 And I think one day AI could, but then what's missing

26:00.080 --> 26:02.240
 is how would you even specify that task

26:02.240 --> 26:04.440
 to a program right now?

26:04.440 --> 26:07.560
 And the way I would do it if I was telling a human to do it

26:07.560 --> 26:10.800
 or a human games designer to do it is I would say,

26:10.800 --> 26:14.120
 something like Go, I would say, come up with a game

26:14.120 --> 26:16.080
 that only takes five minutes to learn,

26:16.080 --> 26:17.880
 which Go does because it's got simple rules,

26:17.880 --> 26:20.280
 but many lifetimes to master, right?

26:20.280 --> 26:22.080
 Or impossible to master in one lifetime

26:22.080 --> 26:23.920
 because it's so deep and so complex.

26:23.920 --> 26:26.520
 And then it's aesthetically beautiful.

26:26.520 --> 26:30.080
 And also it can be completed in three or four hours

26:30.080 --> 26:35.080
 of gameplay time, which is useful for us in a human day.

26:35.280 --> 26:38.560
 And so you might specify these sort of high level concepts

26:38.560 --> 26:40.640
 like that, and then with that

26:40.640 --> 26:42.800
 and then maybe a few other things,

26:42.800 --> 26:47.560
 one could imagine that Go satisfies those constraints.

26:47.560 --> 26:49.600
 But the problem is that we're not able

26:49.600 --> 26:53.040
 to specify abstract notions like that,

26:53.040 --> 26:57.040
 high level abstract notions like that yet to our AI systems.

26:57.040 --> 26:58.840
 And I think there's still something missing there

26:58.840 --> 27:01.840
 in terms of high level concepts or abstractions

27:01.840 --> 27:03.080
 that they truly understand

27:03.080 --> 27:06.560
 and they're combinable and compositional.

27:06.560 --> 27:09.760
 So for the moment, I think AI is capable

27:09.760 --> 27:11.760
 of doing interpolation and extrapolation,

27:11.760 --> 27:13.520
 but not true invention.

27:13.520 --> 27:18.040
 So coming up with rule sets and optimizing

27:18.040 --> 27:20.640
 with complicated objectives around those rule sets,

27:20.640 --> 27:22.280
 we can't currently do.

27:22.280 --> 27:25.480
 But you could take a specific rule set

27:25.480 --> 27:28.360
 and then run a kind of self play experiment

27:28.360 --> 27:32.040
 to see how long, just observe how an AI system

27:32.040 --> 27:35.960
 from scratch learns, how long is that journey of learning?

27:35.960 --> 27:39.160
 And maybe if it satisfies some of those other things

27:39.160 --> 27:41.680
 you mentioned in terms of quickness to learn and so on,

27:41.680 --> 27:44.200
 and you could see a long journey to master

27:44.200 --> 27:46.920
 for even an AI system, then you could say

27:46.920 --> 27:49.280
 that this is a promising game.

27:49.280 --> 27:51.720
 But it would be nice to do almost like AlphaCode

27:51.720 --> 27:53.960
 so programming rules.

27:53.960 --> 27:58.960
 So generating rules that automate even that part

27:59.000 --> 28:00.440
 of the generation of rules.

28:00.440 --> 28:02.960
 So I have thought about systems actually

28:02.960 --> 28:05.680
 that I think would be amazing for a games designer.

28:05.680 --> 28:09.200
 If you could have a system that takes your game,

28:09.200 --> 28:11.960
 plays it tens of millions of times, maybe overnight,

28:11.960 --> 28:13.840
 and then self balances the rules better.

28:13.840 --> 28:18.080
 So it tweaks the rules and maybe the equations

28:18.080 --> 28:22.680
 and the parameters so that the game is more balanced,

28:22.680 --> 28:26.280
 the units in the game or some of the rules could be tweaked.

28:26.280 --> 28:28.320
 So it's a bit of like giving a base set

28:28.320 --> 28:30.800
 and then allowing Monte Carlo Tree Search

28:30.800 --> 28:33.360
 or something like that to sort of explore it.

28:33.360 --> 28:37.080
 And I think that would be super powerful tool actually

28:37.080 --> 28:39.720
 for balancing, auto balancing a game,

28:39.720 --> 28:42.120
 which usually takes thousands of hours

28:42.120 --> 28:44.520
 from hundreds of human games testers normally

28:44.520 --> 28:47.480
 to balance a game like StarCraft,

28:47.480 --> 28:50.640
 which is Blizzard are amazing at balancing their games,

28:50.640 --> 28:52.600
 but it takes them years and years and years.

28:52.600 --> 28:54.120
 So one could imagine at some point

28:54.120 --> 28:57.080
 when this stuff becomes efficient enough

28:57.080 --> 28:59.560
 to you might be able to do that like overnight.

28:59.560 --> 29:04.560
 Do you think a game that is optimal designed by an AI system

29:05.000 --> 29:08.320
 would look very much like a planet earth?

29:09.640 --> 29:11.640
 Maybe, maybe it's only the sort of game

29:11.640 --> 29:16.040
 I would love to make is, and I've tried in my games career,

29:16.040 --> 29:18.560
 the games design career, my first big game

29:18.560 --> 29:21.440
 was designing a theme park, an amusement park.

29:21.440 --> 29:25.200
 Then with games like Republic, I tried to have games

29:25.200 --> 29:28.480
 where we designed whole cities and allowed you to play in.

29:28.480 --> 29:30.320
 So, and of course people like Will Wright

29:30.320 --> 29:32.640
 have written games like SimEarth,

29:32.640 --> 29:35.200
 trying to simulate the whole of earth, pretty tricky,

29:35.200 --> 29:36.040
 but I think.

29:36.040 --> 29:37.600
 SimEarth, I haven't actually played that one.

29:37.600 --> 29:38.440
 So what is it?

29:38.440 --> 29:40.320
 Does it incorporate of evolution or?

29:40.320 --> 29:43.280
 Yeah, it has evolution and it sort of tries to,

29:43.280 --> 29:45.320
 it sort of treats it as an entire biosphere,

29:45.320 --> 29:47.240
 but from quite high level.

29:47.240 --> 29:48.080
 So.

29:48.080 --> 29:50.280
 It'd be nice to be able to sort of zoom in,

29:50.280 --> 29:51.320
 zoom out and zoom in.

29:51.320 --> 29:52.160
 Exactly, exactly.

29:52.160 --> 29:53.440
 So obviously it couldn't do, that was in the 90s.

29:53.440 --> 29:54.920
 I think he wrote that in the 90s.

29:54.920 --> 29:57.560
 So it couldn't, it wasn't able to do that,

29:57.560 --> 30:00.520
 but that would be obviously the ultimate sandbox game.

30:00.520 --> 30:01.480
 Of course.

30:01.480 --> 30:04.760
 On that topic, do you think we're living in a simulation?

30:04.760 --> 30:06.160
 Yes, well, so, okay.

30:06.160 --> 30:07.000
 So I.

30:07.000 --> 30:09.280
 We're gonna jump around from the absurdly philosophical

30:09.280 --> 30:10.120
 to the technical.

30:10.120 --> 30:11.880
 Sure, sure, very, very happy to.

30:11.880 --> 30:13.800
 So I think my answer to that question

30:13.800 --> 30:17.640
 is a little bit complex because there is simulation theory,

30:17.640 --> 30:18.800
 which obviously Nick Bostrom,

30:18.800 --> 30:20.600
 I think famously first proposed.

30:21.680 --> 30:24.720
 And I don't quite believe it in that sense.

30:24.720 --> 30:29.600
 So in the sense that are we in some sort of computer game

30:29.600 --> 30:34.000
 or have our descendants somehow recreated earth

30:34.000 --> 30:36.520
 in the 21st century and some,

30:36.520 --> 30:38.480
 for some kind of experimental reason.

30:38.480 --> 30:41.880
 I think that, but I do think that we,

30:41.880 --> 30:45.600
 that we might be, that the best way to understand physics

30:45.600 --> 30:49.320
 and the universe is from a computational perspective.

30:49.320 --> 30:52.440
 So understanding it as an information universe

30:52.440 --> 30:56.200
 and actually information being the most fundamental unit

30:56.200 --> 30:59.920
 of reality rather than matter or energy.

30:59.920 --> 31:02.400
 So a physicist would say, you know, matter or energy,

31:02.400 --> 31:03.760
 you know, E equals MC squared.

31:03.760 --> 31:06.440
 These are the things that are the fundamentals

31:06.440 --> 31:07.400
 of the universe.

31:07.400 --> 31:09.880
 I'd actually say information,

31:09.880 --> 31:11.760
 which of course itself can be,

31:11.760 --> 31:13.560
 can specify energy or matter, right?

31:13.560 --> 31:14.920
 Matter is actually just, you know,

31:14.920 --> 31:16.880
 we're just out the way our bodies

31:16.880 --> 31:19.720
 and the molecules in our body are arranged as information.

31:19.720 --> 31:23.080
 So I think information may be the most fundamental way

31:23.080 --> 31:24.960
 to describe the universe.

31:24.960 --> 31:28.280
 And therefore you could say we're in some sort of simulation

31:28.280 --> 31:29.880
 because of that.

31:29.880 --> 31:31.040
 But I don't, I do, I'm not,

31:31.040 --> 31:34.200
 I'm not really a subscriber to the idea that, you know,

31:34.200 --> 31:36.920
 these are sort of throw away billions of simulations around.

31:36.920 --> 31:40.640
 I think this is actually very critical and possibly unique,

31:40.640 --> 31:41.760
 this simulation.

31:41.760 --> 31:42.600
 This particular one.

31:42.600 --> 31:43.440
 Yes.

31:43.440 --> 31:48.440
 And you just mean treating the universe as a computer

31:48.760 --> 31:52.240
 that's processing and modifying information

31:52.240 --> 31:54.880
 is a good way to solve the problems of physics,

31:54.880 --> 31:57.160
 of chemistry, of biology,

31:57.160 --> 31:59.720
 and perhaps of humanity and so on.

31:59.720 --> 32:02.240
 Yes, I think understanding physics

32:02.240 --> 32:04.840
 in terms of information theory

32:04.840 --> 32:07.880
 might be the best way to really understand

32:07.880 --> 32:09.360
 what's going on here.

32:09.360 --> 32:13.560
 From our understanding of a universal Turing machine,

32:13.560 --> 32:15.280
 from our understanding of a computer,

32:15.280 --> 32:17.400
 do you think there's something outside

32:17.400 --> 32:19.440
 of the capabilities of a computer

32:19.440 --> 32:21.000
 that is present in our universe?

32:21.000 --> 32:23.560
 You have a disagreement with Roger Penrose

32:23.560 --> 32:25.920
 about the nature of consciousness.

32:25.920 --> 32:27.760
 He thinks that consciousness is more

32:27.760 --> 32:29.040
 than just a computation.

32:30.080 --> 32:32.680
 Do you think all of it, the whole shebangs,

32:32.680 --> 32:34.000
 can be a computation?

32:34.000 --> 32:35.840
 Yeah, I've had many fascinating debates

32:35.840 --> 32:37.680
 with Sir Roger Penrose,

32:37.680 --> 32:39.680
 and obviously he's famously,

32:39.680 --> 32:41.520
 and I read, you know, Emperors of the New Mind

32:41.520 --> 32:45.400
 and his books, his classical books,

32:45.400 --> 32:47.800
 and they were pretty influential in the 90s.

32:47.800 --> 32:50.960
 And he believes that there's something more,

32:50.960 --> 32:53.040
 something quantum that is needed

32:53.040 --> 32:55.840
 to explain consciousness in the brain.

32:55.840 --> 32:58.320
 I think about what we're doing actually at DeepMind

32:58.320 --> 32:59.920
 and what my career is being,

32:59.920 --> 33:01.920
 we're almost like Turing's champion.

33:01.920 --> 33:05.360
 So we are pushing Turing machines or classical computation

33:05.360 --> 33:06.200
 to the limits.

33:06.200 --> 33:09.440
 What are the limits of what classical computing can do?

33:09.440 --> 33:11.760
 Now, and at the same time,

33:11.760 --> 33:14.240
 I've also studied neuroscience to see,

33:14.240 --> 33:15.520
 and that's why I did my PhD in,

33:15.520 --> 33:17.720
 was to see, also to look at, you know,

33:17.720 --> 33:19.240
 is there anything quantum in the brain

33:19.240 --> 33:21.360
 from a neuroscience or biological perspective?

33:21.360 --> 33:24.560
 And so far, I think most neuroscientists

33:24.560 --> 33:26.440
 and most mainstream biologists and neuroscientists

33:26.440 --> 33:29.480
 would say there's no evidence of any quantum systems

33:29.480 --> 33:30.800
 or effects in the brain.

33:30.800 --> 33:33.000
 As far as we can see, it can be mostly explained

33:33.000 --> 33:35.880
 by classical theories.

33:35.880 --> 33:39.280
 So, and then, so there's sort of the search

33:39.280 --> 33:40.600
 from the biology side.

33:40.600 --> 33:42.120
 And then at the same time,

33:42.120 --> 33:44.960
 there's the raising of the water, the bar,

33:44.960 --> 33:47.240
 from what classical Turing machines can do.

33:48.240 --> 33:51.680
 And, you know, including our new AI systems.

33:51.680 --> 33:55.040
 And as you alluded to earlier, you know,

33:55.040 --> 33:57.760
 I think AI, especially in the last decade plus,

33:57.760 --> 34:02.360
 has been a continual story now of surprising events

34:02.360 --> 34:03.920
 and surprising successes,

34:03.920 --> 34:05.800
 knocking over one theory after another

34:05.800 --> 34:07.760
 of what was thought to be impossible, you know,

34:07.760 --> 34:10.080
 from Go to protein folding and so on.

34:10.080 --> 34:14.760
 And so I think I would be very hesitant

34:14.760 --> 34:19.520
 to bet against how far the universal Turing machine

34:19.520 --> 34:23.400
 and classical computation paradigm can go.

34:23.400 --> 34:26.720
 And my betting would be that all of,

34:26.720 --> 34:29.080
 certainly what's going on in our brain,

34:29.080 --> 34:32.160
 can probably be mimicked or approximated

34:32.160 --> 34:34.720
 on a classical machine,

34:34.720 --> 34:38.400
 not requiring something metaphysical or quantum.

34:38.400 --> 34:41.720
 And we'll get there with some of the work with AlphaFold,

34:41.720 --> 34:45.080
 which I think begins the journey of modeling

34:45.080 --> 34:48.160
 this beautiful and complex world of biology.

34:48.160 --> 34:50.160
 So you think all the magic of the human mind

34:50.160 --> 34:54.280
 comes from this, just a few pounds of mush,

34:54.280 --> 34:57.480
 of biological computational mush,

34:57.480 --> 35:00.560
 that's akin to some of the neural networks,

35:01.560 --> 35:03.800
 not directly, but in spirit

35:03.800 --> 35:06.200
 that DeepMind has been working with.

35:06.200 --> 35:08.680
 Well, look, I think it's, you say it's a few, you know,

35:08.680 --> 35:09.680
 of course it's, this is the,

35:09.680 --> 35:11.520
 I think the biggest miracle of the universe

35:11.520 --> 35:15.000
 is that it is just a few pounds of mush in our skulls.

35:15.000 --> 35:18.560
 And yet it's also our brains are the most complex objects

35:18.560 --> 35:20.240
 that we know of in the universe.

35:20.240 --> 35:22.360
 So there's something profoundly beautiful

35:22.360 --> 35:23.920
 and amazing about our brains.

35:23.920 --> 35:28.640
 And I think that it's an incredibly,

35:28.640 --> 35:30.720
 incredible efficient machine.

35:30.720 --> 35:35.520
 And it's, you know, phenomenon basically.

35:35.520 --> 35:37.480
 And I think that building AI,

35:37.480 --> 35:38.920
 one of the reasons I wanna build AI,

35:38.920 --> 35:40.440
 and I've always wanted to is,

35:40.440 --> 35:43.800
 I think by building an intelligent artifact like AI,

35:43.800 --> 35:46.480
 and then comparing it to the human mind,

35:46.480 --> 35:49.560
 that will help us unlock the uniqueness

35:49.560 --> 35:50.960
 and the true secrets of the mind

35:50.960 --> 35:53.480
 that we've always wondered about since the dawn of history,

35:53.480 --> 35:58.480
 like consciousness, dreaming, creativity, emotions,

35:59.160 --> 36:00.760
 what are all these things, right?

36:00.760 --> 36:04.200
 We've wondered about them since the dawn of humanity.

36:04.200 --> 36:05.920
 And I think one of the reasons,

36:05.920 --> 36:08.760
 and, you know, I love philosophy and philosophy of mind is,

36:08.760 --> 36:11.200
 we found it difficult is there haven't been the tools

36:11.200 --> 36:13.680
 for us to really, other than introspection,

36:13.680 --> 36:15.880
 from very clever people in history,

36:15.880 --> 36:17.200
 very clever philosophers,

36:17.200 --> 36:19.360
 to really investigate this scientifically.

36:19.360 --> 36:21.720
 But now suddenly we have a plethora of tools.

36:21.720 --> 36:23.240
 Firstly, we have all of the neuroscience tools,

36:23.240 --> 36:25.920
 fMRI machines, single cell recording, all of this stuff,

36:25.920 --> 36:29.000
 but we also have the ability, computers and AI,

36:29.000 --> 36:31.640
 to build intelligent systems.

36:31.640 --> 36:34.720
 So I think that, you know,

36:34.720 --> 36:37.320
 I think it is amazing what the human mind does.

36:37.320 --> 36:41.120
 And I'm kind of in awe of it really.

36:41.120 --> 36:44.440
 And I think it's amazing that with our human minds,

36:44.440 --> 36:46.760
 we're able to build things like computers

36:46.760 --> 36:48.280
 and actually even, you know,

36:48.280 --> 36:49.880
 think and investigate about these questions.

36:49.880 --> 36:52.720
 I think that's also a testament to the human mind.

36:52.720 --> 36:53.560
 Yeah.

36:53.560 --> 36:56.200
 The universe built the human mind

36:56.200 --> 36:59.600
 that now is building computers that help us understand

36:59.600 --> 37:01.480
 both the universe and our own human mind.

37:01.480 --> 37:02.320
 That's right.

37:02.320 --> 37:03.140
 This is actually it.

37:03.140 --> 37:03.980
 I mean, I think that's one, you know,

37:03.980 --> 37:05.760
 one could say we are,

37:05.760 --> 37:08.160
 maybe we're the mechanism by which the universe

37:08.160 --> 37:09.840
 is going to try and understand itself.

37:09.840 --> 37:10.680
 Yeah.

37:10.680 --> 37:13.160
 It's beautiful.

37:13.160 --> 37:16.960
 So let's go to the basic building blocks of biology

37:16.960 --> 37:20.200
 that I think is another angle at which you can start

37:20.200 --> 37:22.280
 to understand the human mind, the human body,

37:22.280 --> 37:23.400
 which is quite fascinating,

37:23.400 --> 37:26.640
 which is from the basic building blocks,

37:26.640 --> 37:28.960
 start to simulate, start to model

37:28.960 --> 37:30.480
 how from those building blocks,

37:30.480 --> 37:33.080
 you can construct bigger and bigger, more complex systems,

37:33.080 --> 37:35.820
 maybe one day the entirety of the human biology.

37:35.820 --> 37:39.680
 So here's another problem that thought

37:39.680 --> 37:42.720
 to be impossible to solve, which is protein folding.

37:42.720 --> 37:47.720
 And Alpha Fold or specifically Alpha Fold 2 did just that.

37:48.840 --> 37:50.320
 It solved protein folding.

37:50.320 --> 37:53.400
 I think it's one of the biggest breakthroughs,

37:53.400 --> 37:55.140
 certainly in the history of structural biology,

37:55.140 --> 37:58.200
 but in general in science,

38:00.240 --> 38:04.840
 maybe from a high level, what is it and how does it work?

38:04.840 --> 38:08.700
 And then we can ask some fascinating questions after.

38:08.700 --> 38:09.980
 Sure.

38:09.980 --> 38:12.880
 So maybe to explain it to people not familiar

38:12.880 --> 38:14.400
 with protein folding is, you know,

38:14.400 --> 38:16.980
 first of all, explain proteins, which is, you know,

38:16.980 --> 38:18.840
 proteins are essential to all life.

38:18.840 --> 38:21.520
 Every function in your body depends on proteins.

38:21.520 --> 38:23.920
 Sometimes they're called the workhorses of biology.

38:23.920 --> 38:25.340
 And if you look into them and I've, you know,

38:25.340 --> 38:26.660
 obviously as part of Alpha Fold,

38:26.660 --> 38:30.200
 I've been researching proteins and structural biology

38:30.200 --> 38:31.760
 for the last few years, you know,

38:31.760 --> 38:34.760
 they're amazing little bio nano machines proteins.

38:34.760 --> 38:36.460
 They're incredible if you actually watch little videos

38:36.460 --> 38:39.000
 of how they work, animations of how they work.

38:39.000 --> 38:42.600
 And proteins are specified by their genetic sequence

38:42.600 --> 38:44.280
 called the amino acid sequence.

38:44.280 --> 38:47.040
 So you can think of it as their genetic makeup.

38:47.040 --> 38:50.080
 And then in the body in nature,

38:50.080 --> 38:53.360
 they fold up into a 3D structure.

38:53.360 --> 38:55.320
 So you can think of it as a string of beads

38:55.320 --> 38:57.160
 and then they fold up into a ball.

38:57.160 --> 38:59.100
 Now, the key thing is you want to know

38:59.100 --> 39:02.480
 what that 3D structure is because the structure,

39:02.480 --> 39:06.120
 the 3D structure of a protein is what helps to determine

39:06.120 --> 39:08.580
 what does it do, the function it does in your body.

39:08.580 --> 39:12.320
 And also if you're interested in drugs or disease,

39:12.320 --> 39:13.980
 you need to understand that 3D structure

39:13.980 --> 39:15.840
 because if you want to target something

39:15.840 --> 39:18.640
 with a drug compound about to block something

39:18.640 --> 39:21.120
 the protein's doing, you need to understand

39:21.120 --> 39:23.440
 where it's gonna bind on the surface of the protein.

39:23.440 --> 39:24.940
 So obviously in order to do that,

39:24.940 --> 39:26.720
 you need to understand the 3D structure.

39:26.720 --> 39:28.640
 So the structure is mapped to the function.

39:28.640 --> 39:29.880
 The structure is mapped to the function

39:29.880 --> 39:32.560
 and the structure is obviously somehow specified

39:32.560 --> 39:34.840
 by the amino acid sequence.

39:34.840 --> 39:37.420
 And that's the, in essence, the protein folding problem is,

39:37.420 --> 39:39.620
 can you just from the amino acid sequence,

39:39.620 --> 39:42.560
 the one dimensional string of letters,

39:42.560 --> 39:45.600
 can you immediately computationally predict

39:45.600 --> 39:47.120
 the 3D structure?

39:47.120 --> 39:50.020
 And this has been a grand challenge in biology

39:50.020 --> 39:51.500
 for over 50 years.

39:51.500 --> 39:54.360
 So I think it was first articulated by Christian Anfinsen,

39:54.360 --> 39:57.040
 a Nobel prize winner in 1972,

39:57.040 --> 39:59.240
 as part of his Nobel prize winning lecture.

39:59.240 --> 40:01.860
 And he just speculated this should be possible

40:01.860 --> 40:04.960
 to go from the amino acid sequence to the 3D structure,

40:04.960 --> 40:06.060
 but he didn't say how.

40:06.060 --> 40:09.440
 So it's been described to me as equivalent

40:09.440 --> 40:12.320
 to Fermat's last theorem, but for biology.

40:12.320 --> 40:15.120
 You should, as somebody that very well might win

40:15.120 --> 40:16.560
 the Nobel prize in the future.

40:16.560 --> 40:19.240
 But outside of that, you should do more

40:19.240 --> 40:20.080
 of that kind of thing.

40:20.080 --> 40:22.160
 In the margin, just put random things

40:22.160 --> 40:24.440
 that will take like 200 years to solve.

40:24.440 --> 40:26.000
 Set people off for 200 years.

40:26.000 --> 40:27.720
 It should be possible.

40:27.720 --> 40:29.040
 And just don't give any details.

40:29.040 --> 40:29.880
 Exactly.

40:29.880 --> 40:31.500
 I think everyone exactly should be,

40:31.500 --> 40:33.520
 I'll have to remember that for future.

40:33.520 --> 40:34.800
 So yeah, so he set off, you know,

40:34.800 --> 40:37.040
 with this one throwaway remark, just like Fermat,

40:37.040 --> 40:42.040
 you know, he set off this whole 50 year field really

40:42.640 --> 40:44.400
 of computational biology.

40:44.400 --> 40:46.240
 And they had, you know, they got stuck.

40:46.240 --> 40:48.520
 They hadn't really got very far with doing this.

40:48.520 --> 40:52.500
 And until now, until AlphaFold came along,

40:52.500 --> 40:54.320
 this is done experimentally, right?

40:54.320 --> 40:55.500
 Very painstakingly.

40:55.500 --> 40:57.440
 So the rule of thumb is, and you have to like

40:57.440 --> 40:59.820
 crystallize the protein, which is really difficult.

40:59.820 --> 41:03.060
 Some proteins can't be crystallized like membrane proteins.

41:03.060 --> 41:05.940
 And then you have to use very expensive electron microscopes

41:05.940 --> 41:08.200
 or X ray crystallography machines.

41:08.200 --> 41:10.680
 Really painstaking work to get the 3D structure

41:10.680 --> 41:12.400
 and visualize the 3D structure.

41:12.400 --> 41:14.840
 So the rule of thumb in experimental biology

41:14.840 --> 41:16.860
 is that it takes one PhD student,

41:16.860 --> 41:19.400
 their entire PhD to do one protein.

41:20.320 --> 41:23.440
 And with AlphaFold 2, we were able to predict

41:23.440 --> 41:26.400
 the 3D structure in a matter of seconds.

41:26.400 --> 41:28.700
 And so we were, you know, over Christmas,

41:28.700 --> 41:30.240
 we did the whole human proteome

41:30.240 --> 41:33.280
 or every protein in the human body or 20,000 proteins.

41:33.280 --> 41:34.760
 So the human proteomes like the equivalent

41:34.760 --> 41:37.560
 of the human genome, but on protein space.

41:37.560 --> 41:40.240
 And sort of revolutionized really

41:40.240 --> 41:43.300
 what a structural biologist can do.

41:43.300 --> 41:45.720
 Because now they don't have to worry

41:45.720 --> 41:47.960
 about these painstaking experimental,

41:47.960 --> 41:49.560
 should they put all of that effort in or not?

41:49.560 --> 41:51.120
 They can almost just look up the structure

41:51.120 --> 41:53.280
 of their proteins like a Google search.

41:53.280 --> 41:56.880
 And so there's a data set on which it's trained

41:56.880 --> 41:58.800
 and how to map this amino acid sequence.

41:58.800 --> 42:00.760
 First of all, it's incredible that a protein,

42:00.760 --> 42:02.480
 this little chemical computer is able to do

42:02.480 --> 42:05.720
 that computation itself in some kind of distributed way

42:05.720 --> 42:07.800
 and do it very quickly.

42:07.800 --> 42:08.840
 That's a weird thing.

42:08.840 --> 42:10.480
 And they evolve that way because, you know,

42:10.480 --> 42:13.200
 in the beginning, I mean, that's a great invention,

42:13.200 --> 42:14.760
 just the protein itself.

42:14.760 --> 42:18.240
 And then there's, I think, probably a history

42:18.240 --> 42:22.740
 of like they evolved to have many of these proteins

42:22.740 --> 42:26.600
 and those proteins figure out how to be computers themselves

42:26.600 --> 42:28.560
 in such a way that you can create structures

42:28.560 --> 42:30.540
 that can interact in complexes with each other

42:30.540 --> 42:32.660
 in order to form high level functions.

42:32.660 --> 42:35.520
 I mean, it's a weird system that they figured it out.

42:35.520 --> 42:36.360
 Well, for sure.

42:36.360 --> 42:37.640
 I mean, you know, maybe we should talk

42:37.640 --> 42:39.000
 about the origins of life too,

42:39.000 --> 42:41.180
 but proteins themselves, I think are magical

42:41.180 --> 42:45.760
 and incredible, as I said, little bio nano machines.

42:45.760 --> 42:50.280
 And actually Leventhal, who was another scientist,

42:50.280 --> 42:55.120
 a contemporary of Amphinson, he coined this Leventhal,

42:55.120 --> 42:56.820
 what became known as Leventhal's paradox,

42:56.820 --> 42:58.320
 which is exactly what you're saying.

42:58.320 --> 43:01.580
 He calculated roughly an average protein,

43:01.580 --> 43:05.080
 which is maybe 2000 amino acids base as long,

43:05.080 --> 43:09.960
 is can fold in maybe 10 to the power 300

43:09.960 --> 43:11.480
 different confirmations.

43:11.480 --> 43:13.320
 So there's 10 to the power 300 different ways

43:13.320 --> 43:14.800
 that protein could fold up.

43:14.800 --> 43:18.160
 And yet somehow in nature, physics solves this,

43:18.160 --> 43:20.520
 solves this in a matter of milliseconds.

43:20.520 --> 43:23.080
 So proteins fold up in your body in, you know,

43:23.080 --> 43:25.600
 sometimes in fractions of a second.

43:25.600 --> 43:29.080
 So physics is somehow solving that search problem.

43:29.080 --> 43:31.200
 And just to be clear, in many of these cases,

43:31.200 --> 43:33.040
 maybe you can correct me if I'm wrong,

43:33.040 --> 43:37.680
 there's often a unique way for that sequence to form itself.

43:37.680 --> 43:41.240
 So among that huge number of possibilities,

43:41.240 --> 43:43.540
 it figures out a way how to stably,

43:45.320 --> 43:47.800
 in some cases there might be a misfunction, so on,

43:47.800 --> 43:50.040
 which leads to a lot of the disorders and stuff like that.

43:50.040 --> 43:52.720
 But most of the time it's a unique mapping

43:52.720 --> 43:54.820
 and that unique mapping is not obvious.

43:54.820 --> 43:55.660
 No, exactly.

43:55.660 --> 43:57.120
 Which is what the problem is.

43:57.120 --> 44:00.720
 Exactly, so there's a unique mapping usually in a healthy,

44:00.720 --> 44:04.040
 if it's healthy, and as you say in disease,

44:04.040 --> 44:05.400
 so for example, Alzheimer's,

44:05.400 --> 44:09.000
 one conjecture is that it's because of misfolded protein,

44:09.000 --> 44:12.040
 a protein that folds in the wrong way, amyloid beta protein.

44:12.040 --> 44:14.560
 So, and then because it folds in the wrong way,

44:14.560 --> 44:17.600
 it gets tangled up, right, in your neurons.

44:17.600 --> 44:20.560
 So it's super important to understand

44:20.560 --> 44:23.600
 both healthy functioning and also disease

44:23.600 --> 44:26.480
 is to understand, you know, what these things are doing

44:26.480 --> 44:27.600
 and how they're structuring.

44:27.600 --> 44:30.540
 Of course, the next step is sometimes proteins change shape

44:30.540 --> 44:32.160
 when they interact with something.

44:32.160 --> 44:35.960
 So they're not just static necessarily in biology.

44:37.200 --> 44:39.780
 Maybe you can give some interesting,

44:39.780 --> 44:43.260
 so beautiful things to you about these early days

44:43.260 --> 44:46.160
 of AlphaFold, of solving this problem,

44:46.160 --> 44:51.160
 because unlike games, this is real physical systems

44:51.280 --> 44:55.640
 that are less amenable to self play type of mechanisms.

44:55.640 --> 44:56.460
 Sure.

44:56.460 --> 44:58.440
 The size of the data set is smaller

44:58.440 --> 44:59.760
 than you might otherwise like,

44:59.760 --> 45:01.800
 so you have to be very clever about certain things.

45:01.800 --> 45:03.600
 Is there something you could speak to

45:04.800 --> 45:06.680
 what was very hard to solve

45:06.680 --> 45:09.920
 and what are some beautiful aspects about the solution?

45:09.920 --> 45:12.800
 Yeah, I would say AlphaFold is the most complex

45:12.800 --> 45:14.600
 and also probably most meaningful system

45:14.600 --> 45:15.860
 we've built so far.

45:15.860 --> 45:18.400
 So it's been an amazing time actually in the last,

45:18.400 --> 45:20.520
 you know, two, three years to see that come through

45:20.520 --> 45:23.200
 because as we talked about earlier, you know,

45:23.200 --> 45:25.480
 games is what we started on

45:25.480 --> 45:27.900
 building things like AlphaGo and AlphaZero,

45:27.900 --> 45:30.400
 but really the ultimate goal was to,

45:30.400 --> 45:31.520
 not just to crack games,

45:31.520 --> 45:33.120
 it was just to build,

45:33.120 --> 45:35.320
 use them to bootstrap general learning systems

45:35.320 --> 45:37.440
 we could then apply to real world challenges.

45:37.440 --> 45:40.640
 Specifically, my passion is scientific challenges

45:40.640 --> 45:41.920
 like protein folding.

45:41.920 --> 45:43.280
 And then AlphaFold of course

45:43.280 --> 45:45.360
 is our first big proof point of that.

45:45.360 --> 45:49.040
 And so, you know, in terms of the data

45:49.040 --> 45:50.920
 and the amount of innovations that had to go into it,

45:50.920 --> 45:52.280
 we, you know, it was like

45:52.280 --> 45:54.480
 more than 30 different component algorithms

45:54.480 --> 45:57.960
 needed to be put together to crack the protein folding.

45:57.960 --> 46:00.800
 I think some of the big innovations were that

46:00.800 --> 46:04.220
 kind of building in some hard coded constraints

46:04.220 --> 46:07.760
 around physics and evolutionary biology

46:07.760 --> 46:10.400
 to constrain sort of things like the bond angles

46:11.640 --> 46:14.240
 in the protein and things like that,

46:15.400 --> 46:18.040
 a lot, but not to impact the learning system.

46:18.040 --> 46:21.000
 So still allowing the system to be able to learn

46:21.000 --> 46:25.540
 the physics itself from the examples that we had.

46:25.540 --> 46:26.640
 And the examples, as you say,

46:26.640 --> 46:28.840
 there are only about 150,000 proteins,

46:28.840 --> 46:31.240
 even after 40 years of experimental biology,

46:31.240 --> 46:33.880
 only around 150,000 proteins have been,

46:33.880 --> 46:35.920
 the structures have been found out about.

46:35.920 --> 46:37.120
 So that was our training set,

46:37.120 --> 46:41.120
 which is much less than normally we would like to use,

46:41.120 --> 46:43.840
 but using various tricks, things like self distillation.

46:43.840 --> 46:48.280
 So actually using AlphaFold predictions,

46:48.280 --> 46:49.480
 some of the best predictions

46:49.480 --> 46:51.000
 that it thought was highly confident in,

46:51.000 --> 46:53.320
 we put them back into the training set, right?

46:53.320 --> 46:55.440
 To make the training set bigger,

46:55.440 --> 46:58.400
 that was critical to AlphaFold working.

46:58.400 --> 47:00.160
 So there was actually a huge number

47:00.160 --> 47:02.720
 of different innovations like that,

47:02.720 --> 47:06.080
 that were required to ultimately crack the problem.

47:06.080 --> 47:09.720
 AlphaFold one, what it produced was a distrogram.

47:09.720 --> 47:13.600
 So a kind of a matrix of the pairwise distances

47:13.600 --> 47:17.880
 between all of the molecules in the protein.

47:17.880 --> 47:20.440
 And then there had to be a separate optimization process

47:20.440 --> 47:23.640
 to create the 3D structure.

47:23.640 --> 47:25.120
 And what we did for AlphaFold two

47:25.120 --> 47:26.920
 is make it truly end to end.

47:26.920 --> 47:31.720
 So we went straight from the amino acid sequence of bases

47:31.720 --> 47:33.860
 to the 3D structure directly

47:33.860 --> 47:36.080
 without going through this intermediate step.

47:36.080 --> 47:38.600
 And in machine learning, what we've always found is

47:38.600 --> 47:40.920
 that the more end to end you can make it,

47:40.920 --> 47:42.160
 the better the system.

47:42.160 --> 47:46.160
 And it's probably because in the end,

47:46.160 --> 47:48.560
 the system's better at learning what the constraints are

47:48.560 --> 47:51.920
 than we are as the human designers of specifying it.

47:51.920 --> 47:54.040
 So anytime you can let it flow end to end

47:54.040 --> 47:55.400
 and actually just generate what it is

47:55.400 --> 47:58.440
 you're really looking for, in this case, the 3D structure,

47:58.440 --> 48:00.560
 you're better off than having this intermediate step,

48:00.560 --> 48:03.360
 which you then have to handcraft the next step for.

48:03.360 --> 48:06.160
 So it's better to let the gradients and the learning

48:06.160 --> 48:09.000
 flow all the way through the system from the end point,

48:09.000 --> 48:10.880
 the end output you want to the inputs.

48:10.880 --> 48:13.040
 So that's a good way to start on a new problem.

48:13.040 --> 48:14.360
 Handcraft a bunch of stuff,

48:14.360 --> 48:16.640
 add a bunch of manual constraints

48:16.640 --> 48:18.640
 with a small end to end learning piece

48:18.640 --> 48:21.560
 or a small learning piece and grow that learning piece

48:21.560 --> 48:22.840
 until it consumes the whole thing.

48:22.840 --> 48:23.680
 That's right.

48:23.680 --> 48:25.320
 And so you can also see,

48:25.320 --> 48:26.960
 this is a bit of a method we've developed

48:26.960 --> 48:29.640
 over doing many sort of successful alpha,

48:29.640 --> 48:32.200
 we call them alpha X projects, right?

48:32.200 --> 48:34.600
 And the easiest way to see that is the evolution

48:34.600 --> 48:36.720
 of alpha go to alpha zero.

48:36.720 --> 48:39.640
 So alpha go was a learning system,

48:39.640 --> 48:42.280
 but it was specifically trained to only play go, right?

48:42.280 --> 48:45.360
 So, and what we wanted to do with first version of alpha go

48:45.360 --> 48:47.520
 is just get to world champion performance

48:47.520 --> 48:49.200
 no matter how we did it, right?

48:49.200 --> 48:51.400
 And then of course, alpha go zero,

48:51.400 --> 48:55.280
 we remove the need to use human games as a starting point,

48:55.280 --> 48:56.120
 right?

48:56.120 --> 48:57.960
 So it could just play against itself

48:57.960 --> 49:00.280
 from random starting point from the beginning.

49:00.280 --> 49:03.720
 So that removed the need for human knowledge about go.

49:03.720 --> 49:05.960
 And then finally alpha zero then generalized it

49:05.960 --> 49:08.920
 so that any things we had in there, the system,

49:08.920 --> 49:12.240
 including things like symmetry of the go board were removed.

49:12.240 --> 49:14.600
 So the alpha zero could play from scratch

49:14.600 --> 49:16.440
 any two player game and then mu zero,

49:16.440 --> 49:18.360
 which is the final, our latest version

49:18.360 --> 49:20.680
 of that set of things was then extending it

49:20.680 --> 49:22.120
 so that you didn't even have to give it

49:22.120 --> 49:23.200
 the rules of the game.

49:23.200 --> 49:24.880
 It would learn that for itself.

49:24.880 --> 49:26.600
 So it could also deal with computer games

49:26.600 --> 49:27.760
 as well as board games.

49:27.760 --> 49:30.400
 So that line of alpha go, alpha go zero, alpha zero,

49:30.400 --> 49:33.480
 mu zero, that's the full trajectory

49:33.480 --> 49:37.200
 of what you can take from imitation learning

49:37.200 --> 49:40.440
 to full self supervised learning.

49:40.440 --> 49:41.640
 Yeah, exactly.

49:41.640 --> 49:44.720
 And learning the entire structure

49:44.720 --> 49:47.640
 of the environment you're put in from scratch, right?

49:47.640 --> 49:51.840
 And bootstrapping it through self play yourself.

49:51.840 --> 49:53.720
 But the thing is it would have been impossible, I think,

49:53.720 --> 49:55.960
 or very hard for us to build alpha zero

49:55.960 --> 49:58.600
 or mu zero first out of the box.

49:58.600 --> 50:01.400
 Even psychologically, because you have to believe

50:01.400 --> 50:03.040
 in yourself for a very long time.

50:03.040 --> 50:04.640
 You're constantly dealing with doubt

50:04.640 --> 50:06.680
 because a lot of people say that it's impossible.

50:06.680 --> 50:08.640
 Exactly, so it's hard enough just to do go.

50:08.640 --> 50:10.920
 As you were saying, everyone thought that was impossible

50:10.920 --> 50:14.160
 or at least a decade away from when we did it

50:14.160 --> 50:17.320
 back in 2015, 2016.

50:17.320 --> 50:20.960
 And so yes, it would have been psychologically

50:20.960 --> 50:22.960
 probably very difficult as well as the fact

50:22.960 --> 50:26.400
 that of course we learn a lot by building alpha go first.

50:26.400 --> 50:28.520
 Right, so I think this is why I call AI

50:28.520 --> 50:29.880
 an engineering science.

50:29.880 --> 50:32.280
 It's one of the most fascinating science disciplines,

50:32.280 --> 50:34.200
 but it's also an engineering science in the sense

50:34.200 --> 50:38.200
 that unlike natural sciences, the phenomenon you're studying

50:38.200 --> 50:39.440
 doesn't exist out in nature.

50:39.440 --> 50:40.880
 You have to build it first.

50:40.880 --> 50:42.480
 So you have to build the artifact first,

50:42.480 --> 50:46.480
 and then you can study and pull it apart and how it works.

50:46.480 --> 50:50.000
 This is tough to ask you this question

50:50.000 --> 50:51.480
 because you probably will say it's everything,

50:51.480 --> 50:54.360
 but let's try to think through this

50:54.360 --> 50:56.480
 because you're in a very interesting position

50:56.480 --> 50:59.520
 where DeepMind is a place of some of the most brilliant

50:59.520 --> 51:01.760
 ideas in the history of AI,

51:01.760 --> 51:04.600
 but it's also a place of brilliant engineering.

51:05.880 --> 51:08.040
 So how much of solving intelligence,

51:08.040 --> 51:09.880
 this big goal for DeepMind,

51:09.880 --> 51:12.120
 how much of it is science?

51:12.120 --> 51:13.320
 How much is engineering?

51:13.320 --> 51:14.720
 So how much is the algorithms?

51:14.720 --> 51:16.160
 How much is the data?

51:16.160 --> 51:19.840
 How much is the hardware compute infrastructure?

51:19.840 --> 51:22.800
 How much is it the software compute infrastructure?

51:23.960 --> 51:24.800
 What else is there?

51:24.800 --> 51:27.200
 How much is the human infrastructure?

51:27.200 --> 51:30.280
 And like just the humans interacting certain kinds of ways

51:30.280 --> 51:31.720
 in all the space of all those ideas.

51:31.720 --> 51:33.640
 And how much is maybe like philosophy?

51:33.640 --> 51:35.160
 How much, what's the key?

51:35.160 --> 51:40.160
 If you were to sort of look back,

51:40.680 --> 51:43.200
 like if we go forward 200 years and look back,

51:43.200 --> 51:46.320
 what was the key thing that solved intelligence?

51:46.320 --> 51:47.800
 Is it the ideas or the engineering?

51:47.800 --> 51:49.040
 I think it's a combination.

51:49.040 --> 51:49.880
 First of all, of course,

51:49.880 --> 51:51.360
 it's a combination of all those things,

51:51.360 --> 51:54.760
 but the ratios of them changed over time.

51:54.760 --> 51:57.480
 So even in the last 12 years,

51:57.480 --> 51:59.400
 so we started DeepMind in 2010,

51:59.400 --> 52:01.920
 which is hard to imagine now because 2010,

52:01.920 --> 52:03.400
 it's only 12 short years ago,

52:03.400 --> 52:05.600
 but nobody was talking about AI.

52:05.600 --> 52:07.600
 I don't know if you remember back to your MIT days,

52:07.600 --> 52:08.720
 no one was talking about it.

52:08.720 --> 52:11.080
 I did a postdoc at MIT back around then.

52:11.080 --> 52:12.880
 And it was sort of thought of as a,

52:12.880 --> 52:14.200
 well, look, we know AI doesn't work.

52:14.200 --> 52:17.040
 We tried this hard in the 90s at places like MIT,

52:17.040 --> 52:19.880
 mostly using logic systems and old fashioned,

52:19.880 --> 52:22.600
 sort of good old fashioned AI, we would call it now.

52:22.600 --> 52:25.320
 People like Minsky and Patrick Winston,

52:25.320 --> 52:26.720
 and you know all these characters, right?

52:26.720 --> 52:28.280
 And used to debate a few of them.

52:28.280 --> 52:30.120
 And they used to think I was mad thinking about

52:30.120 --> 52:32.360
 that some new advance could be done with learning systems.

52:32.360 --> 52:34.720
 And I was actually pleased to hear that

52:34.720 --> 52:36.960
 because at least you know you're on a unique track

52:36.960 --> 52:37.840
 at that point, right?

52:37.840 --> 52:41.880
 Even if all of your professors are telling you you're mad.

52:41.880 --> 52:43.840
 And of course in industry,

52:43.840 --> 52:47.680
 we couldn't get, it was difficult to get two cents together,

52:47.680 --> 52:48.920
 which is hard to imagine now as well,

52:48.920 --> 52:51.560
 given that it's the biggest sort of buzzword in VCs

52:51.560 --> 52:54.720
 and fundraisings easy and all these kinds of things today.

52:54.720 --> 52:57.720
 So back in 2010, it was very difficult.

52:57.720 --> 52:59.360
 And the reason we started then,

52:59.360 --> 53:02.480
 and Shane and I used to discuss

53:02.480 --> 53:04.920
 what were the sort of founding tenants of DeepMind.

53:04.920 --> 53:06.120
 And it was various things.

53:06.120 --> 53:08.680
 One was algorithmic advances.

53:08.680 --> 53:09.760
 So deep learning, you know,

53:09.760 --> 53:12.360
 Jeff Hinton and Co had just sort of invented that

53:12.360 --> 53:15.200
 in academia, but no one in industry knew about it.

53:15.200 --> 53:16.640
 We love reinforcement learning.

53:16.640 --> 53:18.240
 We thought that could be scaled up.

53:18.240 --> 53:20.160
 But also understanding about the human brain

53:20.160 --> 53:23.920
 had advanced quite a lot in the decade prior

53:23.920 --> 53:25.440
 with fMRI machines and other things.

53:25.440 --> 53:28.840
 So we could get some good hints about architectures

53:28.840 --> 53:32.480
 and algorithms and sort of representations maybe

53:32.480 --> 53:33.400
 that the brain uses.

53:33.400 --> 53:36.920
 So at a systems level, not at a implementation level.

53:37.760 --> 53:41.040
 And then the other big things were compute and GPUs, right?

53:41.040 --> 53:44.160
 So we could see a compute was going to be really useful

53:44.160 --> 53:46.960
 and had got to a place where it become commoditized

53:46.960 --> 53:48.560
 mostly through the games industry

53:48.560 --> 53:50.760
 and that could be taken advantage of.

53:50.760 --> 53:52.800
 And then the final thing was also mathematical

53:52.800 --> 53:54.960
 and theoretical definitions of intelligence.

53:54.960 --> 53:57.560
 So things like AIXI, AIXE,

53:57.560 --> 54:00.160
 which Shane worked on with his supervisor, Marcus Hutter,

54:00.160 --> 54:03.360
 which is this sort of theoretical proof really

54:03.360 --> 54:05.280
 of universal intelligence,

54:05.280 --> 54:08.000
 which is actually a reinforcement learning system

54:08.000 --> 54:08.840
 in the limit.

54:08.840 --> 54:10.640
 I mean, it assumes infinite compute and infinite memory

54:10.640 --> 54:12.920
 in the way, you know, like a Turing machine proves.

54:12.920 --> 54:15.840
 But I was also waiting to see something like that too,

54:15.840 --> 54:19.440
 to, you know, like Turing machines and computation theory

54:19.440 --> 54:21.520
 that people like Turing and Shannon came up with

54:21.520 --> 54:23.680
 underpins modern computer science.

54:24.800 --> 54:26.400
 You know, I was waiting for a theory like that

54:26.400 --> 54:28.880
 to sort of underpin AGI research.

54:28.880 --> 54:30.120
 So when I, you know, met Shane

54:30.120 --> 54:32.000
 and saw he was working on something like that,

54:32.000 --> 54:33.680
 you know, that to me was a sort of final piece

54:33.680 --> 54:34.560
 of the jigsaw.

54:34.560 --> 54:38.320
 So in the early days, I would say that ideas

54:38.320 --> 54:40.040
 were the most important.

54:40.040 --> 54:42.440
 You know, for us, it was deep reinforcement learning,

54:42.440 --> 54:44.600
 scaling up deep learning.

54:44.600 --> 54:46.240
 Of course, we've seen transformers.

54:46.240 --> 54:48.920
 So huge leaps, I would say, you know, three or four

54:48.920 --> 54:51.520
 from, if you think from 2010 till now,

54:51.520 --> 54:53.680
 huge evolutions, things like AlphaGo.

54:53.680 --> 54:57.920
 And maybe there's a few more still needed.

54:57.920 --> 55:01.120
 But as we get closer to AI, AGI,

55:02.000 --> 55:04.600
 I think engineering becomes more and more important

55:04.600 --> 55:07.800
 and data because scale and of course the recent,

55:07.800 --> 55:10.440
 you know, results of GPT3 and all the big language models

55:10.440 --> 55:12.800
 and large models, including our ones,

55:12.800 --> 55:16.000
 has shown that scale and large models

55:16.000 --> 55:18.080
 are clearly gonna be a necessary,

55:18.080 --> 55:21.960
 but perhaps not sufficient part of an AGI solution.

55:21.960 --> 55:24.560
 And throughout that, like you said,

55:24.560 --> 55:26.720
 and I'd like to give you a big thank you.

55:26.720 --> 55:30.640
 You're one of the pioneers in this is sticking by ideas

55:30.640 --> 55:33.480
 like reinforcement learning, that this can actually work

55:34.560 --> 55:38.480
 given actually limited success in the past.

55:38.480 --> 55:41.480
 And also, which we still don't know,

55:41.480 --> 55:46.480
 but proudly having the best researchers in the world

55:46.760 --> 55:49.400
 and talking about solving intelligence.

55:49.400 --> 55:50.920
 So talking about whatever you call it,

55:50.920 --> 55:54.720
 AGI or something like this, speaking of MIT,

55:54.720 --> 55:57.240
 that's just something you wouldn't bring up.

55:57.240 --> 56:02.240
 Not maybe you did in like 40, 50 years ago,

56:03.560 --> 56:08.560
 but that was, AI was a place where you do tinkering,

56:09.320 --> 56:12.560
 very small scale, not very ambitious projects.

56:12.560 --> 56:16.160
 And maybe the biggest ambitious projects

56:16.160 --> 56:17.480
 were in the space of robotics

56:17.480 --> 56:19.200
 and doing like the DARPA challenge.

56:19.200 --> 56:23.400
 But the task of solving intelligence and believing you can,

56:23.400 --> 56:24.560
 that's really, really powerful.

56:24.560 --> 56:27.680
 So in order for engineering to do its work,

56:27.680 --> 56:30.960
 to have great engineers, build great systems,

56:30.960 --> 56:32.360
 you have to have that belief,

56:32.360 --> 56:33.920
 that threads throughout the whole thing

56:33.920 --> 56:35.040
 that you can actually solve

56:35.040 --> 56:36.640
 some of these impossible challenges.

56:36.640 --> 56:37.480
 Yeah, that's right.

56:37.480 --> 56:42.280
 And back in 2010, our mission statement and still is today,

56:42.280 --> 56:45.600
 it was used to be solving step one, solve intelligence,

56:45.600 --> 56:47.520
 step two, use it to solve everything else.

56:47.520 --> 56:51.120
 So if you can imagine pitching that to a VC in 2010,

56:51.120 --> 56:52.680
 the kind of looks we got,

56:52.680 --> 56:55.880
 we managed to find a few kooky people to back us,

56:55.880 --> 56:57.680
 but it was tricky.

56:57.680 --> 57:00.160
 And it got to the point where we wouldn't mention it

57:00.160 --> 57:03.120
 to any of our professors because they would just eye roll

57:03.120 --> 57:05.760
 and think we committed career suicide.

57:05.760 --> 57:10.040
 And so it was, there's a lot of things that we had to do,

57:10.040 --> 57:11.560
 but we always believed it.

57:11.560 --> 57:13.240
 And one reason, by the way,

57:13.240 --> 57:16.160
 one reason I've always believed in reinforcement learning

57:16.160 --> 57:19.120
 is that if you look at neuroscience,

57:19.120 --> 57:22.720
 that is the way that the primate brain learns.

57:22.720 --> 57:24.880
 One of the main mechanisms is the dopamine system

57:24.880 --> 57:26.440
 implements some form of TD learning.

57:26.440 --> 57:28.600
 It was a very famous result in the late 90s

57:29.680 --> 57:31.320
 where they saw this in monkeys

57:31.320 --> 57:34.520
 and as a propagating prediction error.

57:34.520 --> 57:36.800
 So again, in the limit,

57:36.800 --> 57:39.480
 this is what I think you can use neuroscience for is,

57:39.480 --> 57:43.160
 at mathematics, when you're doing something as ambitious

57:43.160 --> 57:44.560
 as trying to solve intelligence

57:44.560 --> 57:47.760
 and it's blue sky research, no one knows how to do it,

57:47.760 --> 57:50.160
 you need to use any evidence

57:50.160 --> 57:52.120
 or any source of information you can

57:52.120 --> 57:54.280
 to help guide you in the right direction

57:54.280 --> 57:56.680
 or give you confidence you're going in the right direction.

57:56.680 --> 57:59.840
 So that was one reason we pushed so hard on that.

57:59.840 --> 58:01.840
 And just going back to your earlier question

58:01.840 --> 58:04.280
 about organization, the other big thing

58:04.280 --> 58:06.000
 that I think we innovated with at DeepMind

58:06.000 --> 58:10.320
 to encourage invention and innovation

58:10.320 --> 58:12.920
 was the multidisciplinary organization we built

58:12.920 --> 58:14.160
 and we still have today.

58:14.160 --> 58:16.680
 So DeepMind originally was a confluence

58:16.680 --> 58:19.400
 of the most cutting edge knowledge in neuroscience

58:19.400 --> 58:22.840
 with machine learning, engineering and mathematics, right?

58:22.840 --> 58:24.400
 And gaming.

58:24.400 --> 58:26.760
 And then since then we've built that out even further.

58:26.760 --> 58:30.280
 So we have philosophers here and ethicists,

58:30.280 --> 58:33.160
 but also other types of scientists, physicists and so on.

58:33.160 --> 58:35.160
 And that's what brings together,

58:35.160 --> 58:38.760
 I tried to build a sort of new type of Bell Labs,

58:38.760 --> 58:41.200
 but in its golden era, right?

58:41.200 --> 58:45.680
 And a new expression of that to try and foster

58:45.680 --> 58:48.480
 this incredible sort of innovation machine.

58:48.480 --> 58:50.600
 So talking about the humans in the machine,

58:50.600 --> 58:53.080
 DeepMind itself is a learning machine

58:53.080 --> 58:55.600
 with lots of amazing human minds in it

58:55.600 --> 58:58.920
 coming together to try and build these learning systems.

59:00.360 --> 59:04.960
 If we return to the big ambitious dream of AlphaFold,

59:04.960 --> 59:08.400
 that may be the early steps on a very long journey

59:08.400 --> 59:13.400
 in biology, do you think the same kind of approach

59:14.200 --> 59:16.400
 can use to predict the structure and function

59:16.400 --> 59:18.720
 of more complex biological systems?

59:18.720 --> 59:21.480
 So multi protein interaction,

59:21.480 --> 59:24.400
 and then, I mean, you can go out from there,

59:24.400 --> 59:26.920
 just simulating bigger and bigger systems

59:26.920 --> 59:29.560
 that eventually simulate something like the human brain

59:29.560 --> 59:32.560
 or the human body, just the big mush,

59:32.560 --> 59:36.480
 the mess of the beautiful, resilient mess of biology.

59:36.480 --> 59:39.600
 Do you see that as a long term vision?

59:39.600 --> 59:42.560
 I do, and I think, if you think about

59:42.560 --> 59:45.680
 what are the top things I wanted to apply AI to

59:45.680 --> 59:47.680
 once we had powerful enough systems,

59:47.680 --> 59:52.240
 biology and curing diseases and understanding biology

59:52.240 --> 59:54.120
 was right up there, top of my list.

59:54.120 --> 59:56.760
 That's one of the reasons I personally pushed that myself

59:56.760 --> 59:59.240
 and with AlphaFold, but I think AlphaFold,

1:00:00.200 --> 1:00:03.000
 amazing as it is, is just the beginning.

1:00:03.000 --> 1:00:07.160
 And I hope it's evidence of what could be done

1:00:07.160 --> 1:00:08.800
 with computational methods.

1:00:08.800 --> 1:00:12.200
 So AlphaFold solved this huge problem

1:00:12.200 --> 1:00:15.240
 of the structure of proteins, but biology is dynamic.

1:00:15.240 --> 1:00:16.880
 So really what I imagine from here,

1:00:16.880 --> 1:00:18.640
 and we're working on all these things now,

1:00:18.640 --> 1:00:23.160
 is protein, protein interaction, protein ligand binding,

1:00:23.160 --> 1:00:25.400
 so reacting with molecules,

1:00:25.400 --> 1:00:27.640
 then you wanna build up to pathways,

1:00:27.640 --> 1:00:30.000
 and then eventually a virtual cell.

1:00:30.000 --> 1:00:32.680
 That's my dream, maybe in the next 10 years.

1:00:32.680 --> 1:00:33.600
 And I've been talking actually

1:00:33.600 --> 1:00:35.000
 to a lot of biologists, friends of mine,

1:00:35.000 --> 1:00:36.760
 Paul Nurse, who runs the Crick Institute,

1:00:36.760 --> 1:00:39.080
 amazing biologists, Nobel Prize winning biologists.

1:00:39.080 --> 1:00:42.080
 We've been discussing for 20 years now, virtual cells.

1:00:42.080 --> 1:00:44.720
 Could you build a virtual simulation of a cell?

1:00:44.720 --> 1:00:46.240
 And if you could, that would be incredible

1:00:46.240 --> 1:00:48.120
 for biology and disease discovery,

1:00:48.120 --> 1:00:49.520
 because you could do loads of experiments

1:00:49.520 --> 1:00:52.400
 on the virtual cell, and then only at the last stage,

1:00:52.400 --> 1:00:53.920
 validate it in the wet lab.

1:00:53.920 --> 1:00:56.400
 So you could, in terms of the search space

1:00:56.400 --> 1:00:59.200
 of discovering new drugs, it takes 10 years roughly

1:00:59.200 --> 1:01:03.360
 to go from identifying a target,

1:01:03.360 --> 1:01:06.480
 to having a drug candidate.

1:01:06.480 --> 1:01:09.760
 Maybe that could be shortened by an order of magnitude,

1:01:09.760 --> 1:01:13.120
 if you could do most of that work in silico.

1:01:13.120 --> 1:01:15.720
 So in order to get to a virtual cell,

1:01:15.720 --> 1:01:18.320
 we have to build up understanding

1:01:18.320 --> 1:01:20.760
 of different parts of biology and the interactions.

1:01:20.760 --> 1:01:24.560
 And so every few years we talk about this,

1:01:24.560 --> 1:01:25.600
 I talked about this with Paul.

1:01:25.600 --> 1:01:27.840
 And then finally, last year after AlphaFold,

1:01:27.840 --> 1:01:30.600
 I said, now's the time we can finally go for it.

1:01:30.600 --> 1:01:32.360
 And AlphaFold is the first proof point

1:01:32.360 --> 1:01:33.800
 that this might be possible.

1:01:33.800 --> 1:01:35.920
 And he's very excited, and we have some collaborations

1:01:35.920 --> 1:01:38.480
 with his lab, they're just across the road actually

1:01:38.480 --> 1:01:40.960
 from us, it's wonderful being here in King's Cross

1:01:40.960 --> 1:01:42.880
 with the Crick Institute across the road.

1:01:42.880 --> 1:01:45.960
 And I think the next steps,

1:01:45.960 --> 1:01:48.040
 I think there's gonna be some amazing advances

1:01:48.040 --> 1:01:50.960
 in biology built on top of things like AlphaFold.

1:01:50.960 --> 1:01:53.160
 We're already seeing that with the community doing that

1:01:53.160 --> 1:01:56.000
 after we've open sourced it and released it.

1:01:56.000 --> 1:02:01.000
 And I often say that I think if you think of mathematics

1:02:02.360 --> 1:02:05.080
 is the perfect description language for physics,

1:02:05.080 --> 1:02:06.920
 I think AI might be end up being

1:02:06.920 --> 1:02:09.280
 the perfect description language for biology

1:02:09.280 --> 1:02:13.040
 because biology is so messy, it's so emergent,

1:02:13.040 --> 1:02:15.320
 so dynamic and complex.

1:02:15.320 --> 1:02:16.920
 I think I find it very hard to believe

1:02:16.920 --> 1:02:18.600
 we'll ever get to something as elegant

1:02:18.600 --> 1:02:21.760
 as Newton's laws of motions to describe a cell, right?

1:02:21.760 --> 1:02:23.600
 It's just too complicated.

1:02:23.600 --> 1:02:26.160
 So I think AI is the right tool for that.

1:02:26.160 --> 1:02:29.480
 So you have to start at the basic building blocks

1:02:29.480 --> 1:02:31.680
 and use AI to run the simulation

1:02:31.680 --> 1:02:32.880
 for all those building blocks.

1:02:32.880 --> 1:02:36.040
 So have a very strong way to do prediction

1:02:36.040 --> 1:02:37.800
 of what given these building blocks,

1:02:37.800 --> 1:02:40.880
 what kind of biology, how the function

1:02:40.880 --> 1:02:43.640
 and the evolution of that biological system.

1:02:43.640 --> 1:02:45.280
 It's almost like a cellular automata,

1:02:45.280 --> 1:02:47.880
 you have to run it, you can't analyze it from a high level.

1:02:47.880 --> 1:02:49.840
 You have to take the basic ingredients,

1:02:49.840 --> 1:02:51.960
 figure out the rules and let it run.

1:02:51.960 --> 1:02:53.960
 But in this case, the rules are very difficult

1:02:53.960 --> 1:02:56.200
 to figure out, you have to learn them.

1:02:56.200 --> 1:02:57.040
 That's exactly it.

1:02:57.040 --> 1:03:00.800
 So the biology is too complicated to figure out the rules.

1:03:00.800 --> 1:03:03.600
 It's too emergent, too dynamic,

1:03:03.600 --> 1:03:05.080
 say compared to a physics system,

1:03:05.080 --> 1:03:07.040
 like the motion of a planet, right?

1:03:07.040 --> 1:03:09.200
 And so you have to learn the rules

1:03:09.200 --> 1:03:11.920
 and that's exactly the type of systems that we're building.

1:03:11.920 --> 1:03:14.800
 So you mentioned you've open sourced AlphaFold

1:03:14.800 --> 1:03:16.640
 and even the data involved.

1:03:16.640 --> 1:03:20.040
 To me personally, also really happy

1:03:20.040 --> 1:03:22.640
 and a big thank you for open sourcing Mojoko,

1:03:23.520 --> 1:03:27.080
 the physics simulation engine that's often used

1:03:27.080 --> 1:03:29.080
 for robotics research and so on.

1:03:29.080 --> 1:03:31.120
 So I think that's a pretty gangster move.

1:03:31.120 --> 1:03:36.120
 So what's the, I mean, very few companies

1:03:37.200 --> 1:03:39.080
 or people do that kind of thing.

1:03:39.080 --> 1:03:41.240
 What's the philosophy behind that?

1:03:41.240 --> 1:03:42.920
 You know, it's a case by case basis.

1:03:42.920 --> 1:03:44.040
 And in both of those cases,

1:03:44.040 --> 1:03:47.360
 we felt that was the maximum benefit to humanity to do that.

1:03:47.360 --> 1:03:50.040
 And the scientific community, in one case,

1:03:50.040 --> 1:03:53.360
 the robotics physics community with Mojoko, so.

1:03:53.360 --> 1:03:54.200
 We purchased it.

1:03:54.200 --> 1:03:55.840
 We purchased it for, yes,

1:03:55.840 --> 1:03:58.520
 we purchased it for the express principle to open source it.

1:03:58.520 --> 1:04:02.440
 So, you know, I hope people appreciate that.

1:04:02.440 --> 1:04:04.040
 It's great to hear that you do.

1:04:04.040 --> 1:04:05.800
 And then the second thing was,

1:04:05.800 --> 1:04:08.040
 and mostly we did it because the person building it

1:04:08.040 --> 1:04:11.920
 was not able to cope with supporting it anymore

1:04:11.920 --> 1:04:13.600
 because it got too big for him.

1:04:13.600 --> 1:04:16.720
 He's an amazing professor who built it in the first place.

1:04:16.720 --> 1:04:18.240
 So we helped him out with that.

1:04:18.240 --> 1:04:20.520
 And then with AlphaFold is even bigger, I would say.

1:04:20.520 --> 1:04:21.960
 And I think in that case,

1:04:21.960 --> 1:04:25.520
 we decided that there were so many downstream applications

1:04:25.520 --> 1:04:29.400
 of AlphaFold that we couldn't possibly even imagine

1:04:29.400 --> 1:04:30.480
 what they all were.

1:04:30.480 --> 1:04:34.360
 So the best way to accelerate drug discovery

1:04:34.360 --> 1:04:38.680
 and also fundamental research would be to give all

1:04:38.680 --> 1:04:43.240
 that data away and the system itself.

1:04:43.240 --> 1:04:45.280
 You know, it's been so gratifying to see

1:04:45.280 --> 1:04:47.040
 what people have done that within just one year,

1:04:47.040 --> 1:04:49.240
 which is a short amount of time in science.

1:04:49.240 --> 1:04:54.160
 And it's been used by over 500,000 researchers have used it.

1:04:54.160 --> 1:04:56.560
 We think that's almost every biologist in the world.

1:04:56.560 --> 1:04:58.840
 I think there's roughly 500,000 biologists in the world,

1:04:58.840 --> 1:05:00.000
 professional biologists,

1:05:00.000 --> 1:05:03.320
 have used it to look at their proteins of interest.

1:05:04.480 --> 1:05:06.520
 We've seen amazing fundamental research done.

1:05:06.520 --> 1:05:09.040
 So a couple of weeks ago, front cover,

1:05:09.040 --> 1:05:10.840
 there was a whole special issue of science,

1:05:10.840 --> 1:05:12.040
 including the front cover,

1:05:12.040 --> 1:05:14.000
 which had the nuclear pore complex on it,

1:05:14.000 --> 1:05:15.800
 which is one of the biggest proteins in the body.

1:05:15.800 --> 1:05:18.960
 The nuclear pore complex is a protein that governs

1:05:18.960 --> 1:05:21.680
 all the nutrients going in and out of your cell nucleus.

1:05:21.680 --> 1:05:24.760
 So they're like little gateways that open and close

1:05:24.760 --> 1:05:27.320
 to let things go in and out of your cell nucleus.

1:05:27.320 --> 1:05:29.400
 So they're really important, but they're huge

1:05:29.400 --> 1:05:31.680
 because they're massive donut ring shaped things.

1:05:31.680 --> 1:05:33.440
 And they've been looking to try and figure out

1:05:33.440 --> 1:05:34.960
 that structure for decades.

1:05:34.960 --> 1:05:37.160
 And they have lots of experimental data,

1:05:37.160 --> 1:05:39.600
 but it's too low resolution, there's bits missing.

1:05:39.600 --> 1:05:43.080
 And they were able to, like a giant Lego jigsaw puzzle,

1:05:43.080 --> 1:05:46.200
 use alpha fold predictions plus experimental data

1:05:46.200 --> 1:05:49.760
 and combined those two independent sources of information,

1:05:49.760 --> 1:05:51.240
 actually four different groups around the world

1:05:51.240 --> 1:05:54.600
 were able to put it together more or less simultaneously

1:05:54.600 --> 1:05:56.280
 using alpha fold predictions.

1:05:56.280 --> 1:05:57.720
 So that's been amazing to see.

1:05:57.720 --> 1:05:59.400
 And pretty much every pharma company,

1:05:59.400 --> 1:06:01.440
 every drug company executive I've spoken to

1:06:01.440 --> 1:06:03.760
 has said that their teams are using alpha fold

1:06:03.760 --> 1:06:08.040
 to accelerate whatever drugs they're trying to discover.

1:06:08.040 --> 1:06:11.440
 So I think the knock on effect has been enormous

1:06:11.440 --> 1:06:15.240
 in terms of the impact that alpha fold has made.

1:06:15.240 --> 1:06:17.840
 And it's probably bringing in, it's creating biologists,

1:06:17.840 --> 1:06:20.800
 it's bringing more people into the field,

1:06:20.800 --> 1:06:23.320
 both on the excitement and both on the technical skills

1:06:23.320 --> 1:06:28.320
 involved in, it's almost like a gateway drug to biology.

1:06:28.760 --> 1:06:29.600
 Yes, it is.

1:06:29.600 --> 1:06:32.640
 And to get more computational people involved too, hopefully.

1:06:32.640 --> 1:06:35.920
 And I think for us, the next stage, as I said,

1:06:35.920 --> 1:06:37.960
 in future we have to have other considerations too.

1:06:37.960 --> 1:06:39.640
 We're building on top of alpha fold

1:06:39.640 --> 1:06:41.200
 and these other ideas I discussed with you

1:06:41.200 --> 1:06:44.800
 about protein interactions and genomics and other things.

1:06:44.800 --> 1:06:46.200
 And not everything will be open source.

1:06:46.200 --> 1:06:48.000
 Some of it we'll do commercially

1:06:48.000 --> 1:06:49.000
 because that will be the best way

1:06:49.000 --> 1:06:51.720
 to actually get the most resources and impact behind it.

1:06:51.720 --> 1:06:53.480
 In other ways, some other projects

1:06:53.480 --> 1:06:55.280
 we'll do nonprofit style.

1:06:55.280 --> 1:06:58.520
 And also we have to consider for future things as well,

1:06:58.520 --> 1:06:59.720
 safety and ethics as well.

1:06:59.720 --> 1:07:03.600
 Like synthetic biology, there is dual use.

1:07:03.600 --> 1:07:05.080
 And we have to think about that as well.

1:07:05.080 --> 1:07:08.600
 With alpha fold, we consulted with 30 different bioethicists

1:07:08.600 --> 1:07:10.240
 and other people expert in this field

1:07:10.240 --> 1:07:13.280
 to make sure it was safe before we released it.

1:07:13.280 --> 1:07:15.280
 So there'll be other considerations in future.

1:07:15.280 --> 1:07:17.120
 But for right now, I think alpha fold

1:07:17.120 --> 1:07:20.840
 is a kind of a gift from us to the scientific community.

1:07:20.840 --> 1:07:24.200
 So I'm pretty sure that something like alpha fold

1:07:25.600 --> 1:07:29.080
 will be part of Nobel prizes in the future.

1:07:29.080 --> 1:07:30.840
 But us humans, of course,

1:07:30.840 --> 1:07:32.480
 are horrible with credit assignment.

1:07:32.480 --> 1:07:34.520
 So we'll of course give it to the humans.

1:07:35.560 --> 1:07:37.440
 Do you think there will be a day

1:07:37.440 --> 1:07:42.440
 when AI system can't be denied

1:07:42.520 --> 1:07:45.120
 that it earned that Nobel prize?

1:07:45.120 --> 1:07:47.400
 Do you think we will see that in 21st century?

1:07:47.400 --> 1:07:50.200
 It depends what type of AIs we end up building, right?

1:07:50.200 --> 1:07:53.600
 Whether they're goal seeking agents

1:07:53.600 --> 1:07:57.800
 who specifies the goals, who comes up with the hypotheses,

1:07:57.800 --> 1:08:00.320
 who determines which problems to tackle, right?

1:08:00.320 --> 1:08:01.160
 So I think...

1:08:01.160 --> 1:08:02.440
 And tweets about it, announcement of the results.

1:08:02.440 --> 1:08:05.440
 Yes, and tweets about results exactly as part of it.

1:08:05.440 --> 1:08:07.760
 So I think right now, of course,

1:08:07.760 --> 1:08:12.200
 it's amazing human ingenuity that's behind these systems.

1:08:12.200 --> 1:08:15.120
 And then the system, in my opinion, is just a tool.

1:08:15.120 --> 1:08:18.400
 Be a bit like saying with Galileo and his telescope,

1:08:18.400 --> 1:08:21.160
 the ingenuity that the credit should go to the telescope.

1:08:21.160 --> 1:08:23.560
 I mean, it's clearly Galileo building the tool

1:08:23.560 --> 1:08:25.160
 which he then uses.

1:08:25.160 --> 1:08:27.320
 So I still see that in the same way today,

1:08:27.320 --> 1:08:30.440
 even though these tools learn for themselves.

1:08:30.440 --> 1:08:32.960
 There, I think of things like alpha fold

1:08:32.960 --> 1:08:35.840
 and the things we're building as the ultimate tools

1:08:35.840 --> 1:08:38.560
 for science and for acquiring new knowledge

1:08:38.560 --> 1:08:41.160
 to help us as scientists acquire new knowledge.

1:08:41.160 --> 1:08:43.200
 I think one day there will come a point

1:08:43.200 --> 1:08:46.360
 where an AI system may solve

1:08:46.360 --> 1:08:48.800
 or come up with something like general relativity

1:08:48.800 --> 1:08:52.040
 of its own bat, not just by averaging everything

1:08:52.040 --> 1:08:55.240
 on the internet or averaging everything on PubMed,

1:08:55.240 --> 1:08:56.320
 although that would be interesting to see

1:08:56.320 --> 1:08:58.520
 what that would come up with.

1:08:58.520 --> 1:09:00.400
 So that to me is a bit like our earlier debate

1:09:00.400 --> 1:09:03.240
 about creativity, you know, inventing go

1:09:03.240 --> 1:09:06.280
 rather than just coming up with a good go move.

1:09:06.280 --> 1:09:10.400
 And so I think solving, I think to, you know,

1:09:10.400 --> 1:09:11.800
 if we wanted to give it the credit

1:09:11.800 --> 1:09:13.520
 of like a Nobel type of thing,

1:09:13.520 --> 1:09:15.800
 then it would need to invent go

1:09:15.800 --> 1:09:19.280
 and sort of invent that new conjecture out of the blue

1:09:19.280 --> 1:09:22.720
 rather than being specified by the human scientists

1:09:22.720 --> 1:09:23.560
 or the human creators.

1:09:23.560 --> 1:09:26.280
 So I think right now it's definitely just a tool.

1:09:26.280 --> 1:09:27.880
 Although it is interesting how far you get

1:09:27.880 --> 1:09:29.960
 by averaging everything on the internet, like you said,

1:09:29.960 --> 1:09:33.160
 because, you know, a lot of people do see science

1:09:33.160 --> 1:09:35.640
 as you're always standing on the shoulders of giants.

1:09:35.640 --> 1:09:40.040
 And the question is how much are you really reaching

1:09:40.040 --> 1:09:42.000
 up above the shoulders of giants?

1:09:42.000 --> 1:09:44.700
 Maybe it's just simulating different kinds

1:09:44.700 --> 1:09:49.360
 of results of the past with ultimately this new perspective

1:09:49.360 --> 1:09:51.120
 that gives you this breakthrough idea.

1:09:51.120 --> 1:09:54.860
 But that idea may not be novel in the way

1:09:54.860 --> 1:09:56.740
 that it can't be already discovered on the internet.

1:09:56.740 --> 1:10:00.080
 Maybe the Nobel prizes of the next 100 years

1:10:00.080 --> 1:10:03.040
 are already all there on the internet to be discovered.

1:10:03.040 --> 1:10:04.560
 They could be, they could be.

1:10:04.560 --> 1:10:08.560
 I mean, I think this is one of the big mysteries,

1:10:08.560 --> 1:10:11.720
 I think is that I, first of all,

1:10:11.720 --> 1:10:13.760
 I believe a lot of the big new breakthroughs

1:10:13.760 --> 1:10:15.280
 that are gonna come in the next few decades

1:10:15.280 --> 1:10:17.400
 and even in the last decade are gonna come

1:10:17.400 --> 1:10:20.200
 at the intersection between different subject areas

1:10:20.200 --> 1:10:23.480
 where there'll be some new connection that's found

1:10:23.480 --> 1:10:26.180
 between what seemingly were disparate areas.

1:10:26.180 --> 1:10:28.840
 And one can even think of DeepMind, as I said earlier,

1:10:28.840 --> 1:10:31.720
 as a sort of interdisciplinary between neuroscience ideas

1:10:31.720 --> 1:10:35.040
 and AI engineering ideas originally.

1:10:35.040 --> 1:10:37.960
 And so I think there's that.

1:10:37.960 --> 1:10:40.380
 And then one of the things we can't imagine today is,

1:10:40.380 --> 1:10:41.720
 and one of the reasons I think people,

1:10:41.720 --> 1:10:44.440
 we were so surprised by how well large models worked

1:10:44.440 --> 1:10:47.900
 is that actually it's very hard for our human minds,

1:10:47.900 --> 1:10:49.440
 our limited human minds to understand

1:10:49.440 --> 1:10:52.020
 what it would be like to read the whole internet, right?

1:10:52.020 --> 1:10:53.520
 I think we can do a thought experiment

1:10:53.520 --> 1:10:54.680
 and I used to do this of like,

1:10:54.680 --> 1:10:57.600
 well, what if I read the whole of Wikipedia?

1:10:57.600 --> 1:10:58.440
 What would I know?

1:10:58.440 --> 1:11:00.480
 And I think our minds can just about comprehend

1:11:00.480 --> 1:11:01.920
 maybe what that would be like,

1:11:01.920 --> 1:11:04.440
 but the whole internet is beyond comprehension.

1:11:04.440 --> 1:11:07.420
 So I think we just don't understand what it would be like

1:11:07.420 --> 1:11:10.320
 to be able to hold all of that in mind potentially, right?

1:11:10.320 --> 1:11:12.920
 And then active at once,

1:11:12.920 --> 1:11:14.520
 and then maybe what are the connections

1:11:14.520 --> 1:11:15.780
 that are available there?

1:11:15.780 --> 1:11:17.520
 So I think no doubt there are huge things

1:11:17.520 --> 1:11:19.280
 to be discovered just like that.

1:11:19.280 --> 1:11:22.280
 But I do think there is this other type of creativity

1:11:22.280 --> 1:11:25.400
 of true spark of new knowledge, new idea,

1:11:25.400 --> 1:11:26.680
 never thought before about,

1:11:26.680 --> 1:11:29.320
 can't be averaged from things that are known,

1:11:29.320 --> 1:11:32.000
 that really, of course, everything come,

1:11:32.000 --> 1:11:33.680
 nobody creates in a vacuum,

1:11:33.680 --> 1:11:35.420
 so there must be clues somewhere,

1:11:35.420 --> 1:11:38.280
 but just a unique way of putting those things together.

1:11:38.280 --> 1:11:40.480
 I think some of the greatest scientists in history

1:11:40.480 --> 1:11:42.240
 have displayed that I would say,

1:11:42.240 --> 1:11:45.120
 although it's very hard to know going back to their time,

1:11:45.120 --> 1:11:48.120
 what was exactly known when they came up with those things.

1:11:48.120 --> 1:11:52.440
 Although you're making me really think because just a thought

1:11:52.440 --> 1:11:57.360
 experiment of deeply knowing a hundred Wikipedia pages.

1:11:57.360 --> 1:11:59.200
 I don't think I can,

1:11:59.200 --> 1:12:03.400
 I've been really impressed by Wikipedia for technical topics.

1:12:03.400 --> 1:12:07.040
 So if you know a hundred pages or a thousand pages,

1:12:07.040 --> 1:12:10.120
 I don't think we can truly comprehend

1:12:10.120 --> 1:12:13.400
 what kind of intelligence that is.

1:12:13.400 --> 1:12:14.760
 That's a pretty powerful intelligence.

1:12:14.760 --> 1:12:16.120
 If you know how to use that

1:12:16.120 --> 1:12:18.320
 and integrate that information correctly,

1:12:18.320 --> 1:12:20.000
 I think you can go really far.

1:12:20.000 --> 1:12:22.080
 You can probably construct thought experiments

1:12:22.080 --> 1:12:25.840
 based on that, like simulate different ideas.

1:12:25.840 --> 1:12:28.840
 So if this is true, let me run this thought experiment

1:12:28.840 --> 1:12:30.160
 that maybe this is true.

1:12:30.160 --> 1:12:31.360
 It's not really invention.

1:12:31.360 --> 1:12:34.640
 It's like just taking literally the knowledge

1:12:34.640 --> 1:12:37.240
 and using it to construct the very basic simulation

1:12:37.240 --> 1:12:38.080
 of the world.

1:12:38.080 --> 1:12:40.080
 I mean, some argue it's romantic in part,

1:12:40.080 --> 1:12:42.400
 but Einstein would do the same kind of things

1:12:42.400 --> 1:12:43.720
 with a thought experiment.

1:12:43.720 --> 1:12:46.320
 Yeah, one could imagine doing that systematically

1:12:46.320 --> 1:12:48.440
 across millions of Wikipedia pages,

1:12:48.440 --> 1:12:50.400
 plus PubMed, all these things.

1:12:50.400 --> 1:12:53.680
 I think there are many, many things to be discovered

1:12:53.680 --> 1:12:55.280
 like that that are hugely useful.

1:12:55.280 --> 1:12:56.200
 You could imagine,

1:12:56.200 --> 1:12:58.520
 and I want us to do some of these things in material science

1:12:58.520 --> 1:13:00.000
 like room temperature superconductors

1:13:00.000 --> 1:13:01.560
 is something on my list one day.

1:13:01.560 --> 1:13:05.000
 I'd like to have an AI system to help build

1:13:05.000 --> 1:13:06.640
 better optimized batteries,

1:13:06.640 --> 1:13:09.000
 all of these sort of mechanical things.

1:13:09.000 --> 1:13:11.600
 I think a systematic sort of search

1:13:11.600 --> 1:13:14.360
 could be guided by a model,

1:13:14.360 --> 1:13:17.120
 could be extremely powerful.

1:13:17.120 --> 1:13:18.160
 So speaking of which,

1:13:18.160 --> 1:13:20.160
 you have a paper on nuclear fusion,

1:13:21.320 --> 1:13:23.120
 magnetic control of tachymic plasmas

1:13:23.120 --> 1:13:24.720
 through deep reinforcement learning.

1:13:24.720 --> 1:13:29.720
 So you're seeking to solve nuclear fusion with deep RL.

1:13:29.800 --> 1:13:31.840
 So it's doing control of high temperature plasmas.

1:13:31.840 --> 1:13:33.520
 Can you explain this work

1:13:33.520 --> 1:13:37.240
 and can AI eventually solve nuclear fusion?

1:13:37.240 --> 1:13:40.200
 It's been very fun last year or two and very productive

1:13:40.200 --> 1:13:43.360
 because we've been taking off a lot of my dream projects,

1:13:43.360 --> 1:13:44.960
 if you like, of things that I've collected

1:13:44.960 --> 1:13:46.960
 over the years of areas of science

1:13:46.960 --> 1:13:48.200
 that I would like to,

1:13:48.200 --> 1:13:51.200
 I think could be very transformative if we helped accelerate

1:13:51.200 --> 1:13:53.600
 and really interesting problems,

1:13:53.600 --> 1:13:55.760
 scientific challenges in of themselves.

1:13:55.760 --> 1:13:57.040
 So this is energy.

1:13:57.040 --> 1:13:58.520
 So energy, yes, exactly.

1:13:58.520 --> 1:13:59.960
 So energy and climate.

1:13:59.960 --> 1:14:01.760
 So we talked about disease and biology

1:14:01.760 --> 1:14:04.520
 as being one of the biggest places I think AI can help with.

1:14:04.520 --> 1:14:07.120
 I think energy and climate is another one.

1:14:07.120 --> 1:14:09.240
 So maybe they would be my top two.

1:14:09.240 --> 1:14:12.520
 And fusion is one area I think AI can help with.

1:14:12.520 --> 1:14:15.360
 Now, fusion has many challenges,

1:14:15.360 --> 1:14:17.240
 mostly physics and material science

1:14:17.240 --> 1:14:18.600
 and engineering challenges as well

1:14:18.600 --> 1:14:20.520
 to build these massive fusion reactors

1:14:20.520 --> 1:14:21.920
 and contain the plasma.

1:14:21.920 --> 1:14:22.760
 And what we try to do,

1:14:22.760 --> 1:14:26.280
 and whenever we go into a new field to apply our systems,

1:14:26.280 --> 1:14:29.240
 is we look for, we talk to domain experts.

1:14:29.240 --> 1:14:30.680
 We try and find the best people in the world

1:14:30.680 --> 1:14:31.680
 to collaborate with.

1:14:33.000 --> 1:14:34.120
 In this case, in fusion,

1:14:34.120 --> 1:14:36.400
 we collaborated with EPFL in Switzerland,

1:14:36.400 --> 1:14:38.280
 the Swiss Technical Institute, who are amazing.

1:14:38.280 --> 1:14:39.640
 They have a test reactor.

1:14:39.640 --> 1:14:41.360
 They were willing to let us use,

1:14:41.360 --> 1:14:43.400
 which I double checked with the team

1:14:43.400 --> 1:14:46.120
 we were gonna use carefully and safely.

1:14:46.120 --> 1:14:47.760
 I was impressed they managed to persuade them

1:14:47.760 --> 1:14:49.160
 to let us use it.

1:14:49.160 --> 1:14:53.440
 And it's an amazing test reactor they have there.

1:14:53.440 --> 1:14:57.000
 And they try all sorts of pretty crazy experiments on it.

1:14:57.000 --> 1:14:59.720
 And what we tend to look at is,

1:14:59.720 --> 1:15:01.760
 if we go into a new domain like fusion,

1:15:01.760 --> 1:15:04.160
 what are all the bottleneck problems?

1:15:04.160 --> 1:15:05.960
 Like thinking from first principles,

1:15:05.960 --> 1:15:07.000
 what are all the bottleneck problems

1:15:07.000 --> 1:15:09.280
 that are still stopping fusion working today?

1:15:09.280 --> 1:15:12.080
 And then we look at, we get a fusion expert to tell us,

1:15:12.080 --> 1:15:13.760
 and then we look at those bottlenecks

1:15:13.760 --> 1:15:14.600
 and we look at the ones,

1:15:14.600 --> 1:15:18.920
 which ones are amenable to our AI methods today, right?

1:15:18.920 --> 1:15:22.200
 And would be interesting from a research perspective,

1:15:22.200 --> 1:15:24.400
 from our point of view, from an AI point of view,

1:15:24.400 --> 1:15:26.760
 and that would address one of their bottlenecks.

1:15:26.760 --> 1:15:29.720
 And in this case, plasma control was perfect.

1:15:29.720 --> 1:15:32.480
 So, the plasma, it's a million degrees Celsius,

1:15:32.480 --> 1:15:34.640
 something like that, it's hotter than the sun.

1:15:34.640 --> 1:15:37.640
 And there's obviously no material that can contain it.

1:15:37.640 --> 1:15:39.440
 So, they have to be containing these magnetic,

1:15:39.440 --> 1:15:42.520
 very powerful and superconducting magnetic fields.

1:15:42.520 --> 1:15:43.960
 But the problem is plasma,

1:15:43.960 --> 1:15:45.360
 it's pretty unstable as you imagine,

1:15:45.360 --> 1:15:49.320
 you're kind of holding a mini sun, mini star in a reactor.

1:15:49.320 --> 1:15:52.520
 So, you kind of want to predict ahead of time,

1:15:52.520 --> 1:15:54.040
 what the plasma is gonna do.

1:15:54.040 --> 1:15:56.240
 So, you can move the magnetic field

1:15:56.240 --> 1:15:58.440
 within a few milliseconds,

1:15:58.440 --> 1:16:00.960
 to basically contain what it's gonna do next.

1:16:00.960 --> 1:16:03.160
 So, it seems like a perfect problem if you think of it

1:16:03.160 --> 1:16:06.280
 for like a reinforcement learning prediction problem.

1:16:06.280 --> 1:16:09.720
 So, you got controller, you're gonna move the magnetic field.

1:16:09.720 --> 1:16:12.560
 And until we came along, they were doing it

1:16:12.560 --> 1:16:16.720
 with traditional operational research type of controllers,

1:16:16.720 --> 1:16:18.320
 which are kind of handcrafted.

1:16:18.320 --> 1:16:19.160
 And the problem is, of course,

1:16:19.160 --> 1:16:20.480
 they can't react in the moment

1:16:20.480 --> 1:16:21.640
 to something the plasma is doing,

1:16:21.640 --> 1:16:23.040
 they have to be hard coded.

1:16:23.040 --> 1:16:26.040
 And again, knowing that that's normally our go to solution

1:16:26.040 --> 1:16:27.960
 is we would like to learn that instead.

1:16:27.960 --> 1:16:30.320
 And they also had a simulator of these plasma.

1:16:30.320 --> 1:16:31.480
 So, there were lots of criteria

1:16:31.480 --> 1:16:34.760
 that matched what we like to use.

1:16:34.760 --> 1:16:38.440
 So, can AI eventually solve nuclear fusion?

1:16:38.440 --> 1:16:39.760
 Well, so with this problem,

1:16:39.760 --> 1:16:42.040
 and we published it in a nature paper last year,

1:16:42.040 --> 1:16:46.160
 we held the fusion, we held the plasma in a specific shapes.

1:16:46.160 --> 1:16:48.360
 So, actually, it's almost like carving the plasma

1:16:48.360 --> 1:16:51.000
 into different shapes and hold it there

1:16:51.000 --> 1:16:52.880
 for a record amount of time.

1:16:52.880 --> 1:16:57.600
 So, that's one of the problems of fusion sort of solved.

1:16:57.600 --> 1:16:59.840
 So, have a controller that's able to,

1:16:59.840 --> 1:17:01.480
 no matter the shape.

1:17:01.480 --> 1:17:02.360
 Contain it. Contain it.

1:17:02.360 --> 1:17:04.160
 Yeah, contain it and hold it in structure.

1:17:04.160 --> 1:17:05.760
 And there's different shapes that are better

1:17:05.760 --> 1:17:10.080
 for the energy productions called droplets and so on.

1:17:10.080 --> 1:17:11.880
 So, that was huge.

1:17:11.880 --> 1:17:12.720
 And now we're looking,

1:17:12.720 --> 1:17:14.400
 we're talking to lots of fusion startups

1:17:14.400 --> 1:17:17.400
 to see what's the next problem we can tackle

1:17:17.400 --> 1:17:19.360
 in the fusion area.

1:17:19.360 --> 1:17:23.080
 So, another fascinating place in a paper titled,

1:17:23.080 --> 1:17:25.120
 Pushing the Frontiers of Density Functionals

1:17:25.120 --> 1:17:27.520
 by Solving the Fractional Electron Problem.

1:17:27.520 --> 1:17:30.880
 So, you're taking on modeling and simulating

1:17:30.880 --> 1:17:33.320
 the quantum mechanical behavior of electrons.

1:17:33.320 --> 1:17:34.160
 Yes.

1:17:36.040 --> 1:17:39.240
 Can you explain this work and can AI model

1:17:39.240 --> 1:17:41.560
 and simulate arbitrary quantum mechanical systems

1:17:41.560 --> 1:17:42.400
 in the future?

1:17:42.400 --> 1:17:44.240
 Yeah, so this is another problem I've had my eye on

1:17:44.240 --> 1:17:47.160
 for a decade or more,

1:17:47.160 --> 1:17:51.200
 which is sort of simulating the properties of electrons.

1:17:51.200 --> 1:17:54.280
 If you can do that, you can basically describe

1:17:54.280 --> 1:17:58.040
 how elements and materials and substances work.

1:17:58.040 --> 1:18:00.040
 So, it's kind of like fundamental

1:18:00.040 --> 1:18:02.840
 if you want to advance material science.

1:18:02.840 --> 1:18:05.240
 And we have Schrodinger's equation

1:18:05.240 --> 1:18:06.480
 and then we have approximations

1:18:06.480 --> 1:18:08.400
 to that density functional theory.

1:18:08.400 --> 1:18:10.560
 These things are famous.

1:18:10.560 --> 1:18:13.200
 And people try and write approximations

1:18:13.200 --> 1:18:17.040
 to these functionals and kind of come up

1:18:17.040 --> 1:18:19.880
 with descriptions of the electron clouds,

1:18:19.880 --> 1:18:20.720
 where they're going to go,

1:18:20.720 --> 1:18:22.120
 how they're going to interact

1:18:22.120 --> 1:18:24.240
 when you put two elements together.

1:18:24.240 --> 1:18:26.760
 And what we try to do is learn a simulation,

1:18:27.680 --> 1:18:30.560
 learn a functional that will describe more chemistry,

1:18:30.560 --> 1:18:31.760
 types of chemistry.

1:18:31.760 --> 1:18:35.560
 So, until now, you can run expensive simulations,

1:18:35.560 --> 1:18:38.760
 but then you can only simulate very small molecules,

1:18:38.760 --> 1:18:40.160
 very simple molecules.

1:18:40.160 --> 1:18:43.080
 We would like to simulate large materials.

1:18:43.080 --> 1:18:45.760
 And so, today there's no way of doing that.

1:18:45.760 --> 1:18:48.560
 And we're building up towards building functionals

1:18:48.560 --> 1:18:51.240
 that approximate Schrodinger's equation

1:18:51.240 --> 1:18:55.600
 and then allow you to describe what the electrons are doing.

1:18:55.600 --> 1:18:57.480
 And all material sort of science

1:18:57.480 --> 1:18:59.920
 and material properties are governed by the electrons

1:18:59.920 --> 1:19:01.360
 and how they interact.

1:19:01.360 --> 1:19:05.840
 So, have a good summarization of the simulation

1:19:05.840 --> 1:19:07.080
 through the functional,

1:19:08.720 --> 1:19:11.360
 but one that is still close

1:19:11.360 --> 1:19:13.200
 to what the actual simulation would come out with.

1:19:13.200 --> 1:19:16.720
 So, how difficult is that task?

1:19:16.720 --> 1:19:17.760
 What's involved in that task?

1:19:17.760 --> 1:19:20.720
 Is it running those complicated simulations

1:19:20.720 --> 1:19:23.280
 and learning the task of mapping

1:19:23.280 --> 1:19:24.560
 from the initial conditions

1:19:24.560 --> 1:19:26.400
 and the parameters of the simulation,

1:19:26.400 --> 1:19:27.720
 learning what the functional would be?

1:19:27.720 --> 1:19:28.560
 Yeah.

1:19:28.560 --> 1:19:29.440
 So, it's pretty tricky.

1:19:29.440 --> 1:19:31.320
 And we've done it with,

1:19:31.320 --> 1:19:35.440
 the nice thing is we can run a lot of the simulations,

1:19:35.440 --> 1:19:39.080
 the molecular dynamic simulations on our compute clusters.

1:19:39.080 --> 1:19:40.840
 And so, that generates a lot of data.

1:19:40.840 --> 1:19:42.800
 So, in this case, the data is generated.

1:19:42.800 --> 1:19:45.880
 So, we like those sort of systems and that's why we use games.

1:19:45.880 --> 1:19:48.480
 It's simulated, generated data.

1:19:48.480 --> 1:19:51.160
 And we can kind of create as much of it as we want, really.

1:19:51.160 --> 1:19:53.280
 And just let's leave some,

1:19:53.280 --> 1:19:55.280
 if any computers are free in the cloud,

1:19:55.280 --> 1:19:57.680
 we just run, we run some of these calculations, right?

1:19:57.680 --> 1:19:59.360
 Compute cluster calculation.

1:19:59.360 --> 1:20:01.080
 I like how the free compute time

1:20:01.080 --> 1:20:02.200
 is used up on quantum mechanics.

1:20:02.200 --> 1:20:03.560
 Yeah, quantum mechanics, exactly.

1:20:03.560 --> 1:20:06.280
 Simulations and protein simulations and other things.

1:20:06.280 --> 1:20:09.880
 And so, when you're not searching on YouTube

1:20:09.880 --> 1:20:11.360
 for free video, cat videos,

1:20:11.360 --> 1:20:13.960
 we're using those computers usefully in quantum chemistry.

1:20:13.960 --> 1:20:14.800
 It's the idea.

1:20:14.800 --> 1:20:15.640
 Finally.

1:20:15.640 --> 1:20:17.000
 And putting them to good use.

1:20:17.000 --> 1:20:19.760
 And then, yeah, and then all of that computational data

1:20:19.760 --> 1:20:20.840
 that's generated,

1:20:20.840 --> 1:20:23.480
 we can then try and learn the functionals from that,

1:20:23.480 --> 1:20:25.640
 which of course are way more efficient

1:20:25.640 --> 1:20:27.080
 once we learn the functional

1:20:27.080 --> 1:20:30.560
 than running those simulations would be.

1:20:30.560 --> 1:20:33.120
 Do you think one day AI may allow us

1:20:33.120 --> 1:20:36.360
 to do something like basically crack open physics?

1:20:36.360 --> 1:20:39.520
 So, do something like travel faster than the speed of light?

1:20:39.520 --> 1:20:41.600
 My ultimate aim is always being with AI

1:20:41.600 --> 1:20:45.560
 is the reason I am personally working on AI

1:20:45.560 --> 1:20:48.200
 for my whole life, it was to build a tool

1:20:48.200 --> 1:20:50.360
 to help us understand the universe.

1:20:50.360 --> 1:20:53.800
 So, I wanted to, and that means physics, really,

1:20:53.800 --> 1:20:54.920
 and the nature of reality.

1:20:54.920 --> 1:20:58.000
 So, I don't think we have systems

1:20:58.000 --> 1:20:59.400
 that are capable of doing that yet,

1:20:59.400 --> 1:21:01.000
 but when we get towards AGI,

1:21:01.000 --> 1:21:02.920
 I think that's one of the first things

1:21:02.920 --> 1:21:05.320
 I think we should apply AGI to.

1:21:05.320 --> 1:21:07.160
 I would like to test the limits of physics

1:21:07.160 --> 1:21:08.600
 and our knowledge of physics.

1:21:08.600 --> 1:21:10.080
 There's so many things we don't know.

1:21:10.080 --> 1:21:12.320
 This is one thing I find fascinating about science.

1:21:12.320 --> 1:21:15.080
 And as a huge proponent of the scientific method

1:21:15.080 --> 1:21:17.880
 as being one of the greatest ideas humanity has ever had

1:21:17.880 --> 1:21:20.160
 and allowed us to progress with our knowledge,

1:21:20.160 --> 1:21:22.000
 but I think as a true scientist,

1:21:22.000 --> 1:21:25.200
 I think what you find is the more you find out,

1:21:25.200 --> 1:21:27.040
 the more you realize we don't know.

1:21:27.040 --> 1:21:29.880
 And I always think that it's surprising

1:21:29.880 --> 1:21:31.880
 that more people aren't troubled.

1:21:31.880 --> 1:21:34.000
 Every night I think about all these things

1:21:34.000 --> 1:21:35.240
 we interact with all the time,

1:21:35.240 --> 1:21:36.880
 that we have no idea how they work.

1:21:36.880 --> 1:21:41.440
 Time, consciousness, gravity, life, we can't,

1:21:41.440 --> 1:21:43.840
 I mean, these are all the fundamental things of nature.

1:21:43.840 --> 1:21:47.320
 I think the way we don't really know what they are.

1:21:47.320 --> 1:21:51.480
 To live life, we pin certain assumptions on them

1:21:51.480 --> 1:21:55.240
 and kind of treat our assumptions as if they're a fact.

1:21:55.240 --> 1:21:57.560
 That allows us to sort of box them off somehow.

1:21:57.560 --> 1:21:59.000
 Yeah, box them off somehow.

1:21:59.000 --> 1:22:02.320
 But the reality is when you think of time,

1:22:02.320 --> 1:22:03.560
 you should remind yourself,

1:22:03.560 --> 1:22:06.760
 you should take it off the shelf

1:22:06.760 --> 1:22:09.040
 and realize like, no, we have a bunch of assumptions.

1:22:09.040 --> 1:22:11.520
 There's still a lot of, there's even now a lot of debate.

1:22:11.520 --> 1:22:15.520
 There's a lot of uncertainty about exactly what is time.

1:22:15.520 --> 1:22:17.480
 Is there an error of time?

1:22:17.480 --> 1:22:19.480
 You know, there's a lot of fundamental questions

1:22:19.480 --> 1:22:21.160
 that you can't just make assumptions about.

1:22:21.160 --> 1:22:26.160
 And maybe AI allows you to not put anything on the shelf.

1:22:27.680 --> 1:22:28.520
 Yeah.

1:22:28.520 --> 1:22:30.200
 Not make any hard assumptions

1:22:30.200 --> 1:22:32.080
 and really open it up and see what's.

1:22:32.080 --> 1:22:34.640
 Exactly, I think we should be truly open minded about that.

1:22:34.640 --> 1:22:39.040
 And exactly that, not be dogmatic to a particular theory.

1:22:39.040 --> 1:22:41.960
 It'll also allow us to build better tools,

1:22:41.960 --> 1:22:44.400
 experimental tools eventually,

1:22:44.400 --> 1:22:46.280
 that can then test certain theories

1:22:46.280 --> 1:22:48.080
 that may not be testable today.

1:22:48.080 --> 1:22:51.240
 Things about like what we spoke about at the beginning

1:22:51.240 --> 1:22:53.520
 about the computational nature of the universe.

1:22:53.520 --> 1:22:55.320
 How one might, if that was true,

1:22:55.320 --> 1:22:57.360
 how one might go about testing that, right?

1:22:57.360 --> 1:22:59.840
 And how much, you know, there are people

1:22:59.840 --> 1:23:02.520
 who've conjectured people like Scott Aaronson and others

1:23:02.520 --> 1:23:04.720
 about, you know, how much information

1:23:04.720 --> 1:23:08.040
 can a specific plank unit of space and time

1:23:08.040 --> 1:23:09.200
 contain, right?

1:23:09.200 --> 1:23:11.960
 So one might be able to think about testing those ideas

1:23:11.960 --> 1:23:15.360
 if you had AI helping you build

1:23:15.360 --> 1:23:19.400
 some new exquisite experimental tools.

1:23:19.400 --> 1:23:20.960
 This is what I imagine that, you know,

1:23:20.960 --> 1:23:23.120
 many decades from now we'll be able to do.

1:23:23.120 --> 1:23:25.840
 And what kind of questions can be answered

1:23:25.840 --> 1:23:28.840
 through running a simulation of them?

1:23:28.840 --> 1:23:30.760
 So there's a bunch of physics simulations

1:23:30.760 --> 1:23:32.600
 you can imagine that could be run

1:23:32.600 --> 1:23:35.760
 in some kind of efficient way,

1:23:35.760 --> 1:23:38.760
 much like you're doing in the quantum simulation work.

1:23:40.320 --> 1:23:42.160
 And perhaps even the origin of life.

1:23:42.160 --> 1:23:45.160
 So figuring out how going even back

1:23:45.160 --> 1:23:47.640
 before the work of AlphaFold begins

1:23:47.640 --> 1:23:52.640
 of how this whole thing emerges from a rock.

1:23:52.640 --> 1:23:53.480
 Yes.

1:23:53.480 --> 1:23:54.320
 From a static thing.

1:23:54.320 --> 1:23:57.040
 What do you think AI will allow us to,

1:23:57.040 --> 1:23:58.920
 is that something you have your eye on?

1:23:58.920 --> 1:24:01.560
 It's trying to understand the origin of life.

1:24:01.560 --> 1:24:06.320
 First of all, yourself, what do you think,

1:24:06.320 --> 1:24:08.760
 how the heck did life originate on Earth?

1:24:08.760 --> 1:24:11.120
 Yeah, well, maybe I'll come to that in a second,

1:24:11.120 --> 1:24:13.800
 but I think the ultimate use of AI

1:24:13.800 --> 1:24:18.120
 is to kind of use it to accelerate science to the maximum.

1:24:18.120 --> 1:24:21.040
 So I think of it a little bit

1:24:21.040 --> 1:24:22.600
 like the tree of all knowledge.

1:24:22.600 --> 1:24:24.160
 If you imagine that's all the knowledge there is

1:24:24.160 --> 1:24:25.840
 in the universe to attain.

1:24:25.840 --> 1:24:29.320
 And we sort of barely scratched the surface of that so far.

1:24:29.320 --> 1:24:31.960
 And even though we've done pretty well

1:24:31.960 --> 1:24:34.320
 since the enlightenment, right, as humanity.

1:24:34.320 --> 1:24:36.840
 And I think AI will turbocharge all of that,

1:24:36.840 --> 1:24:38.600
 like we've seen with AlphaFold.

1:24:38.600 --> 1:24:41.400
 And I want to explore as much of that tree of knowledge

1:24:41.400 --> 1:24:42.920
 as is possible to do.

1:24:42.920 --> 1:24:46.400
 And I think that involves AI helping us

1:24:46.400 --> 1:24:49.680
 with understanding or finding patterns,

1:24:49.680 --> 1:24:52.200
 but also potentially designing and building new tools,

1:24:52.200 --> 1:24:53.600
 experimental tools.

1:24:53.600 --> 1:24:54.840
 So I think that's all,

1:24:56.040 --> 1:24:58.920
 and also running simulations and learning simulations,

1:24:58.920 --> 1:25:03.920
 all of that we're sort of doing at a baby steps level here.

1:25:05.000 --> 1:25:08.560
 But I can imagine that in the decades to come

1:25:08.560 --> 1:25:12.920
 as what's the full flourishing of that line of thinking.

1:25:12.920 --> 1:25:15.160
 It's gonna be truly incredible, I would say.

1:25:15.160 --> 1:25:17.320
 If I visualized this tree of knowledge,

1:25:17.320 --> 1:25:20.840
 something tells me that that tree of knowledge for humans

1:25:20.840 --> 1:25:24.440
 is much smaller in the set of all possible trees

1:25:24.440 --> 1:25:26.600
 of knowledge, it's actually quite small

1:25:26.600 --> 1:25:30.320
 given our cognitive limitations,

1:25:31.480 --> 1:25:33.680
 limited cognitive capabilities,

1:25:33.680 --> 1:25:35.720
 that even with the tools we build,

1:25:35.720 --> 1:25:38.120
 we still won't be able to understand a lot of things.

1:25:38.120 --> 1:25:41.160
 And that's perhaps what nonhuman systems

1:25:41.160 --> 1:25:44.920
 might be able to reach farther, not just as tools,

1:25:44.920 --> 1:25:47.200
 but in themselves understanding something

1:25:47.200 --> 1:25:48.480
 that they can bring back.

1:25:48.480 --> 1:25:50.200
 Yeah, it could well be.

1:25:50.200 --> 1:25:51.800
 So, I mean, there's so many things

1:25:51.800 --> 1:25:55.000
 that are sort of encapsulated in what you just said there.

1:25:55.000 --> 1:25:58.320
 I think first of all, there's two different things.

1:25:58.320 --> 1:26:00.560
 There's like, what do we understand today?

1:26:00.560 --> 1:26:02.680
 What could the human mind understand?

1:26:02.680 --> 1:26:06.400
 And what is the totality of what is there to be understood?

1:26:06.400 --> 1:26:08.640
 And so there's three concentric,

1:26:08.640 --> 1:26:10.720
 you can think of them as three larger and larger trees

1:26:10.720 --> 1:26:12.880
 or exploring more branches of that tree.

1:26:12.880 --> 1:26:15.960
 And I think with AI, we're gonna explore that whole lot.

1:26:15.960 --> 1:26:19.120
 Now, the question is, if you think about

1:26:19.120 --> 1:26:21.840
 what is the totality of what could be understood,

1:26:22.680 --> 1:26:24.800
 there may be some fundamental physics reasons

1:26:24.800 --> 1:26:26.280
 why certain things can't be understood,

1:26:26.280 --> 1:26:29.000
 like what's outside a simulation or outside the universe.

1:26:29.000 --> 1:26:32.320
 Maybe it's not understandable from within the universe.

1:26:32.320 --> 1:26:34.840
 So there may be some hard constraints like that.

1:26:34.840 --> 1:26:36.000
 It could be smaller constraints,

1:26:36.000 --> 1:26:40.520
 like we think of space time as fundamental.

1:26:40.520 --> 1:26:42.880
 Our human brains are really used to this idea

1:26:42.880 --> 1:26:46.040
 of a three dimensional world with time, maybe.

1:26:46.040 --> 1:26:47.760
 But our tools could go beyond that.

1:26:47.760 --> 1:26:49.760
 They wouldn't have that limitation necessarily.

1:26:49.760 --> 1:26:51.760
 They could think in 11 dimensions, 12 dimensions,

1:26:51.760 --> 1:26:52.920
 whatever is needed.

1:26:52.920 --> 1:26:55.640
 But we could still maybe understand that

1:26:55.640 --> 1:26:56.720
 in several different ways.

1:26:56.720 --> 1:26:59.040
 The example I always give is,

1:26:59.040 --> 1:27:01.400
 when I play Garry Kasparov for speed chess,

1:27:01.400 --> 1:27:04.400
 or we've talked about chess and these kinds of things,

1:27:04.400 --> 1:27:07.520
 you know, if you're reasonably good at chess,

1:27:07.520 --> 1:27:11.200
 you can't come up with the move Garry comes up with

1:27:11.200 --> 1:27:13.320
 in his move, but he can explain it to you.

1:27:13.320 --> 1:27:14.160
 And you can understand.

1:27:14.160 --> 1:27:16.720
 And you can understand post hoc the reasoning.

1:27:16.720 --> 1:27:19.400
 So I think there's an even further level of like,

1:27:19.400 --> 1:27:21.640
 well, maybe you couldn't have invented that thing,

1:27:21.640 --> 1:27:24.320
 but going back to using language again,

1:27:24.320 --> 1:27:27.040
 perhaps you can understand and appreciate that.

1:27:27.040 --> 1:27:28.920
 Same way that you can appreciate, you know,

1:27:28.920 --> 1:27:31.120
 Vivaldi or Mozart or something without,

1:27:31.120 --> 1:27:32.680
 you can appreciate the beauty of that

1:27:32.680 --> 1:27:35.800
 without being able to construct it yourself, right?

1:27:35.800 --> 1:27:37.400
 Invent the music yourself.

1:27:37.400 --> 1:27:39.280
 So I think we see this in all forms of life.

1:27:39.280 --> 1:27:42.440
 So it will be that times, you know, a million,

1:27:42.440 --> 1:27:45.800
 but you can imagine also one sign of intelligence

1:27:45.800 --> 1:27:49.320
 is the ability to explain things clearly and simply, right?

1:27:49.320 --> 1:27:50.400
 You know, people like Richard Feynman,

1:27:50.400 --> 1:27:52.400
 another one of my old time heroes used to say that, right?

1:27:52.400 --> 1:27:54.480
 If you can't, you know, if you can explain it

1:27:54.480 --> 1:27:57.360
 something simply, then that's the best sign,

1:27:57.360 --> 1:27:58.640
 a complex topic simply,

1:27:58.640 --> 1:28:00.680
 then that's one of the best signs of you understanding it.

1:28:00.680 --> 1:28:01.520
 Yeah.

1:28:01.520 --> 1:28:04.600
 I can see myself talking trash in the AI system in that way.

1:28:04.600 --> 1:28:05.680
 Yes.

1:28:05.680 --> 1:28:07.800
 It gets frustrated how dumb I am

1:28:07.800 --> 1:28:09.880
 and trying to explain something to me.

1:28:09.880 --> 1:28:11.600
 I was like, well, that means you're not intelligent

1:28:11.600 --> 1:28:12.720
 because if you were intelligent,

1:28:12.720 --> 1:28:14.440
 you'd be able to explain it simply.

1:28:14.440 --> 1:28:16.720
 Yeah, of course, you know, there's also the other option.

1:28:16.720 --> 1:28:19.560
 Of course, we could enhance ourselves and with our devices,

1:28:19.560 --> 1:28:23.120
 we are already sort of symbiotic with our compute devices,

1:28:23.120 --> 1:28:24.600
 right, with our phones and other things.

1:28:24.600 --> 1:28:27.120
 And, you know, there's stuff like Neuralink and Xceptra

1:28:27.120 --> 1:28:30.000
 that could advance that further.

1:28:30.000 --> 1:28:33.880
 So I think there's lots of really amazing possibilities

1:28:33.880 --> 1:28:35.360
 that I could foresee from here.

1:28:35.360 --> 1:28:37.040
 Well, let me ask you some wild questions.

1:28:37.040 --> 1:28:39.920
 So out there looking for friends,

1:28:39.920 --> 1:28:43.120
 do you think there's a lot of alien civilizations out there?

1:28:43.120 --> 1:28:44.960
 So I guess this also goes back

1:28:44.960 --> 1:28:46.640
 to your origin of life question too,

1:28:46.640 --> 1:28:48.240
 because I think that that's key.

1:28:48.240 --> 1:28:51.360
 My personal opinion, looking at all this,

1:28:51.360 --> 1:28:53.680
 and, you know, it's one of my hobbies, physics, I guess.

1:28:53.680 --> 1:28:56.880
 So, you know, it's something I think about a lot

1:28:56.880 --> 1:29:00.760
 and talk to a lot of experts on and read a lot of books on.

1:29:00.760 --> 1:29:05.280
 And I think my feeling currently is that we are alone.

1:29:05.280 --> 1:29:07.160
 I think that's the most likely scenario

1:29:07.160 --> 1:29:08.800
 given what evidence we have.

1:29:08.800 --> 1:29:13.160
 So, and the reasoning is I think that, you know,

1:29:13.160 --> 1:29:16.120
 we've tried since things like SETI program

1:29:16.120 --> 1:29:19.840
 and I guess since the dawning of the space age,

1:29:19.840 --> 1:29:21.240
 we've, you know, had telescopes,

1:29:21.240 --> 1:29:23.280
 open radio telescopes and other things.

1:29:23.280 --> 1:29:27.280
 And if you think about and try to detect signals,

1:29:27.280 --> 1:29:30.120
 now, if you think about the evolution of humans on earth,

1:29:30.120 --> 1:29:33.960
 we could have easily been a million years ahead

1:29:33.960 --> 1:29:36.280
 of our time now or million years behind,

1:29:36.280 --> 1:29:39.440
 right, easily with just some slightly different quirk

1:29:39.440 --> 1:29:42.120
 thing happening hundreds of thousands of years ago.

1:29:42.120 --> 1:29:43.640
 You know, things could have been slightly different

1:29:43.640 --> 1:29:46.200
 if the meteor would hit the dinosaurs a million years earlier,

1:29:46.200 --> 1:29:48.080
 maybe things would have evolved.

1:29:48.080 --> 1:29:50.920
 We'd be a million years ahead of where we are now.

1:29:50.920 --> 1:29:54.080
 So what that means is if you imagine where humanity will be

1:29:54.080 --> 1:29:56.720
 in a few hundred years, let alone a million years,

1:29:56.720 --> 1:29:59.840
 especially if we hopefully, you know,

1:29:59.840 --> 1:30:02.200
 solve things like climate change and other things,

1:30:02.200 --> 1:30:05.640
 and we continue to flourish and we build things like AI

1:30:05.640 --> 1:30:07.920
 and we do space traveling and all of the stuff

1:30:07.920 --> 1:30:10.800
 that humans have dreamed of forever, right?

1:30:10.800 --> 1:30:14.360
 And sci fi is talked about forever.

1:30:14.360 --> 1:30:16.760
 We will be spreading across the stars, right?

1:30:16.760 --> 1:30:19.240
 And von Neumann famously calculated, you know,

1:30:19.240 --> 1:30:20.800
 it would only take about a million years

1:30:20.800 --> 1:30:23.240
 if you sent out von Neumann probes to the nearest,

1:30:23.240 --> 1:30:26.200
 you know, the nearest other solar systems.

1:30:26.200 --> 1:30:29.040
 And then all they did was build two more versions

1:30:29.040 --> 1:30:30.440
 of themselves and sent those two out

1:30:30.440 --> 1:30:32.240
 to the next nearest systems.

1:30:32.240 --> 1:30:33.480
 You know, within a million years,

1:30:33.480 --> 1:30:35.040
 I think you would have one of these probes

1:30:35.040 --> 1:30:36.920
 in every system in the galaxy.

1:30:36.920 --> 1:30:40.040
 So it's not actually in cosmological time.

1:30:40.040 --> 1:30:42.080
 That's actually a very short amount of time.

1:30:42.080 --> 1:30:44.600
 So, and you know, people like Dyson have thought

1:30:44.600 --> 1:30:47.280
 about constructing Dyson spheres around stars

1:30:47.280 --> 1:30:49.800
 to collect all the energy coming out of the star.

1:30:49.800 --> 1:30:51.800
 You know, there would be constructions like that

1:30:51.800 --> 1:30:54.120
 would be visible across space,

1:30:54.120 --> 1:30:56.000
 probably even across a galaxy.

1:30:56.000 --> 1:30:57.920
 So, and then, you know, if you think about

1:30:57.920 --> 1:31:00.760
 all of our radio, television emissions

1:31:00.760 --> 1:31:04.200
 that have gone out since the, you know, 30s and 40s,

1:31:05.120 --> 1:31:06.720
 imagine a million years of that.

1:31:06.720 --> 1:31:10.000
 And now hundreds of civilizations doing that.

1:31:10.000 --> 1:31:12.200
 When we opened our ears at the point

1:31:12.200 --> 1:31:14.840
 we got technologically sophisticated enough

1:31:14.840 --> 1:31:15.880
 in the space age,

1:31:15.880 --> 1:31:19.120
 we should have heard a cacophony of voices.

1:31:19.120 --> 1:31:20.920
 We should have joined that cacophony of voices.

1:31:20.920 --> 1:31:24.480
 And what we did, we opened our ears and we heard nothing.

1:31:24.480 --> 1:31:27.120
 And many people who argue that there are aliens

1:31:27.120 --> 1:31:28.800
 would say, well, we haven't really done

1:31:28.800 --> 1:31:29.920
 exhaustive search yet.

1:31:29.920 --> 1:31:31.880
 And maybe we're looking in the wrong bands

1:31:31.880 --> 1:31:33.760
 and we've got the wrong devices

1:31:33.760 --> 1:31:36.080
 and we wouldn't notice what an alien form was like

1:31:36.080 --> 1:31:38.280
 because it'd be so different to what we're used to.

1:31:38.280 --> 1:31:40.640
 But, you know, I don't really buy that,

1:31:40.640 --> 1:31:42.640
 that it shouldn't be as difficult as that.

1:31:42.640 --> 1:31:44.320
 Like, I think we've searched enough.

1:31:44.320 --> 1:31:45.680
 There should be everywhere.

1:31:45.680 --> 1:31:47.280
 If it was, yeah, it should be everywhere.

1:31:47.280 --> 1:31:49.240
 We should see Dyson spheres being put up,

1:31:49.240 --> 1:31:50.600
 sun's blinking in and out.

1:31:50.600 --> 1:31:52.000
 You know, there should be a lot of evidence

1:31:52.000 --> 1:31:52.920
 for those things.

1:31:52.920 --> 1:31:54.160
 And then there are other people who argue,

1:31:54.160 --> 1:31:56.000
 well, the sort of safari view of like,

1:31:56.000 --> 1:31:57.840
 well, we're a primitive species still

1:31:57.840 --> 1:31:59.400
 because we're not space faring yet.

1:31:59.400 --> 1:32:01.400
 And we're, you know, there's some kind of global,

1:32:01.400 --> 1:32:03.360
 like universal rule not to interfere,

1:32:03.360 --> 1:32:04.600
 you know, Star Trek rule.

1:32:04.600 --> 1:32:07.360
 But like, look, we can't even coordinate humans

1:32:07.360 --> 1:32:10.040
 to deal with climate change and we're one species.

1:32:10.040 --> 1:32:12.400
 What is the chance that of all of these different

1:32:12.400 --> 1:32:14.800
 human civilization, you know, alien civilizations,

1:32:14.800 --> 1:32:16.760
 they would have the same priorities

1:32:16.760 --> 1:32:20.200
 and agree across these kinds of matters.

1:32:20.200 --> 1:32:21.840
 And even if that was true

1:32:21.840 --> 1:32:25.040
 and we were in some sort of safari for our own good,

1:32:25.040 --> 1:32:26.360
 to me, that's not much different

1:32:26.360 --> 1:32:27.640
 from the simulation hypothesis

1:32:27.640 --> 1:32:29.880
 because what does it mean, the simulation hypothesis?

1:32:29.880 --> 1:32:31.360
 I think in its most fundamental level,

1:32:31.360 --> 1:32:34.960
 it means what we're seeing is not quite reality, right?

1:32:34.960 --> 1:32:37.760
 It's something, there's something more deeper underlying it,

1:32:37.760 --> 1:32:39.120
 maybe computational.

1:32:39.120 --> 1:32:42.600
 Now, if we were in a sort of safari park

1:32:42.600 --> 1:32:44.440
 and everything we were seeing was a hologram

1:32:44.440 --> 1:32:46.520
 and it was projected by the aliens or whatever,

1:32:46.520 --> 1:32:47.840
 that to me is not much different

1:32:47.840 --> 1:32:50.280
 than thinking we're inside of another universe

1:32:50.280 --> 1:32:53.160
 because we still can't see true reality, right?

1:32:53.160 --> 1:32:55.120
 I mean, there's other explanations.

1:32:55.120 --> 1:32:58.000
 It could be that the way they're communicating

1:32:58.000 --> 1:32:59.280
 is just fundamentally different,

1:32:59.280 --> 1:33:02.440
 that we're too dumb to understand the much better methods

1:33:02.440 --> 1:33:03.840
 of communication they have.

1:33:03.840 --> 1:33:06.600
 It could be, I mean, it's silly to say,

1:33:06.600 --> 1:33:09.960
 but our own thoughts could be the methods

1:33:09.960 --> 1:33:11.200
 by which they're communicating.

1:33:11.200 --> 1:33:13.240
 Like the place from which our ideas,

1:33:13.240 --> 1:33:15.160
 writers talk about this, like the muse.

1:33:15.160 --> 1:33:16.000
 Yeah.

1:33:17.120 --> 1:33:20.880
 I mean, it sounds like very kind of wild,

1:33:20.880 --> 1:33:22.160
 but it could be thoughts.

1:33:22.160 --> 1:33:24.600
 It could be some interactions with our mind

1:33:24.600 --> 1:33:27.840
 that we think are originating from us

1:33:27.840 --> 1:33:31.440
 is actually something that is coming

1:33:31.440 --> 1:33:33.040
 from other life forms elsewhere.

1:33:33.040 --> 1:33:34.880
 Consciousness itself might be that.

1:33:34.880 --> 1:33:37.360
 It could be, but I don't see any sensible argument

1:33:37.360 --> 1:33:40.560
 to the why would all of the alien species

1:33:40.560 --> 1:33:41.600
 behave in this way?

1:33:41.600 --> 1:33:43.200
 Yeah, some of them will be more primitive.

1:33:43.200 --> 1:33:44.920
 They will be close to our level.

1:33:44.920 --> 1:33:47.760
 There should be a whole sort of normal distribution

1:33:47.760 --> 1:33:48.680
 of these things, right?

1:33:48.680 --> 1:33:49.640
 Some would be aggressive.

1:33:49.640 --> 1:33:52.120
 Some would be curious.

1:33:52.120 --> 1:33:55.560
 Others would be very historical and philosophical

1:33:55.560 --> 1:33:58.080
 because maybe they're a million years older than us,

1:33:58.080 --> 1:34:00.160
 but it's not, it shouldn't be like,

1:34:00.160 --> 1:34:03.000
 I mean, one alien civilization might be like that,

1:34:03.000 --> 1:34:04.200
 communicating thoughts and others,

1:34:04.200 --> 1:34:07.720
 but I don't see why potentially the hundreds there should be

1:34:07.720 --> 1:34:10.040
 would be uniform in this way, right?

1:34:10.040 --> 1:34:13.040
 It could be a violent dictatorship that the people,

1:34:13.040 --> 1:34:17.000
 the alien civilizations that become successful

1:34:20.560 --> 1:34:23.080
 gain the ability to be destructive,

1:34:23.080 --> 1:34:26.000
 an order of magnitude more destructive,

1:34:26.000 --> 1:34:28.680
 but of course the sad thought,

1:34:29.880 --> 1:34:32.640
 well, either humans are very special.

1:34:32.640 --> 1:34:35.480
 We took a lot of leaps that arrived

1:34:35.480 --> 1:34:36.880
 at what it means to be human.

1:34:38.600 --> 1:34:41.160
 There's a question there, which was the hardest,

1:34:41.160 --> 1:34:42.720
 which was the most special,

1:34:42.720 --> 1:34:45.200
 but also if others have reached this level

1:34:45.200 --> 1:34:47.680
 and maybe many others have reached this level,

1:34:47.680 --> 1:34:52.680
 the great filter that prevented them from going farther

1:34:52.680 --> 1:34:54.800
 to becoming a multi planetary species

1:34:54.800 --> 1:34:57.520
 or reaching out into the stars.

1:34:57.520 --> 1:34:59.960
 And those are really important questions for us,

1:34:59.960 --> 1:35:04.720
 whether there's other alien civilizations out there or not,

1:35:04.720 --> 1:35:06.960
 this is very useful for us to think about.

1:35:06.960 --> 1:35:10.240
 If we destroy ourselves, how will we do it?

1:35:10.240 --> 1:35:11.960
 And how easy is it to do?

1:35:11.960 --> 1:35:14.160
 Yeah, well, these are big questions

1:35:14.160 --> 1:35:15.320
 and I've thought about these a lot,

1:35:15.320 --> 1:35:19.600
 but the interesting thing is that if we're alone,

1:35:19.600 --> 1:35:22.000
 that's somewhat comforting from the great filter perspective

1:35:22.000 --> 1:35:25.240
 because it probably means the great filters were passed us.

1:35:25.240 --> 1:35:26.240
 And I'm pretty sure they are.

1:35:26.240 --> 1:35:29.040
 So going back to your origin of life question,

1:35:29.040 --> 1:35:30.560
 there are some incredible things

1:35:30.560 --> 1:35:31.760
 that no one knows how happened,

1:35:31.760 --> 1:35:35.240
 like obviously the first life form from chemical soup,

1:35:35.240 --> 1:35:36.800
 that seems pretty hard,

1:35:36.800 --> 1:35:38.720
 but I would guess the multicellular,

1:35:38.720 --> 1:35:42.120
 I wouldn't be that surprised if we saw single cell

1:35:42.120 --> 1:35:45.440
 sort of life forms elsewhere, bacteria type things,

1:35:45.440 --> 1:35:48.000
 but multicellular life seems incredibly hard,

1:35:48.000 --> 1:35:50.200
 that step of capturing mitochondria

1:35:50.200 --> 1:35:53.120
 and then sort of using that as part of yourself,

1:35:53.120 --> 1:35:53.960
 you know, when you've just eaten it.

1:35:53.960 --> 1:35:57.560
 Would you say that's the biggest, the most,

1:35:57.560 --> 1:36:01.400
 like if you had to choose one sort of,

1:36:01.400 --> 1:36:04.400
 Hitchhiker's Galaxy, one sentence summary of like,

1:36:04.400 --> 1:36:07.280
 oh, those clever creatures did this,

1:36:07.280 --> 1:36:08.280
 that would be the multicellular.

1:36:08.280 --> 1:36:10.600
 I think that was probably the one that's the biggest.

1:36:10.600 --> 1:36:11.440
 I mean, there's a great book

1:36:11.440 --> 1:36:14.760
 called The 10 Great Inventions of Evolution by Nick Lane,

1:36:14.760 --> 1:36:17.440
 and he speculates on 10 of these, you know,

1:36:17.440 --> 1:36:19.800
 what could be great filters.

1:36:19.800 --> 1:36:21.000
 I think that's one.

1:36:21.000 --> 1:36:23.880
 I think the advent of intelligence

1:36:23.880 --> 1:36:26.360
 and conscious intelligence and in order, you know,

1:36:26.360 --> 1:36:28.600
 to us to be able to do science and things like that

1:36:28.600 --> 1:36:29.880
 is huge as well.

1:36:29.880 --> 1:36:32.840
 I mean, it's only evolved once as far as, you know,

1:36:32.840 --> 1:36:34.880
 in Earth history.

1:36:34.880 --> 1:36:37.160
 So that would be a later candidate,

1:36:37.160 --> 1:36:39.160
 but there's certainly for the early candidates,

1:36:39.160 --> 1:36:41.440
 I think multicellular life forms is huge.

1:36:41.440 --> 1:36:43.560
 By the way, what it's interesting to ask you,

1:36:43.560 --> 1:36:45.760
 if you can hypothesize about

1:36:45.760 --> 1:36:48.000
 what is the origin of intelligence?

1:36:48.000 --> 1:36:53.000
 Is it that we started cooking meat over fire?

1:36:53.640 --> 1:36:55.520
 Is it that we somehow figured out

1:36:55.520 --> 1:36:58.120
 that we could be very powerful when we started collaborating?

1:36:58.120 --> 1:37:03.120
 So cooperation between our ancestors

1:37:03.560 --> 1:37:05.920
 so that we can overthrow the alpha male.

1:37:07.040 --> 1:37:07.880
 What is it, Richard?

1:37:07.880 --> 1:37:08.920
 I talked to Richard Ranham,

1:37:08.920 --> 1:37:10.760
 who thinks we're all just beta males

1:37:10.760 --> 1:37:13.840
 who figured out how to collaborate to defeat the one,

1:37:13.840 --> 1:37:16.360
 the dictator, the authoritarian alpha male

1:37:16.360 --> 1:37:18.360
 that controlled the tribe.

1:37:18.360 --> 1:37:20.120
 Is there other explanation?

1:37:20.120 --> 1:37:24.080
 Was there 2001 Space Odyssey type of monolith

1:37:24.080 --> 1:37:25.280
 that came down to Earth?

1:37:25.280 --> 1:37:27.480
 Well, I think all of those things

1:37:27.480 --> 1:37:28.640
 you suggested are good candidates,

1:37:28.640 --> 1:37:30.680
 fire and cooking, right?

1:37:30.680 --> 1:37:35.520
 So that's clearly important for energy efficiency,

1:37:35.520 --> 1:37:39.600
 cooking our meat and then being able to be more efficient

1:37:39.600 --> 1:37:42.840
 about eating it and consuming the energy.

1:37:42.840 --> 1:37:45.720
 I think that's huge and then utilizing fire and tools

1:37:45.720 --> 1:37:48.640
 I think you're right about the tribal cooperation aspects

1:37:48.640 --> 1:37:51.040
 and probably language is part of that

1:37:51.040 --> 1:37:52.400
 because probably that's what allowed us

1:37:52.400 --> 1:37:53.680
 to outcompete Neanderthals

1:37:53.680 --> 1:37:56.160
 and perhaps less cooperative species.

1:37:56.160 --> 1:37:58.760
 So that may be the case.

1:37:58.760 --> 1:38:02.400
 Tool making, spears, axes, I think that let us,

1:38:02.400 --> 1:38:03.920
 I mean, I think it's pretty clear now

1:38:03.920 --> 1:38:05.080
 that humans were responsible

1:38:05.080 --> 1:38:07.840
 for a lot of the extinctions of megafauna,

1:38:07.840 --> 1:38:10.840
 especially in the Americas when humans arrived.

1:38:10.840 --> 1:38:14.440
 So you can imagine once you discover tool usage

1:38:14.440 --> 1:38:15.800
 how powerful that would have been

1:38:15.800 --> 1:38:17.520
 and how scary for animals.

1:38:17.520 --> 1:38:20.720
 So I think all of those could have been explanations for it.

1:38:20.720 --> 1:38:22.760
 The interesting thing is that it's a bit

1:38:22.760 --> 1:38:24.080
 like general intelligence too,

1:38:24.080 --> 1:38:28.040
 is it's very costly to begin with to have a brain

1:38:28.040 --> 1:38:29.520
 and especially a general purpose brain

1:38:29.520 --> 1:38:30.920
 rather than a special purpose one

1:38:30.920 --> 1:38:32.320
 because the amount of energy our brains use,

1:38:32.320 --> 1:38:34.400
 I think it's like 20% of the body's energy

1:38:34.400 --> 1:38:36.680
 and it's massive and even your thinking chest,

1:38:36.680 --> 1:38:39.000
 one of the funny things that we used to say

1:38:39.000 --> 1:38:41.560
 is it's as much as a racing driver uses

1:38:41.560 --> 1:38:43.560
 for a whole Formula One race,

1:38:43.560 --> 1:38:46.360
 just playing a game of serious high level chess,

1:38:46.360 --> 1:38:49.280
 which you wouldn't think just sitting there

1:38:49.280 --> 1:38:52.040
 because the brain's using so much energy.

1:38:52.040 --> 1:38:54.760
 So in order for an animal, an organism to justify that,

1:38:54.760 --> 1:38:57.840
 there has to be a huge payoff.

1:38:57.840 --> 1:39:00.280
 And the problem with half a brain

1:39:00.280 --> 1:39:05.280
 or half intelligence, say an IQs of like a monkey brain,

1:39:06.720 --> 1:39:10.240
 it's not clear you can justify that evolutionary

1:39:10.240 --> 1:39:12.440
 until you get to the human level brain.

1:39:12.440 --> 1:39:14.720
 And so, but how do you do that jump?

1:39:14.720 --> 1:39:15.560
 It's very difficult,

1:39:15.560 --> 1:39:17.120
 which is why I think it has only been done once

1:39:17.120 --> 1:39:19.800
 from the sort of specialized brains that you see in animals

1:39:19.800 --> 1:39:22.480
 to this sort of general purpose,

1:39:22.480 --> 1:39:25.040
 chewing powerful brains that humans have

1:39:26.200 --> 1:39:28.920
 and which allows us to invent the modern world.

1:39:29.800 --> 1:39:33.600
 And it takes a lot to cross that barrier.

1:39:33.600 --> 1:39:35.600
 And I think we've seen the same with AI systems,

1:39:35.600 --> 1:39:38.160
 which is that maybe until very recently,

1:39:38.160 --> 1:39:40.880
 it's always been easier to craft a specific solution

1:39:40.880 --> 1:39:43.040
 to a problem like chess than it has been

1:39:43.040 --> 1:39:44.480
 to build a general learning system

1:39:44.480 --> 1:39:46.280
 that could potentially do many things.

1:39:46.280 --> 1:39:49.480
 Cause initially that system will be way worse

1:39:49.480 --> 1:39:52.120
 than less efficient than the specialized system.

1:39:52.120 --> 1:39:55.880
 So one of the interesting quirks of the human mind

1:39:55.880 --> 1:40:00.880
 of this evolved system is that it appears to be conscious.

1:40:01.320 --> 1:40:02.920
 This thing that we don't quite understand,

1:40:02.920 --> 1:40:07.360
 but it seems very special is ability

1:40:07.360 --> 1:40:08.760
 to have a subjective experience

1:40:08.760 --> 1:40:12.280
 that it feels like something to eat a cookie,

1:40:12.280 --> 1:40:14.320
 the deliciousness of it or see a color

1:40:14.320 --> 1:40:15.560
 and that kind of stuff.

1:40:15.560 --> 1:40:17.960
 Do you think in order to solve intelligence,

1:40:17.960 --> 1:40:20.680
 we also need to solve consciousness along the way?

1:40:20.680 --> 1:40:23.920
 Do you think AGI systems need to have consciousness

1:40:23.920 --> 1:40:28.000
 in order to be truly intelligent?

1:40:28.000 --> 1:40:29.640
 Yeah, we thought about this a lot actually.

1:40:29.640 --> 1:40:33.440
 And I think that my guess is that consciousness

1:40:33.440 --> 1:40:35.800
 and intelligence are double dissociable.

1:40:35.800 --> 1:40:38.360
 So you can have one without the other both ways.

1:40:38.360 --> 1:40:40.920
 And I think you can see that with consciousness

1:40:40.920 --> 1:40:44.160
 in that I think some animals and pets,

1:40:44.160 --> 1:40:46.240
 if you have a pet dog or something like that,

1:40:46.240 --> 1:40:48.560
 you can see some of the higher animals and dolphins,

1:40:48.560 --> 1:40:51.680
 things like that have self awareness

1:40:51.680 --> 1:40:55.720
 and are very sociable, seem to dream.

1:40:57.360 --> 1:40:59.000
 A lot of the traits one would regard

1:40:59.000 --> 1:41:01.600
 as being kind of conscious and self aware,

1:41:02.800 --> 1:41:05.080
 but yet they're not that smart, right?

1:41:05.080 --> 1:41:06.320
 So they're not that intelligent

1:41:06.320 --> 1:41:08.920
 by say IQ standards or something like that.

1:41:08.920 --> 1:41:11.080
 Yeah, it's also possible that our understanding

1:41:11.080 --> 1:41:14.920
 of intelligence is flawed, like putting an IQ to it.

1:41:14.920 --> 1:41:17.360
 Maybe the thing that a dog can do

1:41:17.360 --> 1:41:20.640
 is actually gone very far along the path of intelligence

1:41:20.640 --> 1:41:23.240
 and we humans are just able to play chess

1:41:23.240 --> 1:41:24.840
 and maybe write poems.

1:41:24.840 --> 1:41:27.040
 Right, but if we go back to the idea of AGI

1:41:27.040 --> 1:41:29.480
 and general intelligence, dogs are very specialized, right?

1:41:29.480 --> 1:41:30.920
 Most animals are pretty specialized.

1:41:30.920 --> 1:41:32.360
 They can be amazing at what they do,

1:41:32.360 --> 1:41:35.600
 but they're like kind of elite sports people or something,

1:41:35.600 --> 1:41:38.040
 right, so they do one thing extremely well

1:41:38.040 --> 1:41:40.080
 because their entire brain is optimized.

1:41:40.080 --> 1:41:41.880
 They have somehow convinced the entirety

1:41:41.880 --> 1:41:44.520
 of the human population to feed them and service them.

1:41:44.520 --> 1:41:46.400
 So in some way they're controlling.

1:41:46.400 --> 1:41:47.240
 Yes, exactly.

1:41:47.240 --> 1:41:50.120
 Well, we co evolved to some crazy degree, right?

1:41:50.120 --> 1:41:53.800
 Including the way the dogs even wag their tails

1:41:53.800 --> 1:41:55.160
 and twitch their noses, right?

1:41:55.160 --> 1:41:57.480
 We find inextricably cute.

1:41:58.640 --> 1:42:01.840
 But I think you can also see intelligence on the other side.

1:42:01.840 --> 1:42:03.800
 So systems like artificial systems

1:42:03.800 --> 1:42:07.240
 that are amazingly smart at certain things

1:42:07.240 --> 1:42:09.800
 like maybe playing go and chess and other things,

1:42:09.800 --> 1:42:13.440
 but they don't feel at all in any shape or form conscious

1:42:13.440 --> 1:42:17.240
 in the way that you do to me or I do to you.

1:42:17.240 --> 1:42:21.440
 And I think actually building AI

1:42:21.440 --> 1:42:24.200
 is these intelligent constructs

1:42:24.200 --> 1:42:25.920
 is one of the best ways to explore

1:42:25.920 --> 1:42:28.000
 the mystery of consciousness, to break it down

1:42:28.000 --> 1:42:31.200
 because we're gonna have devices

1:42:31.200 --> 1:42:34.440
 that are pretty smart at certain things

1:42:34.440 --> 1:42:36.200
 or capable at certain things,

1:42:36.200 --> 1:42:39.160
 but potentially won't have any semblance

1:42:39.160 --> 1:42:40.800
 of self awareness or other things.

1:42:40.800 --> 1:42:43.880
 And in fact, I would advocate if there's a choice,

1:42:43.880 --> 1:42:45.680
 building systems in the first place,

1:42:45.680 --> 1:42:48.640
 AI systems that are not conscious to begin with

1:42:48.640 --> 1:42:52.440
 are just tools until we understand them better

1:42:52.440 --> 1:42:53.960
 and the capabilities better.

1:42:53.960 --> 1:42:58.320
 So on that topic, just not as the CEO of DeepMind,

1:42:58.320 --> 1:43:00.880
 just as a human being, let me ask you

1:43:00.880 --> 1:43:03.480
 about this one particular anecdotal evidence

1:43:03.480 --> 1:43:07.080
 of the Google engineer who made a comment

1:43:07.080 --> 1:43:11.800
 or believed that there's some aspect of a language model,

1:43:11.800 --> 1:43:15.960
 the Lambda language model that exhibited sentience.

1:43:15.960 --> 1:43:18.440
 So you said you believe there might be a responsibility

1:43:18.440 --> 1:43:21.120
 to build systems that are not sentient.

1:43:21.120 --> 1:43:23.560
 And this experience of a particular engineer,

1:43:23.560 --> 1:43:25.880
 I think I'd love to get your general opinion

1:43:25.880 --> 1:43:28.000
 on this kind of thing, but I think it will happen

1:43:28.000 --> 1:43:31.480
 more and more and more, which not when engineers,

1:43:31.480 --> 1:43:33.120
 but when people out there that don't have

1:43:33.120 --> 1:43:34.760
 an engineering background start interacting

1:43:34.760 --> 1:43:37.120
 with increasingly intelligent systems,

1:43:37.120 --> 1:43:38.960
 we anthropomorphize them.

1:43:38.960 --> 1:43:43.960
 They start to have deep, impactful interactions with us

1:43:44.680 --> 1:43:47.920
 in a way that we miss them when they're gone.

1:43:47.920 --> 1:43:51.960
 And we sure as heck feel like they're living entities,

1:43:51.960 --> 1:43:54.200
 self aware entities, and maybe even

1:43:54.200 --> 1:43:55.960
 we project sentience onto them.

1:43:55.960 --> 1:43:59.960
 So what's your thought about this particular system?

1:44:01.320 --> 1:44:04.600
 Have you ever met a language model that's sentient?

1:44:04.600 --> 1:44:06.320
 No, no.

1:44:06.320 --> 1:44:10.200
 What do you make of the case of when you kind of feel

1:44:10.200 --> 1:44:12.920
 that there's some elements of sentience to the system?

1:44:12.920 --> 1:44:15.040
 Yeah, so this is an interesting question

1:44:15.040 --> 1:44:17.760
 and obviously a very fundamental one.

1:44:17.760 --> 1:44:20.280
 So the first thing to say is I think that none

1:44:20.280 --> 1:44:22.200
 of the systems we have today, I would say,

1:44:22.200 --> 1:44:25.080
 even have one iota of semblance

1:44:25.080 --> 1:44:26.320
 of consciousness or sentience.

1:44:26.320 --> 1:44:29.720
 That's my personal feeling interacting with them every day.

1:44:29.720 --> 1:44:32.400
 So I think this way premature to be discussing

1:44:32.400 --> 1:44:34.160
 what that engineer talked about.

1:44:34.160 --> 1:44:36.480
 I think at the moment it's more of a projection

1:44:36.480 --> 1:44:37.840
 of the way our own minds work,

1:44:37.840 --> 1:44:42.840
 which is to see sort of purpose and direction

1:44:43.120 --> 1:44:44.600
 in almost anything that we, you know,

1:44:44.600 --> 1:44:48.200
 our brains are trained to interpret agency,

1:44:48.200 --> 1:44:52.280
 basically in things, even inanimate things sometimes.

1:44:52.280 --> 1:44:54.880
 And of course with a language system,

1:44:54.880 --> 1:44:57.080
 because language is so fundamental to intelligence,

1:44:57.080 --> 1:45:00.440
 that's going to be easy for us to anthropomorphize that.

1:45:00.440 --> 1:45:03.840
 I mean, back in the day, even the first, you know,

1:45:03.840 --> 1:45:05.800
 the dumbest sort of template chatbots ever,

1:45:05.800 --> 1:45:09.200
 Eliza and the ilk of the original chatbots

1:45:09.200 --> 1:45:11.160
 back in the sixties fooled some people

1:45:11.160 --> 1:45:12.600
 under certain circumstances, right?

1:45:12.600 --> 1:45:14.040
 It pretended to be a psychologist.

1:45:14.040 --> 1:45:16.080
 So just basically rabbit back to you

1:45:16.080 --> 1:45:18.120
 the same question you asked it back to you.

1:45:19.240 --> 1:45:21.320
 And some people believe that.

1:45:21.320 --> 1:45:23.280
 So I don't think we can, this is why I think

1:45:23.280 --> 1:45:25.440
 the Turing test is a little bit flawed as a formal test

1:45:25.440 --> 1:45:29.240
 because it depends on the sophistication of the judge,

1:45:29.240 --> 1:45:33.280
 whether or not they are qualified to make that distinction.

1:45:33.280 --> 1:45:36.800
 So I think we should talk to, you know,

1:45:36.800 --> 1:45:38.320
 the top philosophers about this,

1:45:38.320 --> 1:45:41.160
 people like Daniel Dennett and David Chalmers and others

1:45:41.160 --> 1:45:43.680
 who've obviously thought deeply about consciousness.

1:45:43.680 --> 1:45:46.040
 Of course, consciousness itself hasn't been well,

1:45:46.040 --> 1:45:47.760
 there's no agreed definition.

1:45:47.760 --> 1:45:52.160
 If I was to, you know, speculate about that, you know,

1:45:52.160 --> 1:45:55.120
 I kind of, the working definition I like is

1:45:55.120 --> 1:45:58.080
 it's the way information feels when it gets processed.

1:45:58.080 --> 1:46:00.160
 I think maybe Max Tegmark came up with that.

1:46:00.160 --> 1:46:01.040
 I like that idea.

1:46:01.040 --> 1:46:02.280
 I don't know if it helps us get towards

1:46:02.280 --> 1:46:03.920
 any more operational thing,

1:46:03.920 --> 1:46:07.800
 but I think it's a nice way of viewing it.

1:46:07.800 --> 1:46:10.000
 I think we can obviously see from neuroscience

1:46:10.000 --> 1:46:11.720
 certain prerequisites that are required,

1:46:11.720 --> 1:46:14.440
 like self awareness, I think is necessary,

1:46:14.440 --> 1:46:16.080
 but not sufficient component.

1:46:16.080 --> 1:46:18.160
 This idea of a self and other

1:46:18.160 --> 1:46:20.520
 and set of coherent preferences

1:46:20.520 --> 1:46:22.480
 that are coherent over time.

1:46:22.480 --> 1:46:24.800
 You know, these things are maybe memory.

1:46:24.800 --> 1:46:26.200
 These things are probably needed

1:46:26.200 --> 1:46:29.320
 for a sentient or conscious being.

1:46:29.320 --> 1:46:31.160
 But the reason, the difficult thing,

1:46:31.160 --> 1:46:32.240
 I think for us when we get,

1:46:32.240 --> 1:46:33.400
 and I think this is a really interesting

1:46:33.400 --> 1:46:37.280
 philosophical debate is when we get closer to AGI

1:46:37.280 --> 1:46:40.680
 and, you know, and much more powerful systems

1:46:40.680 --> 1:46:42.240
 than we have today,

1:46:42.240 --> 1:46:44.440
 how are we going to make this judgment?

1:46:44.440 --> 1:46:46.960
 And one way, which is the Turing test

1:46:46.960 --> 1:46:48.640
 is sort of a behavioral judgment,

1:46:48.640 --> 1:46:52.080
 is the system exhibiting all the behaviors

1:46:52.080 --> 1:46:56.880
 that a human sentient or a sentient being would exhibit?

1:46:56.880 --> 1:46:58.160
 Is it answering the right questions?

1:46:58.160 --> 1:46:59.160
 Is it saying the right things?

1:46:59.160 --> 1:47:01.960
 Is it indistinguishable from a human?

1:47:01.960 --> 1:47:03.360
 And so on.

1:47:03.360 --> 1:47:05.760
 But I think there's a second thing

1:47:05.760 --> 1:47:09.040
 that makes us as humans regard each other as sentient,

1:47:09.040 --> 1:47:09.880
 right?

1:47:09.880 --> 1:47:10.920
 Why do we think this?

1:47:10.920 --> 1:47:12.720
 And I debated this with Daniel Dennett.

1:47:12.720 --> 1:47:13.880
 And I think there's a second reason

1:47:13.880 --> 1:47:15.600
 that's often overlooked,

1:47:15.600 --> 1:47:18.280
 which is that we're running on the same substrate, right?

1:47:18.280 --> 1:47:21.120
 So if we're exhibiting the same behavior,

1:47:21.120 --> 1:47:22.680
 more or less as humans,

1:47:22.680 --> 1:47:24.400
 and we're running on the same, you know,

1:47:24.400 --> 1:47:26.200
 carbon based biological substrate,

1:47:26.200 --> 1:47:29.560
 the squishy, you know, few pounds of flesh in our skulls,

1:47:29.560 --> 1:47:32.800
 then the most parsimonious, I think, explanation

1:47:32.800 --> 1:47:35.520
 is that you're feeling the same thing as I'm feeling, right?

1:47:35.520 --> 1:47:37.840
 But we will never have that second part,

1:47:37.840 --> 1:47:41.200
 the substrate equivalence with a machine, right?

1:47:41.200 --> 1:47:43.880
 So we will have to only judge based on the behavior.

1:47:43.880 --> 1:47:45.920
 And I think the substrate equivalence

1:47:45.920 --> 1:47:48.200
 is a critical part of why we make assumptions

1:47:48.200 --> 1:47:49.080
 that we're conscious.

1:47:49.080 --> 1:47:51.680
 And in fact, even with animals, high level animals,

1:47:51.680 --> 1:47:52.680
 why we think they might be,

1:47:52.680 --> 1:47:54.160
 because they're exhibiting some of the behaviors

1:47:54.160 --> 1:47:55.880
 we would expect from a sentient animal.

1:47:55.880 --> 1:47:57.600
 And we know they're made of the same things,

1:47:57.600 --> 1:47:58.640
 biological neurons.

1:47:58.640 --> 1:48:02.880
 So we're gonna have to come up with explanations

1:48:02.880 --> 1:48:06.320
 or models of the gap between substrate differences,

1:48:06.320 --> 1:48:08.040
 between machines and humans

1:48:08.040 --> 1:48:10.840
 to get anywhere beyond the behavioral.

1:48:10.840 --> 1:48:12.920
 But to me, sort of the practical question

1:48:12.920 --> 1:48:16.040
 is very interesting and very important.

1:48:16.040 --> 1:48:18.640
 When you have millions, perhaps billions of people

1:48:18.640 --> 1:48:20.800
 believing that you have a sentient AI,

1:48:20.800 --> 1:48:23.000
 believing what that Google engineer believed,

1:48:24.040 --> 1:48:28.760
 which I just see as an obvious, very near term future thing,

1:48:28.760 --> 1:48:31.160
 certainly on the path to AGI,

1:48:31.160 --> 1:48:33.160
 how does that change the world?

1:48:33.160 --> 1:48:35.240
 What's the responsibility of the AI system

1:48:35.240 --> 1:48:37.000
 to help those millions of people?

1:48:38.160 --> 1:48:39.760
 And also what's the ethical thing?

1:48:39.760 --> 1:48:44.760
 Because you can make a lot of people happy

1:48:44.760 --> 1:48:48.040
 by creating a meaningful, deep experience

1:48:48.040 --> 1:48:52.800
 with a system that's faking it before it makes it.

1:48:52.800 --> 1:48:56.120
 And I don't, are we the right,

1:48:56.120 --> 1:48:59.720
 who is to say what's the right thing to do?

1:48:59.720 --> 1:49:01.920
 Should AI always be tools?

1:49:01.920 --> 1:49:05.880
 Why are we constraining AI to always be tools

1:49:05.880 --> 1:49:07.680
 as opposed to friends?

1:49:07.680 --> 1:49:11.800
 Yeah, I think, well, I mean, these are fantastic questions

1:49:11.800 --> 1:49:13.840
 and also critical ones.

1:49:13.840 --> 1:49:16.240
 And we've been thinking about this

1:49:16.240 --> 1:49:18.080
 since the start of DeepMind and before that,

1:49:18.080 --> 1:49:19.560
 because we plan for success

1:49:19.560 --> 1:49:24.640
 and however remote that looked like back in 2010.

1:49:24.640 --> 1:49:26.960
 And we've always had sort of these ethical considerations

1:49:26.960 --> 1:49:28.440
 as fundamental at DeepMind.

1:49:29.400 --> 1:49:32.000
 And my current thinking on the language models

1:49:32.000 --> 1:49:33.920
 and large models is they're not ready,

1:49:33.920 --> 1:49:36.480
 we don't understand them well enough yet.

1:49:36.480 --> 1:49:40.240
 And in terms of analysis tools and guard rails,

1:49:40.240 --> 1:49:42.080
 what they can and can't do and so on,

1:49:42.080 --> 1:49:45.440
 to deploy them at scale, because I think,

1:49:45.440 --> 1:49:46.840
 there are big, still ethical questions

1:49:46.840 --> 1:49:48.640
 like should an AI system always announce

1:49:48.640 --> 1:49:50.600
 that it is an AI system to begin with?

1:49:50.600 --> 1:49:51.520
 Probably yes.

1:49:52.800 --> 1:49:55.520
 What do you do about answering those philosophical questions

1:49:55.520 --> 1:49:58.800
 about the feelings people may have about AI systems,

1:49:58.800 --> 1:50:00.760
 perhaps incorrectly attributed?

1:50:00.760 --> 1:50:02.840
 So I think there's a whole bunch of research

1:50:02.840 --> 1:50:06.040
 that needs to be done first to responsibly,

1:50:06.040 --> 1:50:09.120
 before you can responsibly deploy these systems at scale.

1:50:09.120 --> 1:50:12.080
 That will be at least be my current position.

1:50:12.080 --> 1:50:15.080
 Over time, I'm very confident we'll have those tools

1:50:15.080 --> 1:50:20.080
 like interpretability questions and analysis questions.

1:50:20.680 --> 1:50:23.200
 And then with the ethical quandary,

1:50:23.200 --> 1:50:28.200
 I think there it's important to look beyond just science.

1:50:28.520 --> 1:50:31.440
 That's why I think philosophy, social sciences,

1:50:31.440 --> 1:50:34.440
 even theology, other things like that come into it,

1:50:34.440 --> 1:50:37.120
 where arts and humanities,

1:50:37.120 --> 1:50:40.320
 what does it mean to be human and the spirit of being human

1:50:40.320 --> 1:50:43.680
 and to enhance that and the human condition, right?

1:50:43.680 --> 1:50:45.080
 And allow us to experience things

1:50:45.080 --> 1:50:46.400
 we could never experience before

1:50:46.400 --> 1:50:49.080
 and improve the overall human condition

1:50:49.080 --> 1:50:51.640
 and humanity overall, get radical abundance,

1:50:51.640 --> 1:50:54.120
 solve many scientific problems, solve disease.

1:50:54.120 --> 1:50:56.560
 So this is the era I think, this is the amazing era

1:50:56.560 --> 1:50:59.480
 I think we're heading into if we do it right.

1:50:59.480 --> 1:51:00.800
 But we've got to be careful.

1:51:00.800 --> 1:51:02.680
 We've already seen with things like social media,

1:51:02.680 --> 1:51:05.920
 how dual use technologies can be misused by,

1:51:05.920 --> 1:51:10.920
 firstly, by bad actors or naive actors or crazy actors,

1:51:12.040 --> 1:51:14.120
 right, so there's that set of just the common

1:51:14.120 --> 1:51:18.000
 or garden misuse of existing dual use technology.

1:51:18.000 --> 1:51:20.960
 And then of course, there's an additional thing

1:51:20.960 --> 1:51:21.960
 that has to be overcome with AI

1:51:21.960 --> 1:51:24.480
 that eventually it may have its own agency.

1:51:24.480 --> 1:51:28.720
 So it could be good or bad in and of itself.

1:51:28.720 --> 1:51:31.480
 So I think these questions have to be approached

1:51:31.480 --> 1:51:35.360
 very carefully using the scientific method, I would say,

1:51:35.360 --> 1:51:38.680
 in terms of hypothesis generation, careful control testing,

1:51:38.680 --> 1:51:40.680
 not live A, B testing out in the world,

1:51:40.680 --> 1:51:44.400
 because with powerful technologies like AI,

1:51:44.400 --> 1:51:47.640
 if something goes wrong, it may cause a lot of harm

1:51:47.640 --> 1:51:49.120
 before you can fix it.

1:51:49.120 --> 1:51:52.000
 It's not like an imaging app or game app

1:51:52.000 --> 1:51:56.160
 where if something goes wrong, it's relatively easy to fix

1:51:56.160 --> 1:51:57.960
 and the harm is relatively small.

1:51:57.960 --> 1:52:02.720
 So I think it comes with the usual cliche of,

1:52:02.720 --> 1:52:05.240
 like with a lot of power comes a lot of responsibility.

1:52:05.240 --> 1:52:07.800
 And I think that's the case here with things like AI,

1:52:07.800 --> 1:52:11.040
 given the enormous opportunity in front of us.

1:52:11.040 --> 1:52:14.040
 And I think we need a lot of voices

1:52:14.040 --> 1:52:17.160
 and as many inputs into things like the design

1:52:17.160 --> 1:52:19.880
 of the systems and the values they should have

1:52:19.880 --> 1:52:22.400
 and what goals should they be put to.

1:52:22.400 --> 1:52:24.560
 I think as wide a group of voices as possible

1:52:24.560 --> 1:52:27.720
 beyond just the technologists is needed to input into that

1:52:27.720 --> 1:52:29.080
 and to have a say in that,

1:52:29.080 --> 1:52:31.840
 especially when it comes to deployment of these systems,

1:52:31.840 --> 1:52:33.440
 which is when the rubber really hits the road,

1:52:33.440 --> 1:52:35.440
 it really affects the general person in the street

1:52:35.440 --> 1:52:37.400
 rather than fundamental research.

1:52:37.400 --> 1:52:40.240
 And that's why I say, I think as a first step,

1:52:40.240 --> 1:52:42.360
 it would be better if we have the choice

1:52:42.360 --> 1:52:45.120
 to build these systems as tools to give,

1:52:45.120 --> 1:52:47.960
 and I'm not saying that they should never go beyond tools

1:52:47.960 --> 1:52:50.360
 because of course the potential is there

1:52:50.360 --> 1:52:52.960
 for it to go way beyond just tools.

1:52:52.960 --> 1:52:55.800
 But I think that would be a good first step

1:52:55.800 --> 1:52:58.880
 in order for us to allow us to carefully experiment

1:52:58.880 --> 1:53:01.000
 and understand what these things can do.

1:53:01.000 --> 1:53:05.800
 So the leap between tool, the sentient entity being

1:53:05.800 --> 1:53:08.280
 is one we should take very careful of.

1:53:08.280 --> 1:53:11.120
 Let me ask a dark personal question.

1:53:11.120 --> 1:53:13.480
 So you're one of the most brilliant people

1:53:13.480 --> 1:53:16.800
 in the AI community, you're also one of the most kind

1:53:16.800 --> 1:53:20.880
 and if I may say sort of loved people in the community.

1:53:20.880 --> 1:53:25.880
 That said, creation of a super intelligent AI system

1:53:25.880 --> 1:53:30.880
 would be one of the most powerful things in the world,

1:53:32.720 --> 1:53:34.840
 tools or otherwise.

1:53:34.840 --> 1:53:38.400
 And again, as the old saying goes, power corrupts

1:53:38.400 --> 1:53:40.680
 and absolute power corrupts absolutely.

1:53:41.640 --> 1:53:46.640
 You are likely to be one of the people,

1:53:47.280 --> 1:53:50.320
 I would say probably the most likely person

1:53:50.320 --> 1:53:53.280
 to be in the control of such a system.

1:53:53.280 --> 1:53:57.120
 Do you think about the corrupting nature of power

1:53:57.120 --> 1:53:59.560
 when you talk about these kinds of systems

1:53:59.560 --> 1:54:04.560
 that as all dictators and people have caused atrocities

1:54:04.920 --> 1:54:07.760
 in the past, always think they're doing good,

1:54:07.760 --> 1:54:10.400
 but they don't do good because the power

1:54:10.400 --> 1:54:12.560
 has polluted their mind about what is good

1:54:12.560 --> 1:54:13.720
 and what is evil.

1:54:13.720 --> 1:54:14.840
 Do you think about this stuff

1:54:14.840 --> 1:54:16.440
 or are we just focused on language model?

1:54:16.440 --> 1:54:18.760
 No, I think about them all the time

1:54:18.760 --> 1:54:22.360
 and I think what are the defenses against that?

1:54:22.360 --> 1:54:24.840
 I think one thing is to remain very grounded

1:54:24.840 --> 1:54:28.800
 and sort of humble, no matter what you do or achieve.

1:54:28.800 --> 1:54:31.160
 And I try to do that, my best friends

1:54:31.160 --> 1:54:32.200
 are still my set of friends

1:54:32.200 --> 1:54:34.680
 from my undergraduate Cambridge days,

1:54:34.680 --> 1:54:38.080
 my family's and friends are very important.

1:54:39.280 --> 1:54:42.360
 I've always, I think trying to be a multidisciplinary person,

1:54:42.360 --> 1:54:43.760
 it helps to keep you humble

1:54:43.760 --> 1:54:45.880
 because no matter how good you are at one topic,

1:54:45.880 --> 1:54:47.560
 someone will be better than you at that.

1:54:47.560 --> 1:54:50.920
 And always relearning a new topic again from scratch

1:54:50.920 --> 1:54:53.320
 is a new field is very humbling, right?

1:54:53.320 --> 1:54:56.400
 So for me, that's been biology over the last five years,

1:54:56.400 --> 1:55:00.200
 huge area topic and I just love doing that,

1:55:00.200 --> 1:55:01.600
 but it helps to keep you grounded

1:55:01.600 --> 1:55:03.120
 like it keeps you open minded.

1:55:04.320 --> 1:55:06.360
 And then the other important thing

1:55:06.360 --> 1:55:10.040
 is to have a really good, amazing set of people around you

1:55:10.040 --> 1:55:11.840
 at your company or your organization

1:55:11.840 --> 1:55:14.920
 who are also very ethical and grounded themselves

1:55:14.920 --> 1:55:16.840
 and help to keep you that way.

1:55:16.840 --> 1:55:18.880
 And then ultimately just to answer your question,

1:55:18.880 --> 1:55:22.000
 I hope we're gonna be a big part of birthing AI

1:55:22.000 --> 1:55:24.440
 and that being the greatest benefit to humanity

1:55:24.440 --> 1:55:26.800
 of any tool or technology ever,

1:55:26.800 --> 1:55:29.560
 and getting us into a world of radical abundance

1:55:29.560 --> 1:55:33.960
 and curing diseases and solving many of the big challenges

1:55:33.960 --> 1:55:34.840
 we have in front of us.

1:55:34.840 --> 1:55:37.560
 And then ultimately help the ultimate flourishing

1:55:37.560 --> 1:55:39.240
 of humanity to travel the stars

1:55:39.240 --> 1:55:41.160
 and find those aliens if they are there.

1:55:41.160 --> 1:55:43.480
 And if they're not there, find out why they're not there,

1:55:43.480 --> 1:55:45.560
 what is going on here in the universe.

1:55:46.520 --> 1:55:47.360
 This is all to come.

1:55:47.360 --> 1:55:49.440
 And that's what I've always dreamed about.

1:55:50.720 --> 1:55:53.000
 But I think AI is too big an idea.

1:55:53.000 --> 1:55:54.760
 It's not going to be,

1:55:54.760 --> 1:55:57.000
 there'll be a certain set of pioneers who get there first.

1:55:57.000 --> 1:55:58.600
 I hope we're in the vanguard

1:55:58.600 --> 1:56:00.400
 so we can influence how that goes.

1:56:00.400 --> 1:56:02.480
 And I think it matters who builds,

1:56:02.480 --> 1:56:06.480
 which cultures they come from and what values they have,

1:56:06.480 --> 1:56:07.840
 the builders of AI systems.

1:56:07.840 --> 1:56:09.280
 Cause I think even though the AI system

1:56:09.280 --> 1:56:11.560
 is gonna learn for itself most of its knowledge,

1:56:11.560 --> 1:56:14.760
 there'll be a residue in the system of the culture

1:56:14.760 --> 1:56:17.680
 and the values of the creators of that system.

1:56:17.680 --> 1:56:18.720
 And there's interesting questions

1:56:18.720 --> 1:56:21.600
 to discuss about that geopolitically.

1:56:21.600 --> 1:56:22.440
 Different cultures,

1:56:22.440 --> 1:56:24.920
 we're in a more fragmented world than ever, unfortunately.

1:56:24.920 --> 1:56:27.480
 I think in terms of global cooperation,

1:56:27.480 --> 1:56:29.240
 we see that in things like climate

1:56:29.240 --> 1:56:32.000
 where we can't seem to get our act together globally

1:56:32.000 --> 1:56:34.080
 to cooperate on these pressing matters.

1:56:34.080 --> 1:56:35.600
 I hope that will change over time.

1:56:35.600 --> 1:56:38.640
 Perhaps if we get to an era of radical abundance,

1:56:38.640 --> 1:56:40.440
 we don't have to be so competitive anymore.

1:56:40.440 --> 1:56:42.680
 Maybe we can be more cooperative

1:56:42.680 --> 1:56:44.360
 if resources aren't so scarce.

1:56:44.360 --> 1:56:48.240
 It's true that in terms of power corrupting

1:56:48.240 --> 1:56:50.040
 and leading to destructive things,

1:56:50.040 --> 1:56:53.160
 it seems that some of the atrocities of the past happen

1:56:53.160 --> 1:56:56.680
 when there's a significant constraint on resources.

1:56:56.680 --> 1:56:57.560
 I think that's the first thing.

1:56:57.560 --> 1:56:58.400
 I don't think that's enough.

1:56:58.400 --> 1:57:01.560
 I think scarcity is one thing that's led to competition,

1:57:01.560 --> 1:57:03.960
 sort of zero sum game thinking.

1:57:03.960 --> 1:57:06.080
 I would like us to all be in a positive sum world.

1:57:06.080 --> 1:57:08.480
 And I think for that, you have to remove scarcity.

1:57:08.480 --> 1:57:09.840
 I don't think that's enough, unfortunately,

1:57:09.840 --> 1:57:10.800
 to get world peace

1:57:10.800 --> 1:57:12.800
 because there's also other corrupting things

1:57:12.800 --> 1:57:15.520
 like wanting power over people and this kind of stuff,

1:57:15.520 --> 1:57:19.040
 which is not necessarily satisfied by just abundance.

1:57:19.040 --> 1:57:20.280
 But I think it will help.

1:57:22.400 --> 1:57:24.920
 But I think ultimately, AI is not gonna be run

1:57:24.920 --> 1:57:26.800
 by any one person or one organization.

1:57:26.800 --> 1:57:29.600
 I think it should belong to the world, belong to humanity.

1:57:29.600 --> 1:57:33.120
 And I think there'll be many ways this will happen.

1:57:33.120 --> 1:57:36.840
 And ultimately, everybody should have a say in that.

1:57:36.840 --> 1:57:41.840
 Do you have advice for young people in high school,

1:57:42.000 --> 1:57:45.800
 in college, maybe if they're interested in AI

1:57:45.800 --> 1:57:50.640
 or interested in having a big impact on the world,

1:57:50.640 --> 1:57:53.200
 what they should do to have a career they can be proud of

1:57:53.200 --> 1:57:55.000
 or to have a life they can be proud of?

1:57:55.000 --> 1:57:57.400
 I love giving talks to the next generation.

1:57:57.400 --> 1:57:59.120
 What I say to them is actually two things.

1:57:59.120 --> 1:58:02.160
 I think the most important things to learn about

1:58:02.160 --> 1:58:04.520
 and to find out about when you're young

1:58:04.520 --> 1:58:07.080
 is what are your true passions is first of all,

1:58:07.080 --> 1:58:07.920
 as two things.

1:58:07.920 --> 1:58:09.720
 One is find your true passions.

1:58:09.720 --> 1:58:11.720
 And I think you can do that by,

1:58:11.720 --> 1:58:14.600
 the way to do that is to explore as many things as possible

1:58:14.600 --> 1:58:16.520
 when you're young and you have the time

1:58:16.520 --> 1:58:19.160
 and you can take those risks.

1:58:19.160 --> 1:58:21.080
 I would also encourage people to look at

1:58:21.080 --> 1:58:24.600
 finding the connections between things in a unique way.

1:58:24.600 --> 1:58:27.280
 I think that's a really great way to find a passion.

1:58:27.280 --> 1:58:30.600
 Second thing I would say, advise is know yourself.

1:58:30.600 --> 1:58:33.920
 So spend a lot of time understanding

1:58:33.920 --> 1:58:35.600
 how you work best.

1:58:35.600 --> 1:58:37.680
 Like what are the optimal times to work?

1:58:37.680 --> 1:58:39.880
 What are the optimal ways that you study?

1:58:39.880 --> 1:58:42.240
 What are your, how do you deal with pressure?

1:58:42.240 --> 1:58:44.560
 Sort of test yourself in various scenarios

1:58:44.560 --> 1:58:47.240
 and try and improve your weaknesses,

1:58:47.240 --> 1:58:50.720
 but also find out what your unique skills and strengths are

1:58:50.720 --> 1:58:52.160
 and then hone those.

1:58:52.160 --> 1:58:54.520
 So then that's what will be your super value

1:58:54.520 --> 1:58:55.880
 in the world later on.

1:58:55.880 --> 1:58:57.840
 And if you can then combine those two things

1:58:57.840 --> 1:59:01.200
 and find passions that you're genuinely excited about

1:59:01.200 --> 1:59:05.360
 that intersect with what your unique strong skills are,

1:59:05.360 --> 1:59:07.880
 then you're onto something incredible

1:59:07.880 --> 1:59:10.920
 and I think you can make a huge difference in the world.

1:59:10.920 --> 1:59:12.760
 So let me ask about know yourself.

1:59:12.760 --> 1:59:13.600
 This is fun.

1:59:13.600 --> 1:59:14.440
 This is fun.

1:59:14.440 --> 1:59:18.120
 Quick questions about day in the life, the perfect day,

1:59:18.120 --> 1:59:21.200
 the perfect productive day in the life of Demis's Hub.

1:59:21.200 --> 1:59:26.200
 Maybe these days you're, there's a lot involved.

1:59:26.200 --> 1:59:29.000
 So maybe a slightly younger Demis's Hub

1:59:29.000 --> 1:59:31.400
 where you could focus on a single project maybe.

1:59:33.120 --> 1:59:34.440
 How early do you wake up?

1:59:34.440 --> 1:59:35.600
 Are you a night owl?

1:59:35.600 --> 1:59:36.760
 Do you wake up early in the morning?

1:59:36.760 --> 1:59:39.160
 What are some interesting habits?

1:59:39.160 --> 1:59:42.400
 How many dozens of cups of coffees do you drink a day?

1:59:42.400 --> 1:59:46.320
 What's the computer that you use?

1:59:46.320 --> 1:59:47.160
 What's the setup?

1:59:47.160 --> 1:59:47.980
 How many screens?

1:59:47.980 --> 1:59:49.120
 What kind of keyboard?

1:59:49.120 --> 1:59:51.400
 Are we talking Emacs Vim

1:59:51.400 --> 1:59:53.320
 or are we talking something more modern?

1:59:53.320 --> 1:59:54.480
 So there's a bunch of those questions.

1:59:54.480 --> 1:59:58.960
 So maybe day in the life, what's the perfect day involved?

1:59:58.960 --> 2:00:00.880
 Well, these days it's quite different

2:00:00.880 --> 2:00:02.680
 from say 10, 20 years ago.

2:00:02.680 --> 2:00:05.480
 Back 10, 20 years ago, it would have been

2:00:05.480 --> 2:00:10.480
 a whole day of research, individual research or programming,

2:00:10.900 --> 2:00:12.580
 doing some experiment, neuroscience,

2:00:12.580 --> 2:00:14.080
 computer science experiment,

2:00:14.080 --> 2:00:16.640
 reading lots of research papers.

2:00:16.640 --> 2:00:18.420
 And then perhaps at nighttime,

2:00:19.720 --> 2:00:24.720
 reading science fiction books or playing some games.

2:00:25.440 --> 2:00:28.360
 But lots of focus, so like deep focused work

2:00:28.360 --> 2:00:32.440
 on whether it's programming or reading research papers.

2:00:32.440 --> 2:00:35.300
 Yes, so that would be lots of deep focus work.

2:00:35.300 --> 2:00:39.560
 These days for the last sort of, I guess, five to 10 years,

2:00:39.560 --> 2:00:41.020
 I've actually got quite a structure

2:00:41.020 --> 2:00:42.360
 that works very well for me now,

2:00:42.360 --> 2:00:46.140
 which is that I'm a complete night owl, always have been.

2:00:46.140 --> 2:00:47.680
 So I optimize for that.

2:00:47.680 --> 2:00:50.760
 So I'll basically do a normal day's work,

2:00:50.760 --> 2:00:52.560
 get into work about 11 o clock

2:00:52.560 --> 2:00:56.400
 and sort of do work to about seven in the office.

2:00:56.400 --> 2:00:58.960
 And I will arrange back to back meetings

2:00:58.960 --> 2:01:00.920
 for the entire time of that.

2:01:00.920 --> 2:01:03.200
 And with as many, meet as many people as possible.

2:01:03.200 --> 2:01:06.500
 So that's my collaboration management part of the day.

2:01:06.500 --> 2:01:10.680
 Then I go home, spend time with the family and friends,

2:01:10.680 --> 2:01:13.600
 have dinner, relax a little bit.

2:01:13.600 --> 2:01:15.240
 And then I start a second day of work.

2:01:15.240 --> 2:01:18.500
 I call it my second day of work around 10 p.m., 11 p.m.

2:01:18.500 --> 2:01:21.260
 And that's the time to about the small hours of the morning,

2:01:21.260 --> 2:01:24.760
 four or five in the morning, where I will do my thinking

2:01:24.760 --> 2:01:28.060
 and reading and research, writing research papers.

2:01:29.000 --> 2:01:30.960
 Sadly, I don't have time to code anymore,

2:01:30.960 --> 2:01:34.900
 but it's not efficient to do that these days,

2:01:34.900 --> 2:01:37.120
 given the amount of time I have.

2:01:37.120 --> 2:01:38.360
 But that's when I do, you know,

2:01:38.360 --> 2:01:40.760
 maybe do the long kind of stretches

2:01:40.760 --> 2:01:42.440
 of thinking and planning.

2:01:42.440 --> 2:01:45.280
 And then probably, you know, using email, other things,

2:01:45.280 --> 2:01:47.880
 I would set, I would fire off a lot of things to my team

2:01:47.880 --> 2:01:49.360
 to deal with the next morning.

2:01:49.360 --> 2:01:51.640
 But actually thinking about this overnight,

2:01:51.640 --> 2:01:53.200
 we should go for this project

2:01:53.200 --> 2:01:54.880
 or arrange this meeting the next day.

2:01:54.880 --> 2:01:56.120
 When you're thinking through a problem,

2:01:56.120 --> 2:01:58.160
 are you talking about a sheet of paper with a pen?

2:01:58.160 --> 2:02:01.040
 Is there some structured process?

2:02:01.040 --> 2:02:04.360
 I still like pencil and paper best for working out things,

2:02:04.360 --> 2:02:06.720
 but these days it's just so efficient

2:02:06.720 --> 2:02:08.720
 to read research papers just on the screen.

2:02:08.720 --> 2:02:10.220
 I still often print them out, actually.

2:02:10.220 --> 2:02:12.540
 I still prefer to mark out things.

2:02:12.540 --> 2:02:14.880
 And I find it goes into the brain better

2:02:14.880 --> 2:02:16.000
 and sticks in the brain better

2:02:16.000 --> 2:02:19.440
 when you're still using physical pen and pencil and paper.

2:02:19.440 --> 2:02:20.800
 So you take notes with the...

2:02:20.800 --> 2:02:22.440
 I have lots of notes, electronic ones,

2:02:22.440 --> 2:02:27.440
 and also whole stacks of notebooks that I use at home, yeah.

2:02:27.640 --> 2:02:30.320
 On some of these most challenging next steps, for example,

2:02:30.320 --> 2:02:33.800
 stuff none of us know about that you're working on,

2:02:33.800 --> 2:02:35.580
 you're thinking,

2:02:35.580 --> 2:02:37.640
 there's some deep thinking required there, right?

2:02:37.640 --> 2:02:39.420
 Like what is the right problem?

2:02:39.420 --> 2:02:41.280
 What is the right approach?

2:02:41.280 --> 2:02:43.920
 Because you're gonna have to invest a huge amount of time

2:02:43.920 --> 2:02:44.800
 for the whole team.

2:02:44.800 --> 2:02:46.760
 They're going to have to pursue this thing.

2:02:46.760 --> 2:02:48.560
 What's the right way to do it?

2:02:48.560 --> 2:02:50.040
 Is RL gonna work here or not?

2:02:50.040 --> 2:02:50.880
 Yes.

2:02:50.880 --> 2:02:53.120
 What's the right thing to try?

2:02:53.120 --> 2:02:55.120
 What's the right benchmark to use?

2:02:55.120 --> 2:02:57.320
 Do we need to construct a benchmark from scratch?

2:02:57.320 --> 2:02:58.200
 All those kinds of things.

2:02:58.200 --> 2:02:59.040
 Yes.

2:02:59.040 --> 2:03:00.200
 So I think of all those kinds of things

2:03:00.200 --> 2:03:03.480
 in the nighttime phase, but also much more,

2:03:03.480 --> 2:03:07.660
 I find I've always found the quiet hours of the morning

2:03:07.660 --> 2:03:11.420
 when everyone's asleep, it's super quiet outside.

2:03:11.420 --> 2:03:12.280
 I love that time.

2:03:12.280 --> 2:03:13.360
 It's the golden hours,

2:03:13.360 --> 2:03:16.480
 like between one and three in the morning.

2:03:16.480 --> 2:03:18.880
 Put some music on, some inspiring music on,

2:03:18.880 --> 2:03:21.600
 and then think these deep thoughts.

2:03:21.600 --> 2:03:24.240
 So that's when I would read my philosophy books

2:03:24.240 --> 2:03:28.820
 and Spinoza's, my recent favorite can, all these things.

2:03:28.820 --> 2:03:33.660
 And I read about a great scientist of history,

2:03:33.660 --> 2:03:35.640
 how they did things, how they thought things.

2:03:35.640 --> 2:03:37.240
 So that's when you do all your creative,

2:03:37.240 --> 2:03:39.160
 that's when I do all my creative thinking.

2:03:39.160 --> 2:03:41.840
 And it's good, I think people recommend

2:03:41.840 --> 2:03:45.120
 you do your sort of creative thinking in one block.

2:03:45.120 --> 2:03:47.160
 And the way I organize the day,

2:03:47.160 --> 2:03:48.560
 that way I don't get interrupted.

2:03:48.560 --> 2:03:51.460
 There's obviously no one else is up at those times.

2:03:51.460 --> 2:03:55.880
 So I can go, I can sort of get super deep

2:03:55.880 --> 2:03:57.560
 and super into flow.

2:03:57.560 --> 2:03:59.640
 The other nice thing about doing it nighttime wise

2:03:59.640 --> 2:04:02.760
 is if I'm really onto something

2:04:02.760 --> 2:04:04.940
 or I've got really deep into something,

2:04:04.940 --> 2:04:06.840
 I can choose to extend it

2:04:06.840 --> 2:04:09.000
 and I'll go into six in the morning, whatever.

2:04:09.000 --> 2:04:10.760
 And then I'll just pay for it the next day.

2:04:10.760 --> 2:04:12.960
 So I'll be a bit tired and I won't be my best,

2:04:12.960 --> 2:04:13.900
 but that's fine.

2:04:13.900 --> 2:04:16.840
 I can decide looking at my schedule the next day

2:04:16.840 --> 2:04:19.360
 and given where I'm at with this particular thought

2:04:19.360 --> 2:04:22.840
 or creative idea that I'm gonna pay that cost the next day.

2:04:22.840 --> 2:04:26.220
 So I think that's more flexible than morning people

2:04:26.220 --> 2:04:28.780
 who do that, they get up at four in the morning.

2:04:28.780 --> 2:04:31.000
 They can also do those golden hours then,

2:04:31.000 --> 2:04:32.640
 but then their start of their scheduled day

2:04:32.640 --> 2:04:34.440
 starts at breakfast, 8 a.m.,

2:04:34.440 --> 2:04:36.040
 whatever they have their first meeting.

2:04:36.040 --> 2:04:37.880
 And then it's hard, you have to reschedule a day

2:04:37.880 --> 2:04:38.960
 if you're in flow.

2:04:38.960 --> 2:04:39.800
 So I don't have to do that.

2:04:39.800 --> 2:04:41.880
 So that could be a true special thread of thoughts

2:04:41.880 --> 2:04:45.160
 that you're too passionate about.

2:04:45.160 --> 2:04:46.740
 This is where some of the greatest ideas

2:04:46.740 --> 2:04:49.320
 could potentially come is when you just lose yourself

2:04:49.320 --> 2:04:51.360
 late into the night.

2:04:51.360 --> 2:04:53.860
 And for the meetings, I mean, you're loading in

2:04:53.860 --> 2:04:56.520
 really hard problems in a very short amount of time.

2:04:56.520 --> 2:04:58.800
 So you have to do some kind of first principles thinking

2:04:58.800 --> 2:05:00.160
 here, it's like, what's the problem?

2:05:00.160 --> 2:05:01.360
 What's the state of things?

2:05:01.360 --> 2:05:03.120
 What's the right next steps?

2:05:03.120 --> 2:05:05.120
 You have to get really good at context switching,

2:05:05.120 --> 2:05:07.200
 which is one of the hardest things,

2:05:07.200 --> 2:05:09.020
 because especially as we do so many things,

2:05:09.020 --> 2:05:10.800
 if you include all the scientific things we do,

2:05:10.800 --> 2:05:12.600
 scientific fields we're working in,

2:05:12.600 --> 2:05:15.380
 these are complex fields in themselves.

2:05:15.380 --> 2:05:18.960
 And you have to sort of keep abreast of that.

2:05:18.960 --> 2:05:20.000
 But I enjoy it.

2:05:20.000 --> 2:05:23.840
 I've always been a sort of generalist in a way.

2:05:23.840 --> 2:05:25.600
 And that's actually what happened in my games career

2:05:25.600 --> 2:05:26.440
 after chess.

2:05:27.880 --> 2:05:29.260
 One of the reasons I stopped playing chess

2:05:29.260 --> 2:05:30.320
 was because I got into computers,

2:05:30.320 --> 2:05:32.280
 but also I started realizing there were many other

2:05:32.280 --> 2:05:33.880
 great games out there to play too.

2:05:33.880 --> 2:05:36.920
 So I've always been that way inclined, multidisciplinary.

2:05:36.920 --> 2:05:39.120
 And there's too many interesting things in the world

2:05:39.120 --> 2:05:41.680
 to spend all your time just on one thing.

2:05:41.680 --> 2:05:45.640
 So you mentioned Spinoza, gotta ask the big, ridiculously

2:05:45.640 --> 2:05:47.640
 big question about life.

2:05:47.640 --> 2:05:50.480
 What do you think is the meaning of this whole thing?

2:05:50.480 --> 2:05:52.560
 Why are we humans here?

2:05:52.560 --> 2:05:55.120
 You've already mentioned that perhaps the universe

2:05:55.120 --> 2:05:56.720
 created us.

2:05:56.720 --> 2:05:58.920
 Is that why you think we're here?

2:05:58.920 --> 2:06:00.120
 To understand how the universe works?

2:06:00.120 --> 2:06:02.080
 Yeah, I think my answer to that would be,

2:06:02.080 --> 2:06:03.960
 and at least the life I'm living,

2:06:03.960 --> 2:06:08.120
 is to gain and understand the knowledge,

2:06:08.120 --> 2:06:10.600
 to gain knowledge and understand the universe.

2:06:10.600 --> 2:06:13.560
 That's what I think, I can't see any higher purpose

2:06:13.560 --> 2:06:15.720
 than that if you think back to the classical Greeks,

2:06:15.720 --> 2:06:17.560
 the virtue of gaining knowledge.

2:06:17.560 --> 2:06:20.440
 It's, I think it's one of the few true virtues

2:06:20.440 --> 2:06:23.600
 is to understand the world around us

2:06:23.600 --> 2:06:25.680
 and the context and humanity better.

2:06:25.680 --> 2:06:29.140
 And I think if you do that, you become more compassionate

2:06:29.140 --> 2:06:32.080
 and more understanding yourself and more tolerant

2:06:32.080 --> 2:06:33.580
 and all these, I think all these other things

2:06:33.580 --> 2:06:34.760
 may flow from that.

2:06:34.760 --> 2:06:37.640
 And to me, understanding the nature of reality,

2:06:37.640 --> 2:06:38.760
 that is the biggest question.

2:06:38.760 --> 2:06:41.400
 What is going on here is sometimes the colloquial way I say.

2:06:41.400 --> 2:06:43.600
 What is really going on here?

2:06:43.600 --> 2:06:44.900
 It's so mysterious.

2:06:44.900 --> 2:06:47.040
 I feel like we're in some huge puzzle.

2:06:47.040 --> 2:06:49.960
 And it's, but the world is also seems to be,

2:06:49.960 --> 2:06:52.880
 the universe seems to be structured in a way.

2:06:52.880 --> 2:06:54.280
 You know, why is it structured in a way

2:06:54.280 --> 2:06:55.840
 that science is even possible?

2:06:55.840 --> 2:06:58.160
 That, you know, methods, the scientific method works,

2:06:58.160 --> 2:06:59.280
 things are repeatable.

2:07:00.240 --> 2:07:02.560
 It feels like it's almost structured in a way

2:07:02.560 --> 2:07:05.000
 to be conducive to gaining knowledge.

2:07:05.000 --> 2:07:06.480
 So I feel like, and you know,

2:07:06.480 --> 2:07:07.960
 why should computers be even possible?

2:07:07.960 --> 2:07:11.880
 Wasn't that amazing that computational electronic devices

2:07:11.880 --> 2:07:15.300
 can be possible, and they're made of sand,

2:07:15.300 --> 2:07:17.280
 our most common element that we have,

2:07:17.280 --> 2:07:19.960
 you know, silicon on the Earth's crust.

2:07:19.960 --> 2:07:21.480
 It could have been made of diamond or something,

2:07:21.480 --> 2:07:23.800
 then we would have only had one computer.

2:07:23.800 --> 2:07:26.560
 So a lot of things are kind of slightly suspicious to me.

2:07:26.560 --> 2:07:29.220
 It sure as heck sounds, this puzzle sure as heck sounds

2:07:29.220 --> 2:07:30.760
 like something we talked about earlier,

2:07:30.760 --> 2:07:35.120
 what it takes to design a game that's really fun to play

2:07:35.120 --> 2:07:36.620
 for prolonged periods of time.

2:07:36.620 --> 2:07:40.420
 And it does seem like this puzzle, like you mentioned,

2:07:40.420 --> 2:07:42.280
 the more you learn about it,

2:07:42.280 --> 2:07:44.860
 the more you realize how little you know.

2:07:44.860 --> 2:07:46.820
 So it humbles you, but excites you

2:07:46.820 --> 2:07:49.020
 by the possibility of learning more.

2:07:49.020 --> 2:07:53.560
 It's one heck of a puzzle we got going on here.

2:07:53.560 --> 2:07:56.420
 So like I mentioned, of all the people in the world,

2:07:56.420 --> 2:08:01.380
 you're very likely to be the one who creates the AGI system

2:08:02.580 --> 2:08:06.320
 that achieves human level intelligence and goes beyond it.

2:08:06.320 --> 2:08:08.360
 So if you got a chance and very well,

2:08:08.360 --> 2:08:10.340
 you could be the person that goes into the room

2:08:10.340 --> 2:08:13.140
 with the system and have a conversation.

2:08:13.140 --> 2:08:15.260
 Maybe you only get to ask one question.

2:08:15.260 --> 2:08:18.100
 If you do, what question would you ask her?

2:08:19.460 --> 2:08:23.660
 I would probably ask, what is the true nature of reality?

2:08:23.660 --> 2:08:24.560
 I think that's the question.

2:08:24.560 --> 2:08:25.980
 I don't know if I'd understand the answer

2:08:25.980 --> 2:08:28.540
 because maybe it would be 42 or something like that,

2:08:28.540 --> 2:08:30.980
 but that's the question I would ask.

2:08:32.420 --> 2:08:34.820
 And then there'll be a deep sigh from the systems,

2:08:34.820 --> 2:08:37.460
 like, all right, how do I explain to this human?

2:08:37.460 --> 2:08:41.860
 All right, let me, I don't have time to explain.

2:08:41.860 --> 2:08:44.660
 Maybe I'll draw you a picture that it is.

2:08:44.660 --> 2:08:49.660
 I mean, how do you even begin to answer that question?

2:08:51.280 --> 2:08:52.780
 Well, I think it would.

2:08:52.780 --> 2:08:55.680
 What would you think the answer could possibly look like?

2:08:55.680 --> 2:08:58.380
 I think it could start looking like

2:08:59.940 --> 2:09:02.060
 more fundamental explanations of physics

2:09:02.060 --> 2:09:03.900
 would be the beginning.

2:09:03.900 --> 2:09:05.740
 More careful specification of that,

2:09:05.740 --> 2:09:07.700
 taking you, walking us through by the hand

2:09:07.700 --> 2:09:10.620
 as to what one would do to maybe prove those things out.

2:09:10.620 --> 2:09:13.700
 Maybe giving you glimpses of what things

2:09:13.700 --> 2:09:15.740
 you totally miss in the physics of today.

2:09:15.740 --> 2:09:16.700
 Exactly, exactly.

2:09:16.700 --> 2:09:20.740
 Just here's glimpses of, no, like there's a much,

2:09:22.260 --> 2:09:23.640
 a much more elaborate world

2:09:23.640 --> 2:09:25.540
 or a much simpler world or something.

2:09:26.780 --> 2:09:30.260
 A much deeper, maybe simpler explanation of things,

2:09:30.260 --> 2:09:31.900
 right, than the standard model of physics,

2:09:31.900 --> 2:09:34.860
 which we know doesn't work, but we still keep adding to.

2:09:34.860 --> 2:09:37.940
 So, and that's how I think the beginning

2:09:37.940 --> 2:09:38.940
 of an explanation would look.

2:09:38.940 --> 2:09:41.260
 And it would start encompassing many of the mysteries

2:09:41.260 --> 2:09:43.380
 that we have wondered about for thousands of years,

2:09:43.380 --> 2:09:47.900
 like consciousness, dreaming, life, and gravity,

2:09:47.900 --> 2:09:48.820
 all of these things.

2:09:48.820 --> 2:09:52.620
 Yeah, giving us glimpses of explanations for those things.

2:09:52.620 --> 2:09:57.180
 Well, Damasir, one of the special human beings

2:09:57.180 --> 2:09:59.080
 in this giant puzzle of ours,

2:09:59.080 --> 2:10:01.020
 and it's a huge honor that you would take a pause

2:10:01.020 --> 2:10:03.260
 from the bigger puzzle to solve this small puzzle

2:10:03.260 --> 2:10:04.780
 of a conversation with me today.

2:10:04.780 --> 2:10:06.300
 It's truly an honor and a pleasure.

2:10:06.300 --> 2:10:07.140
 Thank you so much.

2:10:07.140 --> 2:10:07.960
 Thank you, I really enjoyed it.

2:10:07.960 --> 2:10:09.100
 Thanks, Lex.

2:10:09.100 --> 2:10:10.580
 Thanks for listening to this conversation

2:10:10.580 --> 2:10:11.980
 with Damas Ashabis.

2:10:11.980 --> 2:10:13.180
 To support this podcast,

2:10:13.180 --> 2:10:15.820
 please check out our sponsors in the description.

2:10:15.820 --> 2:10:17.900
 And now, let me leave you with some words

2:10:17.900 --> 2:10:20.380
 from Edgar Dykstra.

2:10:20.380 --> 2:10:23.500
 Computer science is no more about computers

2:10:23.500 --> 2:10:26.260
 than astronomy is about telescopes.

2:10:26.260 --> 2:10:30.260
 Thank you for listening, and hope to see you next time.

