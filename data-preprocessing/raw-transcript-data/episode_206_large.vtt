WEBVTT

00:00.000 --> 00:03.240
 The following is a conversation with Eshan Mizra,

00:03.240 --> 00:05.800
 research scientist at Facebook AI Research,

00:05.800 --> 00:08.580
 who works on self supervised machine learning

00:08.580 --> 00:10.480
 in the domain of computer vision,

00:10.480 --> 00:14.120
 or in other words, making AI systems understand

00:14.120 --> 00:18.000
 the visual world with minimal help from us humans.

00:18.000 --> 00:21.720
 Transformers and self attention has been successfully used

00:21.720 --> 00:25.600
 by OpenAI's DPT3 and other language models

00:25.600 --> 00:28.560
 to do self supervised learning in the domain of language.

00:28.560 --> 00:31.800
 Eshan, together with Yann LeCun and others,

00:31.800 --> 00:33.960
 is trying to achieve the same success

00:33.960 --> 00:36.400
 in the domain of images and video.

00:36.400 --> 00:38.320
 The goal is to leave a robot

00:38.320 --> 00:40.360
 watching YouTube videos all night,

00:40.360 --> 00:43.600
 and in the morning, come back to a much smarter robot.

00:43.600 --> 00:46.000
 I read the blog post, Self Supervised Learning,

00:46.000 --> 00:50.360
 The Dark Matter of Intelligence by Eshan and Yann LeCun,

00:50.360 --> 00:52.960
 and then listened to Eshan's appearance

00:52.960 --> 00:57.200
 on the excellent Machine Learning Street Talk podcast,

00:57.200 --> 00:59.160
 and I knew I had to talk to him.

00:59.160 --> 01:02.860
 By the way, if you're interested in machine learning and AI,

01:02.860 --> 01:07.860
 I cannot recommend the ML Street Talk podcast highly enough.

01:07.980 --> 01:09.640
 Those guys are great.

01:09.640 --> 01:11.280
 Quick mention of our sponsors.

01:11.280 --> 01:15.400
 Onnit, The Information, Grammarly, and Athletic Greens.

01:15.400 --> 01:18.640
 Check them out in the description to support this podcast.

01:18.640 --> 01:20.480
 As a side note, let me say that,

01:20.480 --> 01:22.560
 for those of you who may have been listening

01:22.560 --> 01:24.960
 for quite a while, this podcast used to be called

01:24.960 --> 01:27.120
 Artificial Intelligence Podcast,

01:27.120 --> 01:29.700
 because my life passion has always been,

01:29.700 --> 01:32.640
 will always be artificial intelligence,

01:32.640 --> 01:35.440
 both narrowly and broadly defined.

01:35.440 --> 01:37.720
 My goal with this podcast is still

01:37.720 --> 01:40.560
 to have many conversations with world class researchers

01:40.560 --> 01:45.120
 in AI, math, physics, biology, and all the other sciences,

01:45.120 --> 01:49.420
 but I also want to talk to historians, musicians, athletes,

01:49.420 --> 01:51.520
 and of course, occasionally comedians.

01:51.520 --> 01:53.600
 In fact, I'm trying out doing this podcast

01:53.600 --> 01:56.200
 three times a week now to give me more freedom

01:56.200 --> 01:59.380
 with guest selection and maybe get a chance

01:59.380 --> 02:00.880
 to have a bit more fun.

02:00.880 --> 02:03.160
 Speaking of fun, in this conversation,

02:03.160 --> 02:05.440
 I challenge the listener to count the number of times

02:05.440 --> 02:08.000
 the word banana is mentioned.

02:08.000 --> 02:12.580
 Ishan and I use the word banana as the canonical example

02:12.580 --> 02:15.200
 at the core of the hard problem of computer vision

02:15.200 --> 02:19.000
 and maybe the hard problem of consciousness.

02:19.880 --> 02:22.640
 This is the Lex Friedman Podcast,

02:22.640 --> 02:26.300
 and here is my conversation with Ishan Mizra.

02:27.240 --> 02:29.880
 What is self supervised learning?

02:29.880 --> 02:32.760
 And maybe even give the bigger basics

02:32.760 --> 02:35.360
 of what is supervised and semi supervised learning,

02:35.360 --> 02:37.640
 and maybe why is self supervised learning

02:37.640 --> 02:40.080
 a better term than unsupervised learning?

02:40.080 --> 02:41.600
 Let's start with supervised learning.

02:41.600 --> 02:43.920
 So typically for machine learning systems,

02:43.920 --> 02:46.920
 the way they're trained is you get a bunch of humans,

02:46.920 --> 02:48.600
 the humans point out particular concepts.

02:48.600 --> 02:50.180
 So if it's in the case of images,

02:50.180 --> 02:52.960
 you want the humans to come and tell you

02:52.960 --> 02:54.400
 what is present in the image,

02:54.400 --> 02:57.240
 draw boxes around them, draw masks of like things,

02:57.240 --> 03:00.520
 pixels, which are of particular categories or not.

03:00.520 --> 03:01.960
 For NLP, again, there are like lots

03:01.960 --> 03:04.760
 of these particular tasks, say about sentiment analysis,

03:04.760 --> 03:06.620
 about entailment and so on.

03:06.620 --> 03:08.080
 So typically for supervised learning,

03:08.080 --> 03:11.280
 we get a big corpus of such annotated or labeled data.

03:11.280 --> 03:12.780
 And then we feed that to a system

03:12.780 --> 03:14.820
 and the system is really trying to mimic.

03:14.820 --> 03:16.600
 So it's taking this input of the data

03:16.600 --> 03:18.360
 and then trying to mimic the output.

03:18.360 --> 03:20.680
 So it looks at an image and the human has tagged

03:20.680 --> 03:22.400
 that this image contains a banana.

03:22.400 --> 03:24.680
 And now the system is basically trying to mimic that.

03:24.680 --> 03:26.680
 So that's its learning signal.

03:26.680 --> 03:28.000
 And so for supervised learning,

03:28.000 --> 03:30.040
 we try to gather lots of such data

03:30.040 --> 03:31.820
 and we train these machine learning models

03:31.820 --> 03:33.460
 to imitate the input output.

03:33.460 --> 03:35.600
 And the hope is basically by doing so,

03:35.600 --> 03:38.080
 now on unseen or like new kinds of data,

03:38.080 --> 03:40.000
 this model can automatically learn

03:40.000 --> 03:41.320
 to predict these concepts.

03:41.320 --> 03:43.400
 So this is a standard sort of supervised setting.

03:43.400 --> 03:45.760
 For semi supervised setting,

03:45.760 --> 03:47.600
 the idea typically is that you have,

03:47.600 --> 03:49.280
 of course, all of the supervised data,

03:49.280 --> 03:50.800
 but you have lots of other data,

03:50.800 --> 03:53.120
 which is unsupervised or which is like not labeled.

03:53.120 --> 03:55.280
 Now, the problem basically with supervised learning

03:55.280 --> 03:57.440
 and why you actually have all of these alternate

03:57.440 --> 03:59.400
 sort of learning paradigms is,

03:59.400 --> 04:01.800
 supervised learning just does not scale.

04:01.800 --> 04:03.900
 So if you look at for computer vision,

04:03.900 --> 04:05.000
 the sort of largest,

04:05.000 --> 04:07.500
 one of the most popular data sets is ImageNet, right?

04:07.500 --> 04:11.680
 So the entire ImageNet data set has about 22,000 concepts

04:11.680 --> 04:13.800
 and about 14 million images.

04:13.800 --> 04:16.160
 So these concepts are basically just nouns

04:16.160 --> 04:18.360
 and they're annotated on images.

04:18.360 --> 04:20.600
 And this entire data set was a mammoth data collection

04:20.600 --> 04:22.320
 effort that actually gave rise

04:22.320 --> 04:23.840
 to a lot of powerful learning algorithms

04:23.840 --> 04:25.640
 is credited with like sort of the rise

04:25.640 --> 04:27.240
 of deep learning as well.

04:27.240 --> 04:30.140
 But this data set took about 22 human years

04:30.140 --> 04:31.960
 to collect, to annotate.

04:31.960 --> 04:33.520
 And it's not even that many concepts, right?

04:33.520 --> 04:34.580
 It's not even that many images,

04:34.580 --> 04:36.800
 14 million is nothing really.

04:36.800 --> 04:39.360
 Like you have about, I think 400 million images or so,

04:39.360 --> 04:41.920
 or even more than that uploaded to most of the popular

04:41.920 --> 04:44.200
 sort of social media websites today.

04:44.200 --> 04:46.440
 So now supervised learning just doesn't scale.

04:46.440 --> 04:48.680
 If I want to now annotate more concepts,

04:48.680 --> 04:51.340
 if I want to have various types of fine grained concepts,

04:51.340 --> 04:53.240
 then it won't really scale.

04:53.240 --> 04:54.880
 So now you come up to these sort of different

04:54.880 --> 04:57.560
 learning paradigms, for example, semi supervised learning,

04:57.560 --> 04:58.600
 where the idea is you, of course,

04:58.600 --> 05:01.400
 you have this annotated corpus of supervised data

05:01.400 --> 05:03.720
 and you have lots of these unlabeled images.

05:03.720 --> 05:05.860
 And the idea is that the algorithm should basically try

05:05.860 --> 05:08.000
 to measure some kind of consistency

05:08.000 --> 05:10.320
 or really try to measure some kind of signal

05:10.320 --> 05:12.200
 on this sort of unlabeled data

05:12.200 --> 05:14.200
 to make itself more confident

05:14.200 --> 05:16.200
 about what it's really trying to predict.

05:16.200 --> 05:19.680
 So by access to this, lots of unlabeled data,

05:19.680 --> 05:22.240
 the idea is that the algorithm actually learns

05:22.240 --> 05:24.560
 to be more confident and actually gets better

05:24.560 --> 05:26.000
 at predicting these concepts.

05:26.920 --> 05:28.520
 And now we come to the other extreme,

05:28.520 --> 05:30.520
 which is like self supervised learning.

05:30.520 --> 05:33.040
 The idea basically is that the machine or the algorithm

05:33.040 --> 05:35.660
 should really discover concepts or discover things

05:35.660 --> 05:38.200
 about the world or learn representations about the world

05:38.200 --> 05:40.080
 which are useful without access

05:40.080 --> 05:41.800
 to explicit human supervision.

05:41.800 --> 05:44.360
 So the word supervision is still

05:44.360 --> 05:46.280
 in the term self supervised.

05:46.280 --> 05:48.560
 So what is the supervision signal?

05:48.560 --> 05:51.240
 And maybe that perhaps is when Yann LeCun

05:51.240 --> 05:52.920
 and you argue that unsupervised

05:52.920 --> 05:55.040
 is the incorrect terminology here.

05:55.040 --> 05:57.440
 So what is the supervision signal

05:57.440 --> 05:59.720
 when the humans aren't part of the picture

05:59.720 --> 06:02.400
 or not a big part of the picture?

06:02.400 --> 06:04.520
 Right, so self supervised,

06:04.520 --> 06:06.840
 the reason that it has the term supervised in itself

06:06.840 --> 06:10.360
 is because you're using the data itself as supervision.

06:10.360 --> 06:13.200
 So because the data serves as its own source of supervision,

06:13.200 --> 06:15.160
 it's self supervised in that way.

06:15.160 --> 06:16.400
 Now, the reason a lot of people,

06:16.400 --> 06:18.380
 I mean, we did it in that blog post with Yann,

06:18.380 --> 06:20.120
 but a lot of other people have also argued

06:20.120 --> 06:22.080
 for using this term self supervised.

06:22.080 --> 06:25.680
 So starting from like 94 from Virginia Desas group,

06:25.680 --> 06:28.800
 I think UCSD, and now she's at UCSD.

06:28.800 --> 06:31.640
 Jeetendra Malik has said this a bunch of times as well.

06:31.640 --> 06:33.080
 So you have supervised,

06:33.080 --> 06:35.200
 and then unsupervised basically means everything

06:35.200 --> 06:36.400
 which is not supervised,

06:36.400 --> 06:38.640
 but that includes stuff like semi supervised,

06:38.640 --> 06:41.280
 that includes other like transductive learning,

06:41.280 --> 06:43.000
 lots of other sort of settings.

06:43.000 --> 06:46.040
 So that's the reason like now people are preferring

06:46.040 --> 06:47.120
 this term self supervised

06:47.120 --> 06:49.240
 because it explicitly says what's happening.

06:49.240 --> 06:51.620
 The data itself is the source of supervision

06:51.620 --> 06:53.120
 and any sort of learning algorithm

06:53.120 --> 06:56.920
 which tries to extract just sort of data supervision signals

06:56.920 --> 06:59.480
 from the data itself is a self supervised algorithm.

06:59.480 --> 07:02.160
 But there is within the data,

07:02.160 --> 07:05.560
 a set of tricks which unlock the supervision.

07:05.560 --> 07:07.200
 So can you give maybe some examples

07:07.200 --> 07:11.360
 and there's innovation ingenuity required

07:11.360 --> 07:12.840
 to unlock that supervision.

07:12.840 --> 07:15.600
 The data doesn't just speak to you some ground truth,

07:15.600 --> 07:17.760
 you have to do some kind of trick.

07:17.760 --> 07:19.560
 So I don't know what your favorite domain is.

07:19.560 --> 07:23.000
 So you specifically specialize in visual learning,

07:23.000 --> 07:24.480
 but is there favorite examples,

07:24.480 --> 07:26.520
 maybe in language or other domains?

07:26.520 --> 07:28.300
 Perhaps the most successful applications

07:28.300 --> 07:31.060
 have been in NLP, not language processing.

07:31.060 --> 07:34.000
 So the idea basically being that you can train models

07:34.000 --> 07:37.360
 that can you have a sentence and you mask out certain words.

07:37.360 --> 07:40.500
 And now these models learn to predict the masked out words.

07:40.500 --> 07:44.000
 So if you have like the cat jumped over the dog,

07:44.000 --> 07:45.940
 so you can basically mask out cat.

07:45.940 --> 07:47.360
 And now you're essentially asking the model

07:47.360 --> 07:50.280
 to predict what was missing, what did I mask out?

07:50.280 --> 07:53.220
 So the model is going to predict basically a distribution

07:53.220 --> 07:55.320
 over all the possible words that it knows.

07:55.320 --> 07:58.360
 And probably it has like if it's a well trained model,

07:58.360 --> 08:00.580
 it has a sort of higher probability density

08:00.580 --> 08:02.560
 for this word cat.

08:02.560 --> 08:05.520
 For vision, I would say the sort of more,

08:05.520 --> 08:07.480
 I mean, the easier example,

08:07.480 --> 08:09.420
 which is not as widely used these days,

08:09.420 --> 08:12.040
 is basically say, for example, video prediction.

08:12.040 --> 08:14.080
 So video is again, a sequence of things.

08:14.080 --> 08:15.040
 So you can ask the model,

08:15.040 --> 08:17.440
 so if you have a video of say 10 seconds,

08:17.440 --> 08:19.840
 you can feed in the first nine seconds to a model

08:19.840 --> 08:21.960
 and then ask it, hey, what happens basically

08:21.960 --> 08:24.500
 in the 10 second, can you predict what's going to happen?

08:24.500 --> 08:26.760
 And the idea basically is because the model

08:26.760 --> 08:29.440
 is predicting something about the data itself.

08:29.440 --> 08:31.380
 Of course, you didn't need any human

08:31.380 --> 08:32.300
 to tell you what was happening

08:32.300 --> 08:34.600
 because the 10 second video was naturally captured.

08:34.600 --> 08:36.680
 Because the model is predicting what's happening there,

08:36.680 --> 08:39.020
 it's going to automatically learn something

08:39.020 --> 08:41.240
 about the structure of the world, how objects move,

08:41.240 --> 08:44.000
 object permanence, and these kinds of things.

08:44.000 --> 08:45.960
 So like, if I have something at the edge of the table,

08:45.960 --> 08:47.520
 it will fall down.

08:47.520 --> 08:49.280
 Things like these, which you really don't have to sit

08:49.280 --> 08:50.280
 and annotate.

08:50.280 --> 08:51.320
 In a supervised learning setting,

08:51.320 --> 08:52.280
 I would have to sit and annotate.

08:52.280 --> 08:55.200
 This is a cup, now I move this cup, this is still a cup,

08:55.200 --> 08:56.640
 and now I move this cup, it's still a cup,

08:56.640 --> 08:58.840
 and then it falls down, and this is a fallen down cup.

08:58.840 --> 09:00.440
 So I won't have to annotate all of these things

09:00.440 --> 09:02.040
 in a self supervised setting.

09:02.040 --> 09:05.280
 Isn't that kind of a brilliant little trick

09:05.280 --> 09:08.320
 of taking a series of data that is consistent

09:08.320 --> 09:11.920
 and removing one element in that series,

09:11.920 --> 09:16.920
 and then teaching the algorithm to predict that element?

09:17.040 --> 09:19.660
 Isn't that, first of all, that's quite brilliant.

09:20.700 --> 09:23.080
 It seems to be applicable in anything

09:23.080 --> 09:27.920
 that has the constraint of being a sequence

09:27.920 --> 09:30.260
 that is consistent with the physical reality.

09:30.260 --> 09:34.400
 The question is, are there other tricks like this

09:34.400 --> 09:37.840
 that can generate the self supervision signal?

09:37.840 --> 09:41.200
 So sequence is possibly the most widely used one in NLP.

09:41.200 --> 09:44.080
 For vision, the one that is actually used for images,

09:44.080 --> 09:45.840
 which is very popular these days,

09:45.840 --> 09:47.600
 is basically taking an image,

09:47.600 --> 09:50.080
 and now taking different crops of that image.

09:50.080 --> 09:51.400
 So you can basically decide to crop,

09:51.400 --> 09:53.100
 say the top left corner,

09:53.100 --> 09:55.280
 and you crop, say the bottom right corner,

09:55.280 --> 09:58.960
 and asking a network to basically present it with a choice,

09:58.960 --> 10:01.360
 saying that, okay, now you have this image,

10:01.360 --> 10:04.480
 you have this image, are these the same or not?

10:04.480 --> 10:06.680
 And so the idea basically is that because different crop,

10:06.680 --> 10:08.480
 like in an image, different parts of the image

10:08.480 --> 10:09.800
 are going to be related.

10:09.800 --> 10:12.420
 So for example, if you have a chair and a table,

10:12.420 --> 10:14.960
 basically these things are going to be close by,

10:14.960 --> 10:16.860
 versus if you take, again,

10:16.860 --> 10:19.520
 if you have like a zoomed in picture of a chair,

10:19.520 --> 10:20.480
 if you're taking different crops,

10:20.480 --> 10:22.340
 it's going to be different parts of the chair.

10:22.340 --> 10:25.020
 So the idea basically is that different crops

10:25.020 --> 10:26.180
 of the image are related,

10:26.180 --> 10:27.900
 and so the features or the representations

10:27.900 --> 10:29.080
 that you get from these different crops

10:29.080 --> 10:30.320
 should also be related.

10:30.320 --> 10:32.720
 So this is possibly the most like widely used trick

10:32.720 --> 10:35.760
 these days for self supervised learning and computer vision.

10:35.760 --> 10:39.080
 So again, using the consistency that's inherent

10:39.080 --> 10:42.000
 to physical reality in visual domain,

10:42.000 --> 10:45.640
 that's, you know, parts of an image are consistent,

10:45.640 --> 10:48.400
 and then in the language domain,

10:48.400 --> 10:50.280
 or anything that has sequences,

10:50.280 --> 10:53.000
 like language or something that's like a time series,

10:53.000 --> 10:55.440
 then you can chop up parts in time.

10:55.440 --> 10:59.400
 It's similar to the story of RNNs and CNNs,

11:00.280 --> 11:02.300
 of RNNs and ConvNets.

11:02.300 --> 11:06.640
 You and Yann LeCun wrote the blog post in March, 2021,

11:06.640 --> 11:08.840
 titled, Self Supervised Learning,

11:08.840 --> 11:11.080
 The Dark Matter of Intelligence.

11:11.080 --> 11:12.640
 Can you summarize this blog post

11:12.640 --> 11:15.660
 and maybe explain the main idea or set of ideas?

11:15.660 --> 11:18.680
 The blog post was mainly about sort of just telling,

11:18.680 --> 11:21.680
 I mean, this is really a accepted fact,

11:21.680 --> 11:22.940
 I would say for a lot of people now,

11:22.940 --> 11:24.360
 that self supervised learning is something

11:24.360 --> 11:27.200
 that is going to play an important role

11:27.200 --> 11:28.320
 for machine learning algorithms

11:28.320 --> 11:30.560
 that come in the future, and even now.

11:30.560 --> 11:33.840
 Let me just comment that we don't yet

11:33.840 --> 11:36.480
 have a good understanding of what dark matter is.

11:36.480 --> 11:37.320
 That's true.

11:37.320 --> 11:40.040
 So the idea basically being...

11:40.040 --> 11:41.840
 So maybe the metaphor doesn't exactly transfer,

11:41.840 --> 11:44.840
 but maybe it's actually perfectly transfers,

11:44.840 --> 11:47.880
 that we don't know, we have an inkling

11:47.880 --> 11:49.280
 that it'll be a big part

11:49.280 --> 11:51.240
 of whatever solving intelligence looks like.

11:51.240 --> 11:52.960
 Right, so I think self supervised learning,

11:52.960 --> 11:54.880
 the way it's done right now is,

11:54.880 --> 11:56.560
 I would say like the first step towards

11:56.560 --> 11:58.600
 what it probably should end up like learning

11:58.600 --> 12:00.540
 or what it should enable us to do.

12:00.540 --> 12:03.760
 So the idea for that particular piece was,

12:03.760 --> 12:06.200
 self supervised learning is going to be a very powerful way

12:06.200 --> 12:08.420
 to learn common sense about the world,

12:08.420 --> 12:10.840
 or like stuff that is really hard to label.

12:10.840 --> 12:13.760
 For example, like is this piece

12:13.760 --> 12:15.640
 over here heavier than the cup?

12:15.640 --> 12:17.520
 Now, for all these kinds of things,

12:17.520 --> 12:18.760
 you'll have to sit and label these things.

12:18.760 --> 12:21.560
 So supervised learning is clearly not going to scale.

12:21.560 --> 12:23.520
 So what is the thing that's actually going to scale?

12:23.520 --> 12:25.060
 It's probably going to be an agent

12:25.060 --> 12:27.920
 that can either actually interact with it to lift it up,

12:27.920 --> 12:29.980
 or observe me doing it.

12:29.980 --> 12:31.580
 So if I'm basically lifting these things up,

12:31.580 --> 12:32.600
 it can probably reason about,

12:32.600 --> 12:34.760
 hey, this is taking him more time to lift up,

12:34.760 --> 12:36.440
 or the velocity is different,

12:36.440 --> 12:37.840
 whereas the velocity for this is different,

12:37.840 --> 12:39.600
 probably this one is heavier.

12:39.600 --> 12:42.000
 So essentially, by observations of the data,

12:42.000 --> 12:44.820
 you should be able to infer a lot of things about the world

12:44.820 --> 12:46.840
 without someone explicitly telling you,

12:46.840 --> 12:48.720
 this is heavy, this is not,

12:48.720 --> 12:50.000
 this is something that can pour,

12:50.000 --> 12:51.200
 this is something that cannot pour,

12:51.200 --> 12:52.480
 this is somewhere that you can sit,

12:52.480 --> 12:53.920
 this is not somewhere that you can sit.

12:53.920 --> 12:57.360
 But you just mentioned ability to interact with the world.

12:57.360 --> 13:01.000
 There's so many questions that are yet,

13:01.000 --> 13:02.840
 that are still open, which is,

13:02.840 --> 13:04.480
 how do you select the set of data

13:04.480 --> 13:08.640
 over which the self supervised learning process works?

13:08.640 --> 13:11.520
 How much interactivity like in the active learning

13:11.520 --> 13:14.400
 or the machine teaching context is there?

13:14.400 --> 13:16.480
 What are the reward signals?

13:16.480 --> 13:18.560
 Like how much actual interaction there is

13:18.560 --> 13:20.080
 with the physical world?

13:20.080 --> 13:21.440
 That kind of thing.

13:21.440 --> 13:24.800
 So that could be a huge question.

13:24.800 --> 13:26.720
 And then on top of that,

13:26.720 --> 13:28.960
 which I have a million questions about,

13:28.960 --> 13:30.420
 which we don't know the answers to,

13:30.420 --> 13:32.840
 but it's worth talking about is,

13:32.840 --> 13:35.120
 how much reasoning is involved?

13:35.120 --> 13:38.520
 How much accumulation of knowledge

13:38.520 --> 13:40.800
 versus something that's more akin to learning

13:40.800 --> 13:43.240
 or whether that's the same thing.

13:43.240 --> 13:46.560
 But so we're like, it is truly dark matter.

13:46.560 --> 13:49.220
 We don't know how exactly to do it.

13:49.220 --> 13:52.040
 But we are, I mean, a lot of us are actually convinced

13:52.040 --> 13:54.200
 that it's going to be a sort of major thing

13:54.200 --> 13:55.040
 in machine learning.

13:55.040 --> 13:56.600
 So let me reframe it then,

13:56.600 --> 14:01.160
 that human supervision cannot be at large scale

14:01.160 --> 14:04.120
 the source of the solution to intelligence.

14:04.120 --> 14:08.000
 So the machines have to discover the supervision

14:08.000 --> 14:10.240
 in the natural signal of the world.

14:10.240 --> 14:11.560
 I mean, the other thing is also

14:11.560 --> 14:14.200
 that humans are not particularly good labelers.

14:14.200 --> 14:16.000
 They're not very consistent.

14:16.000 --> 14:17.860
 For example, like what's the difference

14:17.860 --> 14:19.840
 between a dining table and a table?

14:19.840 --> 14:21.560
 Is it just the fact that one,

14:21.560 --> 14:23.080
 like if you just look at a particular table,

14:23.080 --> 14:24.600
 what makes us say one is dining table

14:24.600 --> 14:26.500
 and the other is not?

14:26.500 --> 14:28.160
 Humans are not particularly consistent.

14:28.160 --> 14:30.100
 They're not like very good sources of supervision

14:30.100 --> 14:32.320
 for a lot of these kinds of edge cases.

14:32.320 --> 14:37.160
 So it may be also the fact that if we want an algorithm

14:37.160 --> 14:39.640
 or want a machine to solve a particular task for us,

14:39.640 --> 14:42.120
 we can maybe just specify the end goal

14:42.120 --> 14:44.240
 and like the stuff in between,

14:44.240 --> 14:46.080
 we really probably should not be specifying

14:46.080 --> 14:49.320
 because we're not maybe going to confuse it a lot actually.

14:49.320 --> 14:51.460
 Well, humans can't even answer the meaning of life.

14:51.460 --> 14:53.920
 So I'm not sure if we're good supervisors

14:53.920 --> 14:55.220
 of the end goal either.

14:55.220 --> 14:56.960
 So let me ask you about categories.

14:56.960 --> 14:59.040
 Humans are not very good at telling the difference

14:59.040 --> 15:01.940
 between what is and isn't a table, like you mentioned.

15:02.800 --> 15:04.520
 Do you think it's possible,

15:04.520 --> 15:08.140
 let me ask you like pretend you're Plato.

15:10.080 --> 15:14.800
 Is it possible to create a pretty good taxonomy

15:14.800 --> 15:16.400
 of objects in the world?

15:16.400 --> 15:19.000
 It seems like a lot of approaches in machine learning

15:19.000 --> 15:21.400
 kind of assume a hopeful vision

15:21.400 --> 15:24.080
 that it's possible to construct a perfect taxonomy

15:24.080 --> 15:26.520
 or it exists perhaps out of our reach,

15:26.520 --> 15:28.800
 but we can always get closer and closer to it.

15:28.800 --> 15:31.240
 Or is that a hopeless pursuit?

15:31.240 --> 15:33.040
 I think it's hopeless in some way.

15:33.040 --> 15:36.080
 So the thing is for any particular categorization

15:36.080 --> 15:36.920
 that you create,

15:36.920 --> 15:38.760
 if you have a discrete sort of categorization,

15:38.760 --> 15:40.520
 I can always take the nearest two concepts

15:40.520 --> 15:42.600
 or I can take a third concept and I can blend it in

15:42.600 --> 15:44.480
 and I can create a new category.

15:44.480 --> 15:46.560
 So if you were to enumerate N categories,

15:46.560 --> 15:48.880
 I will always find an N plus one category for you.

15:48.880 --> 15:50.680
 That's not going to be in the N categories.

15:50.680 --> 15:52.420
 And I can actually create not just N plus one,

15:52.420 --> 15:55.120
 I can very easily create far more than N categories.

15:55.120 --> 15:57.280
 The thing is a lot of things we talk about

15:57.280 --> 15:58.960
 are actually compositional.

15:58.960 --> 16:01.680
 So it's really hard for us to come and sit

16:01.680 --> 16:03.200
 and enumerate all of these out.

16:03.200 --> 16:05.840
 And they compose in various weird ways, right?

16:05.840 --> 16:08.320
 Like you have like a croissant and a donut come together

16:08.320 --> 16:09.680
 to form a cronut.

16:09.680 --> 16:12.400
 So if you were to like enumerate all the foods up until,

16:12.400 --> 16:15.160
 I don't know, whenever the cronut was about 10 years ago

16:15.160 --> 16:16.440
 or 15 years ago,

16:16.440 --> 16:19.000
 then this entire thing called cronut would not exist.

16:19.000 --> 16:21.760
 Yeah, I remember there was the most awesome video

16:21.760 --> 16:23.500
 of a cat wearing a monkey costume.

16:23.500 --> 16:24.340
 Yeah, yes.

16:26.520 --> 16:28.240
 People should look it up, it's great.

16:28.240 --> 16:31.000
 So is that a monkey or is that a cat?

16:31.000 --> 16:33.840
 It's a very difficult philosophical question.

16:33.840 --> 16:37.280
 So there is a concept of similarity between objects.

16:37.280 --> 16:39.860
 So you think that can take us very far?

16:39.860 --> 16:43.200
 Just kind of getting a good function,

16:43.200 --> 16:47.920
 a good way to tell which parts of things are similar

16:47.920 --> 16:50.720
 and which parts of things are very different.

16:50.720 --> 16:51.780
 I think so, yeah.

16:51.780 --> 16:54.320
 So you don't necessarily need to name everything

16:54.320 --> 16:57.840
 or assign a name to everything to be able to use it, right?

16:57.840 --> 16:59.560
 So there are like lots of...

16:59.560 --> 17:01.720
 Shakespeare said that, what's in a name?

17:01.720 --> 17:03.200
 What's in a name, yeah, okay.

17:03.200 --> 17:05.840
 And I mean, lots of like, for example, animals, right?

17:05.840 --> 17:08.120
 They don't have necessarily a well formed

17:08.120 --> 17:09.520
 like syntactic language,

17:09.520 --> 17:11.800
 but they're able to go about their day perfectly.

17:11.800 --> 17:12.880
 The same thing happens for us.

17:12.880 --> 17:17.080
 So, I mean, we probably look at things and we figure out,

17:17.080 --> 17:19.360
 oh, this is similar to something else that I've seen before.

17:19.360 --> 17:22.000
 And then I can probably learn how to use it.

17:22.000 --> 17:26.280
 So I haven't seen all the possible doorknobs in the world.

17:26.280 --> 17:27.800
 But if you show me,

17:27.800 --> 17:29.840
 like I was able to get into this particular place

17:29.840 --> 17:32.120
 fairly easily, I've never seen that particular doorknob.

17:32.120 --> 17:34.360
 So I of course related to all the doorknobs that I've seen

17:34.360 --> 17:36.520
 and I know exactly how it's going to open.

17:36.520 --> 17:39.440
 I have a pretty good idea of how it's going to open.

17:39.440 --> 17:41.800
 And I think this kind of translation between experiences

17:41.800 --> 17:43.720
 only happens because of similarity.

17:43.720 --> 17:45.360
 Because I'm able to relate it to a doorknob.

17:45.360 --> 17:46.600
 If I related it to a hairdryer,

17:46.600 --> 17:50.400
 I would probably be stuck still outside, not able to get in.

17:50.400 --> 17:52.240
 Again, a bit of a philosophical question,

17:52.240 --> 17:55.600
 but can similarity take us all the way

17:55.600 --> 17:57.680
 to understanding a thing?

17:58.680 --> 18:01.940
 Can having a good function that compares objects

18:01.940 --> 18:04.900
 get us to understand something profound

18:04.900 --> 18:07.200
 about singular objects?

18:07.200 --> 18:08.600
 I think I'll ask you a question back.

18:08.600 --> 18:10.600
 What does it mean to understand objects?

18:11.560 --> 18:13.520
 Well, let me tell you what that's similar to.

18:13.520 --> 18:17.680
 No, so there's an idea of sort of reasoning

18:17.680 --> 18:19.760
 by analogy kind of thing.

18:19.760 --> 18:24.760
 I think understanding is the process of placing that thing

18:24.920 --> 18:28.440
 in some kind of network of knowledge that you have.

18:28.440 --> 18:33.160
 That it perhaps is fundamentally related to other concepts.

18:33.160 --> 18:36.480
 So it's not like understanding is fundamentally related

18:36.480 --> 18:39.280
 by composition of other concepts

18:39.280 --> 18:41.480
 and maybe in relation to other concepts.

18:43.160 --> 18:45.800
 And maybe deeper and deeper understanding

18:45.800 --> 18:50.800
 is maybe just adding more edges to that graph somehow.

18:51.840 --> 18:55.080
 So maybe it is a composition of similarities.

18:55.080 --> 18:59.560
 I mean, ultimately, I suppose it is a kind of embedding

18:59.560 --> 19:02.480
 in that wisdom space.

19:02.480 --> 19:06.480
 Yeah, okay, wisdom space is good.

19:06.480 --> 19:08.040
 I think, I do think, right?

19:08.040 --> 19:10.720
 So similarity does get you very, very far.

19:10.720 --> 19:12.320
 Is it the answer to everything?

19:12.320 --> 19:14.120
 I mean, I don't even know what everything is,

19:14.120 --> 19:16.680
 but it's going to take us really far.

19:16.680 --> 19:19.640
 And I think the thing is things are similar

19:19.640 --> 19:21.640
 in very different contexts, right?

19:21.640 --> 19:24.320
 So an elephant is similar to, I don't know,

19:24.320 --> 19:25.600
 another sort of wild animal.

19:25.600 --> 19:28.500
 Let's just pick, I don't know, lion in a different way

19:28.500 --> 19:30.520
 because they're both four legged creatures.

19:30.520 --> 19:32.040
 They're also land animals.

19:32.040 --> 19:33.120
 But of course they're very different

19:33.120 --> 19:33.960
 in a lot of different ways.

19:33.960 --> 19:37.240
 So elephants are like herbivores, lions are not.

19:37.240 --> 19:40.660
 So similarity and particularly dissimilarity

19:40.660 --> 19:43.720
 also actually helps us understand a lot about things.

19:43.720 --> 19:45.200
 And so that's actually why I think

19:45.200 --> 19:47.600
 discrete categorization is very hard.

19:47.600 --> 19:50.060
 Just like forming this particular category of elephant

19:50.060 --> 19:51.840
 and a particular category of lion,

19:51.840 --> 19:54.360
 maybe it's good for just like taxonomy,

19:54.360 --> 19:55.760
 biological taxonomies.

19:55.760 --> 19:59.760
 But when it comes to other things which are not as maybe,

19:59.760 --> 20:01.720
 for example, like grilled cheese, right?

20:01.720 --> 20:02.560
 I have a grilled cheese,

20:02.560 --> 20:03.960
 I dip it in tomato and I keep it outside.

20:03.960 --> 20:05.040
 Now, is that still a grilled cheese

20:05.040 --> 20:06.720
 or is that something else?

20:06.720 --> 20:09.780
 Right, so categorization is still very useful

20:09.780 --> 20:11.240
 for solving problems.

20:11.240 --> 20:15.920
 But is your intuition then sort of the self supervised

20:15.920 --> 20:20.880
 should be the, to borrow Jan Lekun's terminology,

20:20.880 --> 20:23.640
 should be the cake and then categorization,

20:23.640 --> 20:27.360
 the classification, maybe the supervised like layer

20:27.360 --> 20:29.100
 should be just like the thing on top,

20:29.100 --> 20:31.020
 the cherry or the icing or whatever.

20:31.020 --> 20:32.920
 So if you make it the cake,

20:32.920 --> 20:35.520
 it gets in the way of learning.

20:35.520 --> 20:36.360
 If you make it the cake,

20:36.360 --> 20:39.380
 then you won't be able to sit and annotate everything.

20:39.380 --> 20:40.660
 That's as simple as it is.

20:40.660 --> 20:43.080
 Like that's my very practical view on it.

20:43.080 --> 20:44.920
 It's just, I mean, in my PhD,

20:44.920 --> 20:47.000
 I sat down and annotated like a bunch of cards

20:47.000 --> 20:48.480
 for one of my projects.

20:48.480 --> 20:50.640
 And very quickly, I was just like, it was in a video

20:50.640 --> 20:53.560
 and I was basically drawing boxes around all these cards.

20:53.560 --> 20:55.620
 And I think I spent about a week doing all of that

20:55.620 --> 20:57.640
 and I barely got anything done.

20:57.640 --> 21:00.280
 And basically this was, I think my first year of my PhD

21:00.280 --> 21:02.700
 or like a second year of my master's.

21:02.700 --> 21:04.000
 And then by the end of it, I'm like, okay,

21:04.000 --> 21:05.000
 this is just hopeless.

21:05.000 --> 21:05.960
 I can keep doing it.

21:05.960 --> 21:08.480
 And when I'd done that, someone came up to me

21:08.480 --> 21:10.820
 and they basically told me, oh, this is a pickup truck.

21:10.820 --> 21:11.760
 This is not a card.

21:12.760 --> 21:14.800
 And that's when like, aha, this actually makes sense

21:14.800 --> 21:16.140
 because a pickup truck is not really like,

21:16.140 --> 21:17.000
 what was I annotating?

21:17.000 --> 21:19.560
 Was I annotating anything that is mobile

21:19.560 --> 21:21.400
 or was I annotating particular sedans

21:21.400 --> 21:22.660
 or was I annotating SUVs?

21:22.660 --> 21:23.600
 What was I doing?

21:23.600 --> 21:25.720
 By the way, the annotation was bounding boxes?

21:25.720 --> 21:26.960
 Bounding boxes, yeah.

21:26.960 --> 21:30.040
 There's so many deep, profound questions here

21:30.040 --> 21:32.200
 that you're almost cheating your way out of

21:32.200 --> 21:34.400
 by doing self supervised learning, by the way,

21:34.400 --> 21:37.520
 which is like, what makes for an object?

21:37.520 --> 21:39.080
 As opposed to solve intelligence,

21:39.080 --> 21:41.600
 maybe you don't ever need to answer that question.

21:42.480 --> 21:43.720
 I mean, this is the question

21:43.720 --> 21:45.320
 that anyone that's ever done annotation

21:45.320 --> 21:48.040
 because it's so painful gets to ask,

21:48.040 --> 21:53.040
 like, why am I drawing very careful line around this object?

21:55.480 --> 21:57.540
 Like, what is the value?

21:57.540 --> 22:00.200
 I remember when I first saw semantic segmentation

22:00.200 --> 22:03.640
 where you have like instant segmentation

22:03.640 --> 22:06.240
 where you have a very exact line

22:06.240 --> 22:09.520
 around the object in a 2D plane

22:09.520 --> 22:13.440
 of a fundamentally 3D object projected on a 2D plane.

22:13.440 --> 22:15.820
 So you're drawing a line around a car

22:15.820 --> 22:16.960
 that might be occluded.

22:16.960 --> 22:18.880
 There might be another thing in front of it,

22:18.880 --> 22:20.360
 but you're still drawing the line

22:20.360 --> 22:22.720
 of the part of the car that you see.

22:23.640 --> 22:25.880
 How is that the car?

22:25.880 --> 22:27.880
 Why is that the car?

22:27.880 --> 22:31.040
 Like, I had like an existential crisis every time.

22:31.040 --> 22:33.560
 Like, how's that going to help us understand

22:33.560 --> 22:35.360
 a solved computer vision?

22:35.360 --> 22:38.280
 I'm not sure I have a good answer to what's better.

22:38.280 --> 22:41.560
 And I'm not sure I share the confidence that you have

22:41.560 --> 22:46.560
 that self supervised learning can take us far.

22:46.720 --> 22:48.620
 I think I'm more and more convinced

22:48.620 --> 22:50.880
 that it's a very important component,

22:50.880 --> 22:52.840
 but I still feel like we need to understand

22:52.840 --> 22:57.840
 what makes like this dream of maybe what it's called

23:00.120 --> 23:03.080
 like symbolic AI of arriving,

23:03.080 --> 23:05.580
 like once you have this common sense base,

23:05.580 --> 23:10.580
 be able to play with these concepts and build graphs

23:10.960 --> 23:13.440
 or hierarchies of concepts on top

23:13.440 --> 23:18.440
 in order to then like form a deep sense

23:18.800 --> 23:22.040
 of this three dimensional world or four dimensional world

23:22.040 --> 23:25.480
 and be able to reason and then project that onto 2D plane

23:25.480 --> 23:28.520
 in order to interpret a 2D image.

23:28.520 --> 23:30.960
 Can I ask you just an out there question?

23:30.960 --> 23:35.000
 I remember, I think Andre Karpathy had a blog post

23:35.000 --> 23:39.000
 about computer vision, like being really hard.

23:39.000 --> 23:42.080
 I forgot what the title was, but it was many, many years ago.

23:42.080 --> 23:44.760
 And he had, I think President Obama stepping on a scale

23:44.760 --> 23:47.120
 and there was humor and there was a bunch of people laughing

23:47.120 --> 23:48.440
 and whatever.

23:48.440 --> 23:52.000
 And there's a lot of interesting things about that image

23:52.000 --> 23:55.120
 and I think Andre highlighted a bunch of things

23:55.120 --> 23:56.880
 about the image that us humans are able

23:56.880 --> 23:59.000
 to immediately understand.

23:59.000 --> 24:00.960
 Like the idea, I think of gravity

24:00.960 --> 24:04.040
 and that you have the concept of a weight.

24:04.040 --> 24:08.120
 You immediately project because of our knowledge of pose

24:08.120 --> 24:10.360
 and how human bodies are constructed,

24:10.360 --> 24:13.040
 you understand how the forces are being applied

24:13.040 --> 24:14.560
 with the human body.

24:14.560 --> 24:16.040
 The really interesting other thing

24:16.040 --> 24:17.400
 that you're able to understand,

24:17.400 --> 24:20.480
 there's multiple people looking at each other in the image.

24:20.480 --> 24:22.360
 You're able to have a mental model

24:22.360 --> 24:23.760
 of what the people are thinking about.

24:23.760 --> 24:25.320
 You're able to infer like,

24:25.320 --> 24:27.520
 oh, this person is probably thinks,

24:27.520 --> 24:31.240
 like is laughing at how humorous the situation is.

24:31.240 --> 24:34.200
 And this person is confused about what the situation is

24:34.200 --> 24:35.600
 because they're looking this way.

24:35.600 --> 24:37.560
 We're able to infer all of that.

24:37.560 --> 24:40.480
 So that's human vision.

24:41.400 --> 24:45.040
 How difficult is computer vision?

24:45.040 --> 24:48.440
 Like in order to achieve that level of understanding

24:48.440 --> 24:51.440
 and maybe how big of a part

24:51.440 --> 24:54.360
 does self supervised learning play in that, do you think?

24:54.360 --> 24:56.440
 And do you still, you know, back,

24:56.440 --> 24:58.440
 that was like over a decade ago,

24:58.440 --> 25:00.920
 I think Andre and I think a lot of people agreed

25:00.920 --> 25:03.320
 is computer vision is really hard.

25:03.320 --> 25:06.000
 Do you still think computer vision is really hard?

25:06.000 --> 25:07.520
 I think it is, yes.

25:07.520 --> 25:10.640
 And getting to that kind of understanding,

25:10.640 --> 25:12.480
 I mean, it's really out there.

25:12.480 --> 25:15.360
 So if you ask me to solve just that particular problem,

25:15.360 --> 25:17.560
 I can do it the supervised learning route.

25:17.560 --> 25:19.720
 I can always construct a data set and basically predict,

25:19.720 --> 25:21.680
 oh, is there humor in this or not?

25:21.680 --> 25:22.600
 And of course I can do it.

25:22.600 --> 25:23.560
 Actually, that's a good question.

25:23.560 --> 25:25.200
 Do you think you can, okay, okay.

25:25.200 --> 25:29.000
 Do you think you can do human supervised annotation of humor?

25:29.000 --> 25:29.960
 To some extent, yes.

25:29.960 --> 25:30.880
 I'm sure it will work.

25:30.880 --> 25:34.360
 I mean, it won't be as bad as like randomly guessing.

25:34.360 --> 25:36.600
 I'm sure it can still predict whether it's humorous or not

25:36.600 --> 25:37.840
 in some way.

25:37.840 --> 25:40.400
 Yeah, maybe like Reddit upvotes is the signal.

25:40.400 --> 25:41.240
 I don't know.

25:41.240 --> 25:43.800
 I mean, it won't do a great job, but it'll do something.

25:43.800 --> 25:46.040
 It may actually be like, it may find certain things

25:46.040 --> 25:47.560
 which are not humorous, humorous as well,

25:47.560 --> 25:49.160
 which is going to be bad for us.

25:49.160 --> 25:52.120
 But I mean, it'll do, it won't be random.

25:52.120 --> 25:54.520
 Yeah, kind of like my sense of humor.

25:54.520 --> 25:55.920
 Okay, so fine.

25:55.920 --> 25:57.520
 So you can, that particular problem, yes.

25:57.520 --> 25:59.600
 But the general problem you're saying is hard.

25:59.600 --> 26:00.440
 The general problem is hard.

26:00.440 --> 26:02.320
 And I mean, self supervised learning

26:02.320 --> 26:03.920
 is not the answer to everything.

26:03.920 --> 26:04.760
 Of course it's not.

26:04.760 --> 26:07.800
 I think if you have machines that are going to communicate

26:07.800 --> 26:08.760
 with humans at the end of it,

26:08.760 --> 26:10.880
 you want to understand what the algorithm is doing, right?

26:10.880 --> 26:13.720
 You want it to be able to produce an output

26:13.720 --> 26:15.560
 that you can decipher, that you can understand,

26:15.560 --> 26:17.440
 or it's actually useful for something else,

26:17.440 --> 26:19.360
 which again is a human.

26:19.360 --> 26:22.280
 So at some point in this sort of entire loop,

26:22.280 --> 26:23.720
 a human steps in.

26:23.720 --> 26:26.720
 And now this human needs to understand what's going on.

26:26.720 --> 26:28.960
 And at that point, this entire notion of language

26:28.960 --> 26:30.440
 or semantics really comes in.

26:30.440 --> 26:32.600
 If the machine just spits out something

26:32.600 --> 26:34.000
 and if we can't understand it,

26:34.000 --> 26:36.280
 then it's not really that useful for us.

26:36.280 --> 26:38.440
 So self supervised learning is probably going to be useful

26:38.440 --> 26:40.800
 for a lot of the things before that part,

26:40.800 --> 26:42.880
 before the machine really needs to communicate

26:42.880 --> 26:46.080
 a particular kind of output with a human.

26:46.080 --> 26:47.800
 Because, I mean, otherwise,

26:47.800 --> 26:49.920
 how is it going to do that without language?

26:49.920 --> 26:51.880
 Or some kind of communication.

26:51.880 --> 26:53.640
 But you're saying that it's possible to build

26:53.640 --> 26:55.880
 a big base of understanding or whatever,

26:55.880 --> 26:58.280
 of what's a better? Concepts.

26:58.280 --> 26:59.800
 Of concepts. Concepts, yeah.

26:59.800 --> 27:02.280
 Like common sense concepts. Right.

27:02.280 --> 27:06.120
 Supervised learning in the context of computer vision

27:06.120 --> 27:07.520
 is something you've focused on,

27:07.520 --> 27:09.000
 but that's a really hard domain.

27:09.000 --> 27:10.480
 And it's kind of the cutting edge

27:10.480 --> 27:13.040
 of what we're, as a community, working on today.

27:13.040 --> 27:14.760
 Can we take a little bit of a step back

27:14.760 --> 27:16.320
 and look at language?

27:16.320 --> 27:19.000
 Can you summarize the history of success

27:19.000 --> 27:22.480
 of self supervised learning in natural language processing,

27:22.480 --> 27:23.880
 language modeling?

27:23.880 --> 27:25.600
 What are transformers?

27:25.600 --> 27:28.760
 What is the masking, the sentence completion

27:28.760 --> 27:30.040
 that you mentioned before?

27:31.000 --> 27:33.560
 How does it lead us to understand anything?

27:33.560 --> 27:34.800
 Semantic meaning of words,

27:34.800 --> 27:37.640
 syntactic role of words and sentences?

27:37.640 --> 27:40.120
 So I'm, of course, not the expert on NLP.

27:40.120 --> 27:43.480
 I kind of follow it a little bit from the sides.

27:43.480 --> 27:45.760
 So the main sort of reason

27:45.760 --> 27:47.880
 why all of this masking stuff works is,

27:47.880 --> 27:50.880
 I think it's called the distributional hypothesis in NLP.

27:50.880 --> 27:52.640
 The idea basically being that words

27:52.640 --> 27:54.400
 that occur in the same context

27:54.400 --> 27:55.960
 should have similar meaning.

27:55.960 --> 27:59.040
 So if you have the blank jumped over the blank,

27:59.040 --> 28:01.960
 it basically, whatever is like in the first blank

28:01.960 --> 28:04.120
 is basically an object that can actually jump,

28:04.120 --> 28:05.840
 is going to be something that can jump.

28:05.840 --> 28:08.360
 So a cat or a dog, or I don't know, sheep, something,

28:08.360 --> 28:11.680
 all of these things can basically be in that particular context.

28:11.680 --> 28:13.440
 And now, so essentially the idea is that

28:13.440 --> 28:16.080
 if you have words that are in the same context

28:16.080 --> 28:17.360
 and you predict them,

28:17.360 --> 28:20.040
 you're going to learn lots of useful things

28:20.040 --> 28:21.520
 about how words are related,

28:21.520 --> 28:23.600
 because you're predicting by looking at their context

28:23.600 --> 28:24.920
 where the word is going to be.

28:24.920 --> 28:28.280
 So in this particular case, the blank jumped over the fence.

28:28.280 --> 28:30.960
 So now if it's a sheep, the sheep jumped over the fence,

28:30.960 --> 28:32.440
 the dog jumped over the fence.

28:32.440 --> 28:35.600
 So essentially the algorithm or the representation

28:35.600 --> 28:37.640
 basically puts together these two concepts together.

28:37.640 --> 28:40.280
 So it says, okay, dogs are going to be kind of related to sheep

28:40.280 --> 28:42.760
 because both of them occur in the same context.

28:42.760 --> 28:44.480
 Of course, now you can decide

28:44.480 --> 28:46.800
 depending on your particular application downstream,

28:46.800 --> 28:49.200
 you can say that dogs are absolutely not related to sheep

28:49.200 --> 28:52.120
 because well, I don't, I really care about dog food,

28:52.120 --> 28:54.240
 for example, I'm a dog food person

28:54.240 --> 28:55.640
 and I really want to give this dog food

28:55.640 --> 28:57.320
 to this particular animal.

28:57.320 --> 29:00.120
 So depending on what your downstream application is,

29:00.120 --> 29:03.040
 of course, this notion of similarity or this notion

29:03.040 --> 29:04.320
 or this common sense that you've learned

29:04.320 --> 29:05.840
 may not be applicable.

29:05.840 --> 29:08.080
 But the point is basically that this,

29:08.080 --> 29:09.960
 just predicting what the blanks are

29:09.960 --> 29:11.760
 is going to take you really, really far.

29:11.760 --> 29:14.040
 So there's a nice feature of language

29:14.040 --> 29:18.720
 that the number of words in a particular language

29:18.720 --> 29:20.800
 is very large, but it's finite

29:20.800 --> 29:22.080
 and it's actually not that large

29:22.080 --> 29:24.160
 in the grand scheme of things.

29:24.160 --> 29:26.560
 I still got it because we take it for granted.

29:26.560 --> 29:28.400
 So first of all, when you say masking,

29:28.400 --> 29:31.560
 you're talking about this very process of the blank,

29:31.560 --> 29:33.440
 of removing words from a sentence

29:33.440 --> 29:36.760
 and then having the knowledge of what word went there

29:36.760 --> 29:38.520
 in the initial data set,

29:38.520 --> 29:41.080
 that's the ground truth that you're training on

29:41.080 --> 29:43.480
 and then you're asking the neural network

29:43.480 --> 29:45.080
 to predict what goes there.

29:46.560 --> 29:49.240
 That's like a little trick.

29:49.240 --> 29:50.880
 It's a really powerful trick.

29:50.880 --> 29:53.320
 The question is how far that takes us.

29:53.320 --> 29:56.280
 And the other question is, is there other tricks?

29:56.280 --> 29:58.680
 Because to me, it's very possible

29:58.680 --> 30:00.720
 there's other very fascinating tricks.

30:00.720 --> 30:05.200
 I'll give you an example in autonomous driving,

30:05.200 --> 30:06.920
 there's a bunch of tricks

30:06.920 --> 30:10.360
 that give you the self supervised signal back.

30:10.360 --> 30:15.360
 For example, very similar to sentences, but not really,

30:16.280 --> 30:20.240
 which is you have signals from humans driving the car

30:20.240 --> 30:23.640
 because a lot of us drive cars to places.

30:23.640 --> 30:27.800
 And so you can ask the neural network to predict

30:27.800 --> 30:30.240
 what's going to happen the next two seconds

30:30.240 --> 30:33.400
 for a safe navigation through the environment.

30:33.400 --> 30:36.200
 And the signal comes from the fact

30:36.200 --> 30:38.640
 that you also have knowledge of what happened

30:38.640 --> 30:42.080
 in the next two seconds, because you have video of the data.

30:42.080 --> 30:46.760
 The question in autonomous driving, as it is in language,

30:46.760 --> 30:50.200
 can we learn how to drive autonomously

30:50.200 --> 30:53.480
 based on that kind of self supervision?

30:53.480 --> 30:55.360
 Probably the answer is no.

30:55.360 --> 30:57.800
 The question is how good can we get?

30:57.800 --> 31:00.200
 And the same with language, how good can we get?

31:00.200 --> 31:02.160
 And are there other tricks?

31:02.160 --> 31:04.680
 Like we get sometimes super excited by this trick

31:04.680 --> 31:05.720
 that works really well.

31:05.720 --> 31:09.120
 But I wonder, it's almost like mining for gold.

31:09.120 --> 31:12.760
 I wonder how many signals there are in the data

31:12.760 --> 31:15.280
 that could be leveraged that are like there.

31:17.200 --> 31:18.600
 I just wanted to kind of linger on that

31:18.600 --> 31:20.840
 because sometimes it's easy to think

31:20.840 --> 31:24.840
 that maybe this masking process is self supervised learning.

31:24.840 --> 31:27.200
 No, it's only one method.

31:27.200 --> 31:29.280
 So there could be many, many other methods,

31:29.280 --> 31:33.840
 many tricky methods, maybe interesting ways

31:33.840 --> 31:36.880
 to leverage human computation in very interesting ways

31:36.880 --> 31:39.920
 that might actually border on semi supervised learning,

31:39.920 --> 31:40.840
 something like that.

31:40.840 --> 31:43.520
 Obviously the internet is generated by humans

31:43.520 --> 31:44.720
 at the end of the day.

31:44.720 --> 31:48.760
 So all that to say is what's your sense

31:48.760 --> 31:50.680
 in this particular context of language,

31:50.680 --> 31:54.680
 how far can that masking process take us?

31:54.680 --> 31:56.240
 So it has stood the test of time, right?

31:56.240 --> 31:59.800
 I mean, so Word2vec, the initial sort of NLP technique

31:59.800 --> 32:02.120
 that was using this to now, for example,

32:02.120 --> 32:05.880
 like all the BERT and all these big models that we get,

32:05.880 --> 32:07.560
 BERT and Roberta, for example,

32:07.560 --> 32:08.760
 all of them are still sort of based

32:08.760 --> 32:10.600
 on the same principle of masking.

32:10.600 --> 32:12.120
 It's taken us really far.

32:12.120 --> 32:14.400
 I mean, you can actually do things like,

32:14.400 --> 32:16.240
 oh, these two sentences are similar or not,

32:16.240 --> 32:18.680
 whether this particular sentence follows this other sentence

32:18.680 --> 32:20.480
 in terms of logic, so entailment,

32:20.480 --> 32:21.760
 you can do a lot of these things

32:21.760 --> 32:23.640
 with just this masking trick.

32:23.640 --> 32:28.320
 So I'm not sure if I can predict how far it can take us,

32:28.320 --> 32:31.480
 because when it first came out, when Word2vec was out,

32:31.480 --> 32:33.480
 I don't think a lot of us would have imagined

32:33.480 --> 32:35.960
 that this would actually help us do some kind

32:35.960 --> 32:38.520
 of entailment problems and really that well.

32:38.520 --> 32:40.920
 And so just the fact that by just scaling up

32:40.920 --> 32:42.320
 the amount of data that we're training on

32:42.320 --> 32:45.120
 and using better and more powerful neural network

32:45.120 --> 32:47.600
 architectures has taken us from that to this,

32:47.600 --> 32:52.600
 is just showing you how maybe poor predictors we are,

32:52.600 --> 32:54.880
 as humans, how poor we are at predicting

32:54.880 --> 32:57.360
 how successful a particular technique is going to be.

32:57.360 --> 32:58.680
 So I think I can say something now,

32:58.680 --> 33:00.040
 but like 10 years from now,

33:00.040 --> 33:02.800
 I look completely stupid basically predicting this.

33:02.800 --> 33:07.160
 In the language domain, is there something in your work

33:07.160 --> 33:09.560
 that you find useful and insightful

33:09.560 --> 33:12.560
 and transferable to computer vision,

33:12.560 --> 33:15.720
 but also just, I don't know, beautiful and profound

33:15.720 --> 33:18.160
 that I think carries through to the vision domain?

33:18.160 --> 33:21.000
 I mean, the idea of masking has been very powerful.

33:21.000 --> 33:23.680
 It has been used in vision as well for predicting,

33:23.680 --> 33:25.800
 like you say, the next sort of if you have

33:25.800 --> 33:28.080
 and sort of frames and you predict

33:28.080 --> 33:29.360
 what's going to happen in the next frame.

33:29.360 --> 33:30.960
 So that's been very powerful.

33:30.960 --> 33:32.880
 In terms of modeling, like in just terms

33:32.880 --> 33:34.600
 in terms of architecture, I think you would have asked

33:34.600 --> 33:36.880
 about transformers a while back.

33:36.880 --> 33:38.480
 That has really become like,

33:38.480 --> 33:40.800
 it has become super exciting for computer vision now.

33:40.800 --> 33:42.760
 Like in the past, I would say year and a half,

33:42.760 --> 33:44.160
 it's become really powerful.

33:44.160 --> 33:45.240
 What's a transformer?

33:45.240 --> 33:46.080
 Right.

33:46.080 --> 33:47.440
 I mean, the core part of a transformer

33:47.440 --> 33:49.040
 is something called the self attention model.

33:49.040 --> 33:50.440
 So it came out of Google

33:50.440 --> 33:53.760
 and the idea basically is that if you have N elements,

33:53.760 --> 33:56.480
 what you're creating is a way for all of these N elements

33:56.480 --> 33:57.880
 to talk to each other.

33:57.880 --> 34:01.800
 So the idea basically is that you are paying attention.

34:01.800 --> 34:03.160
 Each element is paying attention

34:03.160 --> 34:04.960
 to each of the other element.

34:04.960 --> 34:06.760
 And basically by doing this,

34:06.760 --> 34:08.960
 it's really trying to figure out,

34:08.960 --> 34:11.440
 you're basically getting a much better view of the data.

34:11.440 --> 34:14.480
 So for example, if you have a sentence of like four words,

34:14.480 --> 34:16.320
 the point is if you get a representation

34:16.320 --> 34:18.320
 or a feature for this entire sentence,

34:18.320 --> 34:21.280
 it's constructed in a way such that each word

34:21.280 --> 34:23.840
 has paid attention to everything else.

34:23.840 --> 34:26.120
 Now, the reason it's like different from say,

34:26.120 --> 34:28.440
 what you would do in a ConvNet

34:28.440 --> 34:29.560
 is basically that in the ConvNet,

34:29.560 --> 34:31.400
 you would only pay attention to a local window.

34:31.400 --> 34:33.160
 So each word would only pay attention

34:33.160 --> 34:36.160
 to its next neighbor or like one neighbor after that.

34:36.160 --> 34:37.840
 And the same thing goes for images.

34:37.840 --> 34:40.120
 In images, you would basically pay attention to pixels

34:40.120 --> 34:42.800
 in a three cross three or a seven cross seven neighborhood.

34:42.800 --> 34:43.680
 And that's it.

34:43.680 --> 34:46.000
 Whereas with the transformer, the self attention mainly,

34:46.000 --> 34:48.760
 the sort of idea is that each element

34:48.760 --> 34:50.440
 needs to pay attention to each other element.

34:50.440 --> 34:51.960
 And when you say attention,

34:51.960 --> 34:53.400
 maybe another way to phrase that

34:53.400 --> 34:57.680
 is you're considering a context,

34:57.680 --> 35:01.560
 a wide context in terms of the wide context of the sentence

35:01.560 --> 35:05.160
 in understanding the meaning of a particular word

35:05.160 --> 35:06.960
 and in computer vision that's understanding

35:06.960 --> 35:10.040
 a larger context to understand the local pattern

35:10.040 --> 35:13.080
 of a particular local part of an image.

35:13.080 --> 35:14.960
 Right, so basically if you have say,

35:14.960 --> 35:16.520
 again, a banana in the image,

35:16.520 --> 35:18.600
 you're looking at the full image first.

35:18.600 --> 35:19.920
 So whether it's like, you know,

35:19.920 --> 35:22.200
 you're looking at all the pixels that are off a kitchen

35:22.200 --> 35:23.760
 or for dining table and so on.

35:23.760 --> 35:25.920
 And then you're basically looking at the banana also.

35:25.920 --> 35:27.200
 Yeah, by the way, in terms of,

35:27.200 --> 35:29.240
 if we were to train the funny classifier,

35:29.240 --> 35:32.000
 there's something funny about the word banana.

35:32.000 --> 35:33.840
 Just wanted to anticipate that.

35:33.840 --> 35:36.200
 I am wearing a banana shirt, so yeah.

35:36.200 --> 35:37.480
 Is there bananas on it?

35:39.720 --> 35:42.440
 Okay, so masking has worked for the vision context as well.

35:42.440 --> 35:44.320
 And so this transformer idea has worked as well.

35:44.320 --> 35:46.280
 So basically looking at all the elements

35:46.280 --> 35:48.160
 to understand a particular element

35:48.160 --> 35:49.920
 has been really powerful in vision.

35:49.920 --> 35:52.080
 The reason is like a lot of things

35:52.080 --> 35:53.480
 when you're looking at them in isolation.

35:53.480 --> 35:55.600
 So if you look at just a blob of pixels,

35:55.600 --> 35:57.520
 so Antonio Torralba at MIT used to have

35:57.520 --> 35:58.960
 this like really famous image,

35:58.960 --> 36:01.040
 which I looked at when I was a PhD student.

36:01.040 --> 36:02.840
 But he would basically have a blob of pixels

36:02.840 --> 36:04.960
 and he would ask you, hey, what is this?

36:04.960 --> 36:06.840
 And it looked basically like a shoe

36:06.840 --> 36:08.880
 or like it could look like a TV remote.

36:08.880 --> 36:10.080
 It could look like anything.

36:10.080 --> 36:12.360
 And it turns out it was a beer bottle.

36:12.360 --> 36:14.120
 But I'm not sure it was one of these three things,

36:14.120 --> 36:15.440
 but basically he showed you the full picture

36:15.440 --> 36:17.560
 and then it was very obvious what it was.

36:17.560 --> 36:19.240
 But the point is just by looking at

36:19.240 --> 36:21.880
 that particular local window, you couldn't figure it out.

36:21.880 --> 36:23.880
 Because of resolution, because of other things,

36:23.880 --> 36:26.080
 it's just not easy always to just figure it out

36:26.080 --> 36:27.960
 by looking at just the neighborhood of pixels,

36:27.960 --> 36:29.680
 what these pixels are.

36:29.680 --> 36:32.000
 And the same thing happens for language as well.

36:32.000 --> 36:33.920
 For the parameters that have to learn

36:33.920 --> 36:35.160
 something about the data,

36:35.160 --> 36:37.200
 you need to give it the capacity

36:37.200 --> 36:39.160
 to learn the essential things.

36:39.160 --> 36:42.680
 Like if it's not actually able to receive the signal at all,

36:42.680 --> 36:44.320
 then it's not gonna be able to learn that signal.

36:44.320 --> 36:47.320
 And in order to understand images, to understand language,

36:47.320 --> 36:50.720
 you have to be able to see words in their full context.

36:50.720 --> 36:54.960
 Okay, what is harder to solve, vision or language?

36:54.960 --> 36:57.880
 Visual intelligence or linguistic intelligence?

36:57.880 --> 36:59.840
 So I'm going to say computer vision is harder.

36:59.840 --> 37:01.640
 My reason for this is basically that

37:02.800 --> 37:05.000
 language of course has a big structure to it

37:05.000 --> 37:06.880
 because we developed it.

37:06.880 --> 37:08.720
 Whereas vision is something that is common

37:08.720 --> 37:09.960
 in a lot of animals.

37:09.960 --> 37:12.520
 Everyone is able to get by a lot of these animals

37:12.520 --> 37:15.080
 on earth are actually able to get by without language.

37:15.080 --> 37:18.280
 And a lot of these animals we also deem to be intelligent.

37:18.280 --> 37:20.920
 So clearly intelligence does have

37:20.920 --> 37:22.520
 like a visual component to it.

37:22.520 --> 37:24.240
 And yes, of course, in the case of humans,

37:24.240 --> 37:26.400
 it of course also has a linguistic component.

37:26.400 --> 37:28.720
 But it means that there is something far more fundamental

37:28.720 --> 37:30.840
 about vision than there is about language.

37:30.840 --> 37:32.960
 And I'm sorry to anyone who disagrees,

37:32.960 --> 37:34.360
 but yes, this is what I feel.

37:34.360 --> 37:38.880
 So that's being a little bit reflected in the challenges

37:38.880 --> 37:40.800
 that have to do with the progress

37:40.800 --> 37:42.520
 of self supervised learning, would you say?

37:42.520 --> 37:45.560
 Or is that just a peculiar accidents

37:45.560 --> 37:47.400
 of the progress of the AI community

37:47.400 --> 37:48.600
 that we focused on like,

37:48.600 --> 37:51.680
 or we discovered self attention and transformers

37:51.680 --> 37:53.640
 in the context of language first?

37:53.640 --> 37:55.520
 So like the self supervised learning success

37:55.520 --> 37:58.880
 was actually for vision has not much to do

37:58.880 --> 37:59.960
 with the transformers part.

37:59.960 --> 38:02.480
 I would say it's actually been independent a little bit.

38:02.480 --> 38:05.360
 I think it's just that the signal was a little bit different

38:05.360 --> 38:08.120
 for vision than there was for like NLP

38:08.120 --> 38:11.240
 and probably NLP folks discovered it before.

38:11.240 --> 38:12.680
 So for vision, the main success

38:12.680 --> 38:14.840
 has basically been this like crops so far,

38:14.840 --> 38:16.960
 like taking different crops of images.

38:16.960 --> 38:18.920
 Whereas for NLP, it was this masking thing.

38:18.920 --> 38:20.480
 But also the level of success

38:20.480 --> 38:22.080
 is still much higher for language.

38:22.080 --> 38:22.920
 It has.

38:22.920 --> 38:24.800
 So that has a lot to do with,

38:24.800 --> 38:26.920
 I mean, I can get into a lot of details.

38:26.920 --> 38:29.040
 For this particular question, let's go for it, okay.

38:29.040 --> 38:32.280
 So the first thing is language is very structured.

38:32.280 --> 38:34.080
 So you are going to produce a distribution

38:34.080 --> 38:35.920
 over a finite vocabulary.

38:35.920 --> 38:37.680
 English has a finite number of words.

38:37.680 --> 38:39.520
 It's actually not that large.

38:39.520 --> 38:41.640
 And you need to produce basically,

38:41.640 --> 38:42.760
 when you're doing this masking thing,

38:42.760 --> 38:44.160
 all you need to do is basically tell me

38:44.160 --> 38:46.440
 which one of these like 50,000 words it is.

38:46.440 --> 38:47.280
 That's it.

38:47.280 --> 38:49.560
 Now for vision, let's imagine doing the same thing.

38:49.560 --> 38:51.480
 Okay, we're basically going to blank out

38:51.480 --> 38:52.600
 a particular part of the image

38:52.600 --> 38:54.680
 and we ask the network or this neural network

38:54.680 --> 38:58.080
 to predict what is present in this missing patch.

38:58.080 --> 38:59.960
 It's combinatorially large, right?

38:59.960 --> 39:02.560
 You have 256 pixel values.

39:02.560 --> 39:04.840
 If you're even producing basically a seven cross seven

39:04.840 --> 39:07.960
 or a 14 cross 14 like window of pixels,

39:07.960 --> 39:11.320
 at each of these 169 or each of these 49 locations,

39:11.320 --> 39:13.720
 you have 256 values to predict.

39:13.720 --> 39:15.240
 And so it's really, really large.

39:15.240 --> 39:18.960
 And very quickly, the kind of like prediction problems

39:18.960 --> 39:20.800
 that we're setting up are going to be extremely

39:20.800 --> 39:22.760
 like interactable for us.

39:22.760 --> 39:24.960
 And so the thing is for NLP, it has been really successful

39:24.960 --> 39:27.520
 because we are very good at predicting,

39:27.520 --> 39:30.840
 like doing this like distribution over a finite set.

39:30.840 --> 39:33.480
 And the problem is when this set becomes really large,

39:33.480 --> 39:35.520
 we are going to become really, really bad

39:35.520 --> 39:36.960
 at making these predictions

39:36.960 --> 39:41.000
 and at solving basically this particular set of problems.

39:41.000 --> 39:44.200
 So if you were to do it exactly in the same way

39:44.200 --> 39:47.000
 as NLP for vision, there is very limited success.

39:47.000 --> 39:48.960
 The way stuff is working right now

39:48.960 --> 39:51.640
 is actually not by predicting these masks.

39:51.640 --> 39:53.640
 It's basically by saying that you take these two

39:53.640 --> 39:55.120
 like crops from the image,

39:55.120 --> 39:57.040
 you get a feature representation from it.

39:57.040 --> 39:58.640
 And just saying that these two features,

39:58.640 --> 40:00.400
 so they're like vectors,

40:00.400 --> 40:02.000
 just saying that the distance between these vectors

40:02.000 --> 40:03.200
 should be small.

40:03.200 --> 40:06.640
 And so it's a very different way of learning

40:06.640 --> 40:09.160
 from the visual signal than there is from NLP.

40:09.160 --> 40:11.360
 Okay, the other reason is the distributional hypothesis

40:11.360 --> 40:12.920
 that we talked about for NLP, right?

40:12.920 --> 40:15.160
 So a word given its context,

40:15.160 --> 40:16.560
 basically the context actually supplies

40:16.560 --> 40:18.440
 a lot of meaning to the word.

40:18.440 --> 40:22.280
 Now, because there are just finite number of words

40:22.280 --> 40:25.760
 and there is a finite way in like which we compose them.

40:25.760 --> 40:27.440
 Of course, the same thing holds for pixels,

40:27.440 --> 40:29.760
 but in language, there's a lot of structure, right?

40:29.760 --> 40:31.000
 So I always say whatever,

40:31.000 --> 40:33.760
 the dash jumped over the fence, for example.

40:33.760 --> 40:36.720
 There are lots of these sentences that you'll get.

40:36.720 --> 40:38.680
 And from this, you can actually look at

40:38.680 --> 40:40.160
 this particular sentence might occur

40:40.160 --> 40:41.480
 in a lot of different contexts as well.

40:41.480 --> 40:42.600
 This exact same sentence

40:42.600 --> 40:44.080
 might occur in a different context.

40:44.080 --> 40:45.560
 So the sheep jumped over the fence,

40:45.560 --> 40:46.800
 the cat jumped over the fence,

40:46.800 --> 40:48.160
 the dog jumped over the fence.

40:48.160 --> 40:50.480
 So you immediately get a lot of these words,

40:50.480 --> 40:52.720
 which are because this particular token itself

40:52.720 --> 40:53.560
 has so much meaning,

40:53.560 --> 40:55.480
 you get a lot of these tokens or these words,

40:55.480 --> 40:57.720
 which are actually going to have sort of

40:57.720 --> 41:00.560
 this related meaning across given this context.

41:00.560 --> 41:02.640
 Whereas for vision, it's much harder

41:02.640 --> 41:04.160
 because just by like pure,

41:04.160 --> 41:05.600
 like the way we capture images,

41:05.600 --> 41:07.440
 lighting can be different.

41:07.440 --> 41:09.800
 There might be like different noise in the sensor.

41:09.800 --> 41:12.240
 So the thing is you're capturing a physical phenomenon

41:12.240 --> 41:13.840
 and then you're basically going through

41:13.840 --> 41:16.400
 a very complicated pipeline of like image processing.

41:16.400 --> 41:18.040
 And then you're translating that into

41:18.040 --> 41:20.400
 some kind of like digital signal.

41:20.400 --> 41:23.520
 Whereas with language, you write it down

41:23.520 --> 41:25.040
 and you transfer it to a digital signal,

41:25.040 --> 41:27.520
 almost like it's a lossless like transfer.

41:27.520 --> 41:30.160
 And each of these tokens are very, very well defined.

41:30.160 --> 41:32.840
 There could be a little bit of an argument there

41:32.840 --> 41:36.120
 because language as written down

41:36.120 --> 41:39.400
 is a projection of thought.

41:39.400 --> 41:42.560
 This is one of the open questions is

41:42.560 --> 41:46.320
 if you perfectly can solve language,

41:46.320 --> 41:50.040
 are you getting close to being able to solve easily

41:50.040 --> 41:52.800
 with flying colors past the towing test kind of thing.

41:52.800 --> 41:56.560
 So that's, it's similar, but different

41:56.560 --> 41:59.760
 and the computer vision problem is in the 2D plane

41:59.760 --> 42:02.640
 is a projection with three dimensional world.

42:02.640 --> 42:05.640
 So perhaps there are similar problems there.

42:05.640 --> 42:06.480
 Maybe this is a good.

42:06.480 --> 42:08.560
 I mean, I think what I'm saying is NLP is not easy.

42:08.560 --> 42:09.520
 Of course, don't get me wrong.

42:09.520 --> 42:12.920
 Like abstract thought expressed in knowledge

42:12.920 --> 42:14.600
 or knowledge basically expressed in language

42:14.600 --> 42:16.720
 is really hard to understand, right?

42:16.720 --> 42:19.160
 I mean, we've been communicating with language for so long

42:19.160 --> 42:22.000
 and it is of course a very complicated concept.

42:22.000 --> 42:27.000
 The thing is at least getting like somewhat reasonable,

42:27.000 --> 42:29.880
 like being able to solve some kind of reasonable tasks

42:29.880 --> 42:32.080
 with language, I would say slightly easier

42:32.080 --> 42:33.640
 than it is with computer vision.

42:33.640 --> 42:35.360
 Yeah, I would say, yeah.

42:35.360 --> 42:36.600
 So that's well put.

42:36.600 --> 42:40.840
 I would say getting impressive performance on language

42:40.840 --> 42:43.360
 is easier.

42:43.360 --> 42:45.320
 I feel like for both language and computer vision,

42:45.320 --> 42:49.440
 there's going to be this wall of like,

42:49.440 --> 42:52.240
 like this hump you have to overcome

42:52.240 --> 42:54.800
 to achieve superhuman level performance

42:54.800 --> 42:56.600
 or human level performance.

42:56.600 --> 43:00.200
 And I feel like for language, that wall is farther away.

43:00.200 --> 43:01.880
 So you can get pretty nice.

43:01.880 --> 43:04.080
 You can do a lot of tricks.

43:04.080 --> 43:06.520
 You can show really impressive performance.

43:06.520 --> 43:09.680
 You can even fool people that you're tweeting

43:09.680 --> 43:11.480
 or you write blog posts writing

43:11.480 --> 43:16.480
 or your question answering has intelligence behind it.

43:16.880 --> 43:21.880
 But to truly demonstrate understanding of dialogue,

43:22.360 --> 43:25.000
 of continuous long form dialogue

43:25.000 --> 43:28.600
 that would require perhaps big breakthroughs.

43:28.600 --> 43:30.440
 In the same way in computer vision,

43:30.440 --> 43:33.400
 I think the big breakthroughs need to happen earlier

43:33.400 --> 43:36.600
 to achieve impressive performance.

43:36.600 --> 43:38.760
 This might be a good place to, you already mentioned it,

43:38.760 --> 43:41.120
 but what is contrastive learning

43:41.120 --> 43:43.840
 and what are energy based models?

43:43.840 --> 43:46.840
 Contrastive learning is sort of the paradigm of learning

43:46.840 --> 43:50.680
 where the idea is that you are learning this embedding space

43:50.680 --> 43:52.680
 or so you're learning this sort of vector space

43:52.680 --> 43:54.520
 of all your concepts.

43:54.520 --> 43:56.760
 And the way you learn that is basically by contrasting.

43:56.760 --> 43:59.120
 So the idea is that you have a sample,

43:59.120 --> 44:01.000
 you have another sample that's related to it.

44:01.000 --> 44:02.840
 So that's called the positive

44:02.840 --> 44:05.080
 and you have another sample that's not related to it.

44:05.080 --> 44:06.080
 So that's negative.

44:06.080 --> 44:08.320
 So for example, let's just take an NLP

44:08.320 --> 44:10.960
 or in a simple example in computer vision.

44:10.960 --> 44:14.480
 So you have an image of a cat, you have an image of a dog

44:14.480 --> 44:16.520
 and for whatever application that you're doing,

44:16.520 --> 44:18.840
 say you're trying to figure out what the pets are,

44:18.840 --> 44:20.280
 you're saying that these two images are related.

44:20.280 --> 44:22.280
 So image of a cat and dog are related,

44:22.280 --> 44:25.400
 but now you have another third image of a banana

44:25.400 --> 44:26.960
 because you don't like that word.

44:26.960 --> 44:28.920
 So now you basically have this banana.

44:28.920 --> 44:30.640
 Thank you for speaking to the crowd.

44:30.640 --> 44:32.560
 And so you take both of these images

44:32.560 --> 44:34.440
 and you take the image from the cat,

44:34.440 --> 44:35.280
 the image from the dog,

44:35.280 --> 44:36.760
 you get a feature from both of them.

44:36.760 --> 44:38.160
 And now what you're training the network to do

44:38.160 --> 44:42.080
 is basically pull both of these features together

44:42.080 --> 44:44.720
 while pushing them away from the feature of a banana.

44:44.720 --> 44:45.840
 So this is the contrastive part.

44:45.840 --> 44:47.840
 So you're contrasting against the banana.

44:47.840 --> 44:51.520
 So there's always this notion of a negative and a positive.

44:51.520 --> 44:54.160
 Now, energy based models are like one way

44:54.160 --> 44:57.480
 that Jan sort of explains a lot of these methods.

44:57.480 --> 45:00.680
 So Jan basically, I think a couple of years

45:00.680 --> 45:02.840
 or more than that, like when I joined Facebook,

45:02.840 --> 45:05.080
 Jan used to keep mentioning this word, energy based models.

45:05.080 --> 45:07.200
 And of course I had no idea what he was talking about.

45:07.200 --> 45:09.680
 So then one day I caught him in one of the conference rooms

45:09.680 --> 45:11.240
 and I'm like, can you please tell me what this is?

45:11.240 --> 45:13.120
 So then like very patiently,

45:13.120 --> 45:15.960
 he sat down with like a marker and a whiteboard.

45:15.960 --> 45:18.280
 And his idea basically is that

45:18.280 --> 45:20.280
 rather than talking about probability distributions,

45:20.280 --> 45:21.920
 you can talk about energies of models.

45:21.920 --> 45:24.000
 So models are trying to minimize certain energies

45:24.000 --> 45:24.960
 in certain space,

45:24.960 --> 45:28.200
 or they're trying to maximize a certain kind of energy.

45:28.200 --> 45:29.760
 And the idea basically is that

45:29.760 --> 45:32.200
 you can explain a lot of the contrastive models,

45:32.200 --> 45:33.280
 GANs, for example,

45:33.280 --> 45:36.000
 which are like Generative Adversarial Networks.

45:36.000 --> 45:37.880
 A lot of these modern learning methods

45:37.880 --> 45:39.880
 or VAEs, which are Variational Autoencoders,

45:39.880 --> 45:41.840
 you can really explain them very nicely

45:41.840 --> 45:43.160
 in terms of an energy function

45:43.160 --> 45:45.320
 that they're trying to minimize or maximize.

45:45.320 --> 45:48.360
 And so by putting this common sort of language

45:48.360 --> 45:49.720
 for all of these models,

45:49.720 --> 45:51.800
 what looks very different in machine learning

45:51.800 --> 45:54.160
 that, oh, VAEs are very different from what GANs are,

45:54.160 --> 45:56.440
 are very, very different from what contrastive models are,

45:56.440 --> 45:57.560
 you actually get a sense of like,

45:57.560 --> 46:00.120
 oh, these are actually very, very related.

46:00.120 --> 46:02.520
 It's just that the way or the mechanism

46:02.520 --> 46:04.200
 in which they're sort of maximizing

46:04.200 --> 46:07.000
 or minimizing this energy function is slightly different.

46:07.000 --> 46:08.920
 It's revealing the commonalities

46:08.920 --> 46:10.400
 between all these approaches

46:10.400 --> 46:13.000
 and putting a sexy word on top of it, like energy.

46:13.000 --> 46:14.360
 And so similarities,

46:14.360 --> 46:16.760
 two things that are similar have low energy.

46:16.760 --> 46:20.360
 Like the low energy signifying similarity.

46:20.360 --> 46:21.200
 Right, exactly.

46:21.200 --> 46:23.560
 So basically the idea is that if you were to imagine

46:23.560 --> 46:26.480
 like the embedding as a manifold, a 2D manifold,

46:26.480 --> 46:28.920
 you would get a hill or like a high sort of peak

46:28.920 --> 46:30.600
 in the energy manifold,

46:30.600 --> 46:32.400
 wherever two things are not related.

46:32.400 --> 46:34.080
 And basically you would have like a dip

46:34.080 --> 46:35.520
 where two things are related.

46:35.520 --> 46:37.080
 So you'd get a dip in the manifold.

46:37.080 --> 46:40.200
 And in the self supervised context,

46:40.200 --> 46:42.280
 how do you know two things are related

46:42.280 --> 46:44.120
 and two things are not related?

46:44.120 --> 46:44.960
 Right.

46:44.960 --> 46:46.920
 So this is where all the sort of ingenuity or tricks

46:46.920 --> 46:47.840
 comes in, right?

46:47.840 --> 46:50.840
 So for example, like you can take

46:50.840 --> 46:52.160
 the fill in the blank problem,

46:52.160 --> 46:54.360
 or you can take in the context problem.

46:54.360 --> 46:55.920
 And what you can say is two words

46:55.920 --> 46:57.800
 that are in the same context are related.

46:57.800 --> 47:00.560
 Two words that are in different contexts are not related.

47:00.560 --> 47:02.280
 For images, basically two crops

47:02.280 --> 47:03.960
 from the same image are related.

47:03.960 --> 47:06.440
 And whereas a third image is not related at all.

47:06.440 --> 47:08.200
 Or for a video, it can be two frames

47:08.200 --> 47:09.200
 from that video are related

47:09.200 --> 47:10.800
 because they're likely to contain

47:10.800 --> 47:12.720
 the same sort of concepts in them.

47:12.720 --> 47:13.720
 Whereas a third frame

47:13.720 --> 47:15.600
 from a different video is not related.

47:15.600 --> 47:18.320
 So it basically is, it's a very general term.

47:18.320 --> 47:19.680
 Contrastive learning is nothing really

47:19.680 --> 47:20.840
 to do with self supervised learning.

47:20.840 --> 47:23.240
 It actually is very popular in for example,

47:23.240 --> 47:25.200
 like any kind of metric learning

47:25.200 --> 47:26.920
 or any kind of embedding learning.

47:26.920 --> 47:28.920
 So it's also used in supervised learning.

47:28.920 --> 47:32.080
 And the thing is because we are not really using labels

47:32.080 --> 47:34.560
 to get these positive or negative pairs,

47:34.560 --> 47:37.640
 it can basically also be used for self supervised learning.

47:37.640 --> 47:39.000
 So you mentioned one of the ideas

47:39.000 --> 47:42.760
 in the vision context that works

47:42.760 --> 47:45.280
 is to have different crops.

47:45.280 --> 47:47.080
 So you could think of that as a way

47:47.080 --> 47:49.480
 to sort of manipulating the data

47:49.480 --> 47:53.280
 to generate examples that are similar.

47:53.280 --> 47:55.800
 Obviously, there's a bunch of other techniques.

47:55.800 --> 47:58.440
 You mentioned lighting as a very,

47:58.440 --> 48:01.680
 in images lighting is something that varies a lot

48:01.680 --> 48:04.520
 and you can artificially change those kinds of things.

48:04.520 --> 48:07.720
 There's the whole broad field of data augmentation,

48:07.720 --> 48:11.800
 which manipulates images in order to increase arbitrarily

48:11.800 --> 48:13.400
 the size of the data set.

48:13.400 --> 48:15.840
 First of all, what is data augmentation?

48:15.840 --> 48:18.120
 And second of all, what's the role of data augmentation

48:18.120 --> 48:22.000
 in self supervised learning and contrastive learning?

48:22.000 --> 48:24.760
 So data augmentation is just a way like you said,

48:24.760 --> 48:26.680
 it's basically a way to augment the data.

48:26.680 --> 48:28.640
 So you have say n samples.

48:28.640 --> 48:30.120
 And what you do is you basically define

48:30.120 --> 48:32.280
 some kind of transforms for the sample.

48:32.280 --> 48:33.640
 So you take your say image

48:33.640 --> 48:34.880
 and then you define a transform

48:34.880 --> 48:37.320
 where you can just increase say the colors

48:37.320 --> 48:39.120
 like the colors or the brightness of the image

48:39.120 --> 48:41.320
 or increase or decrease the contrast of the image

48:41.320 --> 48:44.560
 for example, or take different crops of it.

48:44.560 --> 48:46.240
 So data augmentation is just a process

48:46.240 --> 48:49.040
 to like basically perturb the data

48:49.040 --> 48:51.080
 or like augment the data, right?

48:51.080 --> 48:53.160
 And so it has played a fundamental role

48:53.160 --> 48:56.640
 for computer vision for self supervised learning especially.

48:56.640 --> 48:59.160
 The way most of the current methods work

48:59.160 --> 49:02.720
 contrastive or otherwise is by taking an image

49:02.720 --> 49:05.320
 in the case of images is by taking an image

49:05.320 --> 49:08.560
 and then computing basically two perturbations of it.

49:08.560 --> 49:11.480
 So these can be two different crops of the image

49:11.480 --> 49:12.920
 with like different types of lighting

49:12.920 --> 49:15.000
 or different contrast or different colors.

49:15.000 --> 49:17.840
 So you jitter the colors a little bit and so on.

49:17.840 --> 49:21.720
 And now the idea is basically because it's the same object

49:21.720 --> 49:23.440
 or because it's like related concepts

49:23.440 --> 49:25.200
 in both of these perturbations,

49:25.200 --> 49:27.960
 you want the features from both of these perturbations

49:27.960 --> 49:28.920
 to be similar.

49:28.920 --> 49:31.320
 So now you can use a variety of different ways

49:31.320 --> 49:32.600
 to enforce this constraint,

49:32.600 --> 49:34.200
 like these features being similar.

49:34.200 --> 49:36.040
 You can do this by contrastive learning.

49:36.040 --> 49:38.440
 So basically, both of these things are positives,

49:38.440 --> 49:40.440
 a third sort of image is negative.

49:40.440 --> 49:43.480
 You can do this basically by like clustering.

49:43.480 --> 49:46.960
 For example, you can say that both of these images should,

49:46.960 --> 49:48.120
 the features from both of these images

49:48.120 --> 49:50.560
 should belong in the same cluster because they're related,

49:50.560 --> 49:52.280
 whereas image like another image

49:52.280 --> 49:53.880
 should belong to a different cluster.

49:53.880 --> 49:55.160
 So there's a variety of different ways

49:55.160 --> 49:57.560
 to basically enforce this particular constraint.

49:57.560 --> 49:59.080
 By the way, when you say features,

49:59.080 --> 50:01.680
 it means there's a very large neural network

50:01.680 --> 50:03.640
 that extracting patterns from the image

50:03.640 --> 50:05.160
 and the kind of patterns that extracts

50:05.160 --> 50:08.440
 should be either identical or very similar.

50:08.440 --> 50:09.640
 That's what that means.

50:09.640 --> 50:11.880
 So the neural network basically takes in the image

50:11.880 --> 50:14.160
 and then outputs a set of like,

50:14.160 --> 50:16.600
 basically a vector of like numbers,

50:16.600 --> 50:17.720
 and that's the feature.

50:17.720 --> 50:20.000
 And you want this feature for both of these

50:20.000 --> 50:22.120
 like different crops that you computed to be similar.

50:22.120 --> 50:24.520
 So you want this vector to be identical

50:24.520 --> 50:26.120
 in its like entries, for example.

50:26.120 --> 50:28.120
 Be like literally close

50:28.120 --> 50:31.640
 in this multi dimensional space to each other.

50:31.640 --> 50:32.600
 And like you said,

50:32.600 --> 50:35.960
 close can mean part of the same cluster or something like that

50:35.960 --> 50:37.440
 in this large space.

50:37.440 --> 50:38.920
 First of all, that,

50:38.920 --> 50:40.680
 I wonder if there is connection

50:40.680 --> 50:43.760
 to the way humans learn to this,

50:43.760 --> 50:48.040
 almost like maybe subconsciously,

50:48.040 --> 50:50.120
 in order to understand a thing,

50:50.120 --> 50:54.680
 you kind of have to see it from two, three multiple angles.

50:54.680 --> 50:57.320
 I wonder, I have a lot of friends

50:57.320 --> 51:00.200
 who are neuroscientists maybe and cognitive scientists.

51:00.200 --> 51:03.200
 I wonder if that's in there somewhere.

51:03.200 --> 51:08.200
 Like in order for us to place a concept in its proper place,

51:08.560 --> 51:12.440
 we have to basically crop it in all kinds of ways,

51:12.440 --> 51:14.400
 do basic data augmentation on it

51:14.400 --> 51:17.640
 in whatever very clever ways that the brain likes to do.

51:17.640 --> 51:19.040
 Right.

51:19.040 --> 51:21.160
 Like spinning around in our minds somehow

51:21.160 --> 51:23.080
 that that is very effective.

51:23.080 --> 51:25.040
 So I think for some of them, we like need to do it.

51:25.040 --> 51:27.000
 So like babies, for example, pick up objects,

51:27.000 --> 51:30.120
 like move them and put them close to their eye and whatnot.

51:30.120 --> 51:31.200
 But for certain other things,

51:31.200 --> 51:33.800
 actually we are good at imagining it as well, right?

51:33.800 --> 51:35.960
 So if you, I have never seen, for example,

51:35.960 --> 51:36.960
 an elephant from the top.

51:36.960 --> 51:39.560
 I've never basically looked at it from like top down.

51:39.560 --> 51:40.720
 But if you showed me a picture of it,

51:40.720 --> 51:43.760
 I could very well tell you that that's an elephant.

51:43.760 --> 51:45.320
 So I think some of it, we're just like,

51:45.320 --> 51:47.840
 we naturally build it or transfer it from other objects

51:47.840 --> 51:50.920
 that we've seen to imagine what it's going to look like.

51:50.920 --> 51:53.280
 Has anyone done that with augmentation?

51:53.280 --> 51:56.920
 Like imagine all the possible things

51:56.920 --> 51:59.880
 that are occluded or not there,

51:59.880 --> 52:03.360
 but not just like normal things, like wild things,

52:03.360 --> 52:05.880
 but they're nevertheless physically consistent.

52:06.960 --> 52:09.720
 So, I mean, people do kind of like

52:09.720 --> 52:11.800
 occlusion based augmentation as well.

52:11.800 --> 52:14.760
 So you place in like a random like box, gray box

52:14.760 --> 52:17.440
 to sort of mask out a certain part of the image.

52:17.440 --> 52:20.000
 And the thing is basically you're kind of occluding it.

52:20.000 --> 52:23.600
 For example, you place it say on half of a person's face.

52:23.600 --> 52:24.920
 So basically saying that, you know,

52:24.920 --> 52:26.680
 something below their nose is occluded

52:26.680 --> 52:28.280
 because it's grayed out.

52:28.280 --> 52:31.680
 So, you know, I meant like, you have like, what is it?

52:31.680 --> 52:33.880
 A table and you can't see behind the table.

52:33.880 --> 52:37.080
 And you imagine there's a bunch of elves

52:37.080 --> 52:38.840
 with bananas behind the table.

52:38.840 --> 52:40.440
 Like, I wonder if there's useful

52:40.440 --> 52:44.200
 to have a wild imagination for the network

52:44.200 --> 52:46.120
 because that's possible or maybe not elves,

52:46.120 --> 52:49.000
 but like puppies and kittens or something like that.

52:49.000 --> 52:51.240
 Just have a wild imagination

52:51.240 --> 52:55.080
 and like constantly be generating that wild imagination.

52:55.080 --> 52:57.560
 Because in terms of data augmentation,

52:57.560 --> 53:01.200
 as currently applied, it's super ultra, very boring.

53:01.200 --> 53:02.920
 It's very basic data augmentation.

53:02.920 --> 53:07.040
 I wonder if there's a benefit to being wildly imaginable

53:07.040 --> 53:11.880
 while trying to be consistent with physical reality.

53:11.880 --> 53:14.200
 I think it's a kind of a chicken and egg problem, right?

53:14.200 --> 53:16.400
 Because to have like amazing data augmentation,

53:16.400 --> 53:18.520
 you need to understand what the scene is.

53:18.520 --> 53:20.640
 And what we're trying to do data augmentation

53:20.640 --> 53:22.080
 to learn what a scene is anyway.

53:22.080 --> 53:23.760
 So it's basically just keeps going on.

53:23.760 --> 53:24.800
 Before you understand it,

53:24.800 --> 53:26.400
 just put elves with bananas

53:26.400 --> 53:28.120
 until you know it's not to be true.

53:29.360 --> 53:31.680
 Just like children have a wild imagination

53:31.680 --> 53:33.960
 until the adults ruin it all.

53:33.960 --> 53:36.960
 Okay, so what are the different kinds of data augmentation

53:36.960 --> 53:40.800
 that you've seen to be effective in visual intelligence?

53:40.800 --> 53:42.040
 For like vision,

53:42.040 --> 53:44.160
 it's a lot of these image filtering operations.

53:44.160 --> 53:46.520
 So like blurring the image,

53:46.520 --> 53:48.160
 you know, all the kind of Instagram filters

53:48.160 --> 53:49.440
 that you can think of.

53:49.440 --> 53:52.520
 So like arbitrarily like make the red super red,

53:52.520 --> 53:55.840
 make the green super greens, like saturate the image.

53:55.840 --> 53:56.960
 Rotation, cropping.

53:56.960 --> 53:58.440
 Rotation, cropping, exactly.

53:58.440 --> 53:59.560
 All of these kinds of things.

53:59.560 --> 54:02.600
 Like I said, lighting is a really interesting one to me.

54:02.600 --> 54:04.760
 Like that feels like really complicated to do.

54:04.760 --> 54:05.600
 I mean, they don't,

54:05.600 --> 54:08.040
 the augmentations that we work on aren't like

54:08.040 --> 54:08.880
 that involved,

54:08.880 --> 54:09.720
 they're not going to be like

54:09.720 --> 54:11.280
 physically realistic versions of lighting.

54:11.280 --> 54:12.680
 It's not that you're assuming

54:12.680 --> 54:13.680
 that there's a light source up

54:13.680 --> 54:15.080
 and then you're moving it to the right

54:15.080 --> 54:17.000
 and then what does the thing look like?

54:17.000 --> 54:19.160
 It's really more about like brightness of the image,

54:19.160 --> 54:20.400
 overall brightness of the image

54:20.400 --> 54:22.520
 or overall contrast of the image and so on.

54:22.520 --> 54:25.080
 But this is a really important point to me.

54:25.080 --> 54:28.680
 I always thought that data augmentation

54:28.680 --> 54:30.400
 holds an important key

54:31.640 --> 54:33.840
 to big improvements in machine learning.

54:33.840 --> 54:36.640
 And it seems that it is an important aspect

54:36.640 --> 54:39.080
 of self supervised learning.

54:39.080 --> 54:42.560
 So I wonder if there's big improvements to be achieved

54:42.560 --> 54:46.680
 on much more intelligent kinds of data augmentation.

54:46.680 --> 54:48.320
 For example, currently,

54:48.320 --> 54:50.160
 maybe you can correct me if I'm wrong,

54:50.160 --> 54:52.760
 data augmentation is not parameterized.

54:52.760 --> 54:53.600
 Yeah.

54:53.600 --> 54:54.440
 You're not learning.

54:54.440 --> 54:55.280
 You're not learning.

54:55.280 --> 54:59.800
 To me, it seems like data augmentation potentially

54:59.800 --> 55:02.000
 should involve more learning

55:02.000 --> 55:04.160
 than the learning process itself.

55:04.160 --> 55:05.360
 Right.

55:05.360 --> 55:08.800
 You're almost like thinking of like generative kind of,

55:08.800 --> 55:10.240
 it's the elves with bananas.

55:10.240 --> 55:11.080
 You're trying to,

55:11.080 --> 55:13.280
 it's like very active imagination

55:13.280 --> 55:14.880
 of messing with the world

55:14.880 --> 55:17.640
 and teaching that mechanism for messing with the world

55:17.640 --> 55:19.120
 to be realistic.

55:19.120 --> 55:20.480
 Right.

55:20.480 --> 55:22.640
 Because that feels like,

55:22.640 --> 55:24.200
 I mean, it's imagination.

55:24.200 --> 55:25.600
 It's just, as you said,

55:25.600 --> 55:28.160
 it feels like us humans are able to,

55:29.440 --> 55:30.680
 maybe sometimes subconsciously,

55:30.680 --> 55:33.000
 imagine before we see the thing,

55:33.000 --> 55:35.480
 imagine what we're expecting to see,

55:35.480 --> 55:37.240
 like maybe several options.

55:37.240 --> 55:38.800
 And especially, we probably forgot,

55:38.800 --> 55:40.480
 but when we were younger,

55:40.480 --> 55:44.200
 probably the possibilities were wilder, more numerous.

55:44.200 --> 55:45.160
 And then as we get older,

55:45.160 --> 55:47.400
 we become to understand the world

55:47.400 --> 55:51.040
 and the possibilities of what we might see

55:51.040 --> 55:53.120
 becomes less and less and less.

55:53.120 --> 55:55.600
 So I wonder if you think there's a lot of breakthroughs

55:55.600 --> 55:57.160
 yet to be had in data augmentation.

55:57.160 --> 55:59.760
 And maybe also can you just comment on the stuff we have,

55:59.760 --> 56:02.120
 is that a big part of self supervised learning?

56:02.120 --> 56:02.960
 Yes.

56:02.960 --> 56:05.520
 So data augmentation is like key to self supervised learning

56:05.520 --> 56:08.320
 that has like the kind of augmentation that we're using.

56:08.320 --> 56:11.040
 And basically the fact that we're trying to learn

56:11.040 --> 56:13.920
 these neural networks that are predicting these features

56:13.920 --> 56:17.080
 from images that are robust under data augmentation

56:17.080 --> 56:19.560
 has been the key for visual self supervised learning.

56:19.560 --> 56:22.400
 And they play a fairly fundamental role to it.

56:22.400 --> 56:24.600
 Now, the irony of all of this is that

56:24.600 --> 56:26.720
 for like deep learning purists will say

56:26.720 --> 56:28.640
 the entire point of deep learning is that

56:28.640 --> 56:31.160
 you feed in the pixels to the neural network

56:31.160 --> 56:33.120
 and it should figure out the patterns on its own.

56:33.120 --> 56:34.480
 So if it really wants to look at edges,

56:34.480 --> 56:35.640
 it should look at edges.

56:35.640 --> 56:36.720
 You shouldn't really like really go

56:36.720 --> 56:38.600
 and handcraft these like features, right?

56:38.600 --> 56:41.160
 You shouldn't go tell it that look at edges.

56:41.160 --> 56:42.360
 So data augmentation

56:42.360 --> 56:44.400
 should basically be in the same category, right?

56:44.400 --> 56:46.040
 Why should we tell the network

56:46.040 --> 56:48.200
 or tell this entire learning paradigm

56:48.200 --> 56:50.840
 what kinds of data augmentation that we're looking for?

56:50.840 --> 56:55.200
 We are encoding a very sort of human specific bias there

56:55.200 --> 56:57.560
 that we know things are like,

56:57.560 --> 56:59.200
 if you change the contrast of the image,

56:59.200 --> 57:00.280
 it should still be an apple

57:00.280 --> 57:02.240
 or it should still see apple, not banana.

57:02.240 --> 57:05.880
 And basically if we change like colors,

57:05.880 --> 57:08.040
 it should still be the same kind of concept.

57:08.040 --> 57:09.880
 Of course, this is not one,

57:09.880 --> 57:12.480
 this is doesn't feel like super satisfactory

57:12.480 --> 57:14.560
 because a lot of our human knowledge

57:14.560 --> 57:15.760
 or our human supervision

57:15.760 --> 57:17.600
 is actually going into the data augmentation.

57:17.600 --> 57:19.680
 So although we are calling it self supervised learning,

57:19.680 --> 57:21.040
 a lot of the human knowledge

57:21.040 --> 57:23.520
 is actually being encoded in the data augmentation process.

57:23.520 --> 57:24.360
 So it's really like,

57:24.360 --> 57:27.120
 we've kind of sneaked away the supervision at the input

57:27.120 --> 57:28.520
 and we're like really designing

57:28.520 --> 57:30.360
 these nice list of data augmentations

57:30.360 --> 57:31.640
 that are working very well.

57:31.640 --> 57:33.720
 Of course, the idea is that it's much easier

57:33.720 --> 57:36.600
 to design a list of data augmentation than it is to do.

57:36.600 --> 57:39.640
 So humans are doing nevertheless doing less and less work

57:39.640 --> 57:42.600
 and maybe leveraging their creativity more and more.

57:42.600 --> 57:45.080
 And when we say data augmentation is not parameterized,

57:45.080 --> 57:48.200
 it means it's not part of the learning process.

57:48.200 --> 57:50.560
 Do you think it's possible to integrate

57:50.560 --> 57:53.280
 some of the data augmentation into the learning process?

57:53.280 --> 57:54.120
 I think so.

57:54.120 --> 57:54.960
 I think so.

57:54.960 --> 57:57.440
 And in fact, it will be really beneficial for us

57:57.440 --> 57:59.720
 because a lot of these data augmentations

57:59.720 --> 58:01.840
 that we use in vision are very extreme.

58:01.840 --> 58:05.400
 For example, like when you have certain concepts,

58:05.400 --> 58:08.160
 again, a banana, you take the banana

58:08.160 --> 58:10.560
 and then basically you change the color of the banana, right?

58:10.560 --> 58:12.440
 So you make it a purple banana.

58:12.440 --> 58:14.200
 Now this data augmentation process

58:14.200 --> 58:15.920
 is actually independent of the,

58:15.920 --> 58:18.920
 like it has no notion of what is present in the image.

58:18.920 --> 58:20.520
 So it can change this color arbitrarily.

58:20.520 --> 58:22.560
 It can make it a red banana as well.

58:22.560 --> 58:24.040
 And now what we're doing is we're telling

58:24.040 --> 58:26.160
 the neural network that this red banana

58:26.160 --> 58:29.280
 and so a crop of this image which has the red banana

58:29.280 --> 58:30.960
 and a crop of this image where I changed the color

58:30.960 --> 58:32.360
 to a purple banana should be,

58:32.360 --> 58:34.080
 the features should be the same.

58:34.080 --> 58:36.680
 Now bananas aren't red or purple mostly.

58:36.680 --> 58:38.560
 So really the data augmentation process

58:38.560 --> 58:41.120
 should take into account what is present in the image

58:41.120 --> 58:43.080
 and what are the kinds of physical realities

58:43.080 --> 58:43.920
 that are possible.

58:43.920 --> 58:45.840
 It shouldn't be completely independent of the image.

58:45.840 --> 58:48.840
 So you might get big gains if you,

58:48.840 --> 58:51.560
 instead of being drastic, do subtle augmentation

58:51.560 --> 58:53.280
 but realistic augmentation.

58:53.280 --> 58:54.120
 Right, realistic.

58:54.120 --> 58:56.280
 I'm not sure if it's subtle, but like realistic for sure.

58:56.280 --> 58:59.600
 If it's realistic, then even subtle augmentation

58:59.600 --> 59:00.680
 will give you big benefits.

59:00.680 --> 59:01.840
 Exactly, yeah.

59:01.840 --> 59:05.040
 And it will be like for particular domains

59:05.040 --> 59:06.440
 you might actually see like,

59:06.440 --> 59:08.960
 if for example, now we're doing medical imaging,

59:08.960 --> 59:10.160
 there are going to be certain kinds

59:10.160 --> 59:11.440
 of like geometric augmentation

59:11.440 --> 59:13.480
 which are not really going to be very valid

59:13.480 --> 59:15.080
 for the human body.

59:15.080 --> 59:18.280
 So if you were to like actually loop in data augmentation

59:18.280 --> 59:19.480
 into the learning process,

59:19.480 --> 59:21.320
 it will actually be much more useful.

59:21.320 --> 59:23.280
 Now this actually does take us

59:23.280 --> 59:25.120
 to maybe a semi supervised kind of a setting

59:25.120 --> 59:27.480
 because you do want to understand

59:27.480 --> 59:29.080
 what is it that you're trying to solve.

59:29.080 --> 59:30.880
 So currently self supervised learning

59:30.880 --> 59:32.720
 kind of operates in the wild, right?

59:32.720 --> 59:34.960
 So you do the self supervised learning

59:34.960 --> 59:37.560
 and the purists and all of us basically say that,

59:37.560 --> 59:39.440
 okay, this should learn useful representations

59:39.440 --> 59:42.320
 and they should be useful for any kind of end task,

59:42.320 --> 59:44.280
 no matter it's like banana recognition

59:44.280 --> 59:46.240
 or like autonomous driving.

59:46.240 --> 59:47.760
 Now it's a tall order.

59:47.760 --> 59:50.480
 Maybe the first baby step for us should be that,

59:50.480 --> 59:52.640
 okay, if you're trying to loop in this data augmentation

59:52.640 --> 59:53.920
 into the learning process,

59:53.920 --> 59:56.000
 then we at least need to have some sense

59:56.000 --> 59:56.840
 of what we're trying to do.

59:56.840 --> 59:57.760
 Are we trying to distinguish

59:57.760 --> 59:59.560
 between different types of bananas

59:59.560 --> 1:00:02.040
 or are we trying to distinguish between banana and apple

1:00:02.040 --> 1:00:04.400
 or are we trying to do all of these things at once?

1:00:04.400 --> 1:00:07.920
 And so some notion of like what happens at the end

1:00:07.920 --> 1:00:10.840
 might actually help us do much better at this side.

1:00:10.840 --> 1:00:14.320
 Let me ask you a ridiculous question.

1:00:14.320 --> 1:00:16.280
 If I were to give you like a black box,

1:00:16.280 --> 1:00:19.520
 like a choice to have an arbitrary large data set

1:00:19.520 --> 1:00:21.360
 of real natural data

1:00:22.320 --> 1:00:26.640
 versus really good data augmentation algorithms,

1:00:26.640 --> 1:00:31.320
 which would you like to train in a self supervised way on?

1:00:31.320 --> 1:00:35.040
 So natural data from the internet are arbitrary large,

1:00:35.040 --> 1:00:37.360
 so unlimited data,

1:00:37.360 --> 1:00:41.760
 or it's like more controlled good data augmentation

1:00:41.760 --> 1:00:43.600
 on the finite data set.

1:00:43.600 --> 1:00:44.440
 The thing is like,

1:00:44.440 --> 1:00:47.240
 because our learning algorithms for vision right now

1:00:47.240 --> 1:00:49.360
 really rely on data augmentation,

1:00:49.360 --> 1:00:50.480
 even if you were to give me

1:00:50.480 --> 1:00:52.880
 like an infinite source of like image data,

1:00:52.880 --> 1:00:54.600
 I still need a good data augmentation algorithm.

1:00:54.600 --> 1:00:56.080
 You need something that tells you

1:00:56.080 --> 1:00:57.400
 that two things are similar.

1:00:57.400 --> 1:00:58.240
 Right.

1:00:58.240 --> 1:00:59.080
 And so something,

1:00:59.080 --> 1:01:01.600
 because you've given me an arbitrary large data set,

1:01:01.600 --> 1:01:03.760
 I still need to use data augmentation

1:01:03.760 --> 1:01:05.360
 to take that image construct,

1:01:05.360 --> 1:01:06.920
 like these two perturbations of it,

1:01:06.920 --> 1:01:08.240
 and then learn from it.

1:01:08.240 --> 1:01:09.960
 So the thing is our learning paradigm

1:01:09.960 --> 1:01:11.640
 is very primitive right now.

1:01:11.640 --> 1:01:12.480
 Yeah.

1:01:12.480 --> 1:01:13.800
 Even if you were to give me lots of images,

1:01:13.800 --> 1:01:15.200
 it's still not really useful.

1:01:15.200 --> 1:01:16.520
 A good data augmentation algorithm

1:01:16.520 --> 1:01:18.040
 is actually going to be more useful.

1:01:18.040 --> 1:01:21.160
 So you can like reduce down the amount of data

1:01:21.160 --> 1:01:22.920
 that you give me by like 10 times,

1:01:22.920 --> 1:01:23.760
 but if you were to give me

1:01:23.760 --> 1:01:25.040
 a good data augmentation algorithm,

1:01:25.040 --> 1:01:26.440
 that would probably do better

1:01:26.440 --> 1:01:29.040
 than giving me like 10 times the size of that data,

1:01:29.040 --> 1:01:30.800
 but me having to rely on

1:01:30.800 --> 1:01:32.640
 like a very primitive data augmentation algorithm.

1:01:32.640 --> 1:01:35.040
 Like through tagging and all those kinds of things,

1:01:35.040 --> 1:01:37.240
 is there a way to discover things

1:01:37.240 --> 1:01:39.600
 that are semantically similar on the internet?

1:01:39.600 --> 1:01:42.520
 Obviously there is, but they might be extremely noisy.

1:01:42.520 --> 1:01:45.760
 And the difference might be farther away

1:01:45.760 --> 1:01:47.840
 than you would be comfortable with.

1:01:47.840 --> 1:01:49.720
 So, I mean, yes, tagging will help you a lot.

1:01:49.720 --> 1:01:51.480
 It'll actually go a very long way

1:01:51.480 --> 1:01:54.360
 in figuring out what images are related or not.

1:01:54.360 --> 1:01:57.480
 And then, so, but then the purists would argue

1:01:57.480 --> 1:01:58.880
 that when you're using human tags,

1:01:58.880 --> 1:02:01.200
 because these tags are like supervision,

1:02:01.200 --> 1:02:03.960
 is it really self supervised learning now?

1:02:03.960 --> 1:02:05.320
 Because you're using human tags

1:02:05.320 --> 1:02:07.960
 to figure out which images are like similar.

1:02:07.960 --> 1:02:10.440
 Hashtag no filter means a lot of things.

1:02:10.440 --> 1:02:11.280
 Yes.

1:02:11.280 --> 1:02:12.360
 I mean, there are certain tags

1:02:12.360 --> 1:02:15.280
 which are going to be applicable pretty much to anything.

1:02:15.280 --> 1:02:18.240
 So they're pretty useless for learning.

1:02:18.240 --> 1:02:20.800
 But I mean, certain tags are actually like

1:02:20.800 --> 1:02:22.240
 the Eiffel Tower, for example,

1:02:22.240 --> 1:02:23.800
 or the Taj Mahal, for example.

1:02:23.800 --> 1:02:26.480
 These tags are like very indicative of what's going on.

1:02:26.480 --> 1:02:29.440
 And they are, I mean, they are human supervision.

1:02:29.440 --> 1:02:30.280
 Yeah.

1:02:30.280 --> 1:02:31.880
 This is one of the tasks of discovering

1:02:31.880 --> 1:02:34.880
 from human generated data strong signals

1:02:34.880 --> 1:02:39.560
 that could be leveraged for self supervision.

1:02:39.560 --> 1:02:42.240
 Like humans are doing so much work already.

1:02:42.240 --> 1:02:45.120
 Like many years ago, there was something that was called,

1:02:45.120 --> 1:02:48.000
 I guess, human computation back in the day.

1:02:48.000 --> 1:02:50.240
 Humans are doing so much work.

1:02:50.240 --> 1:02:53.480
 It'd be exciting to discover ways to leverage

1:02:53.480 --> 1:02:55.840
 the work they're doing to teach machines

1:02:55.840 --> 1:02:57.960
 without any extra effort from them.

1:02:57.960 --> 1:03:00.160
 An example could be, like we said, driving,

1:03:00.160 --> 1:03:03.000
 humans driving and machines can learn from the driving.

1:03:03.000 --> 1:03:06.760
 I always hope that there could be some supervision signal

1:03:06.760 --> 1:03:08.160
 discovered in video games,

1:03:08.160 --> 1:03:10.720
 because there's so many people that play video games

1:03:10.720 --> 1:03:15.720
 that it feels like so much effort is put into video games,

1:03:15.840 --> 1:03:17.680
 into playing video games,

1:03:17.680 --> 1:03:21.760
 and you can design video games somewhat cheaply

1:03:21.760 --> 1:03:24.640
 to include whatever signals you want.

1:03:24.640 --> 1:03:27.520
 It feels like that could be leverage somehow.

1:03:27.520 --> 1:03:28.680
 So people are using that.

1:03:28.680 --> 1:03:30.840
 Like there are actually folks right here in UT Austin,

1:03:30.840 --> 1:03:33.760
 like Philip Granbull is a professor at UT Austin.

1:03:33.760 --> 1:03:36.160
 He's been like working on video games

1:03:36.160 --> 1:03:38.000
 as a source of supervision.

1:03:38.000 --> 1:03:39.000
 I mean, it's really fun.

1:03:39.000 --> 1:03:40.040
 Like as a PhD student,

1:03:40.040 --> 1:03:42.200
 getting to basically play video games all day.

1:03:42.200 --> 1:03:44.920
 Yeah, but so I do hope that kind of thing scales

1:03:44.920 --> 1:03:48.080
 and like ultimately boils down to discovering

1:03:48.080 --> 1:03:51.600
 some undeniably very good signal.

1:03:51.600 --> 1:03:54.040
 It's like masking in NLP.

1:03:54.040 --> 1:03:57.640
 But that said, there's non contrastive methods.

1:03:57.640 --> 1:04:00.840
 What do non contrastive energy based

1:04:00.840 --> 1:04:03.520
 self supervised learning methods look like?

1:04:03.520 --> 1:04:05.640
 And why are they promising?

1:04:05.640 --> 1:04:07.800
 So like I said about contrastive learning,

1:04:07.800 --> 1:04:10.720
 you have this notion of a positive and a negative.

1:04:10.720 --> 1:04:13.640
 Now, the thing is, this entire learning paradigm

1:04:13.640 --> 1:04:17.160
 really requires access to a lot of negatives

1:04:17.160 --> 1:04:19.040
 to learn a good sort of feature space.

1:04:19.040 --> 1:04:21.680
 The idea is if I tell you, okay,

1:04:21.680 --> 1:04:23.680
 so a cat and a dog are similar,

1:04:23.680 --> 1:04:25.680
 and they're very different from a banana.

1:04:25.680 --> 1:04:28.000
 The thing is, this is a fairly simple analogy, right?

1:04:28.000 --> 1:04:30.840
 Because bananas look visually very different

1:04:30.840 --> 1:04:32.440
 from what cats and dogs do.

1:04:32.440 --> 1:04:34.440
 So very quickly, if this is the only source

1:04:34.440 --> 1:04:36.600
 of supervision that I'm giving you,

1:04:36.600 --> 1:04:38.080
 your learning is not going to be like,

1:04:38.080 --> 1:04:39.760
 after a point, the neural network

1:04:39.760 --> 1:04:41.640
 is really not going to learn a lot.

1:04:41.640 --> 1:04:42.960
 Because the negative that you're getting

1:04:42.960 --> 1:04:43.880
 is going to be so random.

1:04:43.880 --> 1:04:46.640
 So it can be, oh, a cat and a dog are very similar,

1:04:46.640 --> 1:04:49.880
 but they're very different from a Volkswagen Beetle.

1:04:49.880 --> 1:04:51.920
 Now, like this car looks very different

1:04:51.920 --> 1:04:52.920
 from these animals again.

1:04:52.920 --> 1:04:54.880
 So the thing is in contrastive learning,

1:04:54.880 --> 1:04:58.120
 the quality of the negative sample really matters a lot.

1:04:58.120 --> 1:05:00.800
 And so what has happened is basically that

1:05:00.800 --> 1:05:02.840
 typically these methods that are contrastive

1:05:02.840 --> 1:05:04.880
 really require access to lots of negatives,

1:05:04.880 --> 1:05:06.920
 which becomes harder and harder to sort of scale

1:05:06.920 --> 1:05:09.000
 when designing a learning algorithm.

1:05:09.000 --> 1:05:10.920
 So that's been one of the reasons

1:05:10.920 --> 1:05:13.680
 why non contrastive methods have become like popular

1:05:13.680 --> 1:05:16.360
 and why people think that they're going to be more useful.

1:05:16.360 --> 1:05:18.440
 So a non contrastive method, for example,

1:05:18.440 --> 1:05:20.880
 like clustering is one non contrastive method.

1:05:20.880 --> 1:05:22.480
 The idea basically being that you have

1:05:22.480 --> 1:05:25.880
 two of these samples, so the cat and dog

1:05:25.880 --> 1:05:27.680
 or two crops of this image,

1:05:27.680 --> 1:05:29.280
 they belong to the same cluster.

1:05:30.400 --> 1:05:33.320
 And so essentially you're basically doing clustering online

1:05:33.320 --> 1:05:35.080
 when you're learning this network,

1:05:35.080 --> 1:05:36.720
 and which is very different from having access

1:05:36.720 --> 1:05:38.920
 to a lot of negatives explicitly.

1:05:38.920 --> 1:05:40.840
 The other way which has become really popular

1:05:40.840 --> 1:05:43.120
 is something called self distillation.

1:05:43.120 --> 1:05:45.680
 So the idea basically is that you have a teacher network

1:05:45.680 --> 1:05:47.480
 and a student network,

1:05:47.480 --> 1:05:49.520
 and the teacher network produces a feature.

1:05:49.520 --> 1:05:51.080
 So it takes in the image

1:05:51.080 --> 1:05:53.680
 and basically the neural network figures out the patterns

1:05:53.680 --> 1:05:55.240
 gets the feature out.

1:05:55.240 --> 1:05:56.800
 And there's another neural network

1:05:56.800 --> 1:05:57.960
 which is the student neural network

1:05:57.960 --> 1:05:59.920
 and that also produces a feature.

1:05:59.920 --> 1:06:01.640
 And now all you're doing is basically saying

1:06:01.640 --> 1:06:03.960
 that the features produced by the teacher network

1:06:03.960 --> 1:06:06.120
 and the student network should be very similar.

1:06:06.120 --> 1:06:06.960
 That's it.

1:06:06.960 --> 1:06:09.200
 There is no notion of a negative anymore.

1:06:09.200 --> 1:06:10.040
 And that's it.

1:06:10.040 --> 1:06:11.800
 So it's all about similarity maximization

1:06:11.800 --> 1:06:13.680
 between these two features.

1:06:13.680 --> 1:06:16.320
 And so all I need to now do is figure out

1:06:16.320 --> 1:06:18.680
 how to have these two sorts of parallel networks,

1:06:18.680 --> 1:06:20.600
 a student network and a teacher network.

1:06:20.600 --> 1:06:23.000
 And basically researchers have figured out

1:06:23.000 --> 1:06:24.240
 very cheap methods to do this.

1:06:24.240 --> 1:06:26.760
 So you can actually have for free really

1:06:26.760 --> 1:06:29.000
 two types of neural networks.

1:06:29.000 --> 1:06:30.120
 They're kind of related,

1:06:30.120 --> 1:06:32.040
 but they're different enough that you can actually

1:06:32.040 --> 1:06:34.000
 basically have a learning problem set up.

1:06:34.000 --> 1:06:38.200
 So you can ensure that they always remain different enough.

1:06:38.200 --> 1:06:41.040
 So the thing doesn't collapse into something boring.

1:06:41.040 --> 1:06:41.880
 Exactly.

1:06:41.880 --> 1:06:44.360
 So the main sort of enemy of self supervised learning,

1:06:44.360 --> 1:06:47.560
 any kind of similarity maximization technique is collapse.

1:06:47.560 --> 1:06:50.520
 It's a collapse means that you learn the same feature

1:06:50.520 --> 1:06:53.160
 representation for all the images in the world,

1:06:53.160 --> 1:06:54.640
 which is completely useless.

1:06:54.640 --> 1:06:55.640
 Everything's a banana.

1:06:55.640 --> 1:06:56.560
 Everything is a banana.

1:06:56.560 --> 1:06:57.400
 Everything is a cat.

1:06:57.400 --> 1:06:59.200
 Everything is a car.

1:06:59.200 --> 1:07:02.120
 And so all we need to do is basically come up with ways

1:07:02.120 --> 1:07:03.320
 to prevent collapse.

1:07:03.320 --> 1:07:05.360
 Contrastive learning is one way of doing it.

1:07:05.360 --> 1:07:07.840
 And then for example, like clustering or self distillation

1:07:07.840 --> 1:07:09.240
 or other ways of doing it.

1:07:09.240 --> 1:07:11.840
 We also had a recent paper where we used like

1:07:11.840 --> 1:07:15.400
 de correlation between like two sets of features

1:07:15.400 --> 1:07:16.760
 to prevent collapse.

1:07:16.760 --> 1:07:18.880
 So that's inspired a little bit by like Horace Barlow's

1:07:18.880 --> 1:07:20.680
 neuroscience principles.

1:07:20.680 --> 1:07:23.520
 By the way, I should comment that whoever counts

1:07:23.520 --> 1:07:27.760
 the number of times the word banana, apple, cat and dog

1:07:27.760 --> 1:07:30.120
 were using this conversation wins the internet.

1:07:30.120 --> 1:07:31.120
 I wish you luck.

1:07:32.240 --> 1:07:36.760
 What is Suave and the main improvement proposed

1:07:36.760 --> 1:07:40.360
 in the paper on supervised learning of visual features

1:07:40.360 --> 1:07:42.960
 by contrasting cluster assignments?

1:07:42.960 --> 1:07:46.400
 Suave basically is a clustering based technique,

1:07:46.400 --> 1:07:49.240
 which is for again, the same thing for self supervised

1:07:49.240 --> 1:07:52.440
 learning in vision where we have two crops.

1:07:52.440 --> 1:07:55.280
 And the idea basically is that you want the features

1:07:55.280 --> 1:07:58.920
 from these two crops of an image to lie in the same cluster

1:07:58.920 --> 1:08:02.520
 and basically crops that are coming from different images

1:08:02.520 --> 1:08:03.960
 to be in different clusters.

1:08:03.960 --> 1:08:05.880
 Now, typically in a sort of,

1:08:05.880 --> 1:08:07.120
 if you were to do this clustering,

1:08:07.120 --> 1:08:09.520
 you would perform clustering offline.

1:08:09.520 --> 1:08:11.040
 What that means is you would,

1:08:11.040 --> 1:08:13.160
 if you have a dataset of N examples,

1:08:13.160 --> 1:08:15.360
 you would run over all of these N examples,

1:08:15.360 --> 1:08:17.520
 get features for them, perform clustering.

1:08:17.520 --> 1:08:19.480
 So basically get some clusters

1:08:19.480 --> 1:08:21.960
 and then repeat the process again.

1:08:21.960 --> 1:08:24.640
 So this is offline basically because I need to do one pass

1:08:24.640 --> 1:08:27.200
 through the data to compute its clusters.

1:08:27.200 --> 1:08:30.200
 Suave is basically just a simple way of doing this online.

1:08:30.200 --> 1:08:31.800
 So as you're going through the data,

1:08:31.800 --> 1:08:34.800
 you're actually computing these clusters online.

1:08:34.800 --> 1:08:37.480
 And so of course there is like a lot of tricks involved

1:08:37.480 --> 1:08:40.120
 in how to do this in a robust manner without collapsing,

1:08:40.120 --> 1:08:42.440
 but this is this sort of key idea to it.

1:08:42.440 --> 1:08:45.480
 Is there a nice way to say what is the key methodology

1:08:45.480 --> 1:08:47.640
 of the clustering that enables that?

1:08:47.640 --> 1:08:51.000
 Right, so the idea basically is that

1:08:51.000 --> 1:08:52.720
 when you have N samples,

1:08:52.720 --> 1:08:54.920
 we assume that we have access to,

1:08:54.920 --> 1:08:57.040
 like there are always K clusters in a dataset.

1:08:57.040 --> 1:08:57.880
 K is a fixed number.

1:08:57.880 --> 1:09:00.160
 So for example, K is 3000.

1:09:00.160 --> 1:09:02.200
 And so if you have any,

1:09:02.200 --> 1:09:04.840
 when you look at any sort of small number of examples,

1:09:04.840 --> 1:09:08.000
 all of them must belong to one of these K clusters.

1:09:08.000 --> 1:09:10.320
 And we impose this equipartition constraint.

1:09:10.320 --> 1:09:13.640
 What this means is that basically

1:09:15.200 --> 1:09:16.880
 your entire set of N samples

1:09:16.880 --> 1:09:19.440
 should be equally partitioned into K clusters.

1:09:19.440 --> 1:09:21.800
 So all your K clusters are basically equal,

1:09:21.800 --> 1:09:24.400
 they have equal contribution to these N samples.

1:09:24.400 --> 1:09:26.520
 And this ensures that we never collapse.

1:09:26.520 --> 1:09:28.280
 So collapse can be viewed as a way

1:09:28.280 --> 1:09:30.640
 in which all samples belong to one cluster, right?

1:09:30.640 --> 1:09:33.160
 So all this, if all features become the same,

1:09:33.160 --> 1:09:35.120
 then you have basically just one mega cluster.

1:09:35.120 --> 1:09:38.120
 You don't even have like 10 clusters or 3000 clusters.

1:09:38.120 --> 1:09:40.960
 So Suave basically ensures that at each point,

1:09:40.960 --> 1:09:42.960
 all these 3000 clusters are being used

1:09:42.960 --> 1:09:45.040
 in the clustering process.

1:09:45.040 --> 1:09:46.240
 And that's it.

1:09:46.240 --> 1:09:48.480
 Basically just figure out how to do this online.

1:09:48.480 --> 1:09:50.960
 And again, basically just make sure

1:09:50.960 --> 1:09:54.160
 that two crops from the same image belong to the same cluster

1:09:54.160 --> 1:09:55.720
 and others don't.

1:09:55.720 --> 1:09:58.840
 And the fact they have a fixed K makes things simpler.

1:09:58.840 --> 1:10:00.360
 Fixed K makes things simpler.

1:10:00.360 --> 1:10:02.560
 Our clustering is not like really hard clustering,

1:10:02.560 --> 1:10:03.720
 it's soft clustering.

1:10:03.720 --> 1:10:06.880
 So basically you can be 0.2 to cluster number one

1:10:06.880 --> 1:10:08.440
 and 0.8 to cluster number two.

1:10:08.440 --> 1:10:09.880
 So it's not really hard.

1:10:09.880 --> 1:10:12.720
 So essentially, even though we have like 3000 clusters,

1:10:12.720 --> 1:10:15.160
 we can actually represent a lot of clusters.

1:10:15.160 --> 1:10:19.200
 What is SEER, S E E R?

1:10:19.200 --> 1:10:23.080
 And what are the key results and insights in the paper,

1:10:23.080 --> 1:10:27.360
 Self Supervised Pre Training of Visual Features in the Wild?

1:10:27.360 --> 1:10:30.680
 What is this big, beautiful SEER system?

1:10:30.680 --> 1:10:32.920
 SEER, so I'll first go to Suave

1:10:32.920 --> 1:10:34.360
 because Suave is actually like one

1:10:34.360 --> 1:10:35.760
 of the key components for SEER.

1:10:35.760 --> 1:10:37.800
 So Suave was, when we use Suave,

1:10:37.800 --> 1:10:39.760
 it was demonstrated on ImageNet.

1:10:39.760 --> 1:10:42.920
 So typically like self supervised methods,

1:10:42.920 --> 1:10:46.160
 the way we sort of operate is like in the research community,

1:10:46.160 --> 1:10:47.160
 we kind of cheat.

1:10:47.160 --> 1:10:49.720
 So we take ImageNet, which of course I talked about

1:10:49.720 --> 1:10:51.280
 as having lots of labels.

1:10:51.280 --> 1:10:52.920
 And then we throw away the labels,

1:10:52.920 --> 1:10:54.920
 like throw away all the hard work that went behind

1:10:54.920 --> 1:10:56.800
 basically the labeling process.

1:10:56.800 --> 1:11:00.240
 And we pretend that it is unsupervised.

1:11:00.240 --> 1:11:02.840
 But the problem here is that we have,

1:11:02.840 --> 1:11:05.120
 like when we collected these images,

1:11:05.120 --> 1:11:08.200
 the ImageNet dataset has a particular distribution

1:11:08.200 --> 1:11:09.920
 of concepts, right?

1:11:09.920 --> 1:11:11.720
 So these images are very curated.

1:11:11.720 --> 1:11:15.240
 And what that means is these images, of course,

1:11:15.240 --> 1:11:17.640
 belong to a certain set of noun concepts.

1:11:17.640 --> 1:11:20.360
 And also ImageNet has this bias that all images

1:11:20.360 --> 1:11:22.440
 contain an object, which is like very big

1:11:22.440 --> 1:11:24.040
 and it's typically in the center.

1:11:24.040 --> 1:11:26.120
 So when you're talking about a dog, it's a well framed dog,

1:11:26.120 --> 1:11:28.320
 it's towards the center of the image.

1:11:28.320 --> 1:11:29.760
 So a lot of the data augmentation,

1:11:29.760 --> 1:11:31.480
 a lot of the sort of hidden assumptions

1:11:31.480 --> 1:11:33.400
 in self supervised learning,

1:11:33.400 --> 1:11:37.360
 actually really exploit this bias of ImageNet.

1:11:37.360 --> 1:11:39.680
 And so, I mean, a lot of my work,

1:11:39.680 --> 1:11:42.000
 a lot of work from other people always uses ImageNet

1:11:42.000 --> 1:11:44.200
 sort of as the benchmark to show the success

1:11:44.200 --> 1:11:45.440
 of self supervised learning.

1:11:45.440 --> 1:11:47.680
 So you're implying that there's particular limitations

1:11:47.680 --> 1:11:49.200
 to this kind of dataset?

1:11:49.200 --> 1:11:51.880
 Yes, I mean, it's basically because our data augmentation

1:11:51.880 --> 1:11:55.320
 that we designed, like all data augmentation

1:11:55.320 --> 1:11:57.480
 that we designed for self supervised learning in vision

1:11:57.480 --> 1:11:59.360
 are kind of overfit to ImageNet.

1:11:59.360 --> 1:12:02.400
 But you're saying a little bit hard coded

1:12:02.400 --> 1:12:03.800
 like the cropping.

1:12:03.800 --> 1:12:05.480
 Exactly, the cropping parameters,

1:12:05.480 --> 1:12:07.280
 the kind of lighting that we're using,

1:12:07.280 --> 1:12:08.800
 the kind of blurring that we're using.

1:12:08.800 --> 1:12:11.960
 Yeah, but you would, for more in the wild dataset,

1:12:11.960 --> 1:12:16.240
 you would need to be clever or more careful

1:12:16.240 --> 1:12:17.520
 in setting the range of parameters

1:12:17.520 --> 1:12:18.920
 and those kinds of things.

1:12:18.920 --> 1:12:21.360
 So for SEER, our main goal was twofold.

1:12:21.360 --> 1:12:24.680
 One, basically to move away from ImageNet for training.

1:12:24.680 --> 1:12:27.680
 So the images that we used were like uncurated images.

1:12:27.680 --> 1:12:28.600
 Now there's a lot of debate

1:12:28.600 --> 1:12:30.040
 whether they're actually curated or not,

1:12:30.040 --> 1:12:32.360
 but I'll talk about that later.

1:12:32.360 --> 1:12:33.880
 But the idea was basically,

1:12:33.880 --> 1:12:36.400
 these are going to be random internet images

1:12:36.400 --> 1:12:37.920
 that we're not going to filter out

1:12:37.920 --> 1:12:40.080
 based on like particular categories.

1:12:40.080 --> 1:12:42.880
 So we did not say that, oh, images that belong to dogs

1:12:42.880 --> 1:12:44.280
 and cats should be the only images

1:12:44.280 --> 1:12:47.000
 that come in this dataset, banana.

1:12:47.000 --> 1:12:50.040
 And basically, other images should be thrown out.

1:12:50.040 --> 1:12:51.800
 So we didn't do any of that.

1:12:51.800 --> 1:12:53.560
 So these are random internet images.

1:12:53.560 --> 1:12:56.040
 And of course, it also goes back to like the problem

1:12:56.040 --> 1:12:57.320
 of scale that you talked about.

1:12:57.320 --> 1:13:00.120
 So these were basically about a billion or so images.

1:13:00.120 --> 1:13:01.560
 And for context ImageNet,

1:13:01.560 --> 1:13:02.800
 the ImageNet version that we use

1:13:02.800 --> 1:13:04.280
 was 1 million images earlier.

1:13:04.280 --> 1:13:05.400
 So this is basically going like

1:13:05.400 --> 1:13:07.600
 three orders of magnitude more.

1:13:07.600 --> 1:13:08.600
 The idea was basically to see

1:13:08.600 --> 1:13:11.800
 if we can train a very large convolutional model

1:13:11.800 --> 1:13:14.440
 in a self supervised way on this uncurated,

1:13:14.440 --> 1:13:16.400
 but really large set of images.

1:13:16.400 --> 1:13:18.280
 And how well would this model do?

1:13:18.280 --> 1:13:21.440
 So is self supervised learning really overfit to ImageNet

1:13:21.440 --> 1:13:23.840
 or can it actually work in the wild?

1:13:23.840 --> 1:13:25.720
 And it was also out of curiosity,

1:13:25.720 --> 1:13:27.520
 what kind of things will this model learn?

1:13:27.520 --> 1:13:30.080
 Will it actually be able to still figure out

1:13:30.080 --> 1:13:32.000
 different types of objects and so on?

1:13:32.000 --> 1:13:33.720
 Would there be particular kinds of tasks

1:13:33.720 --> 1:13:38.160
 that would actually do better than an ImageNet train model?

1:13:38.160 --> 1:13:40.960
 And so for Sear, one of our main findings was that

1:13:40.960 --> 1:13:43.120
 we can actually train very large models

1:13:43.120 --> 1:13:44.800
 in a completely self supervised way

1:13:44.800 --> 1:13:46.360
 on lots of internet images

1:13:46.360 --> 1:13:48.600
 without really necessarily filtering them out.

1:13:48.600 --> 1:13:49.760
 Which was in itself a good thing

1:13:49.760 --> 1:13:51.960
 because it's a fairly simple process, right?

1:13:51.960 --> 1:13:54.080
 So you get images which are uploaded

1:13:54.080 --> 1:13:55.800
 and you basically can immediately use them

1:13:55.800 --> 1:13:57.680
 to train a model in an unsupervised way.

1:13:57.680 --> 1:13:59.720
 You don't really need to sit and filter them out.

1:13:59.720 --> 1:14:02.040
 These images can be cartoons, these can be memes,

1:14:02.040 --> 1:14:04.440
 these can be actual pictures uploaded by people.

1:14:04.440 --> 1:14:06.160
 And you don't really care about what these images are.

1:14:06.160 --> 1:14:08.520
 You don't even care about what concepts they contain.

1:14:08.520 --> 1:14:10.280
 So this was a very sort of simple setup.

1:14:10.280 --> 1:14:12.880
 What image selection mechanism would you say

1:14:12.880 --> 1:14:17.880
 is there like inherent in some aspect of the process?

1:14:18.840 --> 1:14:21.280
 So you're kind of implying that there's almost none,

1:14:21.280 --> 1:14:24.960
 but what is there would you say if you were to introspect?

1:14:24.960 --> 1:14:28.480
 Right, so it's not like uncurated can basically

1:14:28.480 --> 1:14:30.400
 like one way of imagining uncurated

1:14:30.400 --> 1:14:32.920
 is basically you have like cameras

1:14:32.920 --> 1:14:35.200
 that can take pictures at random viewpoints.

1:14:35.200 --> 1:14:37.400
 When people upload pictures to the internet,

1:14:37.400 --> 1:14:40.320
 they are typically going to care about the framing of it.

1:14:40.320 --> 1:14:41.840
 They're not going to upload, say,

1:14:41.840 --> 1:14:43.800
 the picture of a zoomed in wall, for example.

1:14:43.800 --> 1:14:46.080
 Well, when you say internet, do you mean social networks?

1:14:46.080 --> 1:14:47.160
 Yes. Okay.

1:14:47.160 --> 1:14:48.680
 So these are not going to be like pictures

1:14:48.680 --> 1:14:51.400
 of like a zoomed in table or a zoomed in wall.

1:14:51.400 --> 1:14:53.160
 So it's not really completely uncurated

1:14:53.160 --> 1:14:55.800
 because people do have the like photographer's bias

1:14:55.800 --> 1:14:57.040
 where they do want to keep things

1:14:57.040 --> 1:14:58.640
 towards the center a little bit,

1:14:58.640 --> 1:15:01.320
 or like really have like nice looking things

1:15:01.320 --> 1:15:02.680
 and so on in the picture.

1:15:02.680 --> 1:15:05.640
 So that's the kind of bias that typically exists

1:15:05.640 --> 1:15:07.720
 in this data set and also the user base, right?

1:15:07.720 --> 1:15:09.320
 You're not going to get lots of pictures

1:15:09.320 --> 1:15:10.520
 from different parts of the world

1:15:10.520 --> 1:15:12.120
 because there are certain parts of the world

1:15:12.120 --> 1:15:14.320
 where people may not actually be uploading

1:15:14.320 --> 1:15:15.440
 a lot of pictures to the internet

1:15:15.440 --> 1:15:17.360
 or may not even have access to a lot of internet.

1:15:17.360 --> 1:15:21.720
 So this is a giant data set and a giant neural network.

1:15:21.720 --> 1:15:24.800
 I don't think we've talked about what architectures

1:15:24.800 --> 1:15:29.320
 work well for SSL, for self supervised learning.

1:15:29.320 --> 1:15:32.480
 For SEER and for SWAB, we were using convolutional networks,

1:15:32.480 --> 1:15:34.160
 but recently in a work called Dyno,

1:15:34.160 --> 1:15:36.880
 we've basically started using transformers for vision.

1:15:36.880 --> 1:15:39.840
 Both seem to work really well, Connets and transformers.

1:15:39.840 --> 1:15:41.120
 And depending on what you want to do,

1:15:41.120 --> 1:15:43.560
 you might choose to use a particular formulation.

1:15:43.560 --> 1:15:45.400
 So for SEER, it was a Connet.

1:15:45.400 --> 1:15:47.480
 It was particularly a RegNet model,

1:15:47.480 --> 1:15:49.720
 which was also a work from Facebook.

1:15:49.720 --> 1:15:52.640
 RegNets are like really good when it comes to compute

1:15:52.640 --> 1:15:54.760
 versus like accuracy.

1:15:54.760 --> 1:15:56.920
 So because it was a very efficient model,

1:15:56.920 --> 1:15:59.680
 compute and memory wise efficient,

1:15:59.680 --> 1:16:02.480
 and basically it worked really well in terms of scaling.

1:16:02.480 --> 1:16:04.200
 So we used a very large RegNet model

1:16:04.200 --> 1:16:05.480
 and trained it on a billion images.

1:16:05.480 --> 1:16:08.640
 Can you maybe quickly comment on what RegNets are?

1:16:09.680 --> 1:16:13.520
 It comes from this paper, Designing Network Design Spaces.

1:16:13.520 --> 1:16:15.520
 This is a super interesting concept

1:16:15.520 --> 1:16:18.400
 that emphasizes how to create efficient neural networks,

1:16:18.400 --> 1:16:19.520
 large neural networks.

1:16:19.520 --> 1:16:21.800
 So one of the sort of key takeaways from this paper,

1:16:21.800 --> 1:16:23.400
 which the authors, like whenever you hear them

1:16:23.400 --> 1:16:26.040
 present this work, they keep saying is,

1:16:26.040 --> 1:16:27.960
 a lot of neural networks are characterized

1:16:27.960 --> 1:16:29.040
 in terms of flops, right?

1:16:29.040 --> 1:16:31.480
 Flops basically being the floating point operations.

1:16:31.480 --> 1:16:33.320
 And people really love to use flops to say,

1:16:33.320 --> 1:16:36.200
 this model is like really computationally heavy,

1:16:36.200 --> 1:16:39.000
 or like our model is computationally cheap and so on.

1:16:39.000 --> 1:16:41.880
 Now it turns out that flops are really not a good indicator

1:16:41.880 --> 1:16:43.840
 of how well a particular network is,

1:16:43.840 --> 1:16:45.960
 like how efficient it is really.

1:16:45.960 --> 1:16:49.120
 And what a better indicator is, is the activation

1:16:49.120 --> 1:16:52.160
 or the memory that is being used by this particular model.

1:16:52.160 --> 1:16:55.000
 And so designing, like one of the key findings

1:16:55.000 --> 1:16:57.400
 from this paper was basically that you need to design

1:16:57.400 --> 1:17:00.160
 network families or neural network architectures

1:17:00.160 --> 1:17:02.800
 that are actually very efficient in the memory space as well,

1:17:02.800 --> 1:17:04.840
 not just in terms of pure flops.

1:17:04.840 --> 1:17:07.600
 So RegNet is basically a network architecture family

1:17:07.600 --> 1:17:10.280
 that came out of this paper that is particularly good

1:17:10.280 --> 1:17:13.600
 at both flops and the sort of memory required for it.

1:17:13.600 --> 1:17:15.800
 And of course it builds upon like earlier work,

1:17:15.800 --> 1:17:18.640
 like ResNet being like the sort of more popular inspiration

1:17:18.640 --> 1:17:20.440
 for it, where you have residual connections.

1:17:20.440 --> 1:17:22.440
 But one of the things in this work is basically

1:17:22.440 --> 1:17:25.120
 they also use like squeeze excitation blocks.

1:17:25.120 --> 1:17:27.120
 So it's a lot of nice sort of technical innovation

1:17:27.120 --> 1:17:28.760
 in all of this from prior work,

1:17:28.760 --> 1:17:31.440
 and a lot of the ingenuity of these particular authors

1:17:31.440 --> 1:17:34.160
 in how to combine these multiple building blocks.

1:17:34.160 --> 1:17:36.880
 But the key constraint was optimize for both flops

1:17:36.880 --> 1:17:38.360
 and memory when you're basically doing this,

1:17:38.360 --> 1:17:39.600
 don't just look at flops.

1:17:39.600 --> 1:17:42.360
 And that allows you to what have a,

1:17:42.360 --> 1:17:47.320
 sort of have very large networks through this process,

1:17:47.320 --> 1:17:51.280
 can optimize for low, like for efficiency, for low memory.

1:17:51.280 --> 1:17:53.600
 Also in just in terms of pure hardware,

1:17:53.600 --> 1:17:55.880
 they fit very well on GPU memory.

1:17:55.880 --> 1:17:57.920
 So they can be like really powerful neural network

1:17:57.920 --> 1:18:00.200
 architectures with lots of parameters, lots of flops,

1:18:00.200 --> 1:18:02.760
 but also because they're like efficient in terms of

1:18:02.760 --> 1:18:04.040
 the amount of memory that they're using,

1:18:04.040 --> 1:18:06.600
 you can actually fit a lot of these on like a,

1:18:06.600 --> 1:18:09.600
 you can fit a very large model on a single GPU for example.

1:18:09.600 --> 1:18:14.280
 Would you say that the choice of architecture

1:18:14.280 --> 1:18:17.640
 matters more than the choice of maybe data augmentation

1:18:17.640 --> 1:18:18.560
 techniques?

1:18:18.560 --> 1:18:21.720
 Is there a possibility to say what matters more?

1:18:21.720 --> 1:18:24.400
 You kind of imply that you can probably go really far

1:18:24.400 --> 1:18:27.600
 with just using basic conv nuts.

1:18:27.600 --> 1:18:30.600
 All right, I think like data and data augmentation,

1:18:30.600 --> 1:18:33.280
 the algorithm being used for the self supervised training

1:18:33.280 --> 1:18:36.400
 matters a lot more than the particular kind of architecture.

1:18:36.400 --> 1:18:37.680
 With different types of architecture,

1:18:37.680 --> 1:18:40.320
 you will get different like properties in the resulting

1:18:40.320 --> 1:18:41.720
 sort of representation.

1:18:41.720 --> 1:18:44.640
 But really, I mean, the secret sauce is in the augmentation

1:18:44.640 --> 1:18:47.080
 and the algorithm being used to train them.

1:18:47.080 --> 1:18:49.240
 The architectures, I mean, at this point,

1:18:49.240 --> 1:18:51.680
 a lot of them perform very similarly,

1:18:51.680 --> 1:18:53.840
 depending on like the particular task that you care about,

1:18:53.840 --> 1:18:56.400
 they have certain advantages and disadvantages.

1:18:56.400 --> 1:18:58.680
 Is there something interesting to be said about what it

1:18:58.680 --> 1:19:01.920
 takes with Sears to train a giant neural network?

1:19:01.920 --> 1:19:04.160
 You're talking about a huge amount of data,

1:19:04.160 --> 1:19:05.800
 a huge neural network.

1:19:05.800 --> 1:19:08.280
 Is there something interesting to be said of how to

1:19:08.280 --> 1:19:11.280
 effectively train something like that fast?

1:19:11.280 --> 1:19:13.000
 Lots of GPUs.

1:19:13.000 --> 1:19:13.840
 Okay.

1:19:15.480 --> 1:19:18.800
 I mean, so the model was like a billion parameters.

1:19:18.800 --> 1:19:20.840
 And it was trained on a billion images.

1:19:20.840 --> 1:19:23.320
 So if like, basically the same number of parameters

1:19:23.320 --> 1:19:26.160
 as the number of images, and it took a while.

1:19:26.160 --> 1:19:28.600
 I don't remember the exact number, it's in the paper,

1:19:28.600 --> 1:19:29.600
 but it took a while.

1:19:31.840 --> 1:19:34.640
 I guess I'm trying to get at is,

1:19:34.640 --> 1:19:38.680
 when you're thinking of scaling this kind of thing,

1:19:38.680 --> 1:19:42.600
 I mean, one of the exciting possibilities of self

1:19:42.600 --> 1:19:45.920
 supervised learning is the several orders of magnitude

1:19:45.920 --> 1:19:49.000
 scaling of everything, both the neural network

1:19:49.000 --> 1:19:50.920
 and the size of the data.

1:19:50.920 --> 1:19:52.600
 And so the question is,

1:19:52.600 --> 1:19:56.520
 do you think there's some interesting tricks to do large

1:19:56.520 --> 1:19:57.880
 scale distributed compute,

1:19:57.880 --> 1:20:00.920
 or is that really outside of even deep learning?

1:20:00.920 --> 1:20:04.360
 That's more about like hardware engineering.

1:20:04.360 --> 1:20:07.240
 I think more and more there is like this,

1:20:07.240 --> 1:20:10.160
 a lot of like systems are designed,

1:20:10.160 --> 1:20:11.400
 basically taking into account

1:20:11.400 --> 1:20:12.520
 the machine learning needs, right?

1:20:12.520 --> 1:20:14.760
 So because whenever you're doing this kind of

1:20:14.760 --> 1:20:17.040
 distributed training, there is a lot of intercommunication

1:20:17.040 --> 1:20:17.880
 between nodes.

1:20:17.880 --> 1:20:20.680
 So like gradients or the model parameters are being passed.

1:20:20.680 --> 1:20:22.840
 So you really want to minimize communication costs

1:20:22.840 --> 1:20:25.280
 when you really want to scale these models up.

1:20:25.280 --> 1:20:29.240
 You want basically to be able to do as much,

1:20:29.240 --> 1:20:31.520
 like as limited amount of communication as possible.

1:20:31.520 --> 1:20:33.320
 So currently like a dominant paradigm

1:20:33.320 --> 1:20:35.160
 is synchronized sort of training.

1:20:35.160 --> 1:20:38.520
 So essentially after every sort of gradient step,

1:20:38.520 --> 1:20:41.240
 all you basically have like a synchronization step

1:20:41.240 --> 1:20:43.440
 between all the sort of compute chips

1:20:43.440 --> 1:20:44.800
 that you're going on with.

1:20:45.720 --> 1:20:47.880
 I think asynchronous training was popular,

1:20:47.880 --> 1:20:50.440
 but it doesn't seem to perform as well.

1:20:50.440 --> 1:20:53.400
 But in general, I think that's sort of the,

1:20:53.400 --> 1:20:55.320
 I guess it's outside my scope as well.

1:20:55.320 --> 1:21:00.000
 But the main thing is like minimize the amount of

1:21:00.000 --> 1:21:01.960
 synchronization steps that you have.

1:21:01.960 --> 1:21:04.680
 That has been the key takeaway, at least in my experience.

1:21:04.680 --> 1:21:06.600
 The others I have no idea about, how to design the chip.

1:21:06.600 --> 1:21:11.200
 Yeah, there's very few things that I see Jim Keller's eyes

1:21:11.200 --> 1:21:14.200
 light up as much as talking about giant computers doing

1:21:15.360 --> 1:21:18.040
 like that fast communication that you're talking to well

1:21:18.040 --> 1:21:21.240
 when they're training machine learning systems.

1:21:21.240 --> 1:21:26.240
 What is VSSL, V I S S L, the PyTorch based SSL library?

1:21:27.880 --> 1:21:30.120
 What are the use cases that you might have?

1:21:30.120 --> 1:21:33.040
 VSSL basically was born out of a lot of us at Facebook

1:21:33.040 --> 1:21:35.120
 are doing the self supervised learning research.

1:21:35.120 --> 1:21:38.800
 So it's a common framework in which we have like a lot of

1:21:38.800 --> 1:21:41.720
 self supervised learning methods implemented for vision.

1:21:41.720 --> 1:21:45.920
 It's also, it has in itself like a benchmark of tasks

1:21:45.920 --> 1:21:48.800
 that you can evaluate the self supervised representations on.

1:21:48.800 --> 1:21:51.640
 So the use case for it is basically for anyone who's either

1:21:51.640 --> 1:21:53.760
 trying to evaluate their self supervised model

1:21:53.760 --> 1:21:56.000
 or train their self supervised model,

1:21:56.000 --> 1:21:57.800
 or a researcher who's trying to build

1:21:57.800 --> 1:21:59.240
 a new self supervised technique.

1:21:59.240 --> 1:22:01.520
 So it's basically supposed to be all of these things.

1:22:01.520 --> 1:22:04.480
 So as a researcher before VSSL, for example,

1:22:04.480 --> 1:22:06.960
 or like when we started doing this work fairly seriously

1:22:06.960 --> 1:22:09.960
 at Facebook, it was very hard for us to go and implement

1:22:09.960 --> 1:22:11.880
 every self supervised learning model,

1:22:11.880 --> 1:22:14.560
 test it out in a like sort of consistent manner.

1:22:14.560 --> 1:22:16.440
 The experimental setup was very different

1:22:16.440 --> 1:22:18.160
 across different groups.

1:22:18.160 --> 1:22:20.440
 Even when someone said that they were reporting

1:22:20.440 --> 1:22:23.200
 image net accuracy, it could mean lots of different things.

1:22:23.200 --> 1:22:25.400
 So with VSSL, we tried to really sort of standardize that

1:22:25.400 --> 1:22:26.400
 as much as possible.

1:22:26.400 --> 1:22:28.280
 And there was a paper like we did in 2019

1:22:28.280 --> 1:22:29.800
 just about benchmarking.

1:22:29.800 --> 1:22:32.880
 And so VSSL basically builds upon a lot of this kind of work

1:22:32.880 --> 1:22:35.160
 that we did about like benchmarking.

1:22:35.160 --> 1:22:37.200
 And then every time we try to like,

1:22:37.200 --> 1:22:39.000
 we come up with a self supervised learning method,

1:22:39.000 --> 1:22:41.240
 a lot of us try to push that into VSSL as well,

1:22:41.240 --> 1:22:43.480
 just so that it basically is like the central piece

1:22:43.480 --> 1:22:46.400
 where a lot of these methods can reside.

1:22:46.400 --> 1:22:49.240
 Just out of curiosity, people may be,

1:22:49.240 --> 1:22:52.040
 so certainly outside of Facebook, but just researchers,

1:22:52.040 --> 1:22:54.960
 or just even people that know how to program in Python

1:22:54.960 --> 1:22:58.680
 and know how to use PyTorch, what would be the use case?

1:22:58.680 --> 1:23:01.360
 What would be a fun thing to play around with VSSL on?

1:23:01.360 --> 1:23:04.320
 Like what's a fun thing to play around

1:23:04.320 --> 1:23:07.960
 with self supervised learning on, would you say?

1:23:07.960 --> 1:23:09.800
 Is there a good Hello World program?

1:23:09.800 --> 1:23:14.640
 Like is it always about big size that's important to have,

1:23:14.640 --> 1:23:18.880
 or is there fun little smaller case playgrounds

1:23:18.880 --> 1:23:19.760
 to play around with?

1:23:19.760 --> 1:23:22.440
 So we're trying to like push something towards that.

1:23:22.440 --> 1:23:24.360
 I think there are a few setups out there,

1:23:24.360 --> 1:23:26.840
 but nothing like super standard on the smaller scale.

1:23:26.840 --> 1:23:29.320
 I mean, ImageNet in itself is actually pretty big also.

1:23:29.320 --> 1:23:31.440
 So that is not something

1:23:31.440 --> 1:23:33.520
 which is like feasible for a lot of people.

1:23:33.520 --> 1:23:34.880
 But we are trying to like push up

1:23:34.880 --> 1:23:36.400
 with like smaller sort of use cases.

1:23:36.400 --> 1:23:39.000
 The thing is, at a smaller scale,

1:23:39.000 --> 1:23:40.320
 a lot of the observations

1:23:40.320 --> 1:23:41.800
 or a lot of the algorithms that work

1:23:41.800 --> 1:23:43.760
 don't necessarily translate into the medium

1:23:43.760 --> 1:23:45.000
 or the larger scale.

1:23:45.000 --> 1:23:46.160
 So it's really tricky to come up

1:23:46.160 --> 1:23:47.480
 with a good small scale setup

1:23:47.480 --> 1:23:49.160
 where a lot of your empirical observations

1:23:49.160 --> 1:23:51.560
 will really translate to the other setup.

1:23:51.560 --> 1:23:53.280
 So it's been really challenging.

1:23:53.280 --> 1:23:54.920
 I've been trying to do that for a little bit as well

1:23:54.920 --> 1:23:56.880
 because it does take time to train stuff on ImageNet.

1:23:56.880 --> 1:23:59.880
 It does take time to train on like more images,

1:23:59.880 --> 1:24:02.240
 but pretty much every time I've tried to do that,

1:24:02.240 --> 1:24:03.080
 it's been unsuccessful

1:24:03.080 --> 1:24:04.480
 because all the observations I draw

1:24:04.480 --> 1:24:07.440
 from my set of experiments on a smaller data set

1:24:07.440 --> 1:24:09.440
 don't translate into ImageNet

1:24:09.440 --> 1:24:11.760
 or like don't translate into another sort of data set.

1:24:11.760 --> 1:24:14.240
 So it's been hard for us to figure this one out,

1:24:14.240 --> 1:24:15.760
 but it's an important problem.

1:24:15.760 --> 1:24:17.960
 So there's this really interesting idea

1:24:17.960 --> 1:24:20.840
 of learning across multiple modalities.

1:24:20.840 --> 1:24:25.840
 You have a CVPR 2021 best paper candidate

1:24:26.400 --> 1:24:29.280
 titled audio visual instance discrimination

1:24:29.280 --> 1:24:31.440
 with cross modal agreement.

1:24:31.440 --> 1:24:33.880
 What are the key results, insights in this paper

1:24:33.880 --> 1:24:35.240
 and what can you say in general

1:24:35.240 --> 1:24:37.640
 about the promise and power of multimodal learning?

1:24:37.640 --> 1:24:40.000
 For this paper, it actually came as a little bit

1:24:40.000 --> 1:24:41.960
 of a shock to me at how well it worked.

1:24:41.960 --> 1:24:44.160
 So I can describe what the problem set up was.

1:24:44.160 --> 1:24:46.560
 So it's been used in the past by lots of folks

1:24:46.560 --> 1:24:48.400
 like for example, Andrew Owens from MIT,

1:24:48.400 --> 1:24:49.960
 Alyosha Efros from Berkeley,

1:24:49.960 --> 1:24:51.160
 Andrew Zisserman from Oxford.

1:24:51.160 --> 1:24:52.200
 So a lot of these people have been

1:24:52.200 --> 1:24:53.840
 sort of showing results in this.

1:24:53.840 --> 1:24:55.520
 Of course, I was aware of this result,

1:24:55.520 --> 1:24:58.600
 but I wasn't really sure how well it would work in practice

1:24:58.600 --> 1:25:00.600
 for like other sort of downstream tasks.

1:25:00.600 --> 1:25:02.440
 So the results kept getting better.

1:25:02.440 --> 1:25:04.200
 And I wasn't sure if like a lot of our insights

1:25:04.200 --> 1:25:05.920
 from self supervised learning would translate

1:25:05.920 --> 1:25:08.320
 into this multimodal learning problem.

1:25:08.320 --> 1:25:11.880
 So multimodal learning is when you have like,

1:25:12.880 --> 1:25:14.280
 when you have multiple modalities.

1:25:14.280 --> 1:25:15.680
 That's not even cool.

1:25:15.680 --> 1:25:19.400
 Okay, so the particular modalities

1:25:19.400 --> 1:25:22.040
 that we worked on in this work were audio and video.

1:25:22.040 --> 1:25:23.920
 So the idea was basically, if you have a video,

1:25:23.920 --> 1:25:25.880
 you have its corresponding audio track.

1:25:25.880 --> 1:25:27.560
 And you want to use both of these signals,

1:25:27.560 --> 1:25:29.280
 the audio signal and the video signal

1:25:29.280 --> 1:25:31.280
 to learn a good representation for video

1:25:31.280 --> 1:25:32.720
 and good representation for audio.

1:25:32.720 --> 1:25:33.720
 Like this podcast.

1:25:33.720 --> 1:25:35.480
 Like this podcast, exactly.

1:25:35.480 --> 1:25:38.160
 So what we did in this work was basically train

1:25:38.160 --> 1:25:39.400
 two different neural networks,

1:25:39.400 --> 1:25:41.960
 one on the video signal, one on the audio signal.

1:25:41.960 --> 1:25:43.800
 And what we wanted is basically the features

1:25:43.800 --> 1:25:45.400
 that we get from both of these neural networks

1:25:45.400 --> 1:25:46.800
 should be similar.

1:25:46.800 --> 1:25:48.720
 So it should basically be able to produce

1:25:48.720 --> 1:25:51.120
 the same kinds of features from the video

1:25:51.120 --> 1:25:53.240
 and the same kinds of features from the audio.

1:25:53.240 --> 1:25:54.280
 Now, why is this useful?

1:25:54.280 --> 1:25:56.680
 Well, for a lot of these objects that we have,

1:25:56.680 --> 1:25:58.280
 there is a characteristic sound, right?

1:25:58.280 --> 1:25:59.520
 So trains, when they go by,

1:25:59.520 --> 1:26:00.760
 they make a particular kind of sound.

1:26:00.760 --> 1:26:02.480
 Boats make a particular kind of sound.

1:26:02.480 --> 1:26:03.840
 People, when they're jumping around,

1:26:03.840 --> 1:26:06.240
 will like shout, whatever.

1:26:06.240 --> 1:26:07.280
 Bananas don't make a sound.

1:26:07.280 --> 1:26:09.400
 So where you can't learn anything about bananas there.

1:26:09.400 --> 1:26:11.640
 Or when humans mentioned bananas.

1:26:11.640 --> 1:26:13.520
 Well, yes, when they say the word banana, then.

1:26:13.520 --> 1:26:15.080
 So you can't trust basically anything

1:26:15.080 --> 1:26:17.120
 that comes out of a human's mouth as a source,

1:26:17.120 --> 1:26:19.040
 that source of audio is useless.

1:26:19.040 --> 1:26:20.640
 The typical use case is basically like,

1:26:20.640 --> 1:26:22.440
 for example, someone playing a musical instrument.

1:26:22.440 --> 1:26:24.720
 So guitars have a particular kind of sound and so on.

1:26:24.720 --> 1:26:27.120
 So because a lot of these things are correlated,

1:26:27.120 --> 1:26:28.480
 the idea in multimodal learning

1:26:28.480 --> 1:26:30.160
 is to take these two kinds of modalities,

1:26:30.160 --> 1:26:33.160
 video and audio, and learn a common embedding space,

1:26:33.160 --> 1:26:35.240
 a common feature space where both of these

1:26:35.240 --> 1:26:38.560
 related modalities can basically be close together.

1:26:38.560 --> 1:26:40.600
 And again, you use contrastive learning for this.

1:26:40.600 --> 1:26:43.360
 So in contrastive learning, basically the video

1:26:43.360 --> 1:26:45.520
 and the corresponding audio are positives.

1:26:45.520 --> 1:26:48.200
 And you can take any other video or any other audio

1:26:48.200 --> 1:26:49.840
 and that becomes a negative.

1:26:49.840 --> 1:26:51.000
 And so basically that's it.

1:26:51.000 --> 1:26:53.720
 It's just a simple application of contrastive learning.

1:26:53.720 --> 1:26:55.920
 The main sort of finding from this work for us

1:26:56.840 --> 1:26:58.680
 was basically that you can actually learn

1:26:58.680 --> 1:27:00.760
 very, very powerful feature representations,

1:27:00.760 --> 1:27:02.840
 very, very powerful video representations.

1:27:02.840 --> 1:27:05.400
 So you can learn the sort of video network

1:27:05.400 --> 1:27:07.480
 that we ended up learning can actually be used

1:27:07.480 --> 1:27:11.000
 for downstream, for example, recognizing human actions

1:27:11.000 --> 1:27:14.440
 or recognizing different types of sounds, for example.

1:27:14.440 --> 1:27:17.160
 So this was sort of the key finding.

1:27:17.160 --> 1:27:20.200
 Can you give kind of an example of a human action

1:27:20.200 --> 1:27:23.400
 or like just so we can build up intuition

1:27:23.400 --> 1:27:24.360
 of what kind of thing?

1:27:24.360 --> 1:27:26.880
 Right, so there is this data set called kinetics,

1:27:26.880 --> 1:27:28.640
 for example, which has like 400 different types

1:27:28.640 --> 1:27:29.480
 of human actions.

1:27:29.480 --> 1:27:32.880
 So people jumping, people doing different kinds of sports

1:27:32.880 --> 1:27:34.240
 or different types of swimming.

1:27:34.240 --> 1:27:37.600
 So like different strokes and swimming, golf and so on.

1:27:37.600 --> 1:27:39.640
 So there are like just different types of actions

1:27:39.640 --> 1:27:40.560
 right there.

1:27:40.560 --> 1:27:42.600
 And the point is this kind of video network

1:27:42.600 --> 1:27:44.360
 that you learn in a self supervised way

1:27:44.360 --> 1:27:46.920
 can be used very easily to kind of recognize

1:27:46.920 --> 1:27:48.880
 these different types of actions.

1:27:48.880 --> 1:27:50.440
 It can also be used for recognizing

1:27:50.440 --> 1:27:51.760
 different types of objects.

1:27:53.120 --> 1:27:54.760
 And what we did is we tried to visualize

1:27:54.760 --> 1:27:56.080
 whether the network can figure out

1:27:56.080 --> 1:27:57.880
 where the sound is coming from.

1:27:57.880 --> 1:27:59.840
 So basically, give it a video

1:27:59.840 --> 1:28:03.000
 and basically play say of a person just strumming a guitar,

1:28:03.000 --> 1:28:04.760
 but of course, there is no audio in this.

1:28:04.760 --> 1:28:07.160
 And now you give it this sound of a guitar.

1:28:07.160 --> 1:28:08.880
 And you ask like basically try to visualize

1:28:08.880 --> 1:28:12.520
 where the network thinks the sound is coming from.

1:28:12.520 --> 1:28:14.560
 And that can kind of basically draw like

1:28:14.560 --> 1:28:15.400
 when you visualize it,

1:28:15.400 --> 1:28:17.480
 you can see that it's basically focusing on the guitar.

1:28:17.480 --> 1:28:18.320
 Yeah, that's surreal.

1:28:18.320 --> 1:28:20.160
 And the same thing, for example,

1:28:20.160 --> 1:28:21.480
 for certain people's voices,

1:28:21.480 --> 1:28:22.920
 like famous celebrities voices,

1:28:22.920 --> 1:28:26.040
 it can actually figure out where their mouth is.

1:28:26.040 --> 1:28:28.600
 So it can actually distinguish different people's voices,

1:28:28.600 --> 1:28:30.480
 for example, a little bit as well.

1:28:30.480 --> 1:28:33.680
 Without that ever being annotated in any way.

1:28:33.680 --> 1:28:35.520
 Right, so this is all what it had discovered.

1:28:35.520 --> 1:28:38.200
 We never pointed out that this is a guitar

1:28:38.200 --> 1:28:40.080
 and this is the kind of sound it produces.

1:28:40.080 --> 1:28:41.520
 It can actually naturally figure that out

1:28:41.520 --> 1:28:44.200
 because it's seen so many correlations of this sound

1:28:44.200 --> 1:28:46.680
 coming with this kind of like an object

1:28:46.680 --> 1:28:49.040
 that it basically learns to associate this sound

1:28:49.040 --> 1:28:50.000
 with this kind of an object.

1:28:50.000 --> 1:28:52.760
 Yeah, that's really fascinating, right?

1:28:52.760 --> 1:28:53.600
 That's really interesting.

1:28:53.600 --> 1:28:55.200
 So the idea with this kind of network

1:28:55.200 --> 1:28:57.920
 is then you then fine tune it for a particular task.

1:28:57.920 --> 1:29:01.880
 So this is forming like a really good knowledge base

1:29:01.880 --> 1:29:04.320
 within a neural network based on which you could then

1:29:04.320 --> 1:29:07.720
 the train a little bit more to accomplish a specific task.

1:29:07.720 --> 1:29:11.680
 Well, so you don't need a lot of videos of humans

1:29:11.680 --> 1:29:12.800
 doing actions annotated.

1:29:12.800 --> 1:29:16.040
 You can just use a few of them to basically get your.

1:29:16.040 --> 1:29:18.520
 How much insight do you draw from the fact

1:29:18.520 --> 1:29:22.560
 that it can figure out where the sound is coming from?

1:29:23.440 --> 1:29:26.160
 I'm trying to see, so that's kind of very,

1:29:26.160 --> 1:29:28.520
 it's very CVPR beautiful, right?

1:29:28.520 --> 1:29:30.000
 It's a cool little insight.

1:29:30.000 --> 1:29:33.000
 I wonder how profound that is.

1:29:33.000 --> 1:29:38.000
 Does it speak to the idea that multiple modalities

1:29:39.320 --> 1:29:44.120
 are somehow much bigger than the sum of their parts?

1:29:44.120 --> 1:29:48.000
 Or is it really, really useful to have multiple modalities?

1:29:48.000 --> 1:29:50.640
 Or is it just that cool thing that there's parts

1:29:50.640 --> 1:29:55.640
 of our world that can be revealed like effectively

1:29:57.320 --> 1:29:58.400
 through multiple modalities,

1:29:58.400 --> 1:30:01.200
 but most of it is really all about vision

1:30:01.200 --> 1:30:03.880
 or about one of the modalities.

1:30:03.880 --> 1:30:07.760
 I would say a little tending more towards the second part.

1:30:07.760 --> 1:30:10.680
 So most of it can be sort of figured out with one modality,

1:30:10.680 --> 1:30:13.160
 but having an extra modality always helps you.

1:30:13.160 --> 1:30:14.560
 So in this case, for example,

1:30:14.560 --> 1:30:17.720
 like one thing is when you're,

1:30:17.720 --> 1:30:19.400
 if you observe someone cutting something

1:30:19.400 --> 1:30:21.960
 and you don't have any sort of sound there,

1:30:21.960 --> 1:30:25.080
 whether it's an apple or whether it's an onion,

1:30:25.080 --> 1:30:26.720
 it's very hard to figure that out.

1:30:26.720 --> 1:30:28.240
 But if you hear someone cutting it,

1:30:28.240 --> 1:30:30.760
 it's very easy to figure it out because apples and onions

1:30:30.760 --> 1:30:33.560
 make a very different kind of characteristics

1:30:33.560 --> 1:30:34.840
 on when they're cut.

1:30:34.840 --> 1:30:36.880
 So you really figure this out based on audio,

1:30:36.880 --> 1:30:38.240
 it's much easier.

1:30:38.240 --> 1:30:40.040
 So your life will become much easier

1:30:40.040 --> 1:30:42.280
 when you have access to different kinds of modalities.

1:30:42.280 --> 1:30:45.040
 And the other thing is, so I like to relate it in this way,

1:30:45.040 --> 1:30:46.360
 it may be like completely wrong,

1:30:46.360 --> 1:30:49.320
 but the distributional hypothesis in NLP,

1:30:49.320 --> 1:30:53.040
 where context basically gives kind of meaning to that word,

1:30:53.040 --> 1:30:55.040
 sound kind of does that too.

1:30:55.040 --> 1:30:57.160
 So if you have the same sound,

1:30:57.160 --> 1:30:59.840
 so that's the same context across different videos,

1:30:59.840 --> 1:31:03.000
 you're very likely to be observing the same kind of concept.

1:31:03.000 --> 1:31:04.280
 So that's the kind of reason

1:31:04.280 --> 1:31:06.440
 why it figures out the guitar thing, right?

1:31:06.440 --> 1:31:09.760
 It observed the same sound across multiple different videos

1:31:09.760 --> 1:31:11.880
 and it figures out maybe this is the common factor

1:31:11.880 --> 1:31:13.240
 that's actually doing it.

1:31:13.240 --> 1:31:17.440
 I wonder, I used to have this argument with my dad a bunch

1:31:17.440 --> 1:31:19.760
 for creating general intelligence,

1:31:19.760 --> 1:31:22.840
 whether smell is an important,

1:31:22.840 --> 1:31:25.480
 like if that's important sensory information,

1:31:25.480 --> 1:31:27.560
 mostly we're talking about like falling in love

1:31:27.560 --> 1:31:30.000
 with an AI system and for him,

1:31:30.000 --> 1:31:31.440
 smell and touch are important.

1:31:31.440 --> 1:31:33.880
 And I was arguing that it's not at all.

1:31:33.880 --> 1:31:35.320
 It's important, it's nice and everything,

1:31:35.320 --> 1:31:38.400
 but like you can fall in love with just language really,

1:31:38.400 --> 1:31:41.400
 but a voice is very powerful and vision is next

1:31:41.400 --> 1:31:43.880
 and smell is not that important.

1:31:43.880 --> 1:31:46.880
 Can I ask you about this process of active learning?

1:31:46.880 --> 1:31:49.200
 You mentioned interactivity.

1:31:49.200 --> 1:31:50.040
 Right.

1:31:50.040 --> 1:31:52.920
 Is there some value

1:31:52.920 --> 1:31:57.040
 within the self supervised learning context

1:31:57.040 --> 1:32:02.040
 to select parts of the data in intelligent ways

1:32:02.280 --> 1:32:06.880
 such that they would most benefit the learning process?

1:32:06.880 --> 1:32:07.720
 So I think so.

1:32:07.720 --> 1:32:10.320
 I mean, I know I'm talking to an active learning fan here,

1:32:10.320 --> 1:32:12.640
 so of course I know the answer.

1:32:12.640 --> 1:32:14.000
 First you were talking bananas

1:32:14.000 --> 1:32:15.600
 and now you're talking about active learning.

1:32:15.600 --> 1:32:16.720
 I love it.

1:32:16.720 --> 1:32:18.800
 I think Yannakun told me that active learning

1:32:18.800 --> 1:32:20.480
 is not that interesting.

1:32:20.480 --> 1:32:24.400
 I think back then I didn't want to argue with him too much,

1:32:24.400 --> 1:32:26.040
 but when we talk again,

1:32:26.040 --> 1:32:28.400
 we're gonna spend three hours arguing about active learning.

1:32:28.400 --> 1:32:32.760
 My sense was you can go extremely far with active learning,

1:32:32.760 --> 1:32:34.920
 perhaps farther than anything else.

1:32:34.920 --> 1:32:37.960
 Like to me, there's this kind of intuition

1:32:37.960 --> 1:32:40.840
 that similar to data augmentation,

1:32:40.840 --> 1:32:44.160
 you can get a lot from the data,

1:32:45.280 --> 1:32:50.280
 from intelligent optimized usage of the data.

1:32:50.480 --> 1:32:53.200
 I'm trying to speak generally in such a way

1:32:53.200 --> 1:32:55.280
 that includes data augmentation

1:32:55.280 --> 1:32:57.040
 and active learning,

1:32:57.040 --> 1:32:59.880
 that there's something about maybe interactive exploration

1:32:59.880 --> 1:33:03.640
 of the data that at least is part

1:33:03.640 --> 1:33:07.160
 of the solution to intelligence, like an important part.

1:33:07.160 --> 1:33:08.200
 I don't know what your thoughts are

1:33:08.200 --> 1:33:09.320
 on active learning in general.

1:33:09.320 --> 1:33:10.840
 I actually really like active learning.

1:33:10.840 --> 1:33:14.200
 So back in the day we did this largely ignored CVPR paper

1:33:14.200 --> 1:33:16.520
 called learning by asking questions.

1:33:16.520 --> 1:33:18.240
 So the idea was basically you would train an agent

1:33:18.240 --> 1:33:20.120
 that would ask a question about the image.

1:33:20.120 --> 1:33:21.520
 It would get an answer

1:33:21.520 --> 1:33:23.360
 and basically then it would update itself.

1:33:23.360 --> 1:33:24.360
 It would see the next image.

1:33:24.360 --> 1:33:26.800
 It would decide what's the next hardest question

1:33:26.800 --> 1:33:28.760
 that I can ask to learn the most.

1:33:28.760 --> 1:33:31.320
 And the idea was basically because it was being smart

1:33:31.320 --> 1:33:33.480
 about the kinds of questions it was asking,

1:33:33.480 --> 1:33:35.080
 it would learn in fewer samples.

1:33:35.080 --> 1:33:37.880
 It would be more efficient at using data.

1:33:37.880 --> 1:33:39.400
 And we did find to some extent

1:33:39.400 --> 1:33:42.040
 that it was actually better than randomly asking questions.

1:33:42.040 --> 1:33:43.480
 Kind of weird thing about active learning

1:33:43.480 --> 1:33:45.160
 is it's also a chicken and egg problem

1:33:45.160 --> 1:33:47.120
 because when you look at an image,

1:33:47.120 --> 1:33:48.640
 to ask a good question about the image,

1:33:48.640 --> 1:33:50.880
 you need to understand something about the image.

1:33:50.880 --> 1:33:53.440
 You can't ask a completely arbitrarily random question.

1:33:53.440 --> 1:33:55.480
 It may not even apply to that particular image.

1:33:55.480 --> 1:33:57.600
 So there is some amount of understanding or knowledge

1:33:57.600 --> 1:33:59.160
 that basically keeps getting built

1:33:59.160 --> 1:34:01.280
 when you're doing active learning.

1:34:01.280 --> 1:34:04.560
 So I think active learning by itself is really good.

1:34:04.560 --> 1:34:07.240
 And the main thing we need to figure out is basically

1:34:07.240 --> 1:34:09.600
 how do we come up with a technique

1:34:09.600 --> 1:34:13.320
 to first model what the model knows

1:34:13.320 --> 1:34:16.000
 and also model what the model does not know.

1:34:16.000 --> 1:34:18.360
 I think that's the sort of beauty of it.

1:34:18.360 --> 1:34:20.480
 Because when you know that there are certain things

1:34:20.480 --> 1:34:22.120
 that you don't know anything about,

1:34:22.120 --> 1:34:23.640
 asking a question about those concepts

1:34:23.640 --> 1:34:26.480
 is actually going to bring you the most value.

1:34:26.480 --> 1:34:28.360
 And I think that's the sort of key challenge.

1:34:28.360 --> 1:34:29.960
 Now, self supervised learning by itself,

1:34:29.960 --> 1:34:31.480
 like selecting data for it and so on,

1:34:31.480 --> 1:34:32.640
 that's actually really useful.

1:34:32.640 --> 1:34:33.960
 But I think that's a very narrow view

1:34:33.960 --> 1:34:35.080
 of looking at active learning.

1:34:35.080 --> 1:34:36.360
 If you look at it more broadly,

1:34:36.360 --> 1:34:40.040
 it is basically about if the model has a knowledge

1:34:40.040 --> 1:34:41.400
 about N concepts,

1:34:41.400 --> 1:34:43.840
 and it is weak basically about certain things.

1:34:43.840 --> 1:34:45.280
 So it needs to ask questions

1:34:45.280 --> 1:34:46.880
 either to discover new concepts

1:34:46.880 --> 1:34:49.200
 or to basically increase its knowledge

1:34:49.200 --> 1:34:50.400
 about these N concepts.

1:34:50.400 --> 1:34:53.200
 So at that level, it's a very powerful technique.

1:34:53.200 --> 1:34:56.520
 I actually do think it's going to be really useful.

1:34:56.520 --> 1:34:59.040
 Even in like simple things such as like data labeling,

1:34:59.040 --> 1:35:00.240
 it's super useful.

1:35:00.240 --> 1:35:02.920
 So here is like one simple way

1:35:02.920 --> 1:35:04.280
 that you can use active learning.

1:35:04.280 --> 1:35:06.880
 For example, you have your self supervised model,

1:35:06.880 --> 1:35:08.760
 which is very good at predicting similarities

1:35:08.760 --> 1:35:10.760
 and dissimilarities between things.

1:35:10.760 --> 1:35:14.600
 And so if you label a picture as basically say a banana,

1:35:15.480 --> 1:35:17.720
 now you know that all the images

1:35:17.720 --> 1:35:19.200
 that are very similar to this image

1:35:19.200 --> 1:35:21.480
 are also likely to contain bananas.

1:35:21.480 --> 1:35:24.240
 So probably when you want to understand

1:35:24.240 --> 1:35:25.160
 what else is a banana,

1:35:25.160 --> 1:35:26.880
 you're not going to use these other images.

1:35:26.880 --> 1:35:28.160
 You're actually going to use an image

1:35:28.160 --> 1:35:31.120
 that is not completely dissimilar,

1:35:31.120 --> 1:35:32.320
 but somewhere in between,

1:35:32.320 --> 1:35:33.840
 which is not super similar to this image,

1:35:33.840 --> 1:35:35.640
 but not super dissimilar either.

1:35:35.640 --> 1:35:37.120
 And that's going to tell you a lot more

1:35:37.120 --> 1:35:39.520
 about what this concept of a banana is.

1:35:39.520 --> 1:35:41.840
 So that's kind of a heuristic.

1:35:41.840 --> 1:35:46.840
 I wonder if it's possible to also learn ways

1:35:46.840 --> 1:35:50.640
 to discover the most likely,

1:35:50.640 --> 1:35:52.880
 the most beneficial image.

1:35:52.880 --> 1:35:54.920
 So like, so not just looking a thing

1:35:54.920 --> 1:35:58.360
 that's somewhat similar to a banana,

1:35:58.360 --> 1:35:59.920
 but not exactly similar,

1:35:59.920 --> 1:36:03.480
 but have some kind of more complicated learning system,

1:36:03.480 --> 1:36:07.000
 like learned discovering mechanism

1:36:07.000 --> 1:36:09.360
 that tells you what image to look for.

1:36:09.360 --> 1:36:14.240
 Like how, yeah, like actually in a self supervised way,

1:36:14.240 --> 1:36:17.160
 learning strictly a function that says,

1:36:17.160 --> 1:36:20.440
 is this image going to be very useful to me

1:36:20.440 --> 1:36:22.000
 given what I currently know?

1:36:22.000 --> 1:36:23.880
 I think there's a lot of synergy there.

1:36:23.880 --> 1:36:26.600
 It's just, I think, yeah, it's going to be explored.

1:36:27.520 --> 1:36:29.240
 I think very much related to that.

1:36:29.240 --> 1:36:32.280
 I kind of think of what Tesla Autopilot is doing

1:36:33.480 --> 1:36:36.720
 currently as kind of active learning.

1:36:36.720 --> 1:36:39.120
 There's something that Andre Capati and their team

1:36:39.120 --> 1:36:41.440
 are calling a data engine.

1:36:41.440 --> 1:36:45.640
 So you're basically deploying a bunch of instantiations

1:36:45.640 --> 1:36:47.920
 of a neural network into the wild,

1:36:47.920 --> 1:36:50.640
 and they're collecting a bunch of edge cases

1:36:50.640 --> 1:36:53.920
 that are then sent back for annotation for particular,

1:36:53.920 --> 1:36:56.680
 and edge cases as defined as near failure

1:36:56.680 --> 1:36:59.960
 or some weirdness on a particular task

1:36:59.960 --> 1:37:01.400
 that's then sent back.

1:37:01.400 --> 1:37:04.000
 It's that not exactly a banana,

1:37:04.000 --> 1:37:07.200
 but almost the banana cases sent back for annotation.

1:37:07.200 --> 1:37:09.200
 And then there's this loop that keeps going

1:37:09.200 --> 1:37:11.600
 and you keep retraining and retraining.

1:37:11.600 --> 1:37:13.280
 And the active learning step there,

1:37:13.280 --> 1:37:14.800
 or whatever you want to call it,

1:37:14.800 --> 1:37:19.120
 is the cars themselves that are sending you back the data.

1:37:19.120 --> 1:37:20.760
 Like, what the hell happened here?

1:37:20.760 --> 1:37:22.840
 This was weird.

1:37:22.840 --> 1:37:26.440
 What are your thoughts about that sort of deployment

1:37:26.440 --> 1:37:28.240
 of neural networks in the wild?

1:37:28.240 --> 1:37:31.360
 Another way to ask a question from first is your thoughts.

1:37:31.360 --> 1:37:33.840
 And maybe if you want to comment,

1:37:33.840 --> 1:37:36.960
 is there applications for autonomous driving,

1:37:36.960 --> 1:37:40.160
 like computer vision based autonomous driving,

1:37:40.160 --> 1:37:42.040
 applications of self supervised learning

1:37:42.040 --> 1:37:46.080
 in the context of computer vision based autonomous driving?

1:37:47.520 --> 1:37:48.360
 So I think so.

1:37:48.360 --> 1:37:49.560
 I think for self supervised learning

1:37:49.560 --> 1:37:50.800
 to be used in autonomous driving,

1:37:50.800 --> 1:37:51.800
 there are lots of opportunities.

1:37:51.800 --> 1:37:54.880
 I mean, just like pure consistency in predictions

1:37:54.880 --> 1:37:55.840
 is one way, right?

1:37:55.840 --> 1:38:00.280
 So because you have this nice sequence of data

1:38:00.280 --> 1:38:02.320
 that is coming in, a video stream of it,

1:38:02.320 --> 1:38:04.040
 associated of course with the actions

1:38:04.040 --> 1:38:05.240
 that say the car took,

1:38:05.240 --> 1:38:07.640
 you can form a very nice predictive model

1:38:07.640 --> 1:38:08.480
 of what's happening.

1:38:08.480 --> 1:38:11.400
 So for example, like all the way,

1:38:11.400 --> 1:38:14.440
 like one way possibly in which how they're figuring out

1:38:14.440 --> 1:38:15.880
 what data to get labeled is basically

1:38:15.880 --> 1:38:17.440
 through prediction uncertainty, right?

1:38:17.440 --> 1:38:20.360
 So you predict that the car was going to turn right.

1:38:20.360 --> 1:38:21.840
 So this was the action that was going to happen,

1:38:21.840 --> 1:38:23.080
 say in the shadow mode.

1:38:23.080 --> 1:38:24.640
 And now the driver turned left.

1:38:24.640 --> 1:38:27.160
 And this is a really big surprise.

1:38:27.160 --> 1:38:30.120
 So basically by forming these good predictive models,

1:38:30.120 --> 1:38:32.840
 you are, I mean, these are kind of self supervised models.

1:38:32.840 --> 1:38:34.600
 Prediction models are basically being trained

1:38:34.600 --> 1:38:36.800
 just by looking at what's going to happen next

1:38:36.800 --> 1:38:38.960
 and asking them to predict what's going to happen next.

1:38:38.960 --> 1:38:40.720
 So I would say this is really like one use

1:38:40.720 --> 1:38:42.320
 of self supervised learning.

1:38:42.320 --> 1:38:43.440
 It's a predictive model

1:38:43.440 --> 1:38:44.680
 and you're learning a predictive model

1:38:44.680 --> 1:38:46.880
 basically just by looking at what data you have.

1:38:46.880 --> 1:38:49.600
 Is there something about that active learning context

1:38:49.600 --> 1:38:53.000
 that you find insights from?

1:38:53.000 --> 1:38:54.760
 Like that kind of deployment of the system,

1:38:54.760 --> 1:38:59.120
 seeing cases where it doesn't perform as you expected

1:38:59.120 --> 1:39:01.000
 and then retraining the system based on that?

1:39:01.000 --> 1:39:03.600
 I think that, I mean, that really resonates with me.

1:39:03.600 --> 1:39:05.560
 It's super smart to do it that way.

1:39:05.560 --> 1:39:08.520
 Because I mean, the thing is with any kind

1:39:08.520 --> 1:39:11.160
 of like practical system, like autonomous driving,

1:39:11.160 --> 1:39:13.040
 there are those edge cases that are the things

1:39:13.040 --> 1:39:14.520
 that are actually the problem, right?

1:39:14.520 --> 1:39:17.440
 I mean, highway driving or like freeway driving

1:39:17.440 --> 1:39:19.120
 has basically been like,

1:39:19.120 --> 1:39:21.120
 there has been a lot of success in that particular part

1:39:21.120 --> 1:39:22.840
 of autonomous driving for a long time.

1:39:22.840 --> 1:39:25.560
 I would say like since the eighties or something.

1:39:25.560 --> 1:39:28.000
 Now the point is all these failure cases

1:39:28.000 --> 1:39:30.600
 are the sort of reason why autonomous driving

1:39:30.600 --> 1:39:33.800
 hasn't become like super, super mainstream and available

1:39:33.800 --> 1:39:35.640
 like in every possible car right now.

1:39:35.640 --> 1:39:38.200
 And so basically by really scaling this problem out

1:39:38.200 --> 1:39:40.440
 by really trying to get all of these edge cases out

1:39:40.440 --> 1:39:41.880
 as quickly as possible,

1:39:41.880 --> 1:39:43.920
 and then just like using those to improve your model,

1:39:43.920 --> 1:39:45.640
 that's super smart.

1:39:45.640 --> 1:39:47.120
 And prediction uncertainty to do that

1:39:47.120 --> 1:39:49.800
 is like one really nice way of doing it.

1:39:49.800 --> 1:39:52.040
 Let me put you on the spot.

1:39:52.040 --> 1:39:55.240
 So we mentioned offline Jitendra,

1:39:55.240 --> 1:39:58.240
 he thinks that the Tesla computer vision approach

1:39:58.240 --> 1:40:00.800
 or really any approach for autonomous driving

1:40:00.800 --> 1:40:02.680
 is very far away.

1:40:02.680 --> 1:40:05.440
 How many years away,

1:40:05.440 --> 1:40:06.960
 if you have to bet all your money on it,

1:40:06.960 --> 1:40:09.600
 are we to solving autonomous driving

1:40:09.600 --> 1:40:12.000
 with this kind of computer vision only

1:40:12.000 --> 1:40:13.600
 machine learning based approach?

1:40:13.600 --> 1:40:15.400
 Okay, so what does solving autonomous driving mean?

1:40:15.400 --> 1:40:17.200
 Does it mean solving it in the US?

1:40:17.200 --> 1:40:18.480
 Does it mean solving it in India?

1:40:18.480 --> 1:40:19.320
 Because I can tell you

1:40:19.320 --> 1:40:21.200
 that very different types of driving happening.

1:40:21.200 --> 1:40:23.800
 Not India, not Russia.

1:40:23.800 --> 1:40:26.200
 In the United States, autonomous,

1:40:26.200 --> 1:40:31.200
 so what solving means is when the car says it has control,

1:40:31.880 --> 1:40:34.040
 it is fully liable.

1:40:34.040 --> 1:40:37.800
 You can go to sleep, it's driving by itself.

1:40:37.800 --> 1:40:39.720
 So this is highway and city driving,

1:40:39.720 --> 1:40:42.280
 but not everywhere, but mostly everywhere.

1:40:42.280 --> 1:40:45.040
 And it's, let's say significantly better,

1:40:45.040 --> 1:40:50.040
 like say five times less accidents than humans.

1:40:50.480 --> 1:40:53.960
 Sufficiently safer such that the public feels

1:40:53.960 --> 1:40:57.960
 like that transition is enticing beneficial

1:40:57.960 --> 1:40:59.480
 both for our safety and financial

1:40:59.480 --> 1:41:01.040
 and all those kinds of things.

1:41:01.040 --> 1:41:02.240
 Okay, so first disclaimer,

1:41:02.240 --> 1:41:04.200
 I'm not an expert in autonomous driving.

1:41:04.200 --> 1:41:05.920
 So let me put it out there.

1:41:05.920 --> 1:41:08.320
 I would say like at least five to 10 years.

1:41:09.360 --> 1:41:11.760
 This would be my guess from now.

1:41:12.920 --> 1:41:14.640
 Yeah, I'm actually very impressed.

1:41:14.640 --> 1:41:16.760
 Like when I sat in a friend's Tesla recently

1:41:16.760 --> 1:41:20.600
 and of course, like looking on that screen,

1:41:20.600 --> 1:41:22.800
 it basically shows all the detections and everything.

1:41:22.800 --> 1:41:24.640
 The car is doing as you're driving by

1:41:24.640 --> 1:41:26.880
 and that's super distracting for me as a person

1:41:26.880 --> 1:41:29.440
 because all I keep looking at is like the bounding boxes

1:41:29.440 --> 1:41:31.760
 in the cars it's tracking and it's really impressive.

1:41:31.760 --> 1:41:34.280
 Like especially when it's raining and it's able to do that,

1:41:34.280 --> 1:41:36.000
 that was the most impressive part for me.

1:41:36.000 --> 1:41:38.520
 It's actually able to get through rain and do that.

1:41:38.520 --> 1:41:41.720
 And one of the reasons why like a lot of us believed

1:41:41.720 --> 1:41:44.040
 and I would put myself in that category

1:41:44.040 --> 1:41:47.680
 is LIDAR based sort of technology for autonomous driving

1:41:47.680 --> 1:41:48.720
 was the key driver, right?

1:41:48.720 --> 1:41:50.960
 So Waymo was using it for the longest time.

1:41:50.960 --> 1:41:53.280
 And Tesla then decided to go this completely other route

1:41:53.280 --> 1:41:55.760
 that we are not going to even use LIDAR.

1:41:55.760 --> 1:41:58.720
 So their initial system I think was camera and radar based

1:41:58.720 --> 1:41:59.640
 and now they're actually moving

1:41:59.640 --> 1:42:02.000
 to a completely like vision based system.

1:42:02.000 --> 1:42:04.640
 And so that was just like, it sounded completely crazy.

1:42:04.640 --> 1:42:07.000
 Like LIDAR is very useful in cases

1:42:07.000 --> 1:42:09.240
 where you have low visibility.

1:42:09.240 --> 1:42:11.720
 Of course it comes with its own set of complications.

1:42:11.720 --> 1:42:15.160
 But now to see that happen in like on a live Tesla

1:42:15.160 --> 1:42:16.960
 that basically just proves everyone wrong

1:42:16.960 --> 1:42:18.120
 I would say in a way.

1:42:18.120 --> 1:42:20.520
 And that's just working really well.

1:42:20.520 --> 1:42:22.720
 I think there were also like a lot of advancements

1:42:22.720 --> 1:42:23.920
 in camera technology.

1:42:23.920 --> 1:42:26.280
 Now there were like, I know at CMU when I was there

1:42:26.280 --> 1:42:27.960
 there was a particular kind of camera

1:42:27.960 --> 1:42:30.040
 that had been developed that was really good

1:42:30.040 --> 1:42:32.760
 at basically low visibility setting.

1:42:32.760 --> 1:42:34.400
 So like lots of snow and lots of rain

1:42:34.400 --> 1:42:37.640
 it could actually still have a very reasonable visibility.

1:42:37.640 --> 1:42:39.360
 And I think there are lots of these kinds of innovations

1:42:39.360 --> 1:42:40.960
 that will happen on the sensor side itself

1:42:40.960 --> 1:42:42.840
 which is actually going to make this very easy

1:42:42.840 --> 1:42:43.840
 in the future.

1:42:43.840 --> 1:42:46.080
 And so maybe that's actually why I'm more optimistic

1:42:46.080 --> 1:42:49.000
 about vision based self, like autonomous driving.

1:42:49.000 --> 1:42:51.960
 I was going to call it self supervised driving, but.

1:42:51.960 --> 1:42:53.520
 Vision based autonomous driving.

1:42:53.520 --> 1:42:55.480
 That's the reason I'm quite optimistic about it

1:42:55.480 --> 1:42:56.640
 because I think there are going to be lots

1:42:56.640 --> 1:42:58.960
 of these advances on the sensor side itself.

1:42:58.960 --> 1:43:00.720
 So acquiring this data

1:43:00.720 --> 1:43:02.640
 we're actually going to get much better about it.

1:43:02.640 --> 1:43:05.080
 And then of course, once we're able to scale out

1:43:05.080 --> 1:43:06.800
 and get all of these edge cases in

1:43:06.800 --> 1:43:08.720
 as like Andre described

1:43:08.720 --> 1:43:11.720
 I think that's going to make us go very far away.

1:43:11.720 --> 1:43:13.560
 Yeah, so it's funny.

1:43:13.560 --> 1:43:16.280
 I'm very much with you on the five to 10 years

1:43:16.280 --> 1:43:17.840
 maybe 10 years

1:43:17.840 --> 1:43:21.760
 but you made it, I'm not sure how you made it sound

1:43:21.760 --> 1:43:23.640
 but for some people that seem

1:43:23.640 --> 1:43:25.360
 that might seem like really far away.

1:43:25.360 --> 1:43:30.360
 And then for other people, it might seem like very close.

1:43:30.440 --> 1:43:32.320
 There's a lot of fundamental questions

1:43:32.320 --> 1:43:36.880
 about how much game theory is in this whole thing.

1:43:36.880 --> 1:43:41.160
 So like, how much is this simply a collision avoidance

1:43:41.160 --> 1:43:45.200
 problem and how much of it is you still interacting

1:43:45.200 --> 1:43:46.960
 with other humans in the scene

1:43:46.960 --> 1:43:48.800
 and you're trying to create an experience

1:43:48.800 --> 1:43:49.640
 that's compelling.

1:43:49.640 --> 1:43:53.080
 So you want to get from point A to point B quickly

1:43:53.080 --> 1:43:55.280
 you want to navigate the scene in a safe way

1:43:55.280 --> 1:43:58.480
 but you also want to show some level of aggression

1:43:58.480 --> 1:44:02.000
 because well, certainly this is why you're screwed in India

1:44:02.000 --> 1:44:03.320
 because you have to show aggression.

1:44:03.320 --> 1:44:04.840
 Or Jersey or New Jersey.

1:44:04.840 --> 1:44:05.680
 Or Jersey, right.

1:44:05.680 --> 1:44:10.680
 So like, or New York or basically any major city

1:44:11.200 --> 1:44:13.240
 but I think it's probably Elon

1:44:13.240 --> 1:44:14.800
 that I talked the most about this

1:44:14.800 --> 1:44:17.720
 which is a surprise to the level of which

1:44:17.720 --> 1:44:20.080
 they're not considering human beings

1:44:20.080 --> 1:44:22.960
 as a huge problem in this, as a source of problem.

1:44:22.960 --> 1:44:27.960
 Like the driving is fundamentally a robot on robot

1:44:29.000 --> 1:44:31.160
 versus the environment problem

1:44:31.160 --> 1:44:33.960
 versus like you can just consider humans

1:44:33.960 --> 1:44:35.160
 not part of the problem.

1:44:35.160 --> 1:44:38.840
 I used to think humans are almost certainly

1:44:38.840 --> 1:44:41.200
 have to be modeled really well.

1:44:41.200 --> 1:44:44.360
 Pedestrians and cyclists and humans inside other cars

1:44:44.360 --> 1:44:46.320
 you have to have like mental models for them.

1:44:46.320 --> 1:44:48.280
 You cannot just see it as objects

1:44:48.280 --> 1:44:50.360
 but more and more it's like the

1:44:51.400 --> 1:44:53.720
 it's the same kind of intuition breaking thing

1:44:53.720 --> 1:44:57.000
 that's self supervised learning does, which is

1:44:57.000 --> 1:44:58.840
 well maybe through the learning

1:44:58.840 --> 1:45:03.840
 you'll get all the human like human information you need.

1:45:04.080 --> 1:45:04.920
 Right?

1:45:04.920 --> 1:45:07.760
 Like maybe you'll get it just with enough data.

1:45:07.760 --> 1:45:09.680
 You don't need to have explicit good models

1:45:09.680 --> 1:45:10.800
 of human behavior.

1:45:10.800 --> 1:45:12.120
 Maybe you get it through the data.

1:45:12.120 --> 1:45:14.640
 So, I mean my skepticism also just knowing

1:45:14.640 --> 1:45:16.360
 a lot of automotive companies

1:45:16.360 --> 1:45:18.600
 and how difficult it is to be innovative.

1:45:18.600 --> 1:45:22.560
 I was skeptical that they would be able at scale

1:45:22.560 --> 1:45:27.400
 to convert the driving scene across the world

1:45:27.400 --> 1:45:30.640
 into digital form such that you can create

1:45:30.640 --> 1:45:33.160
 this data engine at scale.

1:45:33.160 --> 1:45:36.640
 And the fact that Tesla is at least getting there

1:45:36.640 --> 1:45:41.640
 or are already there makes me think that

1:45:41.640 --> 1:45:43.680
 it's now starting to be coupled

1:45:43.680 --> 1:45:47.600
 to this self supervised learning vision

1:45:47.600 --> 1:45:49.840
 which is like if that's gonna work

1:45:49.840 --> 1:45:52.920
 if through purely this process you can get really far

1:45:52.920 --> 1:45:54.880
 then maybe you can solve driving that way.

1:45:54.880 --> 1:45:55.720
 I don't know.

1:45:55.720 --> 1:46:00.000
 I tend to believe we don't give enough credit

1:46:00.000 --> 1:46:05.000
 to the how amazing humans are both at driving

1:46:05.920 --> 1:46:09.360
 and at supervising autonomous systems.

1:46:09.360 --> 1:46:13.200
 And also we don't, this is, I wish we were.

1:46:13.200 --> 1:46:17.120
 I wish there was much more driver sensing inside Teslas

1:46:17.120 --> 1:46:21.200
 and much deeper consideration of human factors

1:46:21.200 --> 1:46:24.680
 like understanding psychology and drowsiness

1:46:24.680 --> 1:46:26.200
 and all those kinds of things

1:46:26.200 --> 1:46:28.720
 when the car does more and more of the work.

1:46:28.720 --> 1:46:32.960
 How to keep utilizing the little human supervision

1:46:32.960 --> 1:46:35.080
 that are needed to keep this whole thing safe.

1:46:35.080 --> 1:46:38.440
 I mean it's a fascinating dance of human robot interaction.

1:46:38.440 --> 1:46:42.120
 To me autonomous driving for a long time

1:46:42.120 --> 1:46:45.040
 is a human robot interaction problem.

1:46:45.040 --> 1:46:48.040
 It is not a robotics problem or computer vision problem.

1:46:48.040 --> 1:46:50.000
 Like you have to have a human in the loop.

1:46:50.000 --> 1:46:53.320
 But so which is why I think it's 10 years plus.

1:46:53.320 --> 1:46:56.280
 But I do think there'll be a bunch of cities and contexts

1:46:56.280 --> 1:47:01.280
 where geo restricted it will work really, really damn well.

1:47:02.360 --> 1:47:05.000
 So I think for me that gets five if I'm being optimistic

1:47:05.000 --> 1:47:07.360
 and it's going to be five for a lot of cases

1:47:07.360 --> 1:47:09.200
 and 10 plus, yeah, I agree with you.

1:47:09.200 --> 1:47:13.120
 10 plus basically if we want to recover most of the,

1:47:13.120 --> 1:47:15.240
 say, contiguous United States or something.

1:47:15.240 --> 1:47:16.080
 Oh, interesting.

1:47:16.080 --> 1:47:20.280
 So my optimistic is five and pessimistic is 30.

1:47:20.280 --> 1:47:21.120
 30.

1:47:21.120 --> 1:47:22.480
 I have a long tail on this one.

1:47:22.480 --> 1:47:24.440
 I haven't watched enough driving videos.

1:47:24.440 --> 1:47:29.160
 I've watched enough pedestrians to think like we may be,

1:47:29.160 --> 1:47:31.680
 like there's a small part of me still, not a small,

1:47:31.680 --> 1:47:34.360
 like a pretty big part of me that thinks

1:47:34.360 --> 1:47:37.520
 we will have to build AGI to solve driving.

1:47:37.520 --> 1:47:38.440
 Oh, well.

1:47:38.440 --> 1:47:39.640
 Like there's something to me,

1:47:39.640 --> 1:47:41.800
 like because humans are part of the picture,

1:47:41.800 --> 1:47:44.000
 deeply part of the picture,

1:47:44.000 --> 1:47:46.080
 and also human society is part of the picture

1:47:46.080 --> 1:47:47.920
 in that human life is at stake.

1:47:47.920 --> 1:47:50.840
 Anytime a robot kills a human,

1:47:50.840 --> 1:47:54.280
 it's not clear to me that that's not a problem

1:47:54.280 --> 1:47:56.360
 that machine learning will also have to solve.

1:47:56.360 --> 1:47:59.400
 Like it has to, you have to integrate that

1:47:59.400 --> 1:48:00.240
 into the whole thing.

1:48:00.240 --> 1:48:03.280
 Just like Facebook or social networks,

1:48:03.280 --> 1:48:04.600
 one thing is to say how to make

1:48:04.600 --> 1:48:06.720
 a really good recommender system.

1:48:06.720 --> 1:48:08.640
 And then the other thing is to integrate

1:48:08.640 --> 1:48:10.240
 into that recommender system,

1:48:10.240 --> 1:48:12.080
 all the journalists that will write articles

1:48:12.080 --> 1:48:13.880
 about that recommender system.

1:48:13.880 --> 1:48:15.880
 Like you have to consider the society

1:48:15.880 --> 1:48:18.400
 within which the AI system operates.

1:48:18.400 --> 1:48:21.000
 And in order to, and like politicians too,

1:48:21.000 --> 1:48:24.200
 this is the regulatory stuff for autonomous driving.

1:48:24.200 --> 1:48:26.720
 It's kind of fascinating that the more successful

1:48:26.720 --> 1:48:28.720
 your AI system becomes,

1:48:28.720 --> 1:48:31.600
 the more it gets integrated in society

1:48:31.600 --> 1:48:33.560
 and the more precious politicians

1:48:33.560 --> 1:48:36.000
 and the public and the clickbait journalists

1:48:36.000 --> 1:48:38.040
 and all the different fascinating forces

1:48:38.040 --> 1:48:40.360
 of our society start acting on it.

1:48:40.360 --> 1:48:42.240
 And then it's no longer how good you are

1:48:42.240 --> 1:48:43.960
 at doing the initial task.

1:48:43.960 --> 1:48:47.000
 It's also how good you are at navigating human nature,

1:48:47.000 --> 1:48:49.920
 which is a fascinating space.

1:48:49.920 --> 1:48:52.600
 What do you think are the limits of deep learning?

1:48:52.600 --> 1:48:54.800
 If you allow me, we'll zoom out a little bit

1:48:54.800 --> 1:48:58.120
 into the big question of artificial intelligence.

1:48:58.120 --> 1:49:02.080
 You said dark matter of intelligence is self supervised

1:49:02.080 --> 1:49:04.320
 learning, but there could be more.

1:49:04.320 --> 1:49:07.760
 What do you think the limits of self supervised learning

1:49:07.760 --> 1:49:10.720
 and just learning in general, deep learning are?

1:49:10.720 --> 1:49:12.680
 I think like for deep learning in particular,

1:49:12.680 --> 1:49:14.640
 because self supervised learning is I would say

1:49:14.640 --> 1:49:16.800
 a little bit more vague right now.

1:49:16.800 --> 1:49:18.680
 So I wouldn't, like for something that's so vague,

1:49:18.680 --> 1:49:21.960
 it's hard to predict what its limits are going to be.

1:49:21.960 --> 1:49:25.240
 But like I said, I think anywhere you want to interact

1:49:25.240 --> 1:49:27.920
 with human self supervised learning kind of hits a boundary

1:49:27.920 --> 1:49:29.960
 very quickly because you need to have an interface

1:49:29.960 --> 1:49:31.600
 to be able to communicate with the human.

1:49:31.600 --> 1:49:35.040
 So really like if you have just like vacuous concepts

1:49:35.040 --> 1:49:37.360
 or like just like nebulous concepts discovered

1:49:37.360 --> 1:49:39.920
 by a network, it's very hard to communicate those

1:49:39.920 --> 1:49:41.760
 with the human without like inserting some kind

1:49:41.760 --> 1:49:44.560
 of human knowledge or some kind of like human bias there.

1:49:45.600 --> 1:49:47.040
 In general, I think for deep learning,

1:49:47.040 --> 1:49:50.680
 the biggest challenge is just like data efficiency.

1:49:50.680 --> 1:49:52.600
 Even with self supervised learning,

1:49:52.600 --> 1:49:54.920
 even with anything else, if you just see

1:49:54.920 --> 1:49:59.280
 a single concept once, like one image of like,

1:49:59.280 --> 1:50:01.200
 I don't know, whatever you want to call it,

1:50:01.200 --> 1:50:03.840
 like any concept, it's really hard for these methods

1:50:03.840 --> 1:50:07.040
 to generalize by looking at just one or two samples

1:50:07.040 --> 1:50:09.760
 of things and that has been a real challenge.

1:50:09.760 --> 1:50:11.680
 I think that's actually why like these edge cases,

1:50:11.680 --> 1:50:14.520
 for example, for Tesla are actually that important.

1:50:14.520 --> 1:50:18.040
 Because if you see just one instance of the car failing

1:50:18.040 --> 1:50:20.280
 and if you just annotate that and you get that

1:50:20.280 --> 1:50:23.560
 into your data set, you have like very limited guarantee

1:50:23.560 --> 1:50:25.160
 that it's not going to happen again.

1:50:25.160 --> 1:50:26.720
 And you're actually going to be able to recognize

1:50:26.720 --> 1:50:28.640
 this kind of instance in a very different scenario.

1:50:28.640 --> 1:50:31.400
 So like when it was snowing, so you got that thing labeled

1:50:31.400 --> 1:50:33.240
 when it was snowing, but now when it's raining,

1:50:33.240 --> 1:50:34.640
 you're actually not able to get it.

1:50:34.640 --> 1:50:36.600
 Or you basically have the same scenario

1:50:36.600 --> 1:50:37.440
 in a different part of the world.

1:50:37.440 --> 1:50:39.120
 So the lighting was different or so on.

1:50:39.120 --> 1:50:41.000
 So it's just really hard for these models,

1:50:41.000 --> 1:50:42.720
 like deep learning especially to do that.

1:50:42.720 --> 1:50:43.560
 What's your intuition?

1:50:43.560 --> 1:50:47.800
 How do we solve handwritten digit recognition problem

1:50:47.800 --> 1:50:51.200
 when we only have one example for each number?

1:50:51.200 --> 1:50:54.720
 It feels like humans are using something like learning.

1:50:54.720 --> 1:50:55.560
 Right.

1:50:55.560 --> 1:50:59.240
 I think we are good at transferring knowledge a little bit.

1:50:59.240 --> 1:51:02.640
 We are just better at like for a lot of these problems

1:51:02.640 --> 1:51:04.840
 where we are generalizing from a single sample

1:51:04.840 --> 1:51:06.960
 or recognizing from a single sample,

1:51:06.960 --> 1:51:08.760
 we are using a lot of our own domain knowledge

1:51:08.760 --> 1:51:10.320
 and a lot of our like inductive bias

1:51:10.320 --> 1:51:12.280
 into that one sample to generalize it.

1:51:12.280 --> 1:51:15.320
 So I've never seen you write the number nine, for example.

1:51:15.320 --> 1:51:17.440
 And if you were to write it, I would still get it.

1:51:17.440 --> 1:51:19.280
 And if you were to write a different kind of alphabet

1:51:19.280 --> 1:51:20.840
 and like write it in two different ways,

1:51:20.840 --> 1:51:22.360
 I would still probably be able to figure out

1:51:22.360 --> 1:51:24.720
 that these are the same two characters.

1:51:24.720 --> 1:51:26.320
 It's just that I have been very used

1:51:26.320 --> 1:51:29.080
 to seeing handwritten digits in my life.

1:51:29.080 --> 1:51:31.360
 The other sort of problem with any deep learning system

1:51:31.360 --> 1:51:33.080
 or any kind of machine learning system is like,

1:51:33.080 --> 1:51:34.200
 it's guarantees, right?

1:51:34.200 --> 1:51:35.880
 There are no guarantees for it.

1:51:35.880 --> 1:51:38.200
 Now you can argue that humans also don't have any guarantees.

1:51:38.200 --> 1:51:41.160
 Like there is no guarantee that I can recognize a cat

1:51:41.160 --> 1:51:42.280
 in every scenario.

1:51:42.280 --> 1:51:43.920
 I'm sure there are going to be lots of cats

1:51:43.920 --> 1:51:45.720
 that I don't recognize, lots of scenarios

1:51:45.720 --> 1:51:48.120
 in which I don't recognize cats in general.

1:51:48.120 --> 1:51:52.840
 But I think from just a sort of application perspective,

1:51:52.840 --> 1:51:54.760
 you do need guarantees, right?

1:51:54.760 --> 1:51:56.960
 We call these things algorithms.

1:51:56.960 --> 1:51:59.080
 Now algorithms, like traditional CS algorithms

1:51:59.080 --> 1:51:59.960
 have guarantees.

1:51:59.960 --> 1:52:01.480
 Sorting is a guarantee.

1:52:01.480 --> 1:52:05.600
 If you were to call sort on a particular array of numbers,

1:52:05.600 --> 1:52:07.640
 you are guaranteed that it's going to be sorted.

1:52:07.640 --> 1:52:09.320
 Otherwise it's a bug.

1:52:09.320 --> 1:52:10.160
 Now for machine learning,

1:52:10.160 --> 1:52:12.440
 it's very hard to characterize this.

1:52:12.440 --> 1:52:15.440
 We know for a fact that a cat recognition model

1:52:15.440 --> 1:52:17.040
 is not going to recognize cats,

1:52:17.040 --> 1:52:19.720
 every cat in the world in every circumstance.

1:52:19.720 --> 1:52:22.040
 I think most people would agree with that statement,

1:52:22.040 --> 1:52:23.600
 but we are still okay with it.

1:52:23.600 --> 1:52:25.400
 We still don't call this as a bug.

1:52:25.400 --> 1:52:26.720
 Whereas in traditional computer science

1:52:26.720 --> 1:52:27.840
 or traditional science,

1:52:27.840 --> 1:52:29.960
 like if you have this kind of failure case existing,

1:52:29.960 --> 1:52:33.160
 then you think of it as like something is wrong.

1:52:33.160 --> 1:52:34.520
 I think there is this sort of notion

1:52:34.520 --> 1:52:37.000
 of nebulous correctness for machine learning.

1:52:37.000 --> 1:52:38.840
 And that's something we just need to be very comfortable

1:52:38.840 --> 1:52:39.680
 with.

1:52:39.680 --> 1:52:40.520
 And for deep learning,

1:52:40.520 --> 1:52:42.680
 or like for a lot of these machine learning algorithms,

1:52:42.680 --> 1:52:44.680
 it's not clear how do we characterize

1:52:44.680 --> 1:52:46.320
 this notion of correctness.

1:52:46.320 --> 1:52:48.120
 I think limitation in our understanding,

1:52:48.120 --> 1:52:51.160
 or at least a limitation in our phrasing of this.

1:52:51.160 --> 1:52:53.080
 And if we were to come up with better ways

1:52:53.080 --> 1:52:55.040
 to understand this limitation,

1:52:55.040 --> 1:52:57.160
 then it would actually help us a lot.

1:52:57.160 --> 1:52:58.840
 Do you think there's a distinction

1:52:58.840 --> 1:53:01.800
 between the concept of learning

1:53:01.800 --> 1:53:03.360
 and the concept of reasoning?

1:53:04.240 --> 1:53:09.240
 Do you think it's possible for neural networks to reason?

1:53:10.280 --> 1:53:11.680
 So I think of it slightly differently.

1:53:11.680 --> 1:53:14.520
 So for me, learning is whenever

1:53:14.520 --> 1:53:16.040
 I can like make a snap judgment.

1:53:16.040 --> 1:53:17.200
 So if you show me a picture of a dog,

1:53:17.200 --> 1:53:18.880
 I can immediately say it's a dog.

1:53:18.880 --> 1:53:20.680
 But if you give me like a puzzle,

1:53:20.680 --> 1:53:23.480
 like whatever a Goldsberg machine

1:53:23.480 --> 1:53:24.960
 of like things going to happen,

1:53:24.960 --> 1:53:26.440
 then I have to reason because I've never,

1:53:26.440 --> 1:53:27.600
 it's a very complicated setup.

1:53:27.600 --> 1:53:29.280
 I've never seen that particular setup.

1:53:29.280 --> 1:53:32.200
 And I really need to draw and like imagine in my head

1:53:32.200 --> 1:53:34.640
 what's going to happen to figure it out.

1:53:34.640 --> 1:53:36.840
 So I think, yes, neural networks are really good

1:53:36.840 --> 1:53:41.160
 at recognition, but they're not very good at reasoning.

1:53:41.160 --> 1:53:44.120
 Because they have seen something before

1:53:44.120 --> 1:53:46.360
 or seen something similar before, they're very good

1:53:46.360 --> 1:53:48.240
 at making those sort of snap judgments.

1:53:48.240 --> 1:53:50.680
 But if you were to give them a very complicated thing

1:53:50.680 --> 1:53:52.480
 that they've not seen before,

1:53:52.480 --> 1:53:55.320
 they have very limited ability right now

1:53:55.320 --> 1:53:56.560
 to compose different things.

1:53:56.560 --> 1:53:58.240
 Like, oh, I've seen this particular part before.

1:53:58.240 --> 1:54:00.040
 I've seen this particular part before.

1:54:00.040 --> 1:54:01.400
 And now probably like this is how

1:54:01.400 --> 1:54:02.920
 they're going to work in tandem.

1:54:02.920 --> 1:54:04.160
 It's very hard for them to come up

1:54:04.160 --> 1:54:05.200
 with these kinds of things.

1:54:05.200 --> 1:54:08.800
 Well, there's a certain aspect to reasoning

1:54:08.800 --> 1:54:11.880
 that you can maybe convert into the process of programming.

1:54:11.880 --> 1:54:14.320
 And so there's the whole field of program synthesis

1:54:14.320 --> 1:54:17.240
 and people have been applying machine learning

1:54:17.240 --> 1:54:18.920
 to the problem of program synthesis.

1:54:18.920 --> 1:54:22.680
 And the question is, can they, the step of composition,

1:54:22.680 --> 1:54:24.200
 why can't that be learned?

1:54:25.280 --> 1:54:29.400
 You know, this step of like building things on top of you,

1:54:29.400 --> 1:54:33.200
 like little intuitions, concepts on top of each other,

1:54:33.200 --> 1:54:35.280
 can that be learnable?

1:54:35.280 --> 1:54:36.800
 What's your intuition there?

1:54:36.800 --> 1:54:39.440
 Or like, I guess similar set of techniques,

1:54:39.440 --> 1:54:42.040
 do you think that will be applicable?

1:54:42.040 --> 1:54:44.640
 So I think it is, of course, it is learnable

1:54:44.640 --> 1:54:47.080
 because like we are prime examples of machines

1:54:47.080 --> 1:54:49.480
 that have like, or individuals that have learned this, right?

1:54:49.480 --> 1:54:51.080
 Like humans have learned this.

1:54:51.080 --> 1:54:52.760
 So it is, of course, it is a technique

1:54:52.760 --> 1:54:54.200
 that is very easy to learn.

1:54:55.840 --> 1:54:58.400
 I think where we are kind of hitting a wall

1:54:58.400 --> 1:55:00.480
 basically with like current machine learning

1:55:00.480 --> 1:55:03.400
 is the fact that when the network learns

1:55:03.400 --> 1:55:04.640
 all of this information,

1:55:04.640 --> 1:55:07.480
 we basically are not able to figure out

1:55:07.480 --> 1:55:10.640
 how well it's going to generalize to an unseen thing.

1:55:10.640 --> 1:55:13.880
 And we have no, like a priori, no way of characterizing that.

1:55:15.040 --> 1:55:17.640
 And I think that's basically telling us a lot about,

1:55:18.480 --> 1:55:20.720
 like a lot about the fact that we really don't know

1:55:20.720 --> 1:55:22.760
 what this model has learned and how well it's basically,

1:55:22.760 --> 1:55:25.120
 because we don't know how well it's going to transfer.

1:55:25.120 --> 1:55:28.080
 There's also a sense in which it feels like

1:55:28.080 --> 1:55:33.080
 we humans may not be aware of how much like background,

1:55:34.400 --> 1:55:36.760
 how good our background model is,

1:55:36.760 --> 1:55:39.880
 how much knowledge we just have slowly building

1:55:39.880 --> 1:55:41.400
 on top of each other.

1:55:41.400 --> 1:55:42.480
 It feels like neural networks

1:55:42.480 --> 1:55:43.840
 are constantly throwing stuff out.

1:55:43.840 --> 1:55:45.360
 Like you'll do some incredible thing

1:55:45.360 --> 1:55:49.040
 where you're learning a particular task in computer vision,

1:55:49.040 --> 1:55:51.240
 you celebrate your state of the art successes

1:55:51.240 --> 1:55:52.720
 and you throw that out.

1:55:52.720 --> 1:55:54.240
 Like, it feels like it's,

1:55:54.240 --> 1:55:56.720
 you're never using stuff you've learned

1:55:56.720 --> 1:56:00.080
 for your future successes in other domains.

1:56:00.080 --> 1:56:03.240
 And humans are obviously doing that exceptionally well,

1:56:03.240 --> 1:56:05.840
 still throwing stuff away in their mind,

1:56:05.840 --> 1:56:07.840
 but keeping certain kernels of truth.

1:56:07.840 --> 1:56:09.200
 Right, so I think we're like,

1:56:09.200 --> 1:56:11.080
 continual learning is sort of the paradigm

1:56:11.080 --> 1:56:11.920
 for this in machine learning.

1:56:11.920 --> 1:56:15.160
 And I don't think it's a very well explored paradigm.

1:56:15.160 --> 1:56:17.440
 We have like things in deep learning, for example,

1:56:17.440 --> 1:56:20.160
 catastrophic forgetting is like one of the standard things.

1:56:20.160 --> 1:56:23.120
 The thing basically being that if you teach a network

1:56:23.120 --> 1:56:24.760
 like to recognize dogs,

1:56:24.760 --> 1:56:27.400
 and now you teach that same network to recognize cats,

1:56:27.400 --> 1:56:29.040
 it basically forgets how to recognize dogs.

1:56:29.040 --> 1:56:30.800
 So it forgets very quickly.

1:56:30.800 --> 1:56:32.520
 I mean, and whereas a human,

1:56:32.520 --> 1:56:34.560
 if you were to teach someone to recognize dogs

1:56:34.560 --> 1:56:35.880
 and then to recognize cats,

1:56:35.880 --> 1:56:38.440
 they don't forget immediately how to recognize these dogs.

1:56:38.440 --> 1:56:40.640
 I think that's basically sort of what you're trying to get.

1:56:40.640 --> 1:56:42.400
 Yeah, I just, I wonder if like

1:56:42.400 --> 1:56:44.720
 the long term memory mechanisms

1:56:44.720 --> 1:56:47.080
 or the mechanisms that store not just memories,

1:56:47.080 --> 1:56:52.080
 but concepts that allow you to the reason

1:56:54.240 --> 1:56:57.200
 and compose concepts,

1:56:57.200 --> 1:56:59.000
 if those things will look very different

1:56:59.000 --> 1:56:59.880
 than neural networks,

1:56:59.880 --> 1:57:02.320
 or if you can do that within a single neural network

1:57:02.320 --> 1:57:06.040
 with some particular sort of architecture quirks,

1:57:06.040 --> 1:57:07.720
 that seems to be a really open problem.

1:57:07.720 --> 1:57:09.440
 And of course I go up and down on that

1:57:09.440 --> 1:57:14.440
 because there's something so compelling to the symbolic AI

1:57:14.840 --> 1:57:19.840
 or to the ideas of logic based sort of expert systems.

1:57:20.320 --> 1:57:22.440
 You have like human interpretable facts

1:57:22.440 --> 1:57:24.080
 that built on top of each other.

1:57:24.080 --> 1:57:27.800
 It's really annoying like with self supervised learning

1:57:27.800 --> 1:57:31.120
 that the AI is not very explainable.

1:57:31.120 --> 1:57:33.360
 Like you can't like understand

1:57:33.360 --> 1:57:35.520
 all the beautiful things it has learned.

1:57:35.520 --> 1:57:38.400
 You can't ask it like questions,

1:57:38.400 --> 1:57:40.960
 but then again, maybe that's a stupid thing

1:57:40.960 --> 1:57:42.440
 for us humans to want.

1:57:42.440 --> 1:57:45.240
 Right, I think whenever we try to like understand it,

1:57:45.240 --> 1:57:47.840
 we are putting our own subjective human bias into it.

1:57:47.840 --> 1:57:48.680
 Yeah.

1:57:48.680 --> 1:57:50.000
 And I think that's the sort of problem

1:57:50.000 --> 1:57:51.000
 with self supervised learning,

1:57:51.000 --> 1:57:54.280
 the goal is that it should learn naturally from the data.

1:57:54.280 --> 1:57:55.520
 So now if you try to understand it,

1:57:55.520 --> 1:57:58.640
 you are using your own preconceived notions

1:57:58.640 --> 1:58:00.600
 of what this model has learned.

1:58:00.600 --> 1:58:02.480
 And that's the problem.

1:58:03.480 --> 1:58:04.640
 High level question.

1:58:04.640 --> 1:58:07.920
 What do you think it takes to build a system

1:58:07.920 --> 1:58:10.520
 with superhuman, maybe let's say human level

1:58:10.520 --> 1:58:13.520
 or superhuman level general intelligence?

1:58:13.520 --> 1:58:15.560
 We've already kind of started talking about this,

1:58:15.560 --> 1:58:17.760
 but what's your intuition?

1:58:17.760 --> 1:58:20.760
 Like, does this thing have to have a body?

1:58:20.760 --> 1:58:23.920
 Does it have to interact richly with the world?

1:58:25.400 --> 1:58:27.920
 Does it have to have some more human elements

1:58:27.920 --> 1:58:30.480
 like self awareness?

1:58:30.480 --> 1:58:32.240
 I think emotion.

1:58:32.240 --> 1:58:35.720
 I think emotion is something which is like,

1:58:35.720 --> 1:58:37.520
 it's not really attributed typically

1:58:37.520 --> 1:58:38.440
 in standard machine learning.

1:58:38.440 --> 1:58:39.560
 It's not something we think about,

1:58:39.560 --> 1:58:41.040
 like there is NLP, there is vision,

1:58:41.040 --> 1:58:42.560
 there is no like emotion.

1:58:42.560 --> 1:58:44.600
 Emotion is never a part of all of this.

1:58:44.600 --> 1:58:47.080
 And that just seems a little bit weird to me.

1:58:47.080 --> 1:58:50.320
 I think the reason basically being that there is surprise

1:58:50.320 --> 1:58:53.800
 and like, basically emotion is like one of the reasons

1:58:53.800 --> 1:58:55.800
 emotions arise is like what happens

1:58:55.800 --> 1:58:57.120
 and what do you expect to happen, right?

1:58:57.120 --> 1:58:59.440
 There is like a mismatch between these things.

1:58:59.440 --> 1:59:01.080
 And so that gives rise to like,

1:59:01.080 --> 1:59:03.520
 I can either be surprised or I can be saddened

1:59:03.520 --> 1:59:05.320
 or I can be happy and all of this.

1:59:05.320 --> 1:59:07.960
 And so this basically indicates

1:59:07.960 --> 1:59:10.160
 that I already have a predictive model in my head

1:59:10.160 --> 1:59:11.840
 and something that I predicted or something

1:59:11.840 --> 1:59:13.720
 that I thought was likely to happen.

1:59:13.720 --> 1:59:15.120
 And then there was something that I observed

1:59:15.120 --> 1:59:16.720
 that happened that there was a disconnect

1:59:16.720 --> 1:59:18.280
 between these two things.

1:59:18.280 --> 1:59:21.840
 And that basically is like maybe one of the reasons

1:59:21.840 --> 1:59:24.280
 like you have a lot of emotions.

1:59:24.280 --> 1:59:26.880
 Yeah, I think, so I talk to people a lot about them

1:59:26.880 --> 1:59:29.120
 like Lisa Feldman Barrett.

1:59:29.120 --> 1:59:31.720
 I think that's an interesting concept of emotion

1:59:31.720 --> 1:59:36.720
 but I have a sense that emotion primarily

1:59:36.880 --> 1:59:38.080
 in the way we think about it,

1:59:38.080 --> 1:59:40.320
 which is the display of emotion

1:59:40.320 --> 1:59:43.800
 is a communication mechanism between humans.

1:59:43.800 --> 1:59:48.240
 So it's a part of basically human to human interaction,

1:59:48.240 --> 1:59:50.200
 an important part, but just the part.

1:59:50.200 --> 1:59:55.040
 So it's like, I would throw it into the full mix

1:59:55.040 --> 1:59:58.040
 of communication.

1:59:58.040 --> 2:00:01.240
 And to me, communication can be done with objects

2:00:01.240 --> 2:00:04.360
 that don't look at all like humans.

2:00:04.360 --> 2:00:05.440
 Okay.

2:00:05.440 --> 2:00:07.560
 I've seen our ability to anthropomorphize

2:00:07.560 --> 2:00:10.680
 our ability to connect with things that look like a Roomba

2:00:10.680 --> 2:00:12.000
 our ability to connect.

2:00:12.000 --> 2:00:14.720
 First of all, let's talk about other biological systems

2:00:14.720 --> 2:00:17.440
 like dogs, our ability to love things

2:00:17.440 --> 2:00:19.400
 that are very different than humans.

2:00:19.400 --> 2:00:20.960
 But they do display emotion, right?

2:00:20.960 --> 2:00:23.200
 I mean, dogs do display emotion.

2:00:23.200 --> 2:00:25.320
 So they don't have to be anthropomorphic

2:00:25.320 --> 2:00:27.600
 for them to like display the kind of emotions

2:00:27.600 --> 2:00:28.440
 that we don't.

2:00:28.440 --> 2:00:29.280
 Exactly.

2:00:29.280 --> 2:00:32.160
 So, I mean, but then the word emotion starts to lose.

2:00:33.920 --> 2:00:36.280
 So then we have to be, I guess specific, but yeah.

2:00:36.280 --> 2:00:39.520
 So have rich flavorful communication.

2:00:39.520 --> 2:00:40.360
 Communication, yeah.

2:00:40.360 --> 2:00:43.000
 Yeah, so like, yes, it's full of emotion.

2:00:43.000 --> 2:00:48.000
 It's full of wit and humor and moods

2:00:49.080 --> 2:00:50.280
 and all those kinds of things, yeah.

2:00:50.280 --> 2:00:53.720
 So you're talking about like flavor.

2:00:53.720 --> 2:00:54.560
 Flavor, yeah.

2:00:54.560 --> 2:00:55.400
 Okay, let's call it that.

2:00:55.400 --> 2:00:57.240
 So there's content and then there is flavor

2:00:57.240 --> 2:00:58.440
 and I'm talking about the flavor.

2:00:58.440 --> 2:01:00.280
 Do you think it needs to have a body?

2:01:00.280 --> 2:01:02.840
 Do you think like to interact with the physical world?

2:01:02.840 --> 2:01:04.640
 Do you think you can understand the physical world

2:01:04.640 --> 2:01:07.080
 without being able to directly interact with it?

2:01:07.080 --> 2:01:08.440
 I don't think so, yeah.

2:01:08.440 --> 2:01:10.720
 I think at some point we will need to bite the bullet

2:01:10.720 --> 2:01:12.680
 and actually interact with the physical,

2:01:12.680 --> 2:01:15.880
 as much as I like working on like passive computer vision

2:01:15.880 --> 2:01:17.280
 where I just like sit in my arm chair

2:01:17.280 --> 2:01:19.040
 and look at videos and learn.

2:01:19.040 --> 2:01:22.760
 I do think that we will need to have some kind of embodiment

2:01:22.760 --> 2:01:24.600
 or some kind of interaction

2:01:24.600 --> 2:01:26.960
 to figure out things about the world.

2:01:26.960 --> 2:01:28.640
 What about consciousness?

2:01:28.640 --> 2:01:32.320
 Do you think, how often do you think about consciousness

2:01:32.320 --> 2:01:34.320
 when you think about your work?

2:01:34.320 --> 2:01:35.280
 You could think of it

2:01:35.280 --> 2:01:37.760
 as the more simple thing of self awareness,

2:01:38.640 --> 2:01:43.640
 of being aware that you are a perceiving,

2:01:43.880 --> 2:01:46.840
 sensing, acting thing in this world.

2:01:46.840 --> 2:01:50.320
 Or you can think about the bigger version of that,

2:01:50.320 --> 2:01:51.640
 which is consciousness,

2:01:51.640 --> 2:01:56.640
 which is having it feel like something to be that entity,

2:01:57.200 --> 2:01:59.560
 the subjective experience of being in this world.

2:01:59.560 --> 2:02:01.440
 So I think of self awareness a little bit more

2:02:01.440 --> 2:02:03.400
 than like the broader goal of it,

2:02:03.400 --> 2:02:06.120
 because I think self awareness is pretty critical

2:02:06.120 --> 2:02:09.280
 for like any kind of like any kind of AGI

2:02:09.280 --> 2:02:10.680
 or whatever you want to call it that we build,

2:02:10.680 --> 2:02:13.960
 because it needs to contextualize what it is

2:02:13.960 --> 2:02:15.600
 and what role it's playing

2:02:15.600 --> 2:02:17.960
 with respect to all the other things that exist around it.

2:02:17.960 --> 2:02:19.680
 I think that requires self awareness.

2:02:19.680 --> 2:02:23.520
 It needs to understand that it's an autonomous car, right?

2:02:23.520 --> 2:02:24.920
 And what does that mean?

2:02:24.920 --> 2:02:26.240
 What are its limitations?

2:02:26.240 --> 2:02:29.080
 What are the things that it is supposed to do and so on?

2:02:29.080 --> 2:02:30.760
 What is its role in some way?

2:02:30.760 --> 2:02:34.240
 Or, I mean, these are the kinds of things

2:02:34.240 --> 2:02:36.880
 that we kind of expect from it, I would say.

2:02:36.880 --> 2:02:39.360
 And so that's the level of self awareness

2:02:39.360 --> 2:02:42.200
 that's, I would say, basically required at least,

2:02:42.200 --> 2:02:44.280
 if not more than that.

2:02:44.280 --> 2:02:46.440
 Yeah, I tend to, on the emotion side,

2:02:46.440 --> 2:02:48.360
 believe that it has to have,

2:02:48.360 --> 2:02:52.560
 it has to be able to display consciousness.

2:02:52.560 --> 2:02:54.360
 Display consciousness, what do you mean by that?

2:02:54.360 --> 2:02:57.600
 Meaning like for us humans to connect with each other

2:02:57.600 --> 2:03:01.680
 or to connect with other living entities,

2:03:01.680 --> 2:03:04.200
 I think we need to feel,

2:03:04.200 --> 2:03:06.840
 like in order for us to truly feel

2:03:06.840 --> 2:03:09.400
 like that there's another being there,

2:03:09.400 --> 2:03:11.440
 we have to believe that they're conscious.

2:03:11.440 --> 2:03:14.960
 And so we won't ever connect with something

2:03:14.960 --> 2:03:17.320
 that doesn't have elements of consciousness.

2:03:17.320 --> 2:03:21.560
 Now I tend to think that that's easier to achieve

2:03:21.560 --> 2:03:23.080
 than it may sound,

2:03:23.080 --> 2:03:25.720
 because we anthropomorphize stuff so hard.

2:03:25.720 --> 2:03:28.760
 Like you have a mug that just like has wheels

2:03:28.760 --> 2:03:31.920
 and like rotates every once in a while and makes a sound.

2:03:31.920 --> 2:03:34.320
 I think a couple of days in,

2:03:34.320 --> 2:03:39.320
 especially if you don't hang out with humans,

2:03:39.520 --> 2:03:42.200
 you might start to believe that mug on wheels is conscious.

2:03:42.200 --> 2:03:44.840
 So I think we anthropomorphize pretty effectively

2:03:44.840 --> 2:03:46.040
 as human beings.

2:03:46.040 --> 2:03:49.240
 But I do think that it's in the same bucket

2:03:49.240 --> 2:03:50.920
 that we'll call emotion,

2:03:50.920 --> 2:03:54.720
 that show that you're,

2:03:54.720 --> 2:03:57.400
 I think of consciousness as the capacity to suffer.

2:03:58.320 --> 2:04:02.400
 And if you're an entity that's able to feel things

2:04:02.400 --> 2:04:05.560
 in the world and to communicate that to others,

2:04:06.640 --> 2:04:08.520
 I think that's a really powerful way

2:04:08.520 --> 2:04:10.880
 to interact with humans.

2:04:10.880 --> 2:04:13.200
 And in order to create an AGI system,

2:04:13.200 --> 2:04:18.000
 I believe you should be able to richly interact with humans.

2:04:18.000 --> 2:04:21.120
 Like humans would need to want to interact with you.

2:04:21.120 --> 2:04:22.200
 Like it can't be like,

2:04:22.200 --> 2:04:27.200
 it's the self supervised learning versus like,

2:04:27.400 --> 2:04:29.280
 like the robot shouldn't have to pay you

2:04:29.280 --> 2:04:30.400
 to interact with me.

2:04:30.400 --> 2:04:33.600
 So like it should be a natural fun thing.

2:04:33.600 --> 2:04:36.080
 And then you're going to scale up significantly

2:04:36.080 --> 2:04:39.080
 how much interaction it gets.

2:04:39.080 --> 2:04:40.840
 It's the Alexa prize,

2:04:40.840 --> 2:04:43.400
 which they were trying to get me to be a judge

2:04:43.400 --> 2:04:44.680
 on their contest.

2:04:44.680 --> 2:04:46.040
 Let's see if I want to do that.

2:04:46.040 --> 2:04:50.560
 But their challenge is to talk to you,

2:04:50.560 --> 2:04:53.960
 make the human sufficiently interested

2:04:53.960 --> 2:04:56.160
 that the human keeps talking for 20 minutes.

2:04:56.160 --> 2:04:57.000
 To Alexa?

2:04:57.000 --> 2:04:58.600
 To Alexa, yeah.

2:04:58.600 --> 2:05:00.240
 And right now they're not even close to that

2:05:00.240 --> 2:05:02.560
 because it just gets so boring when you're like,

2:05:02.560 --> 2:05:04.280
 when the intelligence is not there,

2:05:04.280 --> 2:05:06.920
 it gets very not interesting to talk to it.

2:05:06.920 --> 2:05:08.960
 And so the robot needs to be interesting.

2:05:08.960 --> 2:05:10.440
 And one of the ways it can be interesting

2:05:10.440 --> 2:05:14.680
 is display the capacity to love, to suffer.

2:05:14.680 --> 2:05:17.480
 And I would say that essentially means

2:05:17.480 --> 2:05:20.920
 the capacity to display consciousness.

2:05:20.920 --> 2:05:25.160
 Like it is an entity, much like a human being.

2:05:25.160 --> 2:05:27.320
 Of course, what that really means,

2:05:27.320 --> 2:05:30.520
 I don't know if that's fundamentally a robotics problem

2:05:30.520 --> 2:05:33.040
 or some kind of problem that we're not yet even aware.

2:05:33.040 --> 2:05:36.040
 Like if it is truly a hard problem of consciousness,

2:05:36.040 --> 2:05:38.600
 I tend to maybe optimistically think it's a,

2:05:38.600 --> 2:05:42.640
 we can pretty effectively fake it till we make it.

2:05:42.640 --> 2:05:46.400
 So we can display a lot of human like elements for a while.

2:05:46.400 --> 2:05:49.080
 And that will be sufficient to form

2:05:49.080 --> 2:05:50.920
 really close connections with humans.

2:05:52.000 --> 2:05:53.720
 What's used the most beautiful idea

2:05:53.720 --> 2:05:55.840
 in self supervised learning?

2:05:55.840 --> 2:05:59.040
 Like when you sit back with, I don't know,

2:05:59.040 --> 2:06:03.200
 with a glass of wine and an armchair

2:06:03.200 --> 2:06:06.080
 and just at a fireplace,

2:06:06.080 --> 2:06:08.720
 just thinking how beautiful this world that you get

2:06:08.720 --> 2:06:10.560
 to explore is, what do you think

2:06:10.560 --> 2:06:12.820
 is the especially beautiful idea?

2:06:13.800 --> 2:06:16.480
 The fact that like object level,

2:06:16.480 --> 2:06:19.960
 what objects are and some notion of objectness emerges

2:06:19.960 --> 2:06:22.800
 from these models by just like self supervised learning.

2:06:23.680 --> 2:06:28.680
 So for example, like one of the things like the dyno paper

2:06:28.920 --> 2:06:33.040
 that I was a part of at Facebook is the object sort

2:06:33.040 --> 2:06:35.600
 of boundaries emerge from these representations.

2:06:35.600 --> 2:06:38.060
 So if you have like a dog running in the field,

2:06:38.060 --> 2:06:39.440
 the boundaries around the dog,

2:06:39.440 --> 2:06:42.320
 the network is basically able to figure out

2:06:42.320 --> 2:06:45.520
 what the boundaries of this dog are automatically.

2:06:45.520 --> 2:06:47.040
 And it was never trained to do that.

2:06:47.040 --> 2:06:50.160
 It was never trained to, no one taught it

2:06:50.160 --> 2:06:52.680
 that this is a dog and these pixels belong to a dog.

2:06:52.680 --> 2:06:55.000
 It's able to group these things together automatically.

2:06:55.000 --> 2:06:56.160
 So that's one.

2:06:56.160 --> 2:07:00.000
 I think in general, that entire notion that this dumb idea

2:07:00.000 --> 2:07:01.960
 that you take like these two crops of an image

2:07:01.960 --> 2:07:04.120
 and then you say that the features should be similar,

2:07:04.120 --> 2:07:06.040
 that has resulted in something like this,

2:07:06.040 --> 2:07:07.920
 like the model is able to figure out

2:07:07.920 --> 2:07:10.320
 what the dog pixels are and so on.

2:07:10.320 --> 2:07:12.120
 That just seems like so surprising.

2:07:13.440 --> 2:07:16.200
 And I mean, I don't think a lot of us even understand

2:07:16.200 --> 2:07:18.120
 how that is happening really.

2:07:18.120 --> 2:07:20.800
 And it's something we are taking for granted,

2:07:20.800 --> 2:07:23.120
 maybe like a lot in terms of how we're setting up

2:07:23.120 --> 2:07:24.920
 these algorithms, but it's just,

2:07:24.920 --> 2:07:26.780
 it's a very beautiful and powerful idea.

2:07:26.780 --> 2:07:30.240
 So it's really fundamentally telling us something about

2:07:30.240 --> 2:07:32.440
 that there is so much signal in the pixels

2:07:32.440 --> 2:07:34.120
 that we can be super dumb about it.

2:07:34.120 --> 2:07:35.200
 How about how we are setting up

2:07:35.200 --> 2:07:37.080
 the self sequencing problem.

2:07:37.080 --> 2:07:39.600
 And despite being like super dumb about it,

2:07:39.600 --> 2:07:41.640
 we'll actually get very good,

2:07:41.640 --> 2:07:44.000
 like we'll actually get something that is able to do

2:07:44.000 --> 2:07:45.720
 very like surprising things.

2:07:45.720 --> 2:07:48.280
 I wonder if there's other like objectness

2:07:48.280 --> 2:07:50.380
 of other concepts that can emerge.

2:07:51.600 --> 2:07:53.600
 I don't know if you follow Francois Chollet,

2:07:53.600 --> 2:07:56.600
 he had the competition for intelligence

2:07:56.600 --> 2:07:59.560
 that basically it's kind of like an IQ test,

2:07:59.560 --> 2:08:02.400
 but for machines, but for an IQ test,

2:08:02.400 --> 2:08:05.360
 you have to have a few concepts that you want to apply.

2:08:05.360 --> 2:08:07.800
 One of them is objectness.

2:08:07.800 --> 2:08:11.520
 I wonder if those concepts can emerge

2:08:11.520 --> 2:08:14.760
 through self supervised learning on billions of images.

2:08:14.760 --> 2:08:16.320
 I think something like object permanence

2:08:16.320 --> 2:08:17.440
 can definitely emerge, right?

2:08:17.440 --> 2:08:20.240
 So that's like a fundamental concept which we have,

2:08:20.240 --> 2:08:21.480
 maybe not through images, through video,

2:08:21.480 --> 2:08:25.160
 but that's another concept that should be emerging from it

2:08:25.160 --> 2:08:26.760
 because it's not something that,

2:08:26.760 --> 2:08:29.120
 like if we don't teach humans that this isn't,

2:08:29.120 --> 2:08:31.520
 this is like about this concept of object permanence,

2:08:31.520 --> 2:08:32.500
 it actually emerges.

2:08:32.500 --> 2:08:34.100
 And the same thing for like animals, like dogs,

2:08:34.100 --> 2:08:36.360
 I think actually permanence automatically

2:08:36.360 --> 2:08:38.080
 is something that they are born with.

2:08:38.080 --> 2:08:40.320
 So I think it should emerge from the data.

2:08:40.320 --> 2:08:42.440
 It should emerge basically very quickly.

2:08:42.440 --> 2:08:45.880
 I wonder if ideas like symmetry, rotation,

2:08:45.880 --> 2:08:47.920
 these kinds of things might emerge.

2:08:47.920 --> 2:08:50.360
 So I think rotation, probably yes.

2:08:50.360 --> 2:08:51.640
 Yeah, rotation, yes.

2:08:51.640 --> 2:08:55.200
 I mean, there's some constraints in the architecture itself,

2:08:55.200 --> 2:08:59.240
 but it's interesting if all of them could be,

2:08:59.240 --> 2:09:04.240
 like counting was another one, being able to kind of

2:09:04.280 --> 2:09:06.240
 understand that there's multiple objects

2:09:06.240 --> 2:09:09.040
 of the same kind in the image and be able to count them.

2:09:10.040 --> 2:09:11.560
 I wonder if all of that could be,

2:09:11.560 --> 2:09:14.360
 if constructed correctly, they can emerge

2:09:14.360 --> 2:09:16.480
 because then you can transfer those concepts

2:09:16.480 --> 2:09:20.680
 to then interpret images at a deeper level.

2:09:20.680 --> 2:09:21.520
 Right.

2:09:21.520 --> 2:09:24.680
 Counting, I do believe, I mean, it should be possible.

2:09:24.680 --> 2:09:25.920
 You don't know like yet,

2:09:25.920 --> 2:09:29.720
 but I do think it's not that far in the realm of possibility.

2:09:29.720 --> 2:09:30.560
 Yeah, that'd be interesting

2:09:30.560 --> 2:09:33.240
 if using self supervised learning on images

2:09:33.240 --> 2:09:36.520
 can then be applied to then solving those kinds of IQ tests,

2:09:36.520 --> 2:09:38.840
 which seem currently to be kind of impossible.

2:09:40.440 --> 2:09:43.320
 What idea do you believe might be true

2:09:43.320 --> 2:09:46.600
 that most people think is not true

2:09:46.600 --> 2:09:48.560
 or don't agree with you on?

2:09:48.560 --> 2:09:50.040
 Is there something like that?

2:09:50.040 --> 2:09:52.400
 So this is going to be a little controversial,

2:09:52.400 --> 2:09:53.500
 but okay, sure.

2:09:53.500 --> 2:09:55.340
 I don't believe in simulation.

2:09:55.340 --> 2:09:58.840
 Like actually using simulation to do things very much.

2:09:58.840 --> 2:10:01.040
 Just to clarify, because this is a podcast

2:10:01.040 --> 2:10:03.600
 where you talk about, are we living in a simulation often?

2:10:03.600 --> 2:10:08.000
 You're referring to using simulation to construct worlds

2:10:08.000 --> 2:10:10.320
 that you then leverage for machine learning.

2:10:10.320 --> 2:10:11.160
 Right, yeah.

2:10:11.160 --> 2:10:13.080
 For example, like one example would be like

2:10:13.080 --> 2:10:15.520
 to train an autonomous car driving system.

2:10:15.520 --> 2:10:17.400
 You basically first build a simulator,

2:10:17.400 --> 2:10:19.840
 which builds like the environment of the world.

2:10:19.840 --> 2:10:22.680
 And then you basically have a lot of like,

2:10:22.680 --> 2:10:25.320
 you train your machine learning system in that.

2:10:25.320 --> 2:10:27.560
 So I believe it is possible,

2:10:27.560 --> 2:10:30.920
 but I think it's a really expensive way of doing things.

2:10:30.920 --> 2:10:33.760
 And at the end of it, you do need the real world.

2:10:33.760 --> 2:10:35.520
 So I'm not sure.

2:10:35.520 --> 2:10:36.920
 So maybe for certain settings,

2:10:36.920 --> 2:10:38.880
 like maybe the payout is so large,

2:10:38.880 --> 2:10:40.880
 like for autonomous driving, the payout is so large

2:10:40.880 --> 2:10:43.360
 that you can actually invest that much money to build it.

2:10:43.360 --> 2:10:45.480
 But I think as a general sort of principle,

2:10:45.480 --> 2:10:47.040
 it does not apply to a lot of concepts.

2:10:47.040 --> 2:10:49.720
 You can't really build simulations of everything.

2:10:49.720 --> 2:10:51.520
 Not only because like one, it's expensive,

2:10:51.520 --> 2:10:54.800
 because second, it's also not possible for a lot of things.

2:10:54.800 --> 2:10:59.400
 So in general, like there's a lot of work

2:10:59.400 --> 2:11:02.120
 on like using synthetic data and like synthetic simulators.

2:11:02.120 --> 2:11:05.840
 I generally am not very, like I don't believe in that.

2:11:05.840 --> 2:11:09.040
 So you're saying it's very challenging visually,

2:11:09.040 --> 2:11:11.960
 like to correctly like simulate the visual,

2:11:11.960 --> 2:11:13.600
 like the lighting, all those kinds of things.

2:11:13.600 --> 2:11:15.680
 I mean, all these companies that you have, right?

2:11:15.680 --> 2:11:17.880
 So like Pixar and like whatever,

2:11:17.880 --> 2:11:19.840
 all these companies are,

2:11:19.840 --> 2:11:21.540
 all this like computer graphics stuff

2:11:21.540 --> 2:11:22.920
 is really about accurately,

2:11:22.920 --> 2:11:26.120
 a lot of them is about like accurately trying to figure out

2:11:26.120 --> 2:11:28.760
 how the lighting is and like how things reflect off

2:11:28.760 --> 2:11:30.440
 of one another and so on,

2:11:30.440 --> 2:11:32.280
 and like how sparkly things look and so on.

2:11:32.280 --> 2:11:34.040
 So it's a very hard problem.

2:11:34.040 --> 2:11:37.200
 So do we really need to solve that first

2:11:37.200 --> 2:11:39.440
 to be able to like do computer vision?

2:11:39.440 --> 2:11:40.640
 Probably not.

2:11:40.640 --> 2:11:43.920
 And for me, in the context of autonomous driving,

2:11:44.800 --> 2:11:48.040
 it's very tempting to be able to use simulation, right?

2:11:48.040 --> 2:11:50.560
 Because it's a safety critical application,

2:11:50.560 --> 2:11:54.960
 but the other limitation of simulation that perhaps

2:11:54.960 --> 2:11:58.440
 is a bigger one than the visual limitation

2:11:58.440 --> 2:12:00.840
 is the behavior of objects.

2:12:00.840 --> 2:12:03.920
 So you're ultimately interested in edge cases.

2:12:03.920 --> 2:12:05.000
 And the question is,

2:12:05.000 --> 2:12:08.800
 how well can you generate edge cases in simulation,

2:12:08.800 --> 2:12:11.080
 especially with human behavior?

2:12:11.080 --> 2:12:13.480
 I think another problem is like for autonomous driving,

2:12:13.480 --> 2:12:15.260
 it's a constantly changing world.

2:12:15.260 --> 2:12:18.600
 So say autonomous driving like in 10 years from now,

2:12:18.600 --> 2:12:20.800
 like there are lots of autonomous cars,

2:12:20.800 --> 2:12:22.440
 but they're still going to be humans.

2:12:22.440 --> 2:12:25.240
 So now there are 50% of the agents say, which are humans,

2:12:25.240 --> 2:12:26.880
 50% of the agents that are autonomous,

2:12:26.880 --> 2:12:28.600
 like car driving agents.

2:12:28.600 --> 2:12:30.120
 So now the mixture has changed.

2:12:30.120 --> 2:12:32.360
 So now the kinds of behaviors that you actually expect

2:12:32.360 --> 2:12:35.200
 from the other agents or other cars on the road

2:12:35.200 --> 2:12:36.760
 are actually going to be very different.

2:12:36.760 --> 2:12:39.120
 And as the proportion of the number of autonomous cars

2:12:39.120 --> 2:12:40.480
 to humans keeps changing,

2:12:40.480 --> 2:12:42.640
 this behavior will actually change a lot.

2:12:42.640 --> 2:12:44.520
 So now if you were to build a simulator based on

2:12:44.520 --> 2:12:46.480
 just like right now to build them today,

2:12:46.480 --> 2:12:48.440
 you don't have that many autonomous cars on the road.

2:12:48.440 --> 2:12:50.560
 So you would try to like make all of the other agents

2:12:50.560 --> 2:12:52.920
 in that simulator behave as humans,

2:12:52.920 --> 2:12:55.760
 but that's not really going to hold true 10, 15, 20,

2:12:55.760 --> 2:12:57.400
 30 years from now.

2:12:57.400 --> 2:12:59.280
 Do you think we're living in a simulation?

2:12:59.280 --> 2:13:00.120
 No.

2:13:01.520 --> 2:13:02.840
 How hard is it?

2:13:02.840 --> 2:13:04.880
 This is why I think it's an interesting question.

2:13:04.880 --> 2:13:07.780
 How hard is it to build a video game,

2:13:07.780 --> 2:13:12.660
 like virtual reality game where it is so real,

2:13:12.660 --> 2:13:15.840
 forget like ultra realistic to where

2:13:15.840 --> 2:13:17.400
 you can't tell the difference,

2:13:17.400 --> 2:13:20.860
 but like it's so nice that you just want to stay there.

2:13:20.860 --> 2:13:24.960
 You just want to stay there and you don't want to come back.

2:13:24.960 --> 2:13:29.380
 Do you think that's doable within our lifetime?

2:13:29.380 --> 2:13:31.700
 Within our lifetime, probably.

2:13:31.700 --> 2:13:32.540
 Yeah.

2:13:32.540 --> 2:13:33.880
 I eat healthy, I live long.

2:13:33.880 --> 2:13:38.360
 Does that make you sad that there'll be like

2:13:39.400 --> 2:13:44.280
 like population of kids that basically spend 95%,

2:13:44.280 --> 2:13:47.520
 99% of their time in a virtual world?

2:13:50.120 --> 2:13:51.920
 Very, very hard question to answer.

2:13:53.380 --> 2:13:55.760
 For certain people, it might be something

2:13:55.760 --> 2:13:58.160
 that they really derive a lot of value out of,

2:13:58.160 --> 2:14:00.760
 derive a lot of enjoyment and like happiness out of,

2:14:00.760 --> 2:14:03.140
 and maybe the real world wasn't giving them that.

2:14:03.140 --> 2:14:03.980
 That's why they did that.

2:14:03.980 --> 2:14:05.960
 So maybe it is good for certain people.

2:14:05.960 --> 2:14:09.400
 So ultimately, if it maximizes happiness,

2:14:09.400 --> 2:14:10.240
 Right, I think if.

2:14:10.240 --> 2:14:11.060
 Or we could judge.

2:14:11.060 --> 2:14:12.780
 Yeah, I think if it's making people happy,

2:14:12.780 --> 2:14:14.440
 maybe it's okay.

2:14:14.440 --> 2:14:16.780
 Again, I think this is a very hard question.

2:14:18.320 --> 2:14:22.600
 So like you've been a part of a lot of amazing papers.

2:14:23.520 --> 2:14:25.640
 What advice would you give to somebody

2:14:25.640 --> 2:14:28.060
 on what it takes to write a good paper?

2:14:29.220 --> 2:14:31.020
 Grad students writing papers now,

2:14:31.020 --> 2:14:34.540
 is there common things that you've learned along the way

2:14:34.540 --> 2:14:35.760
 that you think it takes,

2:14:35.760 --> 2:14:39.020
 both for a good idea and a good paper?

2:14:39.020 --> 2:14:44.020
 Right, so I think both of these have picked up

2:14:44.140 --> 2:14:46.580
 from like lots of people I've worked with in the past.

2:14:46.580 --> 2:14:48.740
 So one of them is picking the right problem

2:14:48.740 --> 2:14:51.100
 to work on in research is as important

2:14:51.100 --> 2:14:53.720
 as like finding the solution to it.

2:14:53.720 --> 2:14:56.220
 So I mean, there are multiple reasons for this.

2:14:56.220 --> 2:14:59.000
 So one is that there are certain problems

2:14:59.000 --> 2:15:02.380
 that can actually be solved in a particular timeframe.

2:15:02.380 --> 2:15:06.420
 So now say you want to work on finding the meaning of life.

2:15:06.420 --> 2:15:07.460
 This is a great problem.

2:15:07.460 --> 2:15:09.460
 I think most people will agree with that.

2:15:09.460 --> 2:15:12.260
 But do you believe that your talents

2:15:12.260 --> 2:15:13.860
 and like the energy that you'll spend on it

2:15:13.860 --> 2:15:17.300
 will make some kind of meaningful progress

2:15:17.300 --> 2:15:18.860
 in your lifetime?

2:15:18.860 --> 2:15:21.020
 If you are optimistic about it, then go ahead.

2:15:21.020 --> 2:15:22.140
 That's why I started this podcast.

2:15:22.140 --> 2:15:24.080
 I keep asking people about the meaning of life.

2:15:24.080 --> 2:15:27.460
 I'm hoping by episode like 2.20, I'll figure it out.

2:15:27.460 --> 2:15:30.300
 Oh, not too many episodes to go.

2:15:30.300 --> 2:15:31.780
 All right, cool.

2:15:31.780 --> 2:15:33.820
 Maybe today, I don't know, but you're right.

2:15:33.820 --> 2:15:36.300
 So that seems intractable at the moment.

2:15:36.300 --> 2:15:39.060
 Right, so I think it's just the fact of like,

2:15:39.060 --> 2:15:41.100
 if you're starting a PhD, for example,

2:15:41.100 --> 2:15:43.020
 what is one problem that you want to focus on

2:15:43.020 --> 2:15:45.740
 that you do think is interesting enough,

2:15:45.740 --> 2:15:47.800
 and you will be able to make a reasonable amount

2:15:47.800 --> 2:15:50.540
 of headway into it that you think you'll be doing a PhD for?

2:15:50.540 --> 2:15:53.100
 So in that kind of a timeframe.

2:15:53.100 --> 2:15:53.920
 So that's one.

2:15:53.920 --> 2:15:54.780
 Of course, there's the second part,

2:15:54.780 --> 2:15:56.380
 which is what excites you genuinely.

2:15:56.380 --> 2:15:57.620
 So you shouldn't just pick problems

2:15:57.620 --> 2:15:59.020
 that you are not excited about,

2:15:59.020 --> 2:16:01.860
 because as a grad student or as a researcher,

2:16:01.860 --> 2:16:03.220
 you really need to be passionate about it

2:16:03.220 --> 2:16:04.580
 to continue doing that,

2:16:04.580 --> 2:16:05.740
 because there are so many other things

2:16:05.740 --> 2:16:07.100
 that you could be doing in life.

2:16:07.100 --> 2:16:08.260
 So you really need to believe in that

2:16:08.260 --> 2:16:10.740
 to be able to do that for that long.

2:16:10.740 --> 2:16:12.660
 In terms of papers, I think the one thing

2:16:12.660 --> 2:16:13.660
 that I've learned is,

2:16:15.580 --> 2:16:17.780
 like in the past, whenever I used to write things,

2:16:17.780 --> 2:16:18.940
 and even now, whenever I do that,

2:16:18.940 --> 2:16:21.420
 I try to cram in a lot of things into the paper,

2:16:21.420 --> 2:16:22.820
 whereas what really matters

2:16:22.820 --> 2:16:25.760
 is just pushing one simple idea, that's it.

2:16:25.760 --> 2:16:29.980
 That's all because the paper is going to be like,

2:16:29.980 --> 2:16:32.180
 whatever, eight or nine pages.

2:16:32.180 --> 2:16:34.240
 If you keep cramming in lots of ideas,

2:16:34.240 --> 2:16:36.240
 it's really hard for the single thing

2:16:36.240 --> 2:16:38.020
 that you believe in to stand out.

2:16:38.020 --> 2:16:40.900
 So if you really try to just focus,

2:16:40.900 --> 2:16:41.940
 especially in terms of writing,

2:16:41.940 --> 2:16:43.820
 really try to focus on one particular idea

2:16:43.820 --> 2:16:46.220
 and articulate it out in multiple different ways,

2:16:46.220 --> 2:16:49.020
 it's far more valuable to the reader as well,

2:16:49.020 --> 2:16:51.600
 and basically to the reader, of course,

2:16:51.600 --> 2:16:53.100
 because they get to,

2:16:53.100 --> 2:16:54.420
 they know that this particular idea

2:16:54.420 --> 2:16:56.140
 is associated with this paper,

2:16:56.140 --> 2:16:59.260
 and also for you, because you have,

2:16:59.260 --> 2:17:01.080
 when you write about a particular idea in different ways,

2:17:01.080 --> 2:17:02.700
 you think about it more deeply.

2:17:02.700 --> 2:17:06.020
 So as a grad student, I used to always wait to it,

2:17:06.020 --> 2:17:08.700
 maybe in the last week or whatever, to write the paper,

2:17:08.700 --> 2:17:10.280
 because I used to always believe

2:17:10.280 --> 2:17:11.380
 that doing the experiments

2:17:11.380 --> 2:17:13.860
 was actually the bigger part of research than writing.

2:17:13.860 --> 2:17:15.260
 And my advisor always told me

2:17:15.260 --> 2:17:16.660
 that you should start writing very early on,

2:17:16.660 --> 2:17:17.900
 and I thought, oh, it doesn't matter,

2:17:17.900 --> 2:17:19.700
 I don't know what he's talking about.

2:17:19.700 --> 2:17:22.020
 But I think more and more I realized that's the case.

2:17:22.020 --> 2:17:24.060
 Whenever I write something that I'm doing,

2:17:24.060 --> 2:17:26.440
 I actually think much better about it.

2:17:26.440 --> 2:17:28.820
 And so if you start writing early on,

2:17:28.820 --> 2:17:31.220
 you actually, I think, get better ideas,

2:17:31.220 --> 2:17:33.820
 or at least you figure out holes in your theory,

2:17:33.820 --> 2:17:36.260
 or particular experiments that you should run

2:17:36.260 --> 2:17:38.740
 to plug those holes, and so on.

2:17:38.740 --> 2:17:40.340
 Yeah, I'm continually surprised

2:17:40.340 --> 2:17:43.620
 how many really good papers throughout history

2:17:43.620 --> 2:17:47.440
 are quite short and quite simple.

2:17:48.340 --> 2:17:50.180
 And there's a lesson to that.

2:17:50.180 --> 2:17:52.620
 If you want to dream about writing a paper

2:17:52.620 --> 2:17:54.180
 that changes the world,

2:17:54.180 --> 2:17:58.120
 and you wanna go by example, they're usually simple.

2:17:58.120 --> 2:18:01.280
 And that's, it's not cramming,

2:18:01.280 --> 2:18:06.280
 or it's focusing on one idea, and thinking deeply.

2:18:07.200 --> 2:18:10.340
 And you're right that the writing process itself

2:18:10.340 --> 2:18:12.280
 reveals the idea.

2:18:12.280 --> 2:18:15.320
 It challenges you to really think about what is the idea

2:18:15.320 --> 2:18:18.020
 that explains it, the thread that ties it all together.

2:18:19.040 --> 2:18:21.540
 And so a lot of famous researchers I know

2:18:21.540 --> 2:18:24.760
 actually would start off, like, first they were,

2:18:24.760 --> 2:18:27.000
 even before the experiments were in,

2:18:27.000 --> 2:18:28.360
 a lot of them would actually start

2:18:28.360 --> 2:18:30.400
 with writing the introduction of the paper,

2:18:30.400 --> 2:18:32.160
 with zero experiments in.

2:18:32.160 --> 2:18:33.800
 Because that at least helps them figure out

2:18:33.800 --> 2:18:35.800
 what they're trying to solve,

2:18:35.800 --> 2:18:38.660
 and how it fits in the context of things right now.

2:18:38.660 --> 2:18:40.680
 And that would really guide their entire research.

2:18:40.680 --> 2:18:42.360
 So a lot of them would actually first write in intros

2:18:42.360 --> 2:18:43.560
 with zero experiments in,

2:18:43.560 --> 2:18:46.040
 and that's how they would start projects.

2:18:46.040 --> 2:18:48.240
 Some basic questions about people maybe

2:18:49.800 --> 2:18:51.960
 that are more like beginners in this field.

2:18:51.960 --> 2:18:54.080
 What's the best programming language to learn

2:18:54.080 --> 2:18:56.600
 if you're interested in machine learning?

2:18:56.600 --> 2:18:57.440
 I would say Python,

2:18:57.440 --> 2:19:00.320
 just because it's the easiest one to learn.

2:19:00.320 --> 2:19:03.160
 And also a lot of like programming

2:19:03.160 --> 2:19:05.000
 and machine learning happens in Python.

2:19:05.000 --> 2:19:07.600
 So if you don't know any other programming language,

2:19:07.600 --> 2:19:09.560
 Python is actually going to get you a long way.

2:19:09.560 --> 2:19:11.680
 Yeah, it seems like sort of a,

2:19:11.680 --> 2:19:14.000
 it's a toss up question because it seems like Python

2:19:14.000 --> 2:19:16.800
 is so much dominating the space now.

2:19:16.800 --> 2:19:18.520
 But I wonder if there's an interesting alternative.

2:19:18.520 --> 2:19:19.960
 Obviously there's like Swift,

2:19:19.960 --> 2:19:22.740
 and there's a lot of interesting alternatives popping up,

2:19:22.740 --> 2:19:23.960
 even JavaScript.

2:19:23.960 --> 2:19:28.880
 So I, or are more like for the data science applications.

2:19:28.880 --> 2:19:31.240
 But it seems like Python more and more

2:19:31.240 --> 2:19:34.160
 is actually being used to teach like introduction

2:19:34.160 --> 2:19:35.880
 to programming at universities.

2:19:35.880 --> 2:19:38.300
 So it just combines everything very nicely.

2:19:39.840 --> 2:19:41.840
 Even harder question.

2:19:41.840 --> 2:19:46.120
 What are the pros and cons of PyTorch versus TensorFlow?

2:19:46.120 --> 2:19:46.960
 I see.

2:19:48.440 --> 2:19:49.280
 Okay.

2:19:49.280 --> 2:19:51.360
 You can go with no comment.

2:19:51.360 --> 2:19:53.400
 So a disclaimer to this is that the last time

2:19:53.400 --> 2:19:56.400
 I used TensorFlow was probably like four years ago.

2:19:56.400 --> 2:19:58.160
 And so it was right when it had come out

2:19:58.160 --> 2:20:02.660
 because so I started on like deep learning in 2014 or so,

2:20:02.660 --> 2:20:06.480
 and the dominant sort of framework for us then

2:20:06.480 --> 2:20:09.040
 for vision was Cafe, which was out of Berkeley.

2:20:09.040 --> 2:20:11.240
 And we used Cafe a lot, it was really nice.

2:20:12.120 --> 2:20:13.360
 And then TensorFlow came in,

2:20:13.360 --> 2:20:15.080
 which was basically like Python first.

2:20:15.080 --> 2:20:17.040
 So Cafe was mainly C++,

2:20:17.040 --> 2:20:19.040
 and it had like very loose kind of Python binding.

2:20:19.040 --> 2:20:21.320
 So Python wasn't really the first language you would use.

2:20:21.320 --> 2:20:24.680
 You would really use either MATLAB or C++

2:20:24.680 --> 2:20:28.240
 like get stuff done in like Cafe.

2:20:28.240 --> 2:20:30.920
 And then Python of course became popular a little bit later.

2:20:30.920 --> 2:20:32.620
 So TensorFlow was basically around that time.

2:20:32.620 --> 2:20:35.240
 So 2015, 2016 is when I last used it.

2:20:36.120 --> 2:20:37.200
 It's been a while.

2:20:37.200 --> 2:20:40.600
 And then what, did you use Torch or did you?

2:20:40.600 --> 2:20:44.040
 So then I moved to LuaTorch, which was the torch in Lua.

2:20:44.040 --> 2:20:46.780
 And then in 2017, I think basically pretty much

2:20:46.780 --> 2:20:48.420
 to PyTorch completely.

2:20:48.420 --> 2:20:49.260
 Oh, interesting.

2:20:49.260 --> 2:20:50.520
 So you went to Lua, cool.

2:20:50.520 --> 2:20:51.480
 Yeah.

2:20:51.480 --> 2:20:54.200
 Huh, so you were there before it was cool.

2:20:54.200 --> 2:20:56.320
 Yeah, I mean, so LuaTorch was really good

2:20:56.320 --> 2:20:59.000
 because it actually allowed you

2:20:59.000 --> 2:21:01.340
 to do a lot of different kinds of things.

2:21:01.340 --> 2:21:03.880
 So which Cafe was very rigid in terms of its structure.

2:21:03.880 --> 2:21:06.800
 Like you would create a neural network once and that's it.

2:21:06.800 --> 2:21:09.320
 Whereas if you wanted like very dynamic graphs and so on,

2:21:09.320 --> 2:21:10.200
 it was very hard to do that.

2:21:10.200 --> 2:21:11.600
 And LuaTorch was much more friendly

2:21:11.600 --> 2:21:13.560
 for all of these things.

2:21:13.560 --> 2:21:15.600
 Okay, so in terms of PyTorch and TensorFlow,

2:21:15.600 --> 2:21:17.280
 my personal bias is PyTorch

2:21:17.280 --> 2:21:19.080
 just because I've been using it longer

2:21:19.080 --> 2:21:20.780
 and I'm more familiar with it.

2:21:20.780 --> 2:21:23.560
 And also that PyTorch is much easier to debug

2:21:23.560 --> 2:21:26.300
 is what I find because it's imperative in nature

2:21:26.300 --> 2:21:28.620
 compared to like TensorFlow, which is not imperative.

2:21:28.620 --> 2:21:30.480
 But that's telling you a lot that basically

2:21:30.480 --> 2:21:33.320
 the imperative design is sort of a way

2:21:33.320 --> 2:21:35.240
 in which a lot of people are taught programming

2:21:35.240 --> 2:21:38.160
 and that's what actually makes debugging easier for them.

2:21:38.160 --> 2:21:40.480
 So like I learned programming in C, C++.

2:21:40.480 --> 2:21:44.040
 And so for me, imperative way of programming is more natural.

2:21:44.040 --> 2:21:45.280
 Do you think it's good to have

2:21:45.280 --> 2:21:48.480
 kind of these two communities, this kind of competition?

2:21:48.480 --> 2:21:50.680
 I think PyTorch is kind of more and more

2:21:50.680 --> 2:21:52.520
 becoming dominant in the research community,

2:21:52.520 --> 2:21:54.600
 but TensorFlow is still very popular

2:21:54.600 --> 2:21:57.920
 in the more sort of application machine learning community.

2:21:57.920 --> 2:21:59.640
 So do you think it's good to have

2:21:59.640 --> 2:22:02.080
 that kind of split in code bases?

2:22:02.080 --> 2:22:06.560
 Or so like the benefit there is the competition challenges

2:22:06.560 --> 2:22:09.980
 the library developers to step up to a game.

2:22:09.980 --> 2:22:12.720
 But the downside is there's these code bases

2:22:12.720 --> 2:22:15.180
 that are in different libraries.

2:22:15.180 --> 2:22:17.080
 Right, so I think the downside is that,

2:22:17.080 --> 2:22:18.480
 I mean, for a lot of research code

2:22:18.480 --> 2:22:19.640
 that's released in one framework

2:22:19.640 --> 2:22:20.600
 and if you're using the other one,

2:22:20.600 --> 2:22:23.800
 it's really hard to like really build on top of it.

2:22:23.800 --> 2:22:25.800
 But thankfully the open source community

2:22:25.800 --> 2:22:27.080
 in machine learning is amazing.

2:22:27.080 --> 2:22:30.840
 So whenever like something pops up in TensorFlow,

2:22:30.840 --> 2:22:33.200
 you wait a few days and someone who's like super sharp

2:22:33.200 --> 2:22:35.340
 will actually come and translate that particular code

2:22:35.340 --> 2:22:38.380
 based into PyTorch and basically have figured that

2:22:38.380 --> 2:22:39.700
 all the nooks and crannies out.

2:22:39.700 --> 2:22:41.800
 So the open source community is amazing

2:22:41.800 --> 2:22:44.280
 and they really like figure out this gap.

2:22:44.280 --> 2:22:47.560
 So I think in terms of like having these two frameworks

2:22:47.560 --> 2:22:49.720
 or multiple, I think of course there are different use cases

2:22:49.720 --> 2:22:51.080
 so there are going to be benefits

2:22:51.080 --> 2:22:52.840
 to using one or the other framework.

2:22:52.840 --> 2:22:54.720
 And like you said, I think competition is just healthy

2:22:54.720 --> 2:22:57.360
 because both of these frameworks keep

2:22:57.360 --> 2:22:59.060
 or like all of these frameworks really sort of

2:22:59.060 --> 2:23:00.120
 keep learning from each other

2:23:00.120 --> 2:23:01.640
 and keep incorporating different things

2:23:01.640 --> 2:23:03.760
 to just make them better and better.

2:23:03.760 --> 2:23:06.320
 What advice would you have for someone

2:23:06.320 --> 2:23:09.680
 new to machine learning, you know,

2:23:09.680 --> 2:23:11.520
 maybe just started or haven't even started

2:23:11.520 --> 2:23:14.880
 but are curious about it and who want to get in the field?

2:23:14.880 --> 2:23:16.620
 Don't be afraid to get your hands dirty.

2:23:16.620 --> 2:23:17.640
 I think that's the main thing.

2:23:17.640 --> 2:23:19.120
 So if something doesn't work,

2:23:19.120 --> 2:23:22.200
 like really drill into why things are not working.

2:23:22.200 --> 2:23:24.520
 Can you elaborate what your hands dirty means?

2:23:24.520 --> 2:23:27.540
 Right, so for example, like if an algorithm,

2:23:27.540 --> 2:23:29.720
 if you try to train the network and it's not converging,

2:23:29.720 --> 2:23:32.240
 whatever, rather than trying to like Google the answer

2:23:32.240 --> 2:23:33.400
 or trying to do something,

2:23:33.400 --> 2:23:36.320
 like really spend those like five, eight, 10, 15, 20,

2:23:36.320 --> 2:23:37.560
 whatever number of hours really trying

2:23:37.560 --> 2:23:39.000
 to figure it out yourself.

2:23:39.000 --> 2:23:41.320
 Because in that process, you'll actually learn a lot more.

2:23:41.320 --> 2:23:42.520
 Yeah.

2:23:42.520 --> 2:23:44.600
 Googling is of course like a good way to solve it

2:23:44.600 --> 2:23:45.960
 when you need a quick answer.

2:23:45.960 --> 2:23:48.120
 But I think initially, especially like when you're starting

2:23:48.120 --> 2:23:51.840
 out, it's much nicer to like figure things out by yourself.

2:23:51.840 --> 2:23:52.960
 And I just say that from experience

2:23:52.960 --> 2:23:54.280
 because like when I started out,

2:23:54.280 --> 2:23:55.480
 there were not a lot of resources.

2:23:55.480 --> 2:23:57.880
 So we would like in the lab, a lot of us,

2:23:57.880 --> 2:23:59.680
 like we would look up to senior students

2:23:59.680 --> 2:24:01.360
 and then the senior students were of course busy

2:24:01.360 --> 2:24:03.080
 and they would be like, hey, why don't you go figure it out?

2:24:03.080 --> 2:24:04.320
 Because I just don't have the time.

2:24:04.320 --> 2:24:06.480
 I'm working on my dissertation or whatever.

2:24:06.480 --> 2:24:07.640
 I'll find a PhD students.

2:24:07.640 --> 2:24:08.760
 And so then we would sit down

2:24:08.760 --> 2:24:10.480
 and like just try to figure it out.

2:24:10.480 --> 2:24:12.440
 And that I think really helped me.

2:24:12.440 --> 2:24:15.040
 That has really helped me figure a lot of things out.

2:24:15.040 --> 2:24:18.720
 I think in general, if I were to generalize that,

2:24:18.720 --> 2:24:22.720
 I feel like persevering through any kind of struggle

2:24:22.720 --> 2:24:25.640
 on a thing you care about is good.

2:24:25.640 --> 2:24:27.960
 So you're basically, you try to make it seem

2:24:27.960 --> 2:24:30.840
 like it's good to spend time debugging,

2:24:30.840 --> 2:24:33.680
 but really any kind of struggle, whatever form that takes,

2:24:33.680 --> 2:24:36.080
 it could be just Googling a lot.

2:24:36.080 --> 2:24:38.720
 Just basically anything, just sticking with it

2:24:38.720 --> 2:24:41.000
 and going through the hard thing that could take a form

2:24:41.000 --> 2:24:43.200
 of implementing stuff from scratch.

2:24:43.200 --> 2:24:45.600
 It could take the form of re implementing

2:24:45.600 --> 2:24:46.520
 with different libraries

2:24:46.520 --> 2:24:48.320
 or different programming languages.

2:24:49.320 --> 2:24:50.560
 It could take a lot of different forms,

2:24:50.560 --> 2:24:53.520
 but struggle is good for the soul.

2:24:53.520 --> 2:24:55.800
 So like in Pittsburgh, where I did my PhD,

2:24:55.800 --> 2:24:58.360
 the thing was it used to snow a lot.

2:24:58.360 --> 2:25:00.800
 And so when it was snowed, you really couldn't do much.

2:25:00.800 --> 2:25:02.880
 So the thing that a lot of people said

2:25:02.880 --> 2:25:05.320
 was snow builds character.

2:25:05.320 --> 2:25:07.480
 Because when it's snowing, you can't do anything else.

2:25:07.480 --> 2:25:09.040
 You focus on work.

2:25:09.040 --> 2:25:10.800
 Do you have advice in general for people,

2:25:10.800 --> 2:25:13.400
 you've already exceptionally successful, you're young,

2:25:13.400 --> 2:25:15.760
 but do you have advice for young people starting out

2:25:15.760 --> 2:25:18.160
 in college or maybe in high school?

2:25:18.160 --> 2:25:21.040
 Advice for their career, advice for their life,

2:25:21.040 --> 2:25:25.760
 how to pave a successful path in career and life?

2:25:25.760 --> 2:25:27.640
 I would say just be hungry.

2:25:27.640 --> 2:25:29.680
 Always be hungry for what you want.

2:25:29.680 --> 2:25:33.280
 And I think I've been inspired by a lot of people

2:25:33.280 --> 2:25:36.720
 who are just driven and who really go for what they want,

2:25:36.720 --> 2:25:39.440
 no matter what, like you shouldn't want it,

2:25:39.440 --> 2:25:40.480
 you should need it.

2:25:40.480 --> 2:25:41.480
 So if you need something,

2:25:41.480 --> 2:25:44.360
 you basically go towards the ends to make it work.

2:25:44.360 --> 2:25:47.840
 How do you know when you come across a thing

2:25:47.840 --> 2:25:50.280
 that's like you need?

2:25:51.120 --> 2:25:53.080
 I think there's not going to be any single thing

2:25:53.080 --> 2:25:53.920
 that you're going to need.

2:25:53.920 --> 2:25:54.920
 There are going to be different types of things

2:25:54.920 --> 2:25:56.600
 that you need, but whenever you need something,

2:25:56.600 --> 2:25:57.920
 you just go push for it.

2:25:57.920 --> 2:26:00.040
 And of course, once you, you may not get it,

2:26:00.040 --> 2:26:01.960
 or you may find that this was not even the thing

2:26:01.960 --> 2:26:03.640
 that you were looking for, it might be a different thing.

2:26:03.640 --> 2:26:06.240
 But the point is like you're pushing through things

2:26:06.240 --> 2:26:08.960
 and that actually brings a lot of skills

2:26:08.960 --> 2:26:12.880
 and builds a certain kind of attitude

2:26:12.880 --> 2:26:15.680
 which will probably help you get the other thing

2:26:15.680 --> 2:26:18.080
 once you figure out what's really the thing that you want.

2:26:18.080 --> 2:26:20.480
 Yeah, I think a lot of people are,

2:26:20.480 --> 2:26:22.520
 I've noticed, kind of afraid of that

2:26:22.520 --> 2:26:24.880
 is because one, it's a fear of commitment.

2:26:24.880 --> 2:26:26.880
 And two, there's so many amazing things in this world,

2:26:26.880 --> 2:26:28.120
 you almost don't want to miss out

2:26:28.120 --> 2:26:29.440
 on all the other amazing things

2:26:29.440 --> 2:26:31.080
 by committing to this one thing.

2:26:31.080 --> 2:26:32.720
 So I think a lot of it has to do with just

2:26:32.720 --> 2:26:37.720
 allowing yourself to notice that thing

2:26:37.920 --> 2:26:41.560
 and just go all the way with it.

2:26:41.560 --> 2:26:43.240
 I mean, I also like failure, right?

2:26:43.240 --> 2:26:47.280
 So I know this is like super cheesy that failure

2:26:47.280 --> 2:26:49.760
 is something that you should be prepared for and so on,

2:26:49.760 --> 2:26:52.520
 but I do think, I mean, especially in research,

2:26:52.520 --> 2:26:54.400
 for example, failure is something that happens

2:26:54.400 --> 2:26:58.160
 almost every day is like experiments failing

2:26:58.160 --> 2:26:59.080
 and not working.

2:26:59.080 --> 2:27:02.240
 And so you really need to be so used to it.

2:27:02.240 --> 2:27:03.880
 You need to have a thick skin,

2:27:03.880 --> 2:27:06.280
 but, and only basically through,

2:27:06.280 --> 2:27:07.880
 like when you get through it is when you find

2:27:07.880 --> 2:27:09.560
 the one thing that's actually working.

2:27:09.560 --> 2:27:11.840
 So Thomas Edison was like one person like that, right?

2:27:11.840 --> 2:27:13.680
 So I really, like when I was a kid,

2:27:13.680 --> 2:27:17.040
 I used to really read about how he found like the filament,

2:27:17.040 --> 2:27:18.680
 the light bulb filament.

2:27:18.680 --> 2:27:20.560
 And then he, I think his thing was like,

2:27:20.560 --> 2:27:23.120
 he tried 990 things that didn't work

2:27:23.120 --> 2:27:24.320
 or something of the sort.

2:27:24.320 --> 2:27:26.920
 And then they asked him like, so what did you learn?

2:27:26.920 --> 2:27:28.480
 Because all of these were failed experiments.

2:27:28.480 --> 2:27:31.600
 And then he says, oh, these 990 things don't work.

2:27:31.600 --> 2:27:32.440
 And I know that.

2:27:32.440 --> 2:27:33.280
 Did you know that?

2:27:33.280 --> 2:27:35.960
 I mean, that's really inspiring.

2:27:35.960 --> 2:27:38.480
 So you spent a few years on this earth

2:27:38.480 --> 2:27:43.480
 performing a self supervised kind of learning process.

2:27:43.960 --> 2:27:46.400
 Have you figured out the meaning of life yet?

2:27:46.400 --> 2:27:47.720
 I told you I'm doing this podcast

2:27:47.720 --> 2:27:49.120
 to try to get the answer.

2:27:49.120 --> 2:27:50.720
 I'm hoping you could tell me,

2:27:50.720 --> 2:27:52.920
 what do you think the meaning of it all is?

2:27:54.320 --> 2:27:55.800
 I don't think I figured this out.

2:27:55.800 --> 2:27:57.120
 No, I have no idea.

2:27:57.120 --> 2:28:02.120
 Do you think AI will help us figure it out

2:28:02.560 --> 2:28:03.880
 or do you think there's no answer?

2:28:03.880 --> 2:28:05.480
 The whole point is to keep searching.

2:28:05.480 --> 2:28:08.800
 I think, yeah, I think it's an endless sort of quest for us.

2:28:08.800 --> 2:28:10.560
 I don't think AI will help us there.

2:28:10.560 --> 2:28:13.600
 This is like a very hard, hard, hard question

2:28:13.600 --> 2:28:15.440
 which so many humans have tried to answer.

2:28:15.440 --> 2:28:16.400
 Well, that's the interesting thing

2:28:16.400 --> 2:28:18.600
 about the difference between AI and humans.

2:28:19.560 --> 2:28:21.880
 Humans don't seem to know what the hell they're doing.

2:28:21.880 --> 2:28:23.720
 And AI is almost always operating

2:28:23.720 --> 2:28:27.440
 under well defined objective functions.

2:28:28.360 --> 2:28:33.360
 And I wonder whether our lack of ability

2:28:33.680 --> 2:28:37.240
 to define good longterm objective functions

2:28:37.240 --> 2:28:40.400
 or introspect what is the objective function

2:28:40.400 --> 2:28:44.400
 under which we operate, if that's a feature or a bug.

2:28:44.400 --> 2:28:45.240
 I would say it's a feature

2:28:45.240 --> 2:28:47.440
 because then everyone actually has very different kinds

2:28:47.440 --> 2:28:49.360
 of objective functions that they're optimizing

2:28:49.360 --> 2:28:51.320
 and those objective functions evolve

2:28:51.320 --> 2:28:53.400
 and change dramatically through the course

2:28:53.400 --> 2:28:54.240
 of their life.

2:28:54.240 --> 2:28:56.000
 That's actually what makes us interesting, right?

2:28:56.000 --> 2:28:58.040
 If otherwise, like if everyone was doing

2:28:58.040 --> 2:29:00.560
 the exact same thing, that would be pretty boring.

2:29:00.560 --> 2:29:02.600
 We do want like people with different kinds

2:29:02.600 --> 2:29:06.160
 of perspectives, also people evolve continuously.

2:29:06.160 --> 2:29:09.320
 That's like, I would say the biggest feature of being human.

2:29:09.320 --> 2:29:11.160
 And then we get to like the ones that die

2:29:11.160 --> 2:29:12.560
 because they do something stupid.

2:29:12.560 --> 2:29:15.440
 We get to watch that, see it and learn from it.

2:29:15.440 --> 2:29:20.360
 And as a species, we take that lesson

2:29:20.360 --> 2:29:22.600
 and become better and better

2:29:22.600 --> 2:29:24.280
 because of all the dumb people in the world

2:29:24.280 --> 2:29:28.160
 that died doing something wild and beautiful.

2:29:29.080 --> 2:29:31.840
 Ishan, thank you so much for this incredible conversation.

2:29:31.840 --> 2:29:36.840
 We did a depth first search through the space

2:29:37.080 --> 2:29:41.640
 of machine learning and it was fun and fascinating.

2:29:41.640 --> 2:29:43.920
 So it's really an honor to meet you

2:29:43.920 --> 2:29:45.760
 and it was a really awesome conversation.

2:29:45.760 --> 2:29:48.200
 Thanks for coming down today and talking with me.

2:29:48.200 --> 2:29:50.240
 Thanks Lex, I mean, I've listened to you.

2:29:50.240 --> 2:29:52.400
 I told you it was unreal for me to actually meet you

2:29:52.400 --> 2:29:55.000
 in person and I'm so happy to be here, thank you.

2:29:55.000 --> 2:29:55.840
 Thanks man.

2:29:56.680 --> 2:29:58.200
 Thanks for listening to this conversation

2:29:58.200 --> 2:30:01.280
 with Ishan Misra and thank you to Onnit,

2:30:01.280 --> 2:30:05.280
 The Information, Grammarly and Athletic Greens.

2:30:05.280 --> 2:30:08.560
 Check them out in the description to support this podcast.

2:30:08.560 --> 2:30:10.440
 And now let me leave you with some words

2:30:10.440 --> 2:30:12.480
 from Arthur C. Clarke.

2:30:12.480 --> 2:30:14.920
 Any sufficiently advanced technology

2:30:14.920 --> 2:30:18.120
 is indistinguishable from magic.

2:30:18.120 --> 2:30:23.120
 Thank you for listening and hope to see you next time.

