WEBVTT

00:00.000 --> 00:02.800
 By the time he gets to 2045,

00:02.800 --> 00:05.320
 we'll be able to multiply our intelligence

00:05.320 --> 00:07.680
 many millions fold.

00:07.680 --> 00:10.840
 And it's just very hard to imagine what that will be like.

00:13.560 --> 00:16.840
 The following is a conversation with Ray Kurzweil,

00:16.840 --> 00:19.480
 author, inventor, and futurist,

00:19.480 --> 00:22.280
 who has an optimistic view of our future

00:22.280 --> 00:24.320
 as a human civilization,

00:24.320 --> 00:27.280
 predicting that exponentially improving technologies

00:27.280 --> 00:29.880
 will take us to a point of a singularity

00:29.880 --> 00:33.480
 beyond which superintelligent artificial intelligence

00:33.480 --> 00:38.380
 will transform our world in nearly unimaginable ways.

00:38.380 --> 00:41.280
 18 years ago, in the book Singularity is Near,

00:41.280 --> 00:44.000
 he predicted that the onset of the singularity

00:44.000 --> 00:47.360
 will happen in the year 2045.

00:47.360 --> 00:50.800
 He still holds to this prediction and estimate.

00:50.800 --> 00:53.440
 In fact, he's working on a new book on this topic

00:53.440 --> 00:55.640
 that will hopefully be out next year.

00:56.520 --> 00:58.320
 This is the Lex Friedman podcast.

00:58.320 --> 01:00.360
 To support it, please check out our sponsors

01:00.360 --> 01:01.640
 in the description.

01:01.640 --> 01:05.360
 And now, dear friends, here's Ray Kurzweil.

01:06.360 --> 01:10.960
 In your 2005 book titled The Singularity is Near,

01:10.960 --> 01:15.400
 you predicted that the singularity will happen in 2045.

01:15.400 --> 01:18.460
 So now, 18 years later, do you still estimate

01:18.460 --> 01:22.480
 that the singularity will happen on 2045?

01:22.480 --> 01:24.960
 And maybe first, what is the singularity,

01:24.960 --> 01:27.760
 the technological singularity, and when will it happen?

01:27.760 --> 01:31.640
 Singularity is where computers really change our view

01:31.640 --> 01:35.840
 of what's important and change who we are.

01:35.840 --> 01:39.560
 But we're getting close to some salient things

01:39.560 --> 01:42.800
 that will change who we are.

01:42.800 --> 01:45.680
 A key thing is 2029,

01:45.680 --> 01:49.060
 when computers will pass the Turing test.

01:50.120 --> 01:51.520
 And there's also some controversy

01:51.520 --> 01:53.680
 whether the Turing test is valid.

01:53.680 --> 01:55.080
 I believe it is.

01:55.080 --> 01:57.920
 Most people do believe that,

01:57.920 --> 01:59.680
 but there's some controversy about that.

01:59.680 --> 02:04.680
 But Stanford got very alarmed at my prediction about 2029.

02:06.520 --> 02:10.520
 I made this in 1999 in my book.

02:10.520 --> 02:12.120
 The Age of Spiritual Machines.

02:12.120 --> 02:12.960
 Right.

02:12.960 --> 02:15.600
 And then you repeated the prediction in 2005.

02:15.600 --> 02:16.600
 In 2005.

02:16.600 --> 02:17.520
 Yeah.

02:17.520 --> 02:19.480
 So they held an international conference,

02:19.480 --> 02:20.800
 you might have been aware of it,

02:20.800 --> 02:25.800
 of AI experts in 1999 to assess this view.

02:26.620 --> 02:29.580
 So people gave different predictions,

02:29.580 --> 02:30.840
 and they took a poll.

02:30.840 --> 02:34.240
 It was really the first time that AI experts worldwide

02:34.240 --> 02:36.620
 were polled on this prediction.

02:37.720 --> 02:39.980
 And the average poll was 100 years.

02:41.420 --> 02:44.320
 20% believed it would never happen.

02:44.320 --> 02:48.120
 And that was the view in 1999.

02:48.120 --> 02:50.640
 80% believed it would happen,

02:50.640 --> 02:53.200
 but not within their lifetimes.

02:53.200 --> 02:55.840
 There's been so many advances in AI

02:56.920 --> 03:01.840
 that the poll of AI experts has come down over the years.

03:01.840 --> 03:05.440
 So a year ago, something called Meticulous,

03:05.440 --> 03:07.080
 which you may be aware of,

03:07.080 --> 03:11.560
 assesses different types of experts on the future.

03:11.560 --> 03:16.440
 They again assessed what AI experts then felt.

03:16.440 --> 03:18.940
 And they were saying 2042.

03:18.940 --> 03:20.440
 For the Turing test.

03:20.440 --> 03:22.440
 For the Turing test.

03:22.440 --> 03:23.560
 So it's coming down.

03:23.560 --> 03:26.320
 And I was still saying 2029.

03:26.320 --> 03:30.200
 A few weeks ago, they again did another poll,

03:30.200 --> 03:31.660
 and it was 2030.

03:32.960 --> 03:37.920
 So AI experts now basically agree with me.

03:37.920 --> 03:41.220
 I haven't changed at all, I've stayed with 2029.

03:42.820 --> 03:44.520
 And AI experts now agree with me,

03:44.520 --> 03:46.840
 but they didn't agree at first.

03:46.840 --> 03:50.120
 So Alan Turing formulated the Turing test,

03:50.120 --> 03:50.960
 and...

03:50.960 --> 03:54.480
 Right, now, what he said was very little about it.

03:54.480 --> 03:55.920
 I mean, the 1950 paper

03:55.920 --> 03:58.060
 where he had articulated the Turing test,

03:59.440 --> 04:04.440
 there's like a few lines that talk about the Turing test.

04:06.840 --> 04:11.840
 And it really wasn't very clear how to administer it.

04:12.040 --> 04:16.520
 And he said if they did it in like 15 minutes,

04:16.520 --> 04:17.680
 that would be sufficient,

04:17.680 --> 04:20.600
 which I don't really think is the case.

04:20.600 --> 04:22.960
 These large language models now,

04:22.960 --> 04:25.560
 some people are convinced by it already.

04:25.560 --> 04:28.440
 I mean, you can talk to it and have a conversation with it.

04:28.440 --> 04:30.340
 You can actually talk to it for hours.

04:31.760 --> 04:35.360
 So it requires a little more depth.

04:35.360 --> 04:38.120
 There's some problems with large language models

04:38.120 --> 04:39.620
 which we can talk about.

04:41.840 --> 04:46.460
 But some people are convinced by the Turing test.

04:46.460 --> 04:50.160
 Now, if somebody passes the Turing test,

04:50.160 --> 04:52.160
 what are the implications of that?

04:52.160 --> 04:53.720
 Does that mean that they're sentient,

04:53.720 --> 04:56.280
 that they're conscious or not?

04:56.280 --> 05:00.880
 It's not necessarily clear what the implications are.

05:00.880 --> 05:05.880
 Anyway, I believe 2029, that's six, seven years from now,

05:07.640 --> 05:10.360
 we'll have something that passes the Turing test

05:10.360 --> 05:12.480
 and a valid Turing test,

05:12.480 --> 05:15.320
 meaning it goes for hours, not just a few minutes.

05:15.320 --> 05:16.600
 Can you speak to that a little bit?

05:16.600 --> 05:21.160
 What is your formulation of the Turing test?

05:21.160 --> 05:23.180
 You've proposed a very difficult version

05:23.180 --> 05:25.420
 of the Turing test, so what does that look like?

05:25.420 --> 05:28.560
 Basically, it's just to assess it over several hours

05:30.760 --> 05:35.760
 and also have a human judge that's fairly sophisticated

05:36.440 --> 05:39.220
 on what computers can do and can't do.

05:40.800 --> 05:43.800
 If you take somebody who's not that sophisticated

05:43.800 --> 05:47.240
 or even an average engineer,

05:48.360 --> 05:52.080
 they may not really assess various aspects of it.

05:52.080 --> 05:55.680
 So you really want the human to challenge the system.

05:55.680 --> 05:57.040
 Exactly, exactly.

05:57.040 --> 05:58.520
 On its ability to do things

05:58.520 --> 06:00.800
 like common sense reasoning, perhaps.

06:00.800 --> 06:04.680
 That's actually a key problem with large language models.

06:04.680 --> 06:08.080
 They don't do these kinds of tests

06:08.080 --> 06:13.080
 that would involve assessing chains of reasoning,

06:17.400 --> 06:18.960
 but you can lose track of that.

06:18.960 --> 06:20.200
 If you talk to them,

06:20.200 --> 06:22.760
 they actually can talk to you pretty well

06:22.760 --> 06:24.840
 and you can be convinced by it,

06:24.840 --> 06:27.400
 but it's somebody that would really convince you

06:27.400 --> 06:32.200
 that it's a human, whatever that takes.

06:32.200 --> 06:34.800
 Maybe it would take days or weeks,

06:34.800 --> 06:38.720
 but it would really convince you that it's human.

06:40.880 --> 06:45.320
 Large language models can appear that way.

06:45.320 --> 06:49.760
 You can read conversations and they appear pretty good.

06:49.760 --> 06:52.260
 There are some problems with it.

06:52.260 --> 06:54.140
 It doesn't do math very well.

06:55.000 --> 06:58.160
 You can ask how many legs did 10 elephants have

06:58.160 --> 07:00.020
 and they'll tell you, well, okay,

07:00.020 --> 07:01.440
 each elephant has four legs

07:01.440 --> 07:03.700
 and it's 10 elephants, so it's 40 legs.

07:03.700 --> 07:05.840
 And you go, okay, that's pretty good.

07:05.840 --> 07:07.960
 How many legs do 11 elephants have?

07:07.960 --> 07:11.520
 And they don't seem to understand the question.

07:11.520 --> 07:14.160
 Do all humans understand that question?

07:14.160 --> 07:15.880
 No, that's the key thing.

07:15.880 --> 07:19.440
 I mean, how advanced a human do you want it to be?

07:19.440 --> 07:21.360
 But we do expect a human

07:21.360 --> 07:23.980
 to be able to do multi chain reasoning,

07:24.840 --> 07:26.320
 to be able to take a few facts

07:26.320 --> 07:29.840
 and put them together, not perfectly.

07:29.840 --> 07:32.800
 And we see that in a lot of polls

07:32.800 --> 07:35.540
 that people don't do that perfectly at all.

07:39.220 --> 07:42.020
 So it's not very well defined,

07:42.020 --> 07:44.320
 but it's something where it really would convince you

07:44.320 --> 07:45.600
 that it's a human.

07:45.600 --> 07:48.840
 Is your intuition that large language models

07:48.840 --> 07:52.320
 will not be solely the kind of system

07:52.320 --> 07:55.600
 that passes the Turing test in 2029?

07:55.600 --> 07:56.800
 Do we need something else?

07:56.800 --> 07:58.720
 No, I think it will be a large language model,

07:58.720 --> 08:02.960
 but they have to go beyond what they're doing now.

08:02.960 --> 08:04.400
 I think we're getting there.

08:05.760 --> 08:09.240
 And another key issue is if somebody

08:09.240 --> 08:12.200
 actually passes the Turing test validly,

08:12.200 --> 08:13.640
 I would believe they're conscious.

08:13.640 --> 08:15.000
 And then not everybody would say that.

08:15.000 --> 08:17.440
 It's okay, we can pass the Turing test,

08:17.440 --> 08:20.080
 but we don't really believe that it's conscious.

08:20.080 --> 08:21.480
 That's a whole nother issue.

08:23.120 --> 08:24.920
 But if it really passes the Turing test,

08:24.920 --> 08:26.720
 I would believe that it's conscious.

08:26.720 --> 08:31.720
 But I don't believe that of large language models today.

08:32.760 --> 08:35.520
 If it appears to be conscious,

08:35.520 --> 08:38.240
 that's as good as being conscious, at least for you,

08:38.240 --> 08:40.700
 in some sense.

08:40.700 --> 08:45.300
 I mean, consciousness is not something that's scientific.

08:46.640 --> 08:48.880
 I mean, I believe you're conscious,

08:49.760 --> 08:51.100
 but it's really just a belief,

08:51.100 --> 08:52.800
 and we believe that about other humans

08:52.800 --> 08:57.400
 that at least appear to be conscious.

08:57.400 --> 09:00.460
 When you go outside of shared human assumption,

09:01.720 --> 09:03.640
 like are animals conscious?

09:04.520 --> 09:06.200
 Some people believe they're not conscious.

09:06.200 --> 09:08.680
 Some people believe they are conscious.

09:08.680 --> 09:13.680
 And would a machine that acts just like a human be conscious?

09:14.520 --> 09:16.200
 I mean, I believe it would be.

09:17.040 --> 09:20.800
 But that's really a philosophical belief.

09:20.800 --> 09:22.720
 You can't prove it.

09:22.720 --> 09:25.480
 I can't take an entity and prove that it's conscious.

09:25.480 --> 09:27.280
 There's nothing that you can do

09:27.280 --> 09:30.360
 that would indicate that.

09:30.360 --> 09:32.780
 It's like saying a piece of art is beautiful.

09:32.780 --> 09:35.000
 You can say it.

09:35.000 --> 09:38.200
 Multiple people can experience a piece of art as beautiful,

09:39.300 --> 09:41.320
 but you can't prove it.

09:41.320 --> 09:44.840
 But it's also an extremely important issue.

09:44.840 --> 09:47.040
 I mean, imagine if you had something

09:47.040 --> 09:49.140
 where nobody's conscious.

09:49.140 --> 09:52.660
 The world may as well not exist.

09:55.660 --> 10:00.060
 And so some people, like say Marvin Minsky,

10:02.620 --> 10:05.940
 said, well, consciousness is not logical,

10:05.940 --> 10:08.380
 it's not scientific, and therefore we should dismiss it,

10:08.380 --> 10:13.380
 and any talk about consciousness is just not to be believed.

10:15.500 --> 10:18.500
 But when he actually engaged with somebody

10:18.500 --> 10:20.660
 who was conscious, he actually acted

10:20.660 --> 10:22.620
 as if they were conscious.

10:22.620 --> 10:24.260
 He didn't ignore that.

10:24.260 --> 10:26.860
 He acted as if consciousness does matter.

10:26.860 --> 10:28.180
 Exactly.

10:28.180 --> 10:30.500
 Whereas he said it didn't matter.

10:30.500 --> 10:31.780
 Well, that's Marvin Minsky.

10:31.780 --> 10:32.620
 Yeah.

10:32.620 --> 10:34.060
 He's full of contradictions.

10:34.060 --> 10:37.660
 But that's true of a lot of people as well.

10:37.660 --> 10:39.620
 But to you, consciousness matters.

10:39.620 --> 10:42.160
 But to me, it's very important.

10:42.160 --> 10:45.640
 But I would say it's not a scientific issue.

10:45.640 --> 10:49.240
 It's a philosophical issue.

10:49.240 --> 10:50.720
 And people have different views.

10:50.720 --> 10:52.800
 Some people believe that anything

10:52.800 --> 10:54.520
 that makes a decision is conscious.

10:54.520 --> 10:56.760
 So your light switch is conscious.

10:56.760 --> 10:59.400
 Its level of consciousness is low,

10:59.400 --> 11:03.400
 not very interesting, but that's a consciousness.

11:05.120 --> 11:09.120
 So a computer that makes a more interesting decision

11:09.120 --> 11:10.440
 is still not at human levels,

11:10.440 --> 11:12.560
 but it's also conscious and at a higher level

11:12.560 --> 11:13.720
 than your light switch.

11:13.720 --> 11:15.980
 So that's one view.

11:17.360 --> 11:20.080
 There's many different views of what consciousness is.

11:20.080 --> 11:22.880
 So if a system passes the Turing test,

11:24.600 --> 11:29.600
 it's not scientific, but in issues of philosophy,

11:30.000 --> 11:32.600
 things like ethics start to enter the picture.

11:32.600 --> 11:35.500
 Do you think there would be,

11:35.500 --> 11:38.920
 we would start contending as a human species

11:39.920 --> 11:42.840
 about the ethics of turning off such a machine?

11:42.840 --> 11:46.480
 Yeah, I mean, that's definitely come up.

11:47.400 --> 11:49.600
 Hasn't come up in reality yet.

11:49.600 --> 11:50.560
 Yet.

11:50.560 --> 11:52.400
 But I'm talking about 2029.

11:52.400 --> 11:54.180
 It's not that many years from now.

11:56.080 --> 11:58.480
 So what are our obligations to it?

11:59.960 --> 12:03.240
 It has a different, I mean, a computer that's conscious,

12:03.240 --> 12:08.240
 it has a little bit different connotations than a human.

12:08.240 --> 12:13.240
 We have a continuous consciousness.

12:15.600 --> 12:19.640
 We're in an entity that does not last forever.

12:22.080 --> 12:27.080
 Now, actually, a significant portion of humans still exist

12:27.400 --> 12:29.240
 and are therefore still conscious.

12:31.760 --> 12:34.880
 But anybody who is over a certain age

12:34.880 --> 12:37.160
 doesn't exist anymore.

12:37.160 --> 12:40.320
 That wouldn't be true of a computer program.

12:40.320 --> 12:42.000
 You could completely turn it off

12:42.000 --> 12:46.120
 and a copy of it could be stored and you could recreate it.

12:46.120 --> 12:49.920
 And so it has a different type of validity.

12:51.160 --> 12:52.920
 You could actually take it back in time.

12:52.920 --> 12:55.840
 You could eliminate its memory and have it go over again.

12:55.840 --> 12:59.800
 I mean, it has a different kind of connotation

12:59.800 --> 13:01.800
 than humans do.

13:01.800 --> 13:04.400
 Well, perhaps it can do the same thing with humans.

13:04.400 --> 13:06.880
 It's just that we don't know how to do that yet.

13:06.880 --> 13:09.400
 It's possible that we figure out all of these things

13:09.400 --> 13:10.780
 on the machine first.

13:12.320 --> 13:15.480
 But that doesn't mean the machine isn't conscious.

13:15.480 --> 13:17.640
 I mean, if you look at the way people react,

13:17.640 --> 13:22.640
 say, 3CPO or other machines that are conscious in movies,

13:25.000 --> 13:26.740
 they don't actually present how it's conscious,

13:26.740 --> 13:30.120
 but we see that they are a machine

13:30.120 --> 13:33.280
 and people will believe that they are conscious

13:33.280 --> 13:34.640
 and they'll actually worry about it

13:34.640 --> 13:37.480
 if they get into trouble and so on.

13:37.480 --> 13:40.840
 So 2029 is going to be the first year

13:40.840 --> 13:43.440
 when a major thing happens.

13:43.440 --> 13:44.280
 Right.

13:44.280 --> 13:46.520
 And that will shake our civilization

13:46.520 --> 13:50.280
 to start to consider the role of AI in this world.

13:50.280 --> 13:51.120
 Yes and no.

13:51.120 --> 13:54.560
 I mean, this one guy at Google claimed

13:54.560 --> 13:58.440
 that the machine was conscious.

13:58.440 --> 14:00.160
 But that's just one person.

14:00.160 --> 14:01.000
 Right.

14:01.000 --> 14:03.080
 When it starts to happen to scale.

14:03.080 --> 14:06.320
 Well, that's exactly right because most people

14:06.320 --> 14:07.760
 have not taken that position.

14:07.760 --> 14:08.940
 I don't take that position.

14:08.940 --> 14:13.940
 I mean, I've used different things like this

14:17.240 --> 14:20.500
 and they don't appear to me to be conscious.

14:20.500 --> 14:22.840
 As we eliminate various problems

14:22.840 --> 14:25.840
 of these large language models,

14:26.960 --> 14:30.480
 more and more people will accept that they're conscious.

14:30.480 --> 14:35.480
 So when we get to 2029, I think a large fraction

14:35.760 --> 14:37.960
 of people will believe that they're conscious.

14:39.080 --> 14:41.040
 So it's not gonna happen all at once.

14:42.440 --> 14:44.360
 I believe it will actually happen gradually

14:44.360 --> 14:46.240
 and it's already started to happen.

14:47.280 --> 14:52.280
 And so that takes us one step closer to the singularity.

14:52.360 --> 14:55.560
 Another step then is in the 2030s

14:55.560 --> 14:59.800
 when we can actually connect our neocortex,

14:59.800 --> 15:04.800
 which is where we do our thinking, to computers.

15:04.880 --> 15:09.280
 And I mean, just as this actually gains a lot

15:09.280 --> 15:12.200
 to being connected to computers

15:12.200 --> 15:15.360
 that will amplify its abilities,

15:15.360 --> 15:17.400
 I mean, if this did not have any connection,

15:17.400 --> 15:19.360
 it would be pretty stupid.

15:19.360 --> 15:21.860
 It could not answer any of your questions.

15:21.860 --> 15:24.400
 If you're just listening to this, by the way,

15:24.400 --> 15:29.400
 Ray's holding up the all powerful smartphone.

15:29.400 --> 15:32.480
 So we're gonna do that directly from our brains.

15:33.520 --> 15:35.040
 I mean, these are pretty good.

15:35.040 --> 15:37.720
 These already have amplified our intelligence.

15:37.720 --> 15:40.040
 I'm already much smarter than I would otherwise be

15:40.040 --> 15:41.480
 if I didn't have this.

15:42.600 --> 15:44.240
 Because I remember my first book,

15:44.240 --> 15:45.920
 The Age of Intelligent Machines,

15:49.060 --> 15:52.080
 there was no way to get information from computers.

15:52.080 --> 15:55.400
 I actually would go to a library, find a book,

15:55.400 --> 15:58.440
 find the page that had an information I wanted,

15:58.440 --> 15:59.920
 and I'd go to the copier,

15:59.920 --> 16:04.360
 and my most significant information tool

16:04.360 --> 16:08.480
 was a roll of quarters where I could feed the copier.

16:08.480 --> 16:11.400
 So we're already greatly advanced

16:11.400 --> 16:13.280
 that we have these things.

16:13.280 --> 16:15.460
 There's a few problems with it.

16:15.460 --> 16:17.280
 First of all, I constantly put it down,

16:17.280 --> 16:19.680
 and I don't remember where I put it.

16:19.680 --> 16:21.220
 I've actually never lost it.

16:21.220 --> 16:26.080
 But you have to find it, and then you have to turn it on.

16:26.080 --> 16:28.160
 So there's a certain amount of steps.

16:28.160 --> 16:30.100
 It would actually be quite useful

16:30.100 --> 16:33.440
 if someone would just listen to your conversation

16:33.440 --> 16:38.440
 and say, oh, that's so and so actress,

16:38.920 --> 16:41.160
 and tell you what you're talking about.

16:41.160 --> 16:43.160
 So going from active to passive,

16:43.160 --> 16:46.240
 where it just permeates your whole life.

16:46.240 --> 16:47.280
 Yeah, exactly.

16:47.280 --> 16:49.560
 The way your brain does when you're awake.

16:49.560 --> 16:51.220
 Your brain is always there.

16:51.220 --> 16:52.060
 Right.

16:52.060 --> 16:53.800
 That's something that could actually

16:53.800 --> 16:55.840
 just about be done today,

16:55.840 --> 16:57.400
 where we'd listen to your conversation,

16:57.400 --> 16:58.600
 understand what you're saying,

16:58.600 --> 17:01.840
 understand what you're not missing,

17:01.840 --> 17:03.600
 and give you that information.

17:04.520 --> 17:07.300
 But another step is to actually go inside your brain.

17:09.720 --> 17:12.740
 And there are some prototypes

17:12.740 --> 17:15.280
 where you can connect your brain.

17:15.280 --> 17:17.040
 They actually don't have the amount

17:17.040 --> 17:19.160
 of bandwidth that we need.

17:19.160 --> 17:21.940
 They can work, but they work fairly slowly.

17:21.940 --> 17:26.160
 So if it actually would connect to your neocortex,

17:26.160 --> 17:30.180
 and the neocortex, which I describe

17:30.180 --> 17:31.740
 in How to Create a Mind,

17:33.000 --> 17:34.820
 the neocortex is actually,

17:36.700 --> 17:38.180
 it has different levels,

17:38.180 --> 17:39.980
 and as you go up the levels,

17:39.980 --> 17:41.780
 it's kind of like a pyramid.

17:41.780 --> 17:44.340
 The top level is fairly small,

17:44.340 --> 17:46.540
 and that's the level where you wanna connect

17:47.820 --> 17:50.140
 these brain extenders.

17:50.140 --> 17:55.140
 And so I believe that will happen in the 2030s.

17:58.100 --> 18:01.580
 So just the way this is greatly amplified

18:01.580 --> 18:03.480
 by being connected to the cloud,

18:04.420 --> 18:07.420
 we can connect our own brain to the cloud,

18:07.420 --> 18:12.420
 and just do what we can do by using this machine.

18:14.260 --> 18:15.660
 Do you think it would look like

18:15.660 --> 18:18.920
 the brain computer interface of like Neuralink?

18:18.920 --> 18:19.760
 So would it be?

18:19.760 --> 18:22.500
 Well, Neuralink, it's an attempt to do that.

18:22.500 --> 18:24.920
 It doesn't have the bandwidth that we need.

18:26.300 --> 18:27.660
 Yet, right?

18:27.660 --> 18:29.240
 Right, but I think,

18:30.320 --> 18:31.980
 I mean, they're gonna get permission for this

18:31.980 --> 18:33.160
 because there are a lot of people

18:33.160 --> 18:36.660
 who absolutely need it because they can't communicate.

18:36.660 --> 18:38.420
 I know a couple people like that

18:38.420 --> 18:41.220
 who have ideas and they cannot,

18:42.660 --> 18:44.580
 they cannot move their muscles and so on.

18:44.580 --> 18:45.800
 They can't communicate.

18:45.800 --> 18:50.800
 And so for them, this would be very valuable,

18:52.040 --> 18:53.320
 but we could all use it.

18:54.820 --> 18:56.600
 Basically, it'd be,

18:59.000 --> 19:02.520
 turn us into something that would be like we have a phone,

19:02.520 --> 19:05.120
 but it would be in our minds.

19:05.120 --> 19:07.360
 It would be kind of instantaneous.

19:07.360 --> 19:09.440
 And maybe communication between two people

19:09.440 --> 19:14.080
 would not require this low bandwidth mechanism of language.

19:14.080 --> 19:15.640
 Yes, exactly.

19:15.640 --> 19:17.280
 We don't know what that would be,

19:17.280 --> 19:22.280
 although we do know that computers can share information

19:22.400 --> 19:24.640
 like language instantly.

19:24.640 --> 19:28.880
 They can share many, many books in a second.

19:28.880 --> 19:31.200
 So we could do that as well.

19:31.200 --> 19:34.240
 If you look at what our brain does,

19:34.240 --> 19:39.120
 it actually can manipulate different parameters.

19:39.120 --> 19:44.120
 So we talk about these large language models.

19:46.560 --> 19:48.320
 I mean, I had written that

19:51.520 --> 19:55.000
 it requires a certain amount of information

19:55.000 --> 19:57.520
 in order to be effective

19:58.600 --> 20:01.920
 and that we would not see AI really being effective

20:01.920 --> 20:03.480
 until it got to that level.

20:04.400 --> 20:06.400
 And we had large language models

20:06.400 --> 20:09.600
 that were like 10 billion bytes, didn't work very well.

20:09.600 --> 20:11.680
 They finally got to a hundred billion bytes

20:11.680 --> 20:13.440
 and now they work fairly well.

20:13.440 --> 20:16.280
 And now we're going to a trillion bytes.

20:16.280 --> 20:21.280
 If you say lambda has a hundred billion bytes,

20:22.520 --> 20:23.520
 what does that mean?

20:23.520 --> 20:27.160
 Well, what if you had something that had one byte,

20:27.160 --> 20:30.000
 one parameter, maybe you wanna tell

20:30.000 --> 20:33.960
 whether or not something's an elephant or not.

20:33.960 --> 20:37.680
 And so you put in something that would detect its trunk.

20:37.680 --> 20:39.160
 If it has a trunk, it's an elephant.

20:39.160 --> 20:41.680
 If it doesn't have a trunk, it's not an elephant.

20:41.680 --> 20:44.400
 That would work fairly well.

20:44.400 --> 20:46.320
 There's a few problems with it.

20:47.440 --> 20:49.720
 And it really wouldn't be able to tell what a trunk is,

20:49.720 --> 20:50.640
 but anyway.

20:50.640 --> 20:54.120
 And maybe other things other than elephants have trunks,

20:54.120 --> 20:55.560
 you might get really confused.

20:55.560 --> 20:56.960
 Yeah, exactly.

20:56.960 --> 20:58.800
 I'm not sure which animals have trunks,

20:58.800 --> 21:02.400
 but how do you define a trunk?

21:02.400 --> 21:04.000
 But yeah, that's one parameter.

21:04.960 --> 21:06.400
 You can do okay.

21:06.400 --> 21:08.760
 So these things have a hundred billion parameters.

21:08.760 --> 21:12.200
 So they're able to deal with very complex issues.

21:12.200 --> 21:14.000
 All kinds of trunks.

21:14.000 --> 21:16.240
 Human beings actually have a little bit more than that,

21:16.240 --> 21:17.960
 but they're getting to the point

21:17.960 --> 21:20.000
 where they can emulate humans.

21:22.400 --> 21:27.400
 If we were able to connect this to our neocortex,

21:27.400 --> 21:32.400
 we would basically add more of these abilities

21:33.400 --> 21:35.400
 to make distinctions,

21:35.400 --> 21:37.600
 and it could ultimately be much smarter

21:37.600 --> 21:39.680
 and also be attached to information

21:39.680 --> 21:42.240
 that we feel is reliable.

21:43.800 --> 21:45.240
 So that's where we're headed.

21:45.240 --> 21:49.120
 So you think that there will be a merger in the 30s,

21:49.120 --> 21:50.880
 an increasing amount of merging

21:50.880 --> 21:55.880
 between the human brain and the AI brain?

21:55.880 --> 21:57.640
 Exactly.

21:57.640 --> 22:02.280
 And the AI brain is really an emulation of human beings.

22:02.280 --> 22:04.480
 I mean, that's why we're creating them,

22:04.480 --> 22:07.200
 because human beings act the same way,

22:07.200 --> 22:09.600
 and this is basically to amplify them.

22:09.600 --> 22:11.600
 I mean, this amplifies our brain.

22:13.800 --> 22:15.560
 It's a little bit clumsy to interact with,

22:15.560 --> 22:20.560
 but it definitely is way beyond what we had 15 years ago.

22:21.840 --> 22:23.520
 But the implementation becomes different,

22:23.520 --> 22:26.680
 just like a bird versus the airplane,

22:26.680 --> 22:30.600
 even though the AI brain is an emulation,

22:30.600 --> 22:34.360
 it starts adding features we might not otherwise have,

22:34.360 --> 22:36.280
 like ability to consume a huge amount

22:36.280 --> 22:38.520
 of information quickly,

22:38.520 --> 22:43.080
 like look up thousands of Wikipedia articles in one take.

22:43.080 --> 22:44.200
 Exactly.

22:44.200 --> 22:46.320
 I mean, we can get, for example,

22:46.320 --> 22:48.120
 issues like simulated biology,

22:48.120 --> 22:53.120
 where it can simulate many different things at once.

22:56.760 --> 22:59.600
 We already had one example of simulated biology,

22:59.600 --> 23:01.480
 which is the Moderna vaccine.

23:04.560 --> 23:06.600
 And that's gonna be now

23:06.600 --> 23:10.200
 the way in which we create medications.

23:11.160 --> 23:13.000
 But they were able to simulate

23:13.000 --> 23:17.760
 what each example of an mRNA would do to a human being,

23:17.760 --> 23:21.400
 and they were able to simulate that quite reliably.

23:21.400 --> 23:23.960
 And we actually simulated billions

23:23.960 --> 23:27.040
 of different mRNA sequences,

23:27.040 --> 23:29.000
 and they found the ones that were the best,

23:29.000 --> 23:31.040
 and they created the vaccine.

23:31.040 --> 23:34.120
 And they did, and talked about doing that quickly,

23:34.120 --> 23:36.280
 they did that in two days.

23:36.280 --> 23:37.880
 Now, how long would a human being take

23:37.880 --> 23:41.000
 to simulate billions of different mRNA sequences?

23:41.000 --> 23:42.840
 I don't know that we could do it at all,

23:42.840 --> 23:45.800
 but it would take many years.

23:45.800 --> 23:50.000
 They did it in two days, and one of the reasons

23:50.000 --> 23:52.800
 that people didn't like vaccines

23:53.720 --> 23:55.520
 is because it was done too quickly,

23:55.520 --> 23:57.000
 it was done too fast.

23:58.200 --> 24:01.280
 And they actually included the time it took to test it out,

24:01.280 --> 24:03.600
 which was 10 months, so they figured,

24:03.600 --> 24:06.320
 okay, it took 10 months to create this.

24:06.320 --> 24:08.080
 Actually, it took us two days.

24:09.080 --> 24:11.880
 And we also will be able to ultimately do the tests

24:11.880 --> 24:14.200
 in a few days as well.

24:14.200 --> 24:16.600
 Oh, because we can simulate how the body will respond to it.

24:16.600 --> 24:19.120
 Yeah, that's a little bit more complicated

24:19.120 --> 24:22.920
 because the body has a lot of different elements,

24:22.920 --> 24:25.400
 and we have to simulate all of that,

24:25.400 --> 24:27.520
 but that's coming as well.

24:27.520 --> 24:30.240
 So ultimately, we could create it in a few days

24:30.240 --> 24:32.840
 and then test it in a few days, and it would be done.

24:34.040 --> 24:35.960
 And we can do that with every type

24:35.960 --> 24:40.240
 of medical insufficiency that we have.

24:40.240 --> 24:45.240
 So curing all diseases, improving certain functions

24:46.680 --> 24:51.680
 of the body, supplements, drugs for recreation,

24:53.120 --> 24:56.080
 for health, for performance, for productivity,

24:56.080 --> 24:56.920
 all that kind of stuff.

24:56.920 --> 24:58.040
 Well, that's where we're headed,

24:58.040 --> 25:00.640
 because I mean, right now we have a very inefficient way

25:00.640 --> 25:02.520
 of creating these new medications.

25:04.560 --> 25:07.120
 But we've already shown it, and the Moderna vaccine

25:07.120 --> 25:11.400
 is actually the best of the vaccines we've had,

25:12.520 --> 25:14.880
 and it literally took two days to create.

25:16.280 --> 25:17.200
 And we'll get to the point

25:17.200 --> 25:20.160
 where we can test it out also quickly.

25:20.160 --> 25:22.280
 Are you impressed by AlphaFold

25:22.280 --> 25:25.760
 and the solution to the protein folding,

25:25.760 --> 25:30.040
 which essentially is simulating, modeling

25:30.040 --> 25:33.080
 this primitive building block of life,

25:33.080 --> 25:36.080
 which is a protein, and its 3D shape?

25:36.080 --> 25:39.040
 It's pretty remarkable that they can actually predict

25:39.040 --> 25:42.000
 what the 3D shape of these things are,

25:42.000 --> 25:45.920
 but they did it with the same type of neural net

25:45.920 --> 25:50.920
 that won, for example, the Go test.

25:51.240 --> 25:52.720
 So it's all the same.

25:52.720 --> 25:53.560
 It's all the same.

25:53.560 --> 25:54.400
 All the same approaches.

25:54.400 --> 25:57.080
 They took that same thing and just changed the rules

25:57.080 --> 26:01.640
 to chess, and within a couple of days,

26:01.640 --> 26:03.960
 it now played a master level of chess

26:03.960 --> 26:06.280
 greater than any human being.

26:09.160 --> 26:12.040
 And the same thing then worked for AlphaFold,

26:13.480 --> 26:14.800
 which no human had done.

26:14.800 --> 26:16.800
 I mean, human beings could do,

26:16.800 --> 26:20.600
 the best humans could maybe do 15, 20%

26:22.640 --> 26:25.840
 of figuring out what the shape would be.

26:25.840 --> 26:30.840
 And after a few takes, it ultimately did just about 100%.

26:30.840 --> 26:32.560
 100%.

26:32.560 --> 26:35.600
 Do you still think the singularity will happen in 2045?

26:37.560 --> 26:38.960
 And what does that look like?

26:40.760 --> 26:45.760
 Once we can amplify our brain with computers directly,

26:46.600 --> 26:48.080
 which will happen in the 2030s,

26:48.080 --> 26:49.800
 that's gonna keep growing.

26:49.800 --> 26:51.040
 That's another whole theme,

26:51.040 --> 26:54.960
 which is the exponential growth of computing power.

26:54.960 --> 26:57.520
 Yeah, so looking at price performance of computation

26:57.520 --> 26:59.800
 from 1939 to 2021.

26:59.800 --> 27:02.920
 Right, so that starts with the very first computer

27:02.920 --> 27:05.600
 actually created by a German during World War II.

27:06.560 --> 27:09.440
 You might have thought that that might be significant,

27:09.440 --> 27:12.880
 but actually the Germans didn't think computers

27:12.880 --> 27:16.640
 were significant, and they completely rejected it.

27:16.640 --> 27:20.360
 The second one is also the ZUSA 2.

27:20.360 --> 27:22.240
 And by the way, we're looking at a plot

27:22.240 --> 27:27.240
 with the X axis being the year from 1935 to 2025.

27:27.240 --> 27:30.920
 And on the Y axis in log scale

27:30.920 --> 27:34.680
 is computation per second per constant dollar.

27:34.680 --> 27:36.800
 So dollar normalized inflation.

27:37.720 --> 27:40.200
 And it's growing linearly on the log scale,

27:40.200 --> 27:41.880
 which means it's growing exponentially.

27:41.880 --> 27:44.480
 The third one was the British computer,

27:44.480 --> 27:47.720
 which the Allies did take very seriously.

27:47.720 --> 27:51.780
 And it cracked the German code

27:51.780 --> 27:55.520
 and enables the British to win the Battle of Britain,

27:55.520 --> 27:57.720
 which otherwise absolutely would not have happened

27:57.720 --> 28:00.780
 if they hadn't cracked the code using that computer.

28:02.120 --> 28:03.640
 But that's an exponential graph.

28:03.640 --> 28:07.300
 So a straight line on that graph is exponential growth.

28:07.300 --> 28:11.600
 And you see 80 years of exponential growth.

28:11.600 --> 28:15.200
 And I would say about every five years,

28:15.200 --> 28:18.280
 and this happened shortly before the pandemic,

28:18.280 --> 28:20.680
 people saying, well, they call it Moore's law,

28:20.680 --> 28:25.480
 which is not the correct, because that's not all intel.

28:25.480 --> 28:29.680
 In fact, this started decades before intel was even created.

28:29.680 --> 28:34.140
 It wasn't with transistors formed into a grid.

28:34.140 --> 28:37.280
 So it's not just transistor count or transistor size.

28:37.280 --> 28:42.280
 Right, it started with relays, then went to vacuum tubes,

28:43.200 --> 28:46.720
 then went to individual transistors,

28:46.720 --> 28:48.820
 and then to integrated circuits.

28:51.080 --> 28:54.000
 And integrated circuits actually starts

28:54.000 --> 28:55.640
 like in the middle of this graph.

28:56.520 --> 28:58.760
 And it has nothing to do with intel.

28:58.760 --> 29:02.880
 Intel actually was a key part of this.

29:02.880 --> 29:07.180
 But a few years ago, they stopped making the fastest chips.

29:08.960 --> 29:12.800
 But if you take the fastest chip of any technology

29:12.800 --> 29:16.640
 in that year, you get this kind of graph.

29:16.640 --> 29:19.760
 And it's definitely continuing for 80 years.

29:19.760 --> 29:23.880
 So you don't think Moore's law, broadly defined, is dead.

29:24.840 --> 29:29.280
 It's been declared dead multiple times throughout this process.

29:29.280 --> 29:31.400
 I don't like the term Moore's law,

29:31.400 --> 29:34.740
 because it has nothing to do with Moore or with intel.

29:34.740 --> 29:39.740
 But yes, the exponential growth of computing is continuing.

29:41.600 --> 29:42.920
 It has never stopped.

29:42.920 --> 29:43.960
 From various sources.

29:43.960 --> 29:45.880
 I mean, it went through World War II,

29:45.880 --> 29:49.120
 it went through global recessions.

29:49.120 --> 29:50.680
 It's just continuing.

29:53.480 --> 29:58.100
 And if you continue that out, along with software gains,

29:58.100 --> 29:59.720
 which is a whole nother issue,

30:01.560 --> 30:02.960
 and they really multiply,

30:02.960 --> 30:04.400
 whatever you get from software gains,

30:04.400 --> 30:07.920
 you multiply by the computer gains,

30:07.920 --> 30:10.080
 you get faster and faster speed.

30:10.940 --> 30:14.320
 This is actually the fastest computer models

30:14.320 --> 30:15.840
 that have been created.

30:15.840 --> 30:19.480
 And that actually expands roughly twice a year.

30:19.480 --> 30:22.840
 Like, every six months it expands by two.

30:22.840 --> 30:27.840
 So we're looking at a plot from 2010 to 2022.

30:28.360 --> 30:31.400
 On the x axis is the publication date of the model,

30:31.400 --> 30:34.240
 and perhaps sometimes the actual paper associated with it.

30:34.240 --> 30:39.240
 And on the y axis is training, compute, and flops.

30:40.240 --> 30:43.880
 And so basically this is looking at the increase

30:43.880 --> 30:46.360
 in the, not transistors,

30:46.360 --> 30:51.360
 but the computational power of neural networks.

30:51.500 --> 30:55.120
 Yes, the computational power that created these models.

30:55.120 --> 30:57.560
 And that's doubled every six months.

30:57.560 --> 31:00.360
 Which is even faster than transistor division.

31:00.360 --> 31:01.200
 Yeah.

31:02.120 --> 31:06.880
 Now actually, since it goes faster than the amount of cost,

31:06.880 --> 31:10.840
 this has actually become a greater investment

31:10.840 --> 31:12.260
 to create these.

31:12.260 --> 31:16.600
 But at any rate, by the time we get to 2045,

31:16.600 --> 31:19.120
 we'll be able to multiply our intelligence

31:19.120 --> 31:21.480
 many millions fold.

31:21.480 --> 31:25.040
 And it's just very hard to imagine what that will be like.

31:25.040 --> 31:28.400
 And that's the singularity where we can't even imagine.

31:28.400 --> 31:30.480
 Right, that's why we call it the singularity.

31:30.480 --> 31:32.760
 Because the singularity in physics,

31:32.760 --> 31:35.120
 something gets sucked into its singularity

31:35.120 --> 31:37.780
 and you can't tell what's going on in there

31:37.780 --> 31:40.440
 because no information can get out of it.

31:40.440 --> 31:42.120
 There's various problems with that,

31:42.120 --> 31:44.080
 but that's the idea.

31:44.080 --> 31:48.960
 It's too much beyond what we can imagine.

31:48.960 --> 31:51.080
 Do you think it's possible we don't notice

31:52.120 --> 31:55.280
 that what the singularity actually feels like

31:56.360 --> 31:58.240
 is we just live through it

31:59.640 --> 32:04.640
 with exponentially increasing cognitive capabilities

32:05.400 --> 32:08.440
 and we almost, because everything's moving so quickly,

32:09.560 --> 32:11.800
 aren't really able to introspect

32:11.800 --> 32:13.680
 that our life has changed.

32:13.680 --> 32:17.480
 Yeah, but I mean, we will have that much greater capacity

32:17.480 --> 32:20.760
 to understand things, so we should be able to look back.

32:20.760 --> 32:23.240
 Looking at history, understand history.

32:23.240 --> 32:26.880
 But we will need people, basically like you and me,

32:26.880 --> 32:29.240
 to actually think about these things.

32:29.240 --> 32:30.680
 But we might be distracted

32:30.680 --> 32:34.160
 by all the other sources of entertainment and fun

32:34.160 --> 32:39.160
 because the exponential power of intellect is growing,

32:39.160 --> 32:41.640
 but also there'll be a lot of fun.

32:41.640 --> 32:46.160
 The amount of ways you can have, you know.

32:46.160 --> 32:48.440
 I mean, we already have a lot of fun with computer games

32:48.440 --> 32:51.400
 and so on that are really quite remarkable.

32:51.400 --> 32:54.760
 What do you think about the digital world,

32:54.760 --> 32:57.640
 the metaverse, virtual reality?

32:57.640 --> 32:59.120
 Will that have a component in this

32:59.120 --> 33:01.840
 or will most of our advancement be in physical reality?

33:01.840 --> 33:04.480
 Well, that's a little bit like Second Life,

33:04.480 --> 33:06.880
 although the Second Life actually didn't work very well

33:06.880 --> 33:09.320
 because it couldn't actually handle too many people.

33:09.320 --> 33:14.200
 And I don't think the metaverse has come to being.

33:14.200 --> 33:16.040
 I think there will be something like that.

33:16.040 --> 33:21.040
 It won't necessarily be from that one company.

33:21.040 --> 33:23.480
 I mean, there's gonna be competitors.

33:23.480 --> 33:26.480
 But yes, we're gonna live increasingly online,

33:26.480 --> 33:28.960
 and particularly if our brains are online.

33:28.960 --> 33:31.320
 I mean, how could we not be online?

33:31.320 --> 33:34.680
 Do you think it's possible that given this merger with AI,

33:34.680 --> 33:39.040
 most of our meaningful interactions

33:39.040 --> 33:43.880
 will be in this virtual world most of our life?

33:43.880 --> 33:46.320
 We fall in love, we make friends,

33:46.320 --> 33:49.480
 we come up with ideas, we do collaborations, we have fun.

33:49.480 --> 33:51.720
 I actually know somebody who's marrying somebody

33:51.720 --> 33:53.000
 that they never met.

33:54.720 --> 33:57.760
 I think they just met her briefly before the wedding,

33:57.760 --> 34:01.560
 but she actually fell in love with this other person,

34:01.560 --> 34:04.160
 never having met them.

34:06.280 --> 34:10.360
 And I think the love is real, so.

34:10.360 --> 34:11.520
 That's a beautiful story,

34:11.520 --> 34:15.080
 but do you think that story is one that might be experienced

34:15.080 --> 34:18.480
 as opposed to by hundreds of thousands of people,

34:18.480 --> 34:22.280
 but instead by hundreds of millions of people?

34:22.280 --> 34:23.880
 I mean, it really gives you appreciation

34:23.880 --> 34:27.240
 for these virtual ways of communicating.

34:28.520 --> 34:30.080
 And if anybody can do it,

34:30.080 --> 34:33.560
 then it's really not such a freak story.

34:34.720 --> 34:37.440
 So I think more and more people will do that.

34:37.440 --> 34:38.720
 But that's turning our back

34:38.720 --> 34:41.920
 on our entire history of evolution.

34:41.920 --> 34:45.520
 The old days, we used to fall in love by holding hands

34:45.520 --> 34:49.360
 and sitting by the fire, that kind of stuff.

34:49.360 --> 34:50.720
 Here, you're playing.

34:50.720 --> 34:54.640
 Actually, I have five patents on where you can hold hands,

34:54.640 --> 34:57.040
 even if you're separated.

34:57.040 --> 34:58.640
 Great.

34:58.640 --> 35:01.960
 So the touch, the sense, it's all just senses.

35:01.960 --> 35:03.040
 It's all just replicated.

35:03.040 --> 35:04.720
 Yeah, I mean, touch is,

35:04.720 --> 35:07.160
 it's not just that you're touching someone or not.

35:07.160 --> 35:10.480
 There's a whole way of doing it, and it's very subtle.

35:11.520 --> 35:15.200
 But ultimately, we can emulate all of that.

35:17.480 --> 35:19.800
 Are you excited by that future?

35:19.800 --> 35:21.560
 Do you worry about that future?

35:23.480 --> 35:25.120
 I have certain worries about the future,

35:25.120 --> 35:27.600
 but not virtual touch.

35:27.600 --> 35:31.520
 Well, I agree with you.

35:31.520 --> 35:33.480
 You described six stages

35:33.480 --> 35:36.600
 in the evolution of information processing in the universe,

35:36.600 --> 35:39.440
 as you started to describe.

35:39.440 --> 35:42.760
 Can you maybe talk through some of those stages

35:42.760 --> 35:46.320
 from the physics and chemistry to DNA and brains,

35:46.320 --> 35:48.800
 and then to the very end,

35:48.800 --> 35:52.000
 to the very beautiful end of this process?

35:52.000 --> 35:54.120
 It actually gets more rapid.

35:54.120 --> 35:57.720
 So physics and chemistry, that's how we started.

35:59.720 --> 36:02.120
 So the very beginning of the universe.

36:02.120 --> 36:05.920
 We had lots of electrons and various things traveling around.

36:07.240 --> 36:11.480
 And that took actually many billions of years,

36:11.480 --> 36:14.840
 kind of jumping ahead here to kind of

36:14.840 --> 36:16.840
 some of the last stages where we have things

36:16.840 --> 36:19.240
 like love and creativity.

36:19.240 --> 36:21.760
 It's really quite remarkable that that happens.

36:21.760 --> 36:26.760
 But finally, physics and chemistry created biology and DNA.

36:29.920 --> 36:33.440
 And now you had actually one type of molecule

36:33.440 --> 36:37.000
 that described the cutting edge of this process.

36:38.680 --> 36:42.600
 And we go from physics and chemistry to biology.

36:44.440 --> 36:47.080
 And finally, biology created brains.

36:48.120 --> 36:51.440
 I mean, not everything that's created by biology

36:51.440 --> 36:56.440
 has a brain, but eventually brains came along.

36:56.440 --> 36:58.840
 And all of this is happening faster and faster.

36:58.840 --> 37:00.320
 Yeah.

37:00.320 --> 37:04.480
 It created increasingly complex organisms.

37:04.480 --> 37:08.400
 Another key thing is actually not just brains,

37:08.400 --> 37:09.960
 but our thumb.

37:12.880 --> 37:15.880
 Because there's a lot of animals

37:15.880 --> 37:18.080
 with brains even bigger than humans.

37:18.080 --> 37:21.480
 I mean, elephants have a bigger brain.

37:21.480 --> 37:22.920
 Whales have a bigger brain.

37:24.160 --> 37:26.080
 But they've not created technology

37:27.080 --> 37:28.680
 because they don't have a thumb.

37:29.800 --> 37:32.280
 So that's one of the really key elements

37:32.280 --> 37:34.120
 in the evolution of humans.

37:34.120 --> 37:37.920
 This physical manipulator device

37:37.920 --> 37:41.320
 that's useful for puzzle solving in the physical reality.

37:41.320 --> 37:43.600
 So I could think, I could look at a tree and go,

37:43.600 --> 37:46.240
 oh, I could actually trip that branch down

37:46.240 --> 37:49.920
 and eliminate the leaves and carve a tip on it

37:49.920 --> 37:51.800
 and I would create technology.

37:53.000 --> 37:56.640
 And you can't do that if you don't have a thumb.

37:56.640 --> 37:57.480
 Yeah.

37:59.160 --> 38:04.160
 So thumbs then created technology

38:04.480 --> 38:08.040
 and technology also had a memory.

38:08.040 --> 38:10.000
 And now those memories are competing

38:10.000 --> 38:15.000
 with the scale and scope of human beings.

38:15.000 --> 38:17.040
 And ultimately we'll go beyond it.

38:18.160 --> 38:22.440
 And then we're gonna merge human technology

38:22.440 --> 38:27.440
 with human intelligence

38:27.520 --> 38:30.960
 and understand how human intelligence works,

38:30.960 --> 38:33.280
 which I think we already do.

38:33.280 --> 38:37.920
 And we're putting that into our human technology.

38:39.360 --> 38:43.120
 So create the technology inspired by our own intelligence

38:43.120 --> 38:45.360
 and then that technology supersedes us

38:45.360 --> 38:47.200
 in terms of its capabilities.

38:47.200 --> 38:48.640
 And we ride along.

38:48.640 --> 38:50.400
 Or do you ultimately see it as...

38:50.400 --> 38:52.720
 And we ride along, but a lot of people don't see that.

38:52.720 --> 38:56.200
 They say, well, you've got humans and you've got machines

38:56.200 --> 38:59.280
 and there's no way we can ultimately compete with humans.

39:00.280 --> 39:02.200
 And you can already see that.

39:02.200 --> 39:07.000
 Lee Soudal, who's like the best Go player in the world,

39:07.000 --> 39:09.040
 says he's not gonna play Go anymore.

39:10.000 --> 39:12.880
 Because playing Go for a human,

39:12.880 --> 39:14.920
 that was like the ultimate in intelligence

39:14.920 --> 39:16.600
 because no one else could do that.

39:18.200 --> 39:22.400
 But now a machine can actually go way beyond him.

39:22.400 --> 39:25.080
 And so he says, well, there's no point playing it anymore.

39:25.080 --> 39:28.880
 That may be more true for games than it is for life.

39:30.080 --> 39:31.360
 I think there's a lot of benefit

39:31.360 --> 39:34.440
 to working together with AI in regular life.

39:34.440 --> 39:37.920
 So if you were to put a probability on it,

39:37.920 --> 39:41.240
 is it more likely that we merge with AI

39:41.240 --> 39:43.560
 or AI replaces us?

39:43.560 --> 39:47.400
 A lot of people just think computers come along

39:47.400 --> 39:48.320
 and they compete with them.

39:48.320 --> 39:50.880
 We can't really compete and that's the end of it.

39:52.320 --> 39:57.200
 As opposed to them increasing our abilities.

39:57.200 --> 39:59.760
 And if you look at most technology,

39:59.760 --> 40:01.920
 it increases our abilities.

40:04.480 --> 40:06.240
 I mean, look at the history of work.

40:07.920 --> 40:11.120
 Look at what people did 100 years ago.

40:11.120 --> 40:13.000
 Does any of that exist anymore?

40:13.000 --> 40:16.560
 People, I mean, if you were to predict

40:16.560 --> 40:19.440
 that all of these jobs would go away

40:19.440 --> 40:21.000
 and would be done by machines,

40:21.000 --> 40:22.760
 people would say, well, there's gonna be,

40:22.760 --> 40:24.040
 no one's gonna have jobs

40:24.040 --> 40:26.800
 and it's gonna be massive unemployment.

40:29.480 --> 40:32.480
 But I show in this book that's coming out

40:34.120 --> 40:36.760
 the amount of people that are working,

40:36.760 --> 40:41.640
 even as a percentage of the population has gone way up.

40:41.640 --> 40:46.160
 We're looking at the x axis year from 1774 to 2024

40:46.160 --> 40:49.520
 and on the y axis, personal income per capita

40:49.520 --> 40:52.720
 in constant dollars and it's growing super linearly.

40:52.720 --> 40:57.720
 I mean, it's 2021 constant dollars and it's gone way up.

40:58.040 --> 40:59.840
 That's not what you would predict

41:00.760 --> 41:01.920
 given that we would predict

41:01.920 --> 41:03.760
 that all these jobs would go away.

41:03.760 --> 41:07.000
 But the reason it's gone up is because

41:07.000 --> 41:09.880
 we've basically enhanced our own capabilities

41:09.880 --> 41:11.280
 by using these machines

41:11.280 --> 41:13.400
 as opposed to them just competing with us.

41:14.280 --> 41:16.280
 That's a key way in which we're gonna be able

41:16.280 --> 41:18.680
 to become far smarter than we are now

41:18.680 --> 41:23.200
 by increasing the number of different parameters

41:23.200 --> 41:25.640
 we can consider in making a decision.

41:26.480 --> 41:28.640
 I was very fortunate, I am very fortunate

41:28.640 --> 41:31.480
 to be able to get a glimpse preview

41:31.480 --> 41:36.480
 of your upcoming book, Singularity is Nearer.

41:37.320 --> 41:41.920
 And one of the themes outside of just discussing

41:41.920 --> 41:44.760
 the increasing exponential growth of technology,

41:44.760 --> 41:48.480
 one of the themes is that things are getting better

41:48.480 --> 41:50.800
 in all aspects of life.

41:50.800 --> 41:53.720
 And you talked just about this.

41:53.720 --> 41:55.640
 So one of the things you're saying is with jobs.

41:55.640 --> 41:57.840
 So let me just ask about that.

41:57.840 --> 42:01.040
 There is a big concern that automation,

42:01.040 --> 42:06.040
 especially powerful AI, will get rid of jobs.

42:06.400 --> 42:07.880
 There are people who lose jobs.

42:07.880 --> 42:10.960
 And as you were saying, the sense is

42:10.960 --> 42:14.000
 throughout the history of the 20th century,

42:14.000 --> 42:16.640
 automation did not do that ultimately.

42:16.640 --> 42:20.600
 And so the question is, will this time be different?

42:20.600 --> 42:22.560
 Right, that is the question.

42:22.560 --> 42:24.480
 Will this time be different?

42:24.480 --> 42:26.360
 And it really has to do with how quickly

42:26.360 --> 42:29.120
 we can merge with this type of intelligence.

42:29.120 --> 42:34.120
 Whether Lambda or GPT3 is out there,

42:34.920 --> 42:38.640
 and maybe it's overcome some of its key problems,

42:40.200 --> 42:43.480
 and we really haven't enhanced human intelligence,

42:43.480 --> 42:45.640
 that might be a negative scenario.

42:49.600 --> 42:53.160
 But I mean, that's why we create technologies,

42:53.160 --> 42:54.640
 to enhance ourselves.

42:56.280 --> 42:58.800
 And I believe we will be enhanced

42:58.800 --> 43:00.720
 when I'm just going to sit here with

43:03.000 --> 43:08.000
 300 million modules in our neocortex.

43:09.040 --> 43:11.080
 We're going to be able to go beyond that.

43:14.000 --> 43:18.360
 Because that's useful, but we can multiply that by 10,

43:19.640 --> 43:22.240
 100, 1,000, a million.

43:22.240 --> 43:27.240
 And you might think, well, what's the point of doing that?

43:30.240 --> 43:33.920
 It's like asking somebody that's never heard music,

43:33.920 --> 43:36.600
 well, what's the value of music?

43:36.600 --> 43:39.920
 I mean, you can't appreciate it until you've created it.

43:41.360 --> 43:45.320
 There's some worry that there'll be a wealth disparity.

43:46.880 --> 43:50.240
 Class or wealth disparity, only the rich people

43:50.240 --> 43:53.120
 will be, basically, the rich people

43:53.120 --> 43:55.520
 will first have access to this kind of thing,

43:55.520 --> 43:58.000
 and then because of this kind of thing,

43:58.000 --> 43:59.480
 because the ability to merge

43:59.480 --> 44:02.680
 will get richer exponentially faster.

44:02.680 --> 44:05.120
 And I say that's just like cell phones.

44:06.280 --> 44:08.080
 I mean, there's like four billion cell phones

44:08.080 --> 44:10.320
 in the world today.

44:10.320 --> 44:13.320
 In fact, when cell phones first came out,

44:13.320 --> 44:14.840
 you had to be fairly wealthy.

44:14.840 --> 44:17.520
 They weren't very inexpensive.

44:17.520 --> 44:20.160
 So you had to have some wealth in order to afford them.

44:20.160 --> 44:22.760
 Yeah, there were these big, sexy phones.

44:22.760 --> 44:24.080
 And they didn't work very well.

44:24.080 --> 44:25.640
 They did almost nothing.

44:26.480 --> 44:31.240
 So you can only afford these things if you're wealthy

44:31.240 --> 44:34.040
 at a point where they really don't work very well.

44:35.760 --> 44:39.880
 So achieving scale and making it inexpensive

44:39.880 --> 44:42.240
 is part of making the thing work well.

44:42.240 --> 44:43.560
 Exactly.

44:43.560 --> 44:46.980
 So these are not totally cheap, but they're pretty cheap.

44:46.980 --> 44:51.980
 I mean, you can get them for a few hundred dollars.

44:52.140 --> 44:55.400
 Especially given the kind of things it provides for you.

44:55.400 --> 44:57.100
 There's a lot of people in the third world

44:57.100 --> 45:00.380
 that have very little, but they have a smartphone.

45:00.380 --> 45:01.980
 Yeah, absolutely.

45:01.980 --> 45:03.820
 And the same will be true with AI.

45:03.820 --> 45:07.640
 I mean, I see homeless people have their own cell phones.

45:07.640 --> 45:12.120
 Yeah, so your sense is any kind of advanced technology

45:12.120 --> 45:13.740
 will take the same trajectory.

45:13.740 --> 45:17.700
 Right, it ultimately becomes cheap and will be affordable.

45:19.180 --> 45:21.060
 I probably would not be the first person

45:21.060 --> 45:26.060
 to put something in my brain to connect to computers

45:28.220 --> 45:30.240
 because I think it will have limitations.

45:30.240 --> 45:33.180
 But once it's really perfected,

45:34.140 --> 45:36.420
 and at that point it'll be pretty inexpensive,

45:36.420 --> 45:38.260
 I think it'll be pretty affordable.

45:39.660 --> 45:43.080
 So in which other ways, as you outline your book,

45:43.080 --> 45:44.460
 is life getting better?

45:44.460 --> 45:45.340
 Because I think...

45:45.340 --> 45:49.220
 Well, I mean, I have 50 charts in there

45:49.220 --> 45:51.780
 where everything is getting better.

45:51.780 --> 45:54.140
 I think there's a kind of cynicism about,

45:55.500 --> 45:58.020
 like even if you look at extreme poverty, for example.

45:58.020 --> 46:00.860
 For example, this is actually a poll

46:00.860 --> 46:05.500
 taken on extreme poverty, and people were asked,

46:05.500 --> 46:08.320
 has poverty gotten better or worse?

46:08.320 --> 46:11.180
 And the options are increased by 50%,

46:11.180 --> 46:13.940
 increased by 25%, remain the same,

46:13.940 --> 46:16.740
 decreased by 25%, decreased by 50%.

46:16.740 --> 46:18.780
 If you're watching this or listening to this,

46:18.780 --> 46:21.500
 try to vote for yourself.

46:21.500 --> 46:24.200
 70% thought it had gotten worse,

46:24.200 --> 46:27.100
 and that's the general impression.

46:27.100 --> 46:31.600
 88% thought it had gotten worse or remained the same.

46:32.500 --> 46:35.700
 Only 1% thought it decreased by 50%,

46:35.700 --> 46:37.620
 and that is the answer.

46:37.620 --> 46:39.540
 It actually decreased by 50%.

46:39.540 --> 46:43.660
 So only 1% of people got the right optimistic estimate

46:43.660 --> 46:45.260
 of how poverty is.

46:45.260 --> 46:47.520
 Right, and this is the reality,

46:47.520 --> 46:51.140
 and it's true of almost everything you look at.

46:51.140 --> 46:54.780
 You don't wanna go back 100 years or 50 years.

46:54.780 --> 46:56.980
 Things were quite miserable then,

46:56.980 --> 47:01.020
 but we tend not to remember that.

47:01.020 --> 47:05.340
 So literacy rate increasing over the past few centuries

47:05.340 --> 47:07.940
 across all the different nations,

47:07.940 --> 47:11.880
 nearly to 100% across many of the nations in the world.

47:11.880 --> 47:12.820
 It's gone way up.

47:12.820 --> 47:15.620
 Average years of education have gone way up.

47:15.620 --> 47:18.560
 Life expectancy is also increasing.

47:18.560 --> 47:23.560
 Life expectancy was 48 in 1900.

47:24.380 --> 47:26.400
 And it's over 80 now.

47:26.400 --> 47:28.140
 And it's gonna continue to go up,

47:28.140 --> 47:30.940
 particularly as we get into more advanced stages

47:30.940 --> 47:33.380
 of simulated biology.

47:33.380 --> 47:35.580
 For life expectancy, these trends are the same

47:35.580 --> 47:37.940
 for at birth, age one, age five, age 10,

47:37.940 --> 47:40.340
 so it's not just the infant mortality.

47:40.340 --> 47:42.620
 And I have 50 more graphs in the book

47:42.620 --> 47:44.580
 about all kinds of things.

47:46.120 --> 47:48.340
 Even spread of democracy,

47:48.340 --> 47:52.740
 which might bring up some sort of controversial issues,

47:52.740 --> 47:55.140
 it still has gone way up.

47:55.140 --> 47:57.260
 Well, that one has gone way up,

47:57.260 --> 47:59.500
 but that one is a bumpy road, right?

47:59.500 --> 48:03.220
 Exactly, and somebody might represent democracy

48:03.220 --> 48:08.220
 and go backwards, but we basically had no democracies

48:08.460 --> 48:10.980
 before the creation of the United States,

48:10.980 --> 48:13.860
 which was a little over two centuries ago,

48:13.860 --> 48:16.460
 which in the scale of human history isn't that long.

48:17.460 --> 48:19.860
 Do you think superintelligence systems will help

48:21.460 --> 48:22.580
 with democracy?

48:23.620 --> 48:25.020
 So what is democracy?

48:25.020 --> 48:29.660
 Democracy is giving a voice to the populace

48:29.660 --> 48:33.700
 and having their ideas, having their beliefs,

48:33.700 --> 48:38.180
 having their views represented.

48:38.180 --> 48:39.440
 Well, I hope so.

48:41.260 --> 48:44.060
 I mean, we've seen social networks

48:44.060 --> 48:47.720
 can spread conspiracy theories,

48:49.180 --> 48:51.340
 which have been quite negative,

48:51.340 --> 48:55.500
 being, for example, being against any kind of stuff

48:55.500 --> 48:58.340
 that would help your health.

48:58.340 --> 49:01.520
 So those kinds of ideas have,

49:03.100 --> 49:06.540
 on social media, what you notice is they increase

49:06.540 --> 49:10.340
 engagement, so dramatic division increases engagement.

49:10.340 --> 49:13.460
 Do you worry about AI systems that will learn

49:13.460 --> 49:15.140
 to maximize that division?

49:17.020 --> 49:20.360
 I mean, I do have some concerns about this,

49:22.040 --> 49:25.740
 and I have a chapter in the book about the perils

49:25.740 --> 49:30.740
 of advanced AI, spreading misinformation

49:32.380 --> 49:34.080
 on social networks is one of them,

49:34.080 --> 49:36.780
 but there are many others.

49:36.780 --> 49:38.780
 What's the one that worries you the most

49:40.640 --> 49:42.900
 that we should think about to try to avoid?

49:47.260 --> 49:49.000
 Well, it's hard to choose.

49:50.820 --> 49:55.340
 We do have the nuclear power that evolved

49:55.340 --> 49:57.660
 when I was a child, I remember,

49:57.660 --> 50:02.660
 and we would actually do these drills against a nuclear war.

50:03.580 --> 50:07.620
 We'd get under our desks and put our hands behind our heads

50:07.620 --> 50:10.120
 to protect us from a nuclear war.

50:11.140 --> 50:13.300
 Seems to work, we're still around, so.

50:15.540 --> 50:17.060
 You're protected.

50:17.060 --> 50:20.080
 But that's still a concern.

50:20.080 --> 50:22.860
 And there are key dangerous situations

50:22.860 --> 50:26.260
 that can take place in biology.

50:27.140 --> 50:32.140
 Someone could create a virus that's very,

50:33.340 --> 50:36.340
 I mean, we have viruses that are hard to spread,

50:40.560 --> 50:42.800
 and they can be very dangerous,

50:42.800 --> 50:46.160
 and we have viruses that are easy to spread,

50:46.160 --> 50:47.800
 but they're not so dangerous.

50:47.800 --> 50:51.600
 Somebody could create something

50:51.600 --> 50:55.580
 that would be very easy to spread and very dangerous,

50:55.580 --> 50:57.400
 and be very hard to stop.

50:58.960 --> 51:02.040
 It could be something that would spread

51:02.040 --> 51:04.640
 without people noticing, because people could get it,

51:04.640 --> 51:08.320
 they'd have no symptoms, and then everybody would get it,

51:08.320 --> 51:11.800
 and then symptoms would occur maybe a month later.

51:11.800 --> 51:16.800
 So I mean, and that actually doesn't occur normally,

51:18.760 --> 51:23.760
 because if we were to have a problem with that,

51:24.680 --> 51:26.920
 we wouldn't exist.

51:26.920 --> 51:30.720
 So the fact that humans exist means that we don't have

51:30.720 --> 51:35.040
 viruses that can spread easily and kill us,

51:35.040 --> 51:37.540
 because otherwise we wouldn't exist.

51:37.540 --> 51:39.080
 Yeah, viruses don't wanna do that.

51:39.080 --> 51:44.080
 They want to spread and keep the host alive somewhat.

51:44.080 --> 51:47.240
 So you can describe various dangers with biology.

51:48.620 --> 51:53.520
 Also nanotechnology, which we actually haven't experienced

51:53.520 --> 51:56.040
 yet, but there are people that are creating nanotechnology,

51:56.040 --> 51:57.960
 and I describe that in the book.

51:57.960 --> 52:00.880
 Now you're excited by the possibilities of nanotechnology,

52:00.880 --> 52:04.920
 of nanobots, of being able to do things inside our body,

52:04.920 --> 52:07.520
 inside our mind, that's going to help.

52:07.520 --> 52:10.880
 What's exciting, what's terrifying about nanobots?

52:10.880 --> 52:13.920
 What's exciting is that that's a way to communicate

52:13.920 --> 52:18.920
 with our neocortex, because each neocortex is pretty small

52:19.000 --> 52:22.360
 and you need a small entity that can actually get in there

52:22.360 --> 52:25.440
 and establish a communication channel.

52:25.440 --> 52:30.320
 And that's gonna really be necessary to connect our brains

52:30.320 --> 52:35.320
 to AI within ourselves, because otherwise it would be hard

52:35.320 --> 52:38.720
 for us to compete with it.

52:38.720 --> 52:40.240
 In a high bandwidth way.

52:40.240 --> 52:41.760
 Yeah, yeah.

52:41.760 --> 52:45.720
 And that's key, actually, because a lot of the things

52:45.720 --> 52:48.980
 like Neuralink are really not high bandwidth yet.

52:49.880 --> 52:52.680
 So nanobots is the way you achieve high bandwidth.

52:52.680 --> 52:55.880
 How much intelligence would those nanobots have?

52:55.880 --> 53:00.320
 Yeah, they don't need a lot, just enough to basically

53:00.320 --> 53:04.400
 establish a communication channel to one nanobot.

53:04.400 --> 53:06.720
 So it's primarily about communication.

53:06.720 --> 53:07.560
 Yeah.

53:07.560 --> 53:09.880
 Between external computing devices

53:09.880 --> 53:14.120
 and our biological thinking machine.

53:15.020 --> 53:17.040
 What worries you about nanobots?

53:17.040 --> 53:19.840
 Is it similar to with the viruses?

53:19.840 --> 53:22.720
 Well, I mean, it's the great goo challenge.

53:22.720 --> 53:23.560
 Yes.

53:24.920 --> 53:29.920
 If you had a nanobot that wanted to create

53:29.920 --> 53:34.920
 any kind of entity and repeat itself,

53:37.520 --> 53:41.520
 and was able to operate in a natural environment,

53:41.520 --> 53:45.240
 it could turn everything into that entity

53:45.240 --> 53:50.240
 and basically destroy all biological life.

53:52.000 --> 53:54.600
 So you mentioned nuclear weapons.

53:54.600 --> 53:55.440
 Yeah.

53:55.440 --> 54:00.440
 I'd love to hear your opinion about the 21st century

54:01.840 --> 54:05.320
 and whether you think we might destroy ourselves.

54:05.320 --> 54:08.840
 And maybe your opinion, if it has changed

54:08.840 --> 54:11.760
 by looking at what's going on in Ukraine,

54:11.760 --> 54:16.760
 that we could have a hot war with nuclear powers involved

54:18.980 --> 54:23.320
 and the tensions building and the seeming forgetting

54:23.320 --> 54:27.460
 of how terrifying and destructive nuclear weapons are.

54:29.340 --> 54:32.940
 Do you think humans might destroy ourselves

54:32.940 --> 54:36.220
 in the 21st century, and if we do, how?

54:36.220 --> 54:37.500
 And how do we avoid it?

54:38.540 --> 54:41.100
 I don't think that's gonna happen

54:41.100 --> 54:45.180
 despite the terrors of that war.

54:45.180 --> 54:50.180
 It is a possibility, but I mean, I don't.

54:50.420 --> 54:52.700
 It's unlikely in your mind.

54:52.700 --> 54:55.380
 Yeah, even with the tensions we've had

54:55.380 --> 54:59.860
 with this one nuclear power plant that's been taken over,

55:02.340 --> 55:07.340
 it's very tense, but I don't actually see a lot of people

55:07.580 --> 55:10.220
 worrying that that's gonna happen.

55:10.220 --> 55:11.900
 I think we'll avoid that.

55:11.900 --> 55:15.940
 We had two nuclear bombs go off in 45,

55:15.940 --> 55:20.860
 so now we're 77 years later.

55:20.860 --> 55:22.400
 Yeah, we're doing pretty good.

55:22.400 --> 55:27.100
 We've never had another one go off through anger.

55:27.100 --> 55:31.020
 People forget the lessons of history.

55:31.020 --> 55:33.540
 Well, yeah, I mean, I am worried about it.

55:33.540 --> 55:37.460
 I mean, that is definitely a challenge.

55:37.460 --> 55:40.620
 But you believe that we'll make it out

55:40.620 --> 55:44.600
 and ultimately superintelligent AI will help us make it out

55:44.600 --> 55:47.680
 as opposed to destroy us.

55:47.680 --> 55:52.420
 I think so, but we do have to be mindful of these dangers.

55:52.420 --> 55:56.340
 And there are other dangers besides nuclear weapons, so.

55:56.340 --> 56:01.060
 So to get back to merging with AI,

56:01.060 --> 56:03.740
 will we be able to upload our mind in a computer

56:06.020 --> 56:09.380
 in a way where we might even transcend

56:09.380 --> 56:11.620
 the constraints of our bodies?

56:11.620 --> 56:15.300
 So copy our mind into a computer and leave the body behind?

56:15.300 --> 56:20.300
 Let me describe one thing I've already done with my father.

56:21.060 --> 56:22.200
 That's a great story.

56:23.700 --> 56:26.660
 So we created a technology, this is public,

56:26.660 --> 56:30.140
 came out, I think, six years ago,

56:30.140 --> 56:33.740
 where you could ask any question

56:33.740 --> 56:35.220
 and the release product,

56:35.220 --> 56:37.620
 which I think is still on the market,

56:37.620 --> 56:40.900
 it would read 200,000 books.

56:40.900 --> 56:45.900
 And then find the one sentence in 200,000 books

56:46.140 --> 56:48.220
 that best answered your question.

56:49.860 --> 56:51.180
 And it's actually quite interesting.

56:51.180 --> 56:52.740
 You can ask all kinds of questions

56:52.740 --> 56:56.240
 and you get the best answer in 200,000 books.

56:57.580 --> 56:59.940
 But I was also able to take it

56:59.940 --> 57:03.180
 and not go through 200,000 books,

57:03.180 --> 57:07.060
 but go through a book that I put together,

57:07.060 --> 57:10.980
 which is basically everything my father had written.

57:10.980 --> 57:14.660
 So everything he had written, I had gathered,

57:14.660 --> 57:16.100
 and we created a book,

57:17.100 --> 57:20.220
 everything that Frederick Herzog had written.

57:20.220 --> 57:23.380
 Now, I didn't think this actually would work that well

57:23.380 --> 57:28.380
 because stuff he had written was stuff about how to lay out.

57:30.900 --> 57:35.900
 I mean, he directed choral groups

57:35.900 --> 57:39.220
 and music groups,

57:39.220 --> 57:44.180
 and he would be laying out how the people should,

57:44.180 --> 57:49.180
 where they should sit and how to fund this

57:49.660 --> 57:52.220
 and all kinds of things

57:52.220 --> 57:55.140
 that really didn't seem that interesting.

57:57.620 --> 57:59.900
 And yet, when you ask a question,

57:59.900 --> 58:00.820
 it would go through it

58:00.820 --> 58:04.760
 and it would actually give you a very good answer.

58:04.760 --> 58:07.860
 So I said, well, who's the most interesting composer?

58:07.860 --> 58:09.500
 And he said, well, definitely Brahms.

58:09.500 --> 58:13.220
 And he would go on about how Brahms was fabulous

58:13.220 --> 58:17.140
 and talk about the importance of music education.

58:18.020 --> 58:21.140
 So you could have essentially a question and answer,

58:21.140 --> 58:21.980
 a conversation with him.

58:21.980 --> 58:23.020
 You could have a conversation with him,

58:23.020 --> 58:25.940
 which was actually more interesting than talking to him

58:25.940 --> 58:27.020
 because if you talked to him,

58:27.020 --> 58:30.080
 he'd be concerned about how they're gonna lay out

58:30.080 --> 58:34.220
 this property to give a choral group.

58:34.220 --> 58:36.060
 He'd be concerned about the day to day

58:36.060 --> 58:37.300
 versus the big questions.

58:37.300 --> 58:39.060
 Exactly, yeah.

58:39.060 --> 58:41.620
 And you did ask about the meaning of life

58:41.620 --> 58:43.260
 and he answered, love.

58:43.260 --> 58:44.100
 Yeah.

58:46.460 --> 58:47.300
 Do you miss him?

58:49.180 --> 58:50.260
 Yes, I do.

58:52.940 --> 58:57.940
 Yeah, you get used to missing somebody after 52 years,

58:58.540 --> 59:02.700
 and I didn't really have intelligent conversations with him

59:02.700 --> 59:04.500
 until later in life.

59:06.940 --> 59:08.780
 In the last few years, he was sick,

59:08.780 --> 59:10.140
 which meant he was home a lot

59:10.140 --> 59:11.900
 and I was actually able to talk to him

59:11.900 --> 59:15.780
 about different things like music and other things.

59:15.780 --> 59:19.820
 And so I miss that very much.

59:19.820 --> 59:22.220
 What did you learn about life from your father?

59:25.180 --> 59:27.820
 What part of him is with you now?

59:29.020 --> 59:31.580
 He was devoted to music.

59:31.580 --> 59:33.860
 And when he would create something to music,

59:33.860 --> 59:35.460
 it put him in a different world.

59:37.540 --> 59:39.680
 Otherwise, he was very shy.

59:42.520 --> 59:43.780
 And if people got together,

59:43.780 --> 59:47.020
 he tended not to interact with people

59:47.020 --> 59:48.580
 just because of his shyness.

59:49.780 --> 59:54.440
 But when he created music, he was like a different person.

59:55.340 --> 59:56.540
 Do you have that in you?

59:56.540 --> 59:59.840
 That kind of light that shines?

59:59.840 --> 1:00:04.840
 I mean, I got involved with technology at like age five.

1:00:06.620 --> 1:00:07.620
 And you fell in love with it

1:00:07.620 --> 1:00:09.380
 in the same way he did with music?

1:00:09.380 --> 1:00:11.300
 Yeah, yeah.

1:00:11.300 --> 1:00:15.900
 I remember this actually happened with my grandmother.

1:00:16.940 --> 1:00:20.060
 She had a manual typewriter

1:00:20.060 --> 1:00:23.060
 and she wrote a book, One Life Is Not Enough,

1:00:23.060 --> 1:00:26.240
 which actually a good title for a book I might write,

1:00:26.240 --> 1:00:30.260
 but it was about a school she had created.

1:00:30.260 --> 1:00:33.800
 Well, actually her mother created it.

1:00:33.800 --> 1:00:38.300
 So my mother's mother's mother created the school in 1868.

1:00:38.300 --> 1:00:40.620
 And it was the first school in Europe

1:00:40.620 --> 1:00:42.660
 that provided higher education for girls.

1:00:42.660 --> 1:00:44.420
 It went through 14th grade.

1:00:45.620 --> 1:00:48.260
 If you were a girl and you were lucky enough

1:00:48.260 --> 1:00:50.700
 to get an education at all,

1:00:50.700 --> 1:00:52.940
 it would go through like ninth grade.

1:00:52.940 --> 1:00:56.660
 And many people didn't have any education as a girl.

1:00:56.660 --> 1:00:58.320
 This went through 14th grade.

1:01:00.620 --> 1:01:04.060
 Her mother created it, she took it over,

1:01:04.060 --> 1:01:09.060
 and the book was about the history of the school

1:01:09.420 --> 1:01:11.120
 and her involvement with it.

1:01:12.980 --> 1:01:14.020
 When she presented it to me,

1:01:14.020 --> 1:01:19.020
 I was not so interested in the story of the school,

1:01:19.020 --> 1:01:24.020
 but I was totally amazed with this manual typewriter.

1:01:25.400 --> 1:01:27.860
 I mean, here is something you could put a blank piece

1:01:27.860 --> 1:01:31.080
 of paper into and you could turn it into something

1:01:31.080 --> 1:01:33.780
 that looked like it came from a book.

1:01:33.780 --> 1:01:34.620
 And you can actually type on it

1:01:34.620 --> 1:01:36.440
 and it looked like it came from a book.

1:01:36.440 --> 1:01:38.180
 It was just amazing to me.

1:01:39.120 --> 1:01:41.740
 And I could see actually how it worked.

1:01:42.720 --> 1:01:44.840
 And I was also interested in magic.

1:01:44.840 --> 1:01:49.840
 But in magic, if somebody actually knows how it works,

1:01:50.420 --> 1:01:52.540
 the magic goes away.

1:01:52.540 --> 1:01:53.780
 The magic doesn't stay there

1:01:53.780 --> 1:01:56.600
 if you actually understand how it works.

1:01:56.600 --> 1:01:57.820
 But here was technology.

1:01:57.820 --> 1:02:01.020
 I didn't have that word when I was five or six.

1:02:01.020 --> 1:02:02.660
 And the magic was still there for you?

1:02:02.660 --> 1:02:05.740
 The magic was still there, even if you knew how it worked.

1:02:06.900 --> 1:02:08.780
 So I became totally interested in this

1:02:08.780 --> 1:02:12.580
 and then went around, collected little pieces

1:02:12.580 --> 1:02:17.580
 of mechanical objects from bicycles, from broken radios.

1:02:17.580 --> 1:02:19.340
 I would go through the neighborhood.

1:02:20.500 --> 1:02:23.700
 This was an era where you would allow five or six year olds

1:02:23.700 --> 1:02:26.340
 to run through the neighborhood and do this.

1:02:26.340 --> 1:02:27.740
 We don't do that anymore.

1:02:27.740 --> 1:02:30.700
 But I didn't know how to put them together.

1:02:30.700 --> 1:02:32.220
 I said, if I could just figure out

1:02:32.220 --> 1:02:36.140
 how to put these things together, I could solve any problem.

1:02:37.340 --> 1:02:41.660
 And I actually remember talking to these very old girls.

1:02:41.660 --> 1:02:42.780
 I think they were 10.

1:02:45.340 --> 1:02:48.340
 And telling them, if I could just figure this out,

1:02:48.340 --> 1:02:50.120
 we could fly, we could do anything.

1:02:50.120 --> 1:02:53.580
 And they said, well, you have quite an imagination.

1:02:56.220 --> 1:03:00.780
 And then when I was in third grade,

1:03:00.780 --> 1:03:02.860
 so I was like eight,

1:03:02.860 --> 1:03:05.900
 created like a virtual reality theater

1:03:05.900 --> 1:03:07.780
 where people could come on stage

1:03:07.780 --> 1:03:09.900
 and they could move their arms.

1:03:09.900 --> 1:03:13.540
 And all of it was controlled through one control box.

1:03:13.540 --> 1:03:15.800
 It was all done with mechanical technology.

1:03:16.980 --> 1:03:19.800
 And it was a big hit in my third grade class.

1:03:21.100 --> 1:03:22.980
 And then I went on to do things

1:03:22.980 --> 1:03:24.900
 in junior high school science fairs

1:03:24.900 --> 1:03:27.660
 and high school science fairs.

1:03:27.660 --> 1:03:30.720
 I won the Westinghouse Science Talent Search.

1:03:30.720 --> 1:03:33.940
 So I mean, I became committed to technology

1:03:33.940 --> 1:03:37.460
 when I was five or six years old.

1:03:37.460 --> 1:03:42.460
 You've talked about how you use lucid dreaming to think,

1:03:43.100 --> 1:03:45.900
 to come up with ideas as a source of creativity.

1:03:45.900 --> 1:03:49.360
 Because you maybe talk through that,

1:03:49.360 --> 1:03:52.020
 maybe the process of how to,

1:03:52.020 --> 1:03:54.060
 you've invented a lot of things.

1:03:54.060 --> 1:03:55.620
 You've came up and thought through

1:03:55.620 --> 1:03:57.180
 some very interesting ideas.

1:03:58.100 --> 1:03:59.520
 What advice would you give,

1:03:59.520 --> 1:04:03.420
 or can you speak to the process of thinking,

1:04:03.420 --> 1:04:07.100
 of how to think, how to think creatively?

1:04:07.100 --> 1:04:10.460
 Well, I mean, sometimes I will think through in a dream

1:04:10.460 --> 1:04:12.340
 and try to interpret that.

1:04:12.340 --> 1:04:17.340
 But I think the key issue that I would tell younger people

1:04:22.220 --> 1:04:25.080
 is to put yourself in the position

1:04:25.080 --> 1:04:28.860
 that what you're trying to create already exists.

1:04:30.660 --> 1:04:33.580
 And then you're explaining, like...

1:04:34.980 --> 1:04:35.820
 How it works.

1:04:35.820 --> 1:04:38.220
 Exactly.

1:04:38.220 --> 1:04:39.220
 That's really interesting.

1:04:39.220 --> 1:04:42.780
 You paint a world that you would like to exist,

1:04:42.780 --> 1:04:45.940
 you think it exists, and reverse engineer that.

1:04:45.940 --> 1:04:47.900
 And then you actually imagine you're giving a speech

1:04:47.900 --> 1:04:50.140
 about how you created this.

1:04:50.140 --> 1:04:51.780
 Well, you'd have to then work backwards

1:04:51.780 --> 1:04:56.780
 as to how you would create it in order to make it work.

1:04:57.320 --> 1:04:58.160
 That's brilliant.

1:04:58.160 --> 1:05:01.420
 And that requires some imagination too,

1:05:01.420 --> 1:05:03.140
 some first principles thinking.

1:05:03.140 --> 1:05:06.040
 You have to visualize that world.

1:05:06.040 --> 1:05:07.720
 That's really interesting.

1:05:07.720 --> 1:05:10.600
 And generally, when I talk about things

1:05:10.600 --> 1:05:13.160
 we're trying to invent, I would use the present tense

1:05:13.160 --> 1:05:14.700
 as if it already exists.

1:05:15.880 --> 1:05:18.280
 Not just to give myself that confidence,

1:05:18.280 --> 1:05:20.280
 but everybody else who's working on it.

1:05:21.840 --> 1:05:26.640
 We just have to kind of do all the steps

1:05:26.640 --> 1:05:31.040
 in order to make it actual.

1:05:31.040 --> 1:05:33.500
 How much of a good idea is about timing?

1:05:35.400 --> 1:05:37.040
 How much is it about your genius

1:05:37.040 --> 1:05:40.000
 versus that its time has come?

1:05:41.620 --> 1:05:42.920
 Timing's very important.

1:05:42.920 --> 1:05:46.200
 I mean, that's really why I got into futurism.

1:05:46.200 --> 1:05:50.500
 I didn't, I wasn't inherently a futurist.

1:05:50.500 --> 1:05:52.620
 That was not really my goal.

1:05:54.320 --> 1:05:57.400
 It's really to figure out when things are feasible.

1:05:57.400 --> 1:06:00.800
 We see that now with large scale models.

1:06:01.680 --> 1:06:06.400
 The very large scale models like GPT3,

1:06:06.400 --> 1:06:08.200
 it started two years ago.

1:06:09.600 --> 1:06:11.160
 Four years ago, it wasn't feasible.

1:06:11.160 --> 1:06:16.160
 In fact, they did create GPT2, which didn't work.

1:06:18.800 --> 1:06:22.360
 So it required a certain amount of timing

1:06:22.360 --> 1:06:24.200
 having to do with this exponential growth

1:06:24.200 --> 1:06:27.400
 of computing power.

1:06:27.400 --> 1:06:31.240
 So futurism in some sense is a study of timing,

1:06:31.240 --> 1:06:34.400
 trying to understand how the world will evolve

1:06:34.400 --> 1:06:38.320
 and when will the capacity for certain ideas emerge.

1:06:38.320 --> 1:06:40.040
 And that's become a thing in itself

1:06:40.040 --> 1:06:42.560
 and to try to time things in the future.

1:06:43.960 --> 1:06:48.960
 But really its original purpose was to time my products.

1:06:48.960 --> 1:06:53.800
 I mean, I did OCR in the 1970s

1:06:55.480 --> 1:07:00.480
 because OCR doesn't require a lot of computation.

1:07:01.440 --> 1:07:02.760
 Optical character recognition.

1:07:02.760 --> 1:07:06.560
 Yeah, so we were able to do that in the 70s

1:07:06.560 --> 1:07:11.000
 and I waited till the 80s to address speech recognition

1:07:11.000 --> 1:07:14.480
 since that requires more computation.

1:07:14.480 --> 1:07:16.000
 So you were thinking through timing

1:07:16.000 --> 1:07:17.480
 when you're developing those things.

1:07:17.480 --> 1:07:18.320
 Yeah.

1:07:18.320 --> 1:07:19.880
 Time come.

1:07:19.880 --> 1:07:21.400
 Yeah.

1:07:21.400 --> 1:07:24.360
 And that's how you've developed that brain power

1:07:24.360 --> 1:07:26.720
 to start to think in a futurist sense

1:07:26.720 --> 1:07:31.040
 when how will the world look like in 2045

1:07:31.040 --> 1:07:33.640
 and work backwards and how it gets there.

1:07:33.640 --> 1:07:35.360
 But that has to become a thing in itself

1:07:35.360 --> 1:07:40.360
 because looking at what things will be like in the future

1:07:40.360 --> 1:07:47.360
 and the future reflects such dramatic changes in how humans will live

1:07:48.680 --> 1:07:51.240
 that was worth communicating also.

1:07:51.240 --> 1:07:56.360
 So you developed that muscle of predicting the future

1:07:56.360 --> 1:07:58.280
 and then applied broadly

1:07:58.280 --> 1:08:02.280
 and started to discuss how it changes the world of technology,

1:08:02.280 --> 1:08:06.800
 how it changes the world of human life on earth.

1:08:06.800 --> 1:08:09.000
 In Danielle, one of your books,

1:08:09.000 --> 1:08:11.600
 you write about someone who has the courage

1:08:11.600 --> 1:08:15.040
 to question assumptions that limit human imagination

1:08:15.040 --> 1:08:16.640
 to solve problems.

1:08:16.640 --> 1:08:18.560
 And you also give advice

1:08:18.560 --> 1:08:22.760
 on how each of us can have this kind of courage.

1:08:22.760 --> 1:08:24.520
 Well, it's good that you picked that quote

1:08:24.520 --> 1:08:27.480
 because I think that does symbolize what Danielle is about.

1:08:27.480 --> 1:08:28.760
 Courage.

1:08:28.760 --> 1:08:30.760
 So how can each of us have that courage

1:08:30.760 --> 1:08:33.760
 to question assumptions?

1:08:33.760 --> 1:08:38.600
 I mean, we see that when people can go beyond

1:08:38.600 --> 1:08:43.600
 the current realm and create something that's new.

1:08:43.880 --> 1:08:45.520
 I mean, take Uber, for example.

1:08:45.520 --> 1:08:48.120
 Before that existed, you never thought

1:08:48.120 --> 1:08:49.960
 that that would be feasible

1:08:49.960 --> 1:08:53.200
 and it did require changes in the way people work.

1:08:54.520 --> 1:08:57.840
 Is there practical advice as you give in the book

1:08:57.840 --> 1:09:02.040
 about what each of us can do to be a Danielle?

1:09:04.880 --> 1:09:06.880
 Well, she looks at the situation

1:09:06.880 --> 1:09:11.880
 and tries to imagine how she can overcome various obstacles

1:09:15.840 --> 1:09:17.960
 and then she goes for it.

1:09:17.960 --> 1:09:19.680
 And she's a very good communicator

1:09:19.680 --> 1:09:24.680
 so she can communicate these ideas to other people.

1:09:25.080 --> 1:09:27.640
 And there's practical advice of learning to program

1:09:27.640 --> 1:09:32.000
 and recording your life and things of this nature.

1:09:32.000 --> 1:09:33.240
 Become a physicist.

1:09:33.240 --> 1:09:36.880
 So you list a bunch of different suggestions

1:09:36.880 --> 1:09:39.120
 of how to throw yourself into this world.

1:09:39.120 --> 1:09:42.200
 Yeah, I mean, it's kind of an idea

1:09:42.200 --> 1:09:46.160
 how young people can actually change the world

1:09:46.160 --> 1:09:51.160
 by learning all of these different skills.

1:09:52.440 --> 1:09:54.760
 And at the core of that is the belief

1:09:54.760 --> 1:09:56.760
 that you can change the world.

1:09:57.840 --> 1:10:00.480
 That your mind, your body can change the world.

1:10:00.480 --> 1:10:02.760
 Yeah, that's right.

1:10:02.760 --> 1:10:05.160
 And not letting anyone else tell you otherwise.

1:10:06.640 --> 1:10:08.920
 That's really good, exactly.

1:10:08.920 --> 1:10:13.440
 When we upload the story you told about your dad

1:10:13.440 --> 1:10:15.280
 and having a conversation with him,

1:10:16.160 --> 1:10:19.960
 we're talking about uploading your mind to the computer.

1:10:21.720 --> 1:10:23.160
 Do you think we'll have a future

1:10:23.160 --> 1:10:25.640
 with something you call afterlife?

1:10:25.640 --> 1:10:29.840
 We'll have avatars that mimic increasingly better and better

1:10:29.840 --> 1:10:33.520
 our behavior, our appearance, all that kind of stuff.

1:10:33.520 --> 1:10:36.800
 Even those that are perhaps no longer with us.

1:10:36.800 --> 1:10:41.800
 Yes, I mean, we need some information about them.

1:10:42.840 --> 1:10:44.520
 I mean, think about my father.

1:10:45.640 --> 1:10:47.040
 I have what he wrote.

1:10:48.080 --> 1:10:50.480
 Now, he didn't have a word processor,

1:10:50.480 --> 1:10:53.680
 so he didn't actually write that much.

1:10:53.680 --> 1:10:56.000
 And our memories of him aren't perfect.

1:10:56.000 --> 1:10:59.840
 So how do you even know if you've created something

1:10:59.840 --> 1:11:00.840
 that's satisfactory?

1:11:00.840 --> 1:11:04.920
 Now, you could do a Frederick Kurzweil Turing test.

1:11:04.920 --> 1:11:07.080
 It seems like Frederick Kurzweil to me.

1:11:07.920 --> 1:11:10.280
 But the people who remember him, like me,

1:11:11.240 --> 1:11:14.400
 don't have a perfect memory.

1:11:14.400 --> 1:11:16.320
 Is there such a thing as a perfect memory?

1:11:16.320 --> 1:11:21.320
 Maybe the whole point is for him to make you feel

1:11:24.760 --> 1:11:25.600
 a certain way.

1:11:25.600 --> 1:11:28.400
 Yeah, well, I think that would be the goal.

1:11:28.400 --> 1:11:30.360
 And that's the connection we have with loved ones.

1:11:30.360 --> 1:11:35.120
 It's not really based on very strict definition of truth.

1:11:35.120 --> 1:11:37.560
 It's more about the experiences we share.

1:11:37.560 --> 1:11:39.880
 And they get morphed through memory.

1:11:39.880 --> 1:11:41.800
 But ultimately, they make us smile.

1:11:41.800 --> 1:11:44.440
 I think we definitely can do that.

1:11:44.440 --> 1:11:46.800
 And that would be very worthwhile.

1:11:46.800 --> 1:11:49.960
 So do you think we'll have a world of replicants?

1:11:49.960 --> 1:11:51.280
 Of copies?

1:11:51.280 --> 1:11:53.800
 There'll be a bunch of Ray Kurzweils.

1:11:53.800 --> 1:11:55.320
 Like, I could hang out with one.

1:11:55.320 --> 1:11:58.200
 I can download it for five bucks

1:11:58.200 --> 1:12:00.040
 and have a best friend, Ray.

1:12:01.680 --> 1:12:04.820
 And you, the original copy, wouldn't even know about it.

1:12:07.160 --> 1:12:10.040
 Is that, do you think that world is,

1:12:11.440 --> 1:12:13.360
 first of all, do you think that world is feasible?

1:12:13.360 --> 1:12:16.320
 And do you think there's ethical challenges there?

1:12:16.320 --> 1:12:18.080
 Like, how would you feel about me hanging out

1:12:18.080 --> 1:12:20.480
 with Ray Kurzweil and you not knowing about it?

1:12:20.480 --> 1:12:25.480
 It doesn't strike me as a problem.

1:12:28.080 --> 1:12:30.240
 Which you, the original?

1:12:30.240 --> 1:12:34.240
 Would you strike, would that cause a problem for you?

1:12:34.240 --> 1:12:37.480
 No, I would really very much enjoy it.

1:12:37.480 --> 1:12:38.760
 No, not just hang out with me,

1:12:38.760 --> 1:12:43.760
 but if somebody hang out with you, a replicant of you.

1:12:43.840 --> 1:12:46.800
 Well, I think I would start, it sounds exciting,

1:12:46.800 --> 1:12:50.640
 but then what if they start doing better than me

1:12:51.560 --> 1:12:54.060
 and take over my friend group?

1:12:55.000 --> 1:13:00.000
 And then, because they may be an imperfect copy

1:13:02.280 --> 1:13:05.320
 or there may be more social, all these kinds of things,

1:13:05.320 --> 1:13:07.640
 and then I become like the old version

1:13:07.640 --> 1:13:10.240
 that's not nearly as exciting.

1:13:10.240 --> 1:13:12.360
 Maybe they're a copy of the best version of me

1:13:12.360 --> 1:13:13.200
 on a good day.

1:13:13.200 --> 1:13:16.880
 Yeah, but if you hang out with a replicant of me

1:13:18.020 --> 1:13:20.200
 and that turned out to be successful,

1:13:20.200 --> 1:13:23.520
 I'd feel proud of that person because it was based on me.

1:13:24.960 --> 1:13:29.960
 So it's, but it is a kind of death of this version of you.

1:13:32.420 --> 1:13:33.960
 Well, not necessarily.

1:13:33.960 --> 1:13:36.360
 I mean, you can still be alive, right?

1:13:36.360 --> 1:13:38.560
 But, and you would be proud, okay,

1:13:38.560 --> 1:13:40.280
 so it's like having kids and you're proud

1:13:40.280 --> 1:13:42.720
 that they've done even more than you were able to do.

1:13:42.720 --> 1:13:43.560
 Yeah, exactly.

1:13:48.280 --> 1:13:50.040
 It does bring up new issues,

1:13:50.040 --> 1:13:53.880
 but it seems like an opportunity.

1:13:55.120 --> 1:13:57.840
 Well, that replicant should probably have the same rights

1:13:57.840 --> 1:13:58.680
 as you do.

1:13:59.680 --> 1:14:02.720
 Well, that gets into a whole issue

1:14:05.420 --> 1:14:07.400
 because when a replicant occurs,

1:14:07.400 --> 1:14:10.320
 they're not necessarily gonna have your rights.

1:14:10.320 --> 1:14:11.680
 And if a replicant occurs,

1:14:11.680 --> 1:14:13.480
 if it's somebody who's already dead,

1:14:14.680 --> 1:14:17.880
 do they have all the obligations

1:14:17.880 --> 1:14:21.160
 and that the original person had?

1:14:21.160 --> 1:14:23.400
 Do they have all the agreements that they had?

1:14:25.840 --> 1:14:30.200
 I think you're gonna have to have laws that say yes.

1:14:30.200 --> 1:14:33.260
 There has to be, if you wanna create a replicant,

1:14:33.260 --> 1:14:35.720
 they have to have all the same rights as human rights.

1:14:35.720 --> 1:14:37.080
 Well, you don't know.

1:14:37.080 --> 1:14:38.400
 Someone can create a replicant and say,

1:14:38.400 --> 1:14:39.240
 well, it's a replicant,

1:14:39.240 --> 1:14:40.920
 but I didn't bother getting their rights.

1:14:40.920 --> 1:14:41.760
 And so.

1:14:41.760 --> 1:14:43.720
 Yeah, but that would be illegal, I mean.

1:14:43.720 --> 1:14:47.600
 Like if you do that, you have to do that in the black market.

1:14:47.600 --> 1:14:49.520
 If you wanna get an official replicant.

1:14:49.520 --> 1:14:51.000
 Okay, it's not so easy.

1:14:51.000 --> 1:14:53.680
 It's supposed to create multiple replicants.

1:14:55.360 --> 1:14:57.940
 The original rights,

1:14:59.840 --> 1:15:03.180
 maybe for one person and not for a whole group of people.

1:15:04.640 --> 1:15:05.480
 Sure.

1:15:08.520 --> 1:15:10.600
 So there has to be at least one.

1:15:10.600 --> 1:15:13.160
 And then all the other ones kinda share the rights.

1:15:14.260 --> 1:15:16.480
 Yeah, I just don't think that,

1:15:16.480 --> 1:15:18.680
 that's very difficult to conceive for us humans,

1:15:18.680 --> 1:15:20.760
 the idea that this country.

1:15:20.760 --> 1:15:23.640
 You create a replicant that has certain,

1:15:24.600 --> 1:15:26.800
 I mean, I've talked to people about this,

1:15:26.800 --> 1:15:30.540
 including my wife, who would like to get back her father.

1:15:32.640 --> 1:15:36.560
 And she doesn't worry about who has rights to what.

1:15:38.280 --> 1:15:40.440
 She would have somebody that she could visit with

1:15:40.440 --> 1:15:42.440
 and might give her some satisfaction.

1:15:44.300 --> 1:15:49.240
 And she wouldn't care about any of these other rights.

1:15:49.240 --> 1:15:52.200
 What does your wife think about multiple rake or as wells?

1:15:53.560 --> 1:15:54.400
 Have you had that discussion?

1:15:54.400 --> 1:15:56.000
 I haven't addressed that with her.

1:15:58.200 --> 1:16:00.640
 I think ultimately that's an important question,

1:16:00.640 --> 1:16:03.560
 loved ones, how they feel about.

1:16:03.560 --> 1:16:05.040
 There's something about love.

1:16:05.040 --> 1:16:06.400
 Well, that's the key thing, right?

1:16:06.400 --> 1:16:07.960
 If the loved one's rejected,

1:16:07.960 --> 1:16:10.560
 it's not gonna work very well, so.

1:16:12.320 --> 1:16:15.960
 So the loved ones really are the key determinant,

1:16:15.960 --> 1:16:18.200
 whether or not this works or not.

1:16:19.760 --> 1:16:21.940
 But there's also ethical rules.

1:16:22.840 --> 1:16:24.200
 We have to contend with the idea,

1:16:24.200 --> 1:16:27.920
 and we have to contend with that idea with AI.

1:16:27.920 --> 1:16:30.320
 But what's gonna motivate it is,

1:16:30.320 --> 1:16:34.680
 I mean, I talk to people who really miss people who are gone

1:16:34.680 --> 1:16:37.760
 and they would love to get something back,

1:16:37.760 --> 1:16:39.500
 even if it isn't perfect.

1:16:40.840 --> 1:16:42.800
 And that's what's gonna motivate this.

1:16:47.120 --> 1:16:51.200
 And that person lives on in some form.

1:16:51.200 --> 1:16:52.880
 And the more data we have,

1:16:52.880 --> 1:16:56.080
 the more we're able to reconstruct that person

1:16:56.080 --> 1:16:57.540
 and allow them to live on.

1:16:59.360 --> 1:17:01.440
 And eventually as we go forward,

1:17:01.440 --> 1:17:03.160
 we're gonna have more and more of this data

1:17:03.160 --> 1:17:06.360
 because we're gonna have none of us

1:17:06.360 --> 1:17:08.360
 that are inside our neocortex

1:17:08.360 --> 1:17:10.200
 and we're gonna collect a lot of data.

1:17:11.120 --> 1:17:14.840
 In fact, anything that's data is always collected.

1:17:15.800 --> 1:17:18.680
 There is something a little bit sad,

1:17:18.680 --> 1:17:23.200
 which is becoming, or maybe it's hopeful,

1:17:23.200 --> 1:17:26.800
 which is more and more common these days,

1:17:26.800 --> 1:17:28.360
 which when a person passes away,

1:17:28.360 --> 1:17:29.960
 you have their Twitter account,

1:17:31.080 --> 1:17:34.080
 and you have the last tweet they tweeted,

1:17:34.080 --> 1:17:35.040
 like something they needed.

1:17:35.040 --> 1:17:36.520
 And you can recreate them now

1:17:36.520 --> 1:17:38.360
 with large language models and so on.

1:17:38.360 --> 1:17:40.880
 I mean, you can create somebody that's just like them

1:17:40.880 --> 1:17:45.040
 and can actually continue to communicate.

1:17:45.040 --> 1:17:46.440
 I think that's really exciting

1:17:46.440 --> 1:17:49.360
 because I think in some sense,

1:17:49.360 --> 1:17:51.760
 like if I were to die today,

1:17:51.760 --> 1:17:55.240
 in some sense I would continue on if I continued tweeting.

1:17:56.120 --> 1:17:57.640
 I tweet, therefore I am.

1:17:58.880 --> 1:18:02.040
 Yeah, well, I mean, that's one of the advantages

1:18:02.040 --> 1:18:06.600
 of a replicant, they can recreate the communications

1:18:06.600 --> 1:18:08.360
 of that person.

1:18:10.320 --> 1:18:14.400
 Do you hope, do you think, do you hope

1:18:14.400 --> 1:18:17.440
 humans will become a multi planetary species?

1:18:17.440 --> 1:18:20.040
 You've talked about the phases, the six epochs,

1:18:20.040 --> 1:18:23.600
 and one of them is reaching out into the stars in part.

1:18:23.600 --> 1:18:28.240
 Yes, but the kind of attempts we're making now

1:18:28.240 --> 1:18:33.040
 to go to other planetary objects

1:18:34.400 --> 1:18:36.560
 doesn't excite me that much

1:18:36.560 --> 1:18:38.840
 because it's not really advancing anything.

1:18:38.840 --> 1:18:41.160
 It's not efficient enough?

1:18:41.160 --> 1:18:45.160
 Yeah, and we're also putting out other human beings,

1:18:48.120 --> 1:18:50.440
 which is a very inefficient way

1:18:50.440 --> 1:18:52.600
 to explore these other objects.

1:18:52.600 --> 1:18:57.600
 What I'm really talking about in the sixth epoch,

1:18:57.800 --> 1:18:59.240
 the universe wakes up.

1:19:00.240 --> 1:19:03.080
 It's where we can spread our super intelligence

1:19:03.080 --> 1:19:05.400
 throughout the universe.

1:19:05.400 --> 1:19:08.120
 And that doesn't mean sending a very soft,

1:19:08.120 --> 1:19:10.200
 squishy creatures like humans.

1:19:10.200 --> 1:19:13.840
 Yeah, the universe wakes up.

1:19:13.840 --> 1:19:18.840
 I mean, we would send intelligence masses of nanobots

1:19:18.840 --> 1:19:23.840
 which can then go out and colonize

1:19:24.880 --> 1:19:27.840
 these other parts of the universe.

1:19:29.000 --> 1:19:31.600
 Do you think there's intelligent alien civilizations

1:19:31.600 --> 1:19:34.120
 out there that our bots might meet?

1:19:35.240 --> 1:19:37.440
 My hunch is no.

1:19:38.760 --> 1:19:40.720
 Most people say yes, absolutely.

1:19:40.720 --> 1:19:43.480
 I mean, and the universe is too big.

1:19:43.480 --> 1:19:46.160
 And they'll cite the Drake equation.

1:19:46.160 --> 1:19:50.720
 And I think in Singularity is Near,

1:19:52.560 --> 1:19:56.200
 I have two analyses of the Drake equation,

1:19:56.200 --> 1:19:58.920
 both with very reasonable assumptions.

1:20:00.000 --> 1:20:04.960
 And one gives you thousands of advanced civilizations

1:20:04.960 --> 1:20:06.240
 in each galaxy.

1:20:07.440 --> 1:20:11.840
 And another one gives you one civilization.

1:20:11.840 --> 1:20:13.680
 And we know of one.

1:20:13.680 --> 1:20:16.600
 A lot of the analyses are forgetting

1:20:16.600 --> 1:20:20.160
 the exponential growth of computation.

1:20:21.200 --> 1:20:24.840
 Because we've gone from where the fastest way

1:20:24.840 --> 1:20:28.480
 I could send a message to somebody was with a pony,

1:20:30.160 --> 1:20:33.400
 which was what, like a century and a half ago?

1:20:34.920 --> 1:20:37.880
 To the advanced civilization we have today.

1:20:37.880 --> 1:20:40.880
 And if you accept what I've said,

1:20:40.880 --> 1:20:42.720
 go forward a few decades,

1:20:42.720 --> 1:20:46.800
 you can have absolutely fantastic amount of civilization

1:20:46.800 --> 1:20:50.400
 compared to a pony, and that's in a couple hundred years.

1:20:50.400 --> 1:20:53.320
 Yeah, the speed and the scale of information transfer

1:20:53.320 --> 1:20:57.560
 is growing exponentially in a blink of an eye.

1:20:58.720 --> 1:21:01.680
 Now think about these other civilizations.

1:21:01.680 --> 1:21:05.160
 They're gonna be spread out at cosmic times.

1:21:06.320 --> 1:21:10.160
 So if something is like ahead of us or behind us,

1:21:10.160 --> 1:21:14.280
 it could be ahead of us or behind us by maybe millions

1:21:14.280 --> 1:21:16.440
 of years, which isn't that much.

1:21:16.440 --> 1:21:21.440
 I mean, the world is billions of years old,

1:21:21.520 --> 1:21:23.960
 14 billion or something.

1:21:23.960 --> 1:21:28.960
 So even a thousand years, if two or 300 years is enough

1:21:29.760 --> 1:21:33.920
 to go from a pony to fantastic amount of civilization,

1:21:33.920 --> 1:21:35.920
 we would see that.

1:21:35.920 --> 1:21:39.720
 So of other civilizations that have occurred,

1:21:39.720 --> 1:21:43.960
 okay, some might be behind us, but some might be ahead of us.

1:21:43.960 --> 1:21:45.800
 If they're ahead of us, they're ahead of us

1:21:45.800 --> 1:21:49.560
 by thousands, millions of years,

1:21:49.560 --> 1:21:51.760
 and they would be so far beyond us,

1:21:51.760 --> 1:21:55.200
 they would be doing galaxy wide engineering.

1:21:56.200 --> 1:22:00.080
 But we don't see anything doing galaxy wide engineering.

1:22:00.080 --> 1:22:05.120
 So either they don't exist, or this very universe

1:22:05.120 --> 1:22:08.340
 is a construction of an alien species.

1:22:08.340 --> 1:22:10.900
 We're living inside a video game.

1:22:11.760 --> 1:22:14.840
 Well, that's another explanation that yes,

1:22:14.840 --> 1:22:19.140
 you've got some teenage kids in another civilization.

1:22:19.140 --> 1:22:22.280
 Do you find compelling the simulation hypothesis

1:22:22.280 --> 1:22:25.640
 as a thought experiment that we're living in a simulation?

1:22:25.640 --> 1:22:29.400
 The universe is computational.

1:22:29.400 --> 1:22:34.400
 So we are an example in a computational world.

1:22:34.400 --> 1:22:39.120
 Therefore, it is a simulation.

1:22:39.120 --> 1:22:41.040
 It doesn't necessarily mean an experiment

1:22:41.040 --> 1:22:44.820
 by some high school kid in another world,

1:22:44.820 --> 1:22:47.800
 but it nonetheless is taking place

1:22:47.800 --> 1:22:50.120
 in a computational world.

1:22:50.120 --> 1:22:51.600
 And everything that's going on

1:22:51.600 --> 1:22:56.080
 is basically a form of computation.

1:22:58.080 --> 1:23:00.640
 So you really have to define what you mean

1:23:00.640 --> 1:23:05.640
 by this whole world being a simulation.

1:23:06.360 --> 1:23:11.360
 Well, then it's the teenager that makes the video game.

1:23:12.400 --> 1:23:16.720
 Us humans with our current limited cognitive capability

1:23:16.720 --> 1:23:20.560
 have strived to understand ourselves

1:23:20.560 --> 1:23:23.840
 and we have created religions.

1:23:23.840 --> 1:23:25.160
 We think of God.

1:23:25.160 --> 1:23:30.160
 Whatever that is, do you think God exists?

1:23:32.240 --> 1:23:34.500
 And if so, who is God?

1:23:35.440 --> 1:23:37.720
 I alluded to this before.

1:23:37.720 --> 1:23:41.960
 We started out with lots of particles going around

1:23:42.840 --> 1:23:47.840
 and there's nothing that represents love and creativity.

1:23:53.160 --> 1:23:55.000
 And somehow we've gotten into a world

1:23:55.000 --> 1:23:56.760
 where love actually exists

1:23:57.920 --> 1:23:59.960
 and that has to do actually with consciousness

1:23:59.960 --> 1:24:03.120
 because you can't have love without consciousness.

1:24:03.120 --> 1:24:06.720
 So to me, that's God, the fact that we have something

1:24:06.720 --> 1:24:11.200
 where love, where you can be devoted to someone else

1:24:11.200 --> 1:24:15.200
 and really feel the love, that's God.

1:24:19.080 --> 1:24:21.040
 And if you look at the Old Testament,

1:24:21.040 --> 1:24:26.040
 it was actually created by several different

1:24:26.680 --> 1:24:29.200
 ravenants in there.

1:24:29.200 --> 1:24:32.500
 And I think they've identified three of them.

1:24:34.080 --> 1:24:39.080
 One of them dealt with God as a person

1:24:39.400 --> 1:24:42.440
 that you can make deals with and he gets angry

1:24:42.440 --> 1:24:47.440
 and he wrecks vengeance on various people.

1:24:48.320 --> 1:24:50.440
 But two of them actually talk about God

1:24:50.440 --> 1:24:55.440
 as a symbol of love and peace and harmony and so forth.

1:24:58.280 --> 1:24:59.980
 That's how they describe God.

1:25:01.360 --> 1:25:06.120
 So that's my view of God, not as a person in the sky

1:25:06.120 --> 1:25:08.140
 that you can make deals with.

1:25:09.120 --> 1:25:13.200
 It's whatever the magic that goes from basic elements

1:25:13.200 --> 1:25:15.960
 to things like consciousness and love.

1:25:15.960 --> 1:25:19.200
 Do you think one of the things I find

1:25:19.200 --> 1:25:22.240
 extremely beautiful and powerful is cellular automata,

1:25:22.240 --> 1:25:23.600
 which you also touch on?

1:25:24.640 --> 1:25:27.720
 Do you think whatever the heck happens in cellular automata

1:25:27.720 --> 1:25:30.460
 where interesting, complicated objects emerge,

1:25:31.480 --> 1:25:33.480
 God is in there too?

1:25:33.480 --> 1:25:38.480
 The emergence of love in this seemingly primitive universe?

1:25:38.480 --> 1:25:41.560
 Well, that's the goal of creating a replicant

1:25:42.600 --> 1:25:47.600
 is that they would love you and you would love them.

1:25:47.600 --> 1:25:50.840
 There wouldn't be much point of doing it

1:25:50.840 --> 1:25:52.720
 if that didn't happen.

1:25:52.720 --> 1:25:54.880
 But all of it, I guess what I'm saying

1:25:54.880 --> 1:25:59.280
 about cellular automata is it's primitive building blocks

1:25:59.280 --> 1:26:03.440
 and they somehow create beautiful things.

1:26:03.440 --> 1:26:06.280
 Is there some deep truth to that

1:26:06.280 --> 1:26:07.960
 about how our universe works?

1:26:07.960 --> 1:26:11.160
 Is the emergence from simple rules,

1:26:11.160 --> 1:26:14.080
 beautiful, complex objects can emerge?

1:26:14.080 --> 1:26:16.680
 Is that the thing that made us?

1:26:16.680 --> 1:26:18.120
 Yeah, well. As we went through

1:26:18.120 --> 1:26:21.560
 all the six phases of reality.

1:26:21.560 --> 1:26:23.660
 That's a good way to look at it.

1:26:23.660 --> 1:26:27.320
 It does make some point to the whole value

1:26:27.320 --> 1:26:29.820
 of having a universe.

1:26:31.400 --> 1:26:34.040
 Do you think about your own mortality?

1:26:34.040 --> 1:26:35.080
 Are you afraid of it?

1:26:36.240 --> 1:26:41.240
 Yes, but I keep going back to my idea

1:26:41.240 --> 1:26:46.240
 of being able to expand human life quickly enough

1:26:48.080 --> 1:26:53.080
 in advance of our getting there, longevity escape velocity,

1:26:55.880 --> 1:26:57.660
 which we're not quite at yet,

1:26:58.600 --> 1:27:01.520
 but I think we're actually pretty close,

1:27:01.520 --> 1:27:05.320
 particularly with, for example, doing simulated biology.

1:27:06.600 --> 1:27:08.920
 I think we can probably get there within,

1:27:08.920 --> 1:27:12.800
 say, by the end of this decade, and that's my goal.

1:27:12.800 --> 1:27:16.400
 Do you hope to achieve the longevity escape velocity?

1:27:16.400 --> 1:27:18.520
 Do you hope to achieve immortality?

1:27:20.900 --> 1:27:22.960
 Well, immortality is hard to say.

1:27:22.960 --> 1:27:26.080
 I can't really come on your program saying I've done it.

1:27:26.080 --> 1:27:30.280
 I've achieved immortality because it's never forever.

1:27:32.480 --> 1:27:35.280
 A long time, a long time of living well.

1:27:35.280 --> 1:27:37.180
 But we'd like to actually advance

1:27:37.180 --> 1:27:41.000
 human life expectancy, advance my life expectancy

1:27:41.000 --> 1:27:44.000
 more than a year every year,

1:27:44.000 --> 1:27:45.820
 and I think we can get there within,

1:27:45.820 --> 1:27:47.800
 by the end of this decade.

1:27:47.800 --> 1:27:49.440
 How do you think we'd do it?

1:27:49.440 --> 1:27:53.640
 So there's practical things in Transcend,

1:27:53.640 --> 1:27:56.200
 the nine steps to living well forever, your book.

1:27:56.200 --> 1:27:58.360
 You describe just that.

1:27:58.360 --> 1:28:00.520
 There's practical things like health,

1:28:00.520 --> 1:28:02.280
 exercise, all those things.

1:28:02.280 --> 1:28:03.920
 Yeah, I mean, we live in a body

1:28:03.920 --> 1:28:08.280
 that doesn't last forever.

1:28:08.280 --> 1:28:10.280
 There's no reason why it can't, though,

1:28:11.160 --> 1:28:14.000
 and we're discovering things, I think, that will extend it.

1:28:17.640 --> 1:28:19.400
 But you do have to deal with,

1:28:19.400 --> 1:28:22.080
 I mean, I've got various issues.

1:28:23.240 --> 1:28:28.240
 Went to Mexico 40 years ago, developed salmonella.

1:28:28.240 --> 1:28:33.060
 I created pancreatitis, which gave me

1:28:33.060 --> 1:28:35.360
 a strange form of diabetes.

1:28:37.380 --> 1:28:42.380
 It's not type one diabetes, because it's an autoimmune

1:28:42.900 --> 1:28:44.860
 disorder that destroys your pancreas.

1:28:44.860 --> 1:28:45.880
 I don't have that.

1:28:46.780 --> 1:28:48.700
 But it's also not type two diabetes,

1:28:48.700 --> 1:28:51.740
 because type two diabetes is your pancreas works fine,

1:28:51.740 --> 1:28:55.740
 but your cells don't absorb the insulin well.

1:28:55.740 --> 1:28:58.460
 I don't have that either.

1:28:58.460 --> 1:29:03.460
 The pancreatitis I had partially damaged my pancreas,

1:29:04.560 --> 1:29:06.100
 but it was a one time thing.

1:29:06.100 --> 1:29:11.100
 It didn't continue, and I've learned now how to control it.

1:29:11.620 --> 1:29:13.860
 But so that's just something that I had to do

1:29:15.300 --> 1:29:18.540
 in order to continue to exist.

1:29:18.540 --> 1:29:20.460
 Since your particular biological system,

1:29:20.460 --> 1:29:22.540
 you had to figure out a few hacks,

1:29:22.540 --> 1:29:24.900
 and the idea is that science would be able

1:29:24.900 --> 1:29:26.420
 to do that much better, actually.

1:29:26.420 --> 1:29:29.340
 Yeah, so I mean, I do spend a lot of time

1:29:29.340 --> 1:29:32.220
 just tinkering with my own body to keep it going.

1:29:34.240 --> 1:29:37.740
 So I do think I'll last till the end of this decade,

1:29:37.740 --> 1:29:41.660
 and I think we'll achieve longevity, escape velocity.

1:29:41.660 --> 1:29:43.540
 I think that we'll start with people

1:29:43.540 --> 1:29:46.180
 who are very diligent about this.

1:29:46.180 --> 1:29:48.860
 Eventually, it'll become sort of routine

1:29:48.860 --> 1:29:51.400
 that people will be able to do it.

1:29:51.400 --> 1:29:54.300
 So if you're talking about kids today,

1:29:54.300 --> 1:29:56.700
 or even people in their 20s or 30s,

1:29:56.700 --> 1:30:00.480
 that's really not a very serious problem.

1:30:01.300 --> 1:30:05.100
 I have had some discussions with relatives

1:30:05.100 --> 1:30:10.100
 who are like almost 100, and saying,

1:30:10.340 --> 1:30:13.380
 well, we're working on it as quickly as possible.

1:30:13.380 --> 1:30:14.980
 I don't know if that's gonna work.

1:30:16.460 --> 1:30:18.400
 Is there a case, this is a difficult question,

1:30:18.400 --> 1:30:23.400
 but is there a case to be made against living forever

1:30:23.400 --> 1:30:28.400
 that a finite life, that mortality is a feature, not a bug,

1:30:29.620 --> 1:30:34.620
 that living a shorter, so dying makes ice cream

1:30:36.020 --> 1:30:40.260
 taste delicious, makes life intensely beautiful

1:30:40.260 --> 1:30:42.220
 more than it otherwise might be?

1:30:42.220 --> 1:30:46.820
 Most people believe that way, except if you present

1:30:46.820 --> 1:30:51.500
 a death of anybody they care about or love,

1:30:51.500 --> 1:30:55.300
 they find that extremely depressing.

1:30:55.300 --> 1:30:58.420
 And I know people who feel that way

1:30:58.420 --> 1:31:03.420
 20, 30, 40 years later, they still want them back.

1:31:06.200 --> 1:31:10.660
 So I mean, death is not something to celebrate,

1:31:11.820 --> 1:31:16.260
 but we've lived in a world where people just accept this.

1:31:16.260 --> 1:31:18.340
 Life is short, you see it all the time on TV,

1:31:18.340 --> 1:31:21.340
 oh, life's short, you have to take advantage of it

1:31:21.340 --> 1:31:23.860
 and nobody accepts the fact that you could actually

1:31:23.860 --> 1:31:27.940
 go beyond normal lifetimes.

1:31:27.940 --> 1:31:31.580
 But anytime we talk about death or a death of a person,

1:31:31.580 --> 1:31:35.420
 even one death is a terrible tragedy.

1:31:35.420 --> 1:31:39.000
 If you have somebody that lives to 100 years old,

1:31:39.000 --> 1:31:42.860
 we still love them in return.

1:31:43.820 --> 1:31:47.660
 And there's no limitation to that.

1:31:47.660 --> 1:31:52.000
 In fact, these kinds of trends are gonna provide

1:31:52.000 --> 1:31:54.700
 greater and greater opportunity for everybody,

1:31:54.700 --> 1:31:56.200
 even if we have more people.

1:31:57.100 --> 1:32:00.320
 So let me ask about an alien species

1:32:00.320 --> 1:32:03.060
 or a super intelligent AI 500 years from now

1:32:03.060 --> 1:32:08.060
 that will look back and remember Ray Kurzweil version zero.

1:32:11.140 --> 1:32:13.140
 Before the replicants spread,

1:32:13.140 --> 1:32:17.020
 how do you hope they remember you

1:32:17.020 --> 1:32:21.420
 in Hitchhiker's Guide to the Galaxy summary of Ray Kurzweil?

1:32:21.420 --> 1:32:23.160
 What do you hope your legacy is?

1:32:24.120 --> 1:32:26.740
 Well, I mean, I do hope to be around, so that's.

1:32:26.740 --> 1:32:27.900
 Some version of you, yes.

1:32:27.900 --> 1:32:28.740
 So.

1:32:29.740 --> 1:32:32.140
 Do you think you'll be the same person around?

1:32:32.140 --> 1:32:37.020
 I mean, am I the same person I was when I was 20 or 10?

1:32:37.020 --> 1:32:39.780
 You would be the same person in that same way,

1:32:39.780 --> 1:32:43.380
 but yes, we're different, we're different.

1:32:44.420 --> 1:32:46.900
 All we have of that, all you have of that person

1:32:46.900 --> 1:32:51.900
 is your memories, which are probably distorted in some way.

1:32:53.700 --> 1:32:55.860
 Maybe you just remember the good parts,

1:32:55.860 --> 1:32:57.860
 depending on your psyche.

1:32:57.860 --> 1:32:59.740
 You might focus on the bad parts,

1:32:59.740 --> 1:33:01.260
 might focus on the good parts.

1:33:02.820 --> 1:33:06.500
 Right, but I mean, I still have a relationship

1:33:06.500 --> 1:33:10.820
 to the way I was when I was earlier, when I was younger.

1:33:11.860 --> 1:33:14.220
 How will you and the other super intelligent AIs

1:33:14.220 --> 1:33:17.720
 remember you of today from 500 years ago?

1:33:18.940 --> 1:33:22.860
 What do you hope to be remembered by this version of you

1:33:22.860 --> 1:33:24.680
 before the singularity?

1:33:25.640 --> 1:33:28.220
 Well, I think it's expressed well in my books,

1:33:28.220 --> 1:33:32.620
 trying to create some new realities that people will accept.

1:33:32.620 --> 1:33:36.800
 I mean, that's something that gives me great pleasure,

1:33:40.320 --> 1:33:45.320
 and greater insight into what makes humans valuable.

1:33:49.720 --> 1:33:54.720
 I'm not the only person who's tempted to comment on that.

1:33:57.220 --> 1:34:00.700
 And optimism that permeates your work.

1:34:00.700 --> 1:34:04.700
 Optimism about the future is ultimately that optimism

1:34:04.700 --> 1:34:06.780
 paves the way for building a better future.

1:34:06.780 --> 1:34:09.080
 Yeah, I agree with that.

1:34:10.100 --> 1:34:15.100
 So you asked your dad about the meaning of life,

1:34:15.360 --> 1:34:19.260
 and he said, love, let me ask you the same question.

1:34:19.260 --> 1:34:21.200
 What's the meaning of life?

1:34:21.200 --> 1:34:22.900
 Why are we here?

1:34:22.900 --> 1:34:26.900
 This beautiful journey that we're on in phase four,

1:34:26.900 --> 1:34:31.900
 reaching for phase five of this evolution

1:34:32.540 --> 1:34:34.660
 and information processing, why?

1:34:35.500 --> 1:34:38.320
 Well, I think I'd give the same answers as my father.

1:34:42.020 --> 1:34:43.860
 Because if there were no love,

1:34:43.860 --> 1:34:45.640
 and we didn't care about anybody,

1:34:46.580 --> 1:34:48.300
 there'd be no point existing.

1:34:49.540 --> 1:34:51.460
 Love is the meaning of life.

1:34:51.460 --> 1:34:54.260
 The AI version of your dad had a good point.

1:34:54.260 --> 1:34:57.820
 Well, I think that's a beautiful way to end it.

1:34:57.820 --> 1:34:59.260
 Ray, thank you for your work.

1:34:59.260 --> 1:35:01.100
 Thank you for being who you are.

1:35:01.100 --> 1:35:03.420
 Thank you for dreaming about a beautiful future

1:35:03.420 --> 1:35:06.460
 and creating it along the way.

1:35:06.460 --> 1:35:09.340
 And thank you so much for spending

1:35:09.340 --> 1:35:10.900
 your really valuable time with me today.

1:35:10.900 --> 1:35:12.260
 This was awesome.

1:35:12.260 --> 1:35:16.300
 It was my pleasure, and you have some great insights,

1:35:16.300 --> 1:35:20.540
 both into me and into humanity as well, so I appreciate that.

1:35:21.440 --> 1:35:22.980
 Thanks for listening to this conversation

1:35:22.980 --> 1:35:24.340
 with Ray Kurzweil.

1:35:24.340 --> 1:35:25.580
 To support this podcast,

1:35:25.580 --> 1:35:28.380
 please check out our sponsors in the description.

1:35:28.380 --> 1:35:30.420
 And now, let me leave you with some words

1:35:30.420 --> 1:35:31.900
 from Isaac Asimov.

1:35:32.820 --> 1:35:37.660
 It is change, continuous change, inevitable change

1:35:37.660 --> 1:35:41.060
 that is the dominant factor in society today.

1:35:41.060 --> 1:35:43.860
 No sensible decision can be made any longer

1:35:43.860 --> 1:35:47.320
 without taking into account not only the world as it is,

1:35:47.320 --> 1:35:49.560
 but the world as it will be.

1:35:49.560 --> 1:35:52.540
 This, in turn, means that our statesmen,

1:35:52.540 --> 1:35:55.340
 our businessmen, our everyman,

1:35:55.340 --> 1:35:58.460
 must take on a science fictional way of thinking.

1:35:58.460 --> 1:36:21.460
 Thank you for listening, and hope to see you next time.

