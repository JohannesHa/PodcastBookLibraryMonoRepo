WEBVTT

00:00.000 --> 00:02.920
 The following is a conversation with Jim Keller,

00:02.920 --> 00:04.960
 his second time in the podcast.

00:04.960 --> 00:08.480
 Jim is a legendary microprocessor architect

00:08.480 --> 00:11.080
 and is widely seen as one of the greatest

00:11.080 --> 00:14.620
 engineering minds of the computing age.

00:14.620 --> 00:18.840
 In a peculiar twist of space time in our simulation,

00:18.840 --> 00:22.200
 Jim is also a brother in law of Jordan Peterson.

00:22.200 --> 00:25.320
 We talk about this and about computing,

00:25.320 --> 00:29.200
 artificial intelligence, consciousness, and life.

00:29.200 --> 00:31.280
 Quick mention of our sponsors.

00:31.280 --> 00:33.780
 Athletic Greens All In One Nutrition Drink,

00:33.780 --> 00:36.600
 Brooklyn and Sheets, ExpressVPN,

00:36.600 --> 00:39.600
 and Belcampo Grass Fed Meat.

00:39.600 --> 00:41.680
 Click the sponsor links to get a discount

00:41.680 --> 00:43.920
 and to support this podcast.

00:43.920 --> 00:46.540
 As a side note, let me say that Jim is someone who,

00:46.540 --> 00:50.160
 on a personal level, inspired me to be myself.

00:50.160 --> 00:53.340
 There was something in his words, on and off the mic,

00:53.340 --> 00:56.200
 or perhaps that he even paid attention to me at all,

00:56.200 --> 00:59.160
 that almost told me, you're all right, kid.

00:59.160 --> 01:01.820
 A kind of pat on the back that can make the difference

01:01.820 --> 01:03.640
 between a mind that flourishes

01:03.640 --> 01:05.760
 and a mind that is broken down

01:05.760 --> 01:08.160
 by the cynicism of the world.

01:08.160 --> 01:10.440
 So I guess that's just my brief few words

01:10.440 --> 01:12.800
 of thank you to Jim, and in general,

01:12.800 --> 01:15.440
 gratitude for the people who have given me a chance

01:15.440 --> 01:19.000
 on this podcast, in my work, and in life.

01:19.000 --> 01:21.200
 If you enjoy this thing, subscribe on YouTube,

01:21.200 --> 01:24.240
 review on Apple Podcast, follow on Spotify,

01:24.240 --> 01:26.360
 support on Patreon, or connect with me

01:26.360 --> 01:28.560
 on Twitter, Alex Friedman.

01:28.560 --> 01:32.340
 And now, here's my conversation with Jim Keller.

01:33.360 --> 01:35.340
 What's the value and effectiveness

01:35.340 --> 01:38.080
 of theory versus engineering, this dichotomy,

01:38.080 --> 01:43.080
 in building good software or hardware systems?

01:43.400 --> 01:45.520
 Well, good design is both.

01:46.440 --> 01:48.680
 I guess that's pretty obvious.

01:48.680 --> 01:51.840
 By engineering, do you mean reduction of practice

01:51.840 --> 01:53.260
 of known methods?

01:53.260 --> 01:55.960
 And then science is the pursuit of discovering things

01:55.960 --> 01:57.780
 that people don't understand.

01:57.780 --> 02:00.340
 Or solving unknown problems.

02:00.340 --> 02:01.960
 Definitions are interesting here,

02:01.960 --> 02:04.120
 but I was thinking more in theory,

02:04.120 --> 02:06.740
 constructing models that kind of generalize

02:06.740 --> 02:08.540
 about how things work.

02:08.540 --> 02:12.760
 And engineering is actually building stuff.

02:12.760 --> 02:16.180
 The pragmatic, like, okay, we have these nice models,

02:16.180 --> 02:17.920
 but how do we actually get things to work?

02:17.920 --> 02:20.740
 Maybe economics is a nice example.

02:20.740 --> 02:22.440
 Like, economists have all these models

02:22.440 --> 02:23.640
 of how the economy works,

02:23.640 --> 02:26.680
 and how different policies will have an effect,

02:26.680 --> 02:29.240
 but then there's the actual, okay,

02:29.240 --> 02:30.480
 let's call it engineering,

02:30.480 --> 02:33.240
 of like, actually deploying the policies.

02:33.240 --> 02:36.380
 So computer design is almost all engineering.

02:36.380 --> 02:38.200
 And reduction of practice of known methods.

02:38.200 --> 02:43.200
 Now, because of the complexity of the computers we built,

02:43.560 --> 02:44.960
 you know, you could think you're,

02:44.960 --> 02:46.600
 well, we'll just go write some code,

02:46.600 --> 02:49.160
 and then we'll verify it, and then we'll put it together,

02:49.160 --> 02:50.920
 and then you find out that the combination

02:50.920 --> 02:53.200
 of all that stuff is complicated.

02:53.200 --> 02:54.700
 And then you have to be inventive

02:54.700 --> 02:56.920
 to figure out how to do it, right?

02:56.920 --> 02:59.760
 So that definitely happens a lot.

02:59.760 --> 03:04.440
 And then, every so often, some big idea happens.

03:04.440 --> 03:06.360
 But it might be one person.

03:06.360 --> 03:08.840
 And that idea is in the space of engineering,

03:08.840 --> 03:10.440
 or is it in the space of...

03:10.440 --> 03:11.380
 Well, I'll give you an example.

03:11.380 --> 03:13.140
 So one of the limits of computer performance

03:13.140 --> 03:14.880
 is branch prediction.

03:14.880 --> 03:17.500
 So, and there's a whole bunch of ideas

03:17.500 --> 03:19.440
 about how good you could predict a branch.

03:19.440 --> 03:21.640
 And people said, there's a limit to it,

03:21.640 --> 03:23.480
 it's an asymptotic curve.

03:23.480 --> 03:24.920
 And somebody came up with a better way

03:24.920 --> 03:27.400
 to do branch prediction, it was a lot better.

03:28.280 --> 03:29.720
 And he published a paper on it,

03:29.720 --> 03:32.760
 and every computer in the world now uses it.

03:32.760 --> 03:34.600
 And it was one idea.

03:34.600 --> 03:37.960
 So the engineers who build branch prediction hardware

03:37.960 --> 03:40.520
 were happy to drop the one kind of training array

03:40.520 --> 03:42.380
 and put it in another one.

03:42.380 --> 03:44.840
 So it was a real idea.

03:44.840 --> 03:48.520
 And branch prediction is one of the key problems

03:48.520 --> 03:51.960
 underlying all of sort of the lowest level of software.

03:51.960 --> 03:53.800
 It boils down to branch prediction.

03:53.800 --> 03:54.860
 Boils down to uncertainty.

03:54.860 --> 03:56.280
 Computers are limited by...

03:56.280 --> 03:58.640
 Single thread computer is limited by two things.

03:58.640 --> 04:01.400
 The predictability of the path of the branches

04:01.400 --> 04:04.140
 and the predictability of the locality of data.

04:05.320 --> 04:07.080
 So we have predictors that now predict

04:07.080 --> 04:09.160
 both of those pretty well.

04:09.160 --> 04:11.880
 So memory is a couple hundred cycles away,

04:11.880 --> 04:14.540
 local cache is a couple cycles away.

04:14.540 --> 04:15.720
 When you're executing fast,

04:15.720 --> 04:19.020
 virtually all the data has to be in the local cache.

04:19.020 --> 04:21.320
 So a simple program says,

04:21.320 --> 04:23.280
 add one to every element in an array,

04:23.280 --> 04:26.680
 it's really easy to see what the stream of data will be.

04:26.680 --> 04:28.520
 But you might have a more complicated program

04:28.520 --> 04:31.080
 that says, get an element of this array,

04:31.080 --> 04:32.800
 look at something, make a decision,

04:32.800 --> 04:35.200
 go get another element, it's kind of random.

04:35.200 --> 04:37.760
 And you can think, that's really unpredictable.

04:37.760 --> 04:39.200
 And then you make this big predictor

04:39.200 --> 04:41.400
 that looks at this kind of pattern and you realize,

04:41.400 --> 04:43.000
 well, if you get this data and this data,

04:43.000 --> 04:44.560
 then you probably want that one.

04:44.560 --> 04:46.440
 And if you get this one and this one and this one,

04:46.440 --> 04:47.960
 you probably want that one.

04:47.960 --> 04:49.920
 And is that theory or is that engineering?

04:49.920 --> 04:51.320
 Like the paper that was written,

04:51.320 --> 04:54.640
 was it asymptotic kind of discussion

04:54.640 --> 04:57.920
 or is it more like, here's a hack that works well?

04:57.920 --> 04:59.100
 It's a little bit of both.

04:59.100 --> 05:01.280
 Like there's information theory in it, I think somewhere.

05:01.280 --> 05:04.320
 Okay, so it's actually trying to prove some kind of stuff.

05:04.320 --> 05:06.360
 But once you know the method,

05:06.360 --> 05:08.660
 implementing it is an engineering problem.

05:09.560 --> 05:10.800
 Now there's a flip side of this,

05:10.800 --> 05:13.400
 which is in a big design team,

05:13.400 --> 05:14.960
 what percentage of people think

05:14.960 --> 05:19.960
 their plan or their life's work is engineering

05:20.800 --> 05:23.480
 versus inventing things?

05:23.480 --> 05:27.520
 So lots of companies will reward you for filing patents.

05:27.520 --> 05:29.280
 Some, many big companies get stuck

05:29.280 --> 05:30.420
 because to get promoted,

05:30.420 --> 05:32.940
 you have to come up with something new.

05:32.940 --> 05:34.740
 And then what happens is everybody's trying

05:34.740 --> 05:36.480
 to do some random new thing,

05:36.480 --> 05:39.120
 99% of which doesn't matter.

05:39.120 --> 05:41.140
 And the basics get neglected.

05:41.140 --> 05:46.140
 Or there's a dichotomy, they think like the cell library

05:47.700 --> 05:52.700
 and the basic CAD tools or basic software validation methods,

05:53.260 --> 05:54.740
 that's simple stuff.

05:54.740 --> 05:56.900
 They wanna work on the exciting stuff.

05:56.900 --> 05:58.460
 And then they spend lots of time

05:58.460 --> 06:00.740
 trying to figure out how to patent something.

06:00.740 --> 06:02.240
 And that's mostly useless.

06:02.240 --> 06:04.580
 But the breakthrough is on simple stuff.

06:04.580 --> 06:08.940
 No, no, you have to do the simple stuff really well.

06:08.940 --> 06:11.460
 If you're building a building out of bricks,

06:11.460 --> 06:13.240
 you want great bricks.

06:13.240 --> 06:14.900
 So you go to two places that sell bricks.

06:14.900 --> 06:17.980
 So one guy says, yeah, they're over there in a ugly pile.

06:17.980 --> 06:19.860
 And the other guy is like lovingly tells you

06:19.860 --> 06:22.300
 about the 50 kinds of bricks and how hard they are

06:22.300 --> 06:26.100
 and how beautiful they are and how square they are.

06:26.100 --> 06:28.220
 Which one are you gonna buy bricks from?

06:28.220 --> 06:30.420
 Which is gonna make a better house?

06:30.420 --> 06:32.020
 So you're talking about the craftsman,

06:32.020 --> 06:33.500
 the person who understands bricks,

06:33.500 --> 06:35.140
 who loves bricks, who loves the varieties.

06:35.140 --> 06:36.540
 That's a good word.

06:36.540 --> 06:39.460
 Good engineering is great craftsmanship.

06:39.460 --> 06:44.460
 And when you start thinking engineering is about invention

06:44.880 --> 06:47.940
 and you set up a system that rewards invention,

06:47.940 --> 06:50.660
 the craftsmanship gets neglected.

06:50.660 --> 06:53.500
 Okay, so maybe one perspective is the theory,

06:53.500 --> 06:57.660
 the science overemphasizes invention

06:57.660 --> 07:00.420
 and engineering emphasizes craftsmanship.

07:00.420 --> 07:03.940
 And therefore, so it doesn't matter what you do,

07:03.940 --> 07:05.060
 theory, engineering. Well, everybody does.

07:05.060 --> 07:06.740
 Like read the tech ranks are always talking

07:06.740 --> 07:09.540
 about some breakthrough or innovation

07:09.540 --> 07:12.460
 and everybody thinks that's the most important thing.

07:12.460 --> 07:13.900
 But the number of innovative ideas

07:13.900 --> 07:15.980
 is actually relatively low.

07:15.980 --> 07:17.260
 We need them, right?

07:17.260 --> 07:19.820
 And innovation creates a whole new opportunity.

07:19.820 --> 07:24.020
 Like when some guy invented the internet, right?

07:24.020 --> 07:25.940
 Like that was a big thing.

07:25.940 --> 07:28.240
 The million people that wrote software against that

07:28.240 --> 07:31.180
 were mostly doing engineering software writing.

07:31.180 --> 07:34.300
 So the elaboration of that idea was huge.

07:34.300 --> 07:35.580
 I don't know if you know Brendan Eich,

07:35.580 --> 07:38.180
 he wrote JavaScript in 10 days.

07:38.180 --> 07:39.540
 That's an interesting story.

07:39.540 --> 07:43.740
 It makes me wonder, and it was famously for many years

07:43.740 --> 07:47.660
 considered to be a pretty crappy programming language.

07:47.660 --> 07:48.780
 Still is perhaps.

07:48.780 --> 07:51.140
 It's been improving sort of consistently.

07:51.140 --> 07:55.580
 But the interesting thing about that guy is,

07:55.580 --> 07:57.380
 you know, he doesn't get any awards.

07:58.540 --> 08:01.140
 You don't get a Nobel Prize or a Fields Medal or.

08:01.140 --> 08:06.140
 For inventing a crappy piece of, you know, software code.

08:06.820 --> 08:08.700
 That is currently the number one programming language

08:08.700 --> 08:10.100
 in the world and runs,

08:10.100 --> 08:13.740
 now is increasingly running the backend of the internet.

08:13.740 --> 08:17.640
 Well, does he know why everybody uses it?

08:17.640 --> 08:19.300
 Like that would be an interesting thing.

08:19.300 --> 08:22.340
 Was it the right thing at the right time?

08:22.340 --> 08:24.900
 Cause like when stuff like JavaScript came out,

08:24.900 --> 08:26.260
 like there was a move from, you know,

08:26.260 --> 08:30.620
 writing C programs and C++ to what they call

08:30.620 --> 08:32.340
 managed code frameworks,

08:32.340 --> 08:35.220
 where you write simple code, it might be interpreted,

08:35.220 --> 08:37.780
 it has lots of libraries, productivity is high,

08:37.780 --> 08:39.520
 and you don't have to be an expert.

08:39.520 --> 08:41.340
 So, you know, Java was supposed to solve

08:41.340 --> 08:42.180
 all the world's problems.

08:42.180 --> 08:43.780
 It was complicated.

08:43.780 --> 08:45.220
 JavaScript came out, you know,

08:45.220 --> 08:47.660
 after a bunch of other scripting languages.

08:47.660 --> 08:49.220
 I'm not an expert on it.

08:49.220 --> 08:51.420
 But was it the right thing at the right time?

08:51.420 --> 08:54.260
 Or was there something, you know, clever?

08:54.260 --> 08:56.300
 Cause he wasn't the only one.

08:56.300 --> 08:57.420
 There's a few elements.

08:57.420 --> 08:59.500
 And maybe if he figured out what it was,

08:59.500 --> 09:00.880
 then he'd get a prize.

09:02.020 --> 09:02.860
 Like that.

09:02.860 --> 09:06.860
 Yeah, you know, maybe his problem is he hasn't defined this.

09:06.860 --> 09:08.500
 Or he just needs a good promoter.

09:09.500 --> 09:11.900
 Well, I think there was a bunch of blog posts

09:11.900 --> 09:13.620
 written about it, which is like,

09:13.620 --> 09:18.620
 wrong is right, which is like doing the crappy thing fast.

09:19.340 --> 09:21.340
 Just like hacking together the thing

09:21.340 --> 09:23.260
 that answers some of the needs.

09:23.260 --> 09:26.100
 And then iterating over time, listening to developers.

09:26.100 --> 09:28.220
 Like listening to people who actually use the thing.

09:28.220 --> 09:30.500
 This is something you can do more in software.

09:31.540 --> 09:33.760
 But the right time, like you have to sense,

09:33.760 --> 09:35.140
 you have to have a good instinct

09:35.140 --> 09:37.580
 of when is the right time for the right tool.

09:37.580 --> 09:39.400
 And make it super simple.

09:40.260 --> 09:42.720
 And just get it out there.

09:42.720 --> 09:45.200
 The problem is, this is true with hardware.

09:45.200 --> 09:46.420
 This is less true with software.

09:46.420 --> 09:48.420
 Is there's backward compatibility

09:48.420 --> 09:51.740
 that just drags behind you as, you know,

09:51.740 --> 09:53.820
 as you try to fix all the mistakes of the past.

09:53.820 --> 09:55.820
 But the timing.

09:55.820 --> 09:56.640
 It was good.

09:56.640 --> 09:57.480
 There's something about that.

09:57.480 --> 09:58.820
 And it wasn't accidental.

09:58.820 --> 10:02.580
 You have to like give yourself over to the,

10:02.580 --> 10:05.380
 you have to have this like broad sense

10:05.380 --> 10:06.900
 of what's needed now.

10:07.740 --> 10:10.860
 Both scientifically and like the community.

10:10.860 --> 10:15.500
 And just like this, it was obvious that there was no,

10:15.500 --> 10:17.980
 the interesting thing about JavaScript

10:17.980 --> 10:20.900
 is everything that ran in the browser at the time,

10:20.900 --> 10:24.460
 like Java and I think other like Scheme,

10:24.460 --> 10:25.940
 other programming languages,

10:25.940 --> 10:30.500
 they were all in a separate external container.

10:30.500 --> 10:32.500
 And then JavaScript was literally

10:32.500 --> 10:34.620
 just injected into the webpage.

10:34.620 --> 10:36.380
 It was the dumbest possible thing

10:36.380 --> 10:39.340
 running in the same thread as everything else.

10:39.340 --> 10:43.100
 And like it was inserted as a comment.

10:43.100 --> 10:47.420
 So JavaScript code is inserted as a comment in the HTML code.

10:47.420 --> 10:50.260
 And it was, I mean, there's,

10:50.260 --> 10:53.100
 it's either genius or super dumb, but it's like.

10:53.100 --> 10:55.980
 Right, so it had no apparatus for like a virtual machine

10:55.980 --> 10:58.460
 and container, it just executed in the framework

10:58.460 --> 10:59.780
 of the program that's already running.

10:59.780 --> 11:00.940
 Yeah, that's cool.

11:00.940 --> 11:04.140
 And then because something about that accessibility,

11:04.140 --> 11:09.140
 the ease of its use resulted in then developers innovating

11:10.060 --> 11:11.420
 of how to actually use it.

11:11.420 --> 11:13.660
 I mean, I don't even know what to make of that,

11:13.660 --> 11:18.340
 but it does seem to echo across different software,

11:18.340 --> 11:19.740
 like stories of different software.

11:19.740 --> 11:22.900
 PHP has the same story, really crappy language.

11:22.900 --> 11:24.420
 They just took over the world.

11:25.380 --> 11:28.340
 I always have a joke that the random length instructions,

11:28.340 --> 11:30.660
 variable length instructions, that's always one,

11:30.660 --> 11:33.060
 even though they're obviously worse.

11:33.060 --> 11:34.460
 Like nobody knows why.

11:34.460 --> 11:38.660
 X86 is arguably the worst architecture on the planet.

11:38.660 --> 11:40.500
 It's one of the most popular ones.

11:40.500 --> 11:43.700
 Well, I mean, isn't that also the story of risk versus,

11:43.700 --> 11:46.220
 I mean, is that simplicity?

11:46.220 --> 11:49.420
 There's something about simplicity that us

11:49.420 --> 11:53.500
 in this evolutionary process is valued.

11:53.500 --> 11:58.500
 If it's simple, it spreads faster, it seems like.

11:58.820 --> 11:59.980
 Or is that not always true?

11:59.980 --> 12:01.140
 Not always true.

12:01.140 --> 12:04.260
 Yeah, it could be simple is good, but too simple is bad.

12:04.260 --> 12:06.460
 So why did risk win, you think, so far?

12:06.460 --> 12:07.300
 Did risk win?

12:08.700 --> 12:10.580
 In the long archivist tree.

12:10.580 --> 12:11.420
 We don't know.

12:11.420 --> 12:12.700
 So who's gonna win?

12:12.700 --> 12:15.900
 What's risk, what's CISC, and who's gonna win in that space

12:15.900 --> 12:17.580
 in these instruction sets?

12:17.580 --> 12:21.140
 AI software's gonna win, but there'll be little computers

12:21.140 --> 12:23.940
 that run little programs like normal all over the place.

12:24.980 --> 12:28.580
 But we're going through another transformation, so.

12:28.580 --> 12:32.420
 But you think instruction sets underneath it all will change?

12:32.420 --> 12:33.700
 Yeah, they evolve slowly.

12:33.700 --> 12:35.500
 They don't matter very much.

12:35.500 --> 12:36.820
 They don't matter very much, okay.

12:36.820 --> 12:40.420
 I mean, the limits of performance are predictability

12:40.420 --> 12:41.700
 of instructions and data.

12:41.700 --> 12:43.420
 I mean, that's the big thing.

12:43.420 --> 12:48.420
 And then the usability of it is some quality of design,

12:49.180 --> 12:52.180
 quality of tools, availability.

12:52.180 --> 12:56.460
 Like right now, x86 is proprietary with Intel and AMD,

12:56.460 --> 12:59.740
 but they can change it any way they want independently.

12:59.740 --> 13:01.660
 ARM is proprietary to ARM,

13:01.660 --> 13:03.700
 and they won't let anybody else change it.

13:03.700 --> 13:05.740
 So it's like a sole point.

13:05.740 --> 13:09.140
 And RISC 5 is open source, so anybody can change it,

13:09.140 --> 13:10.660
 which is super cool.

13:10.660 --> 13:12.500
 But that also might mean it gets changed

13:12.500 --> 13:16.340
 too many random ways that there's no common subset of it

13:16.340 --> 13:17.700
 that people can use.

13:17.700 --> 13:19.940
 Do you like open or do you like closed?

13:19.940 --> 13:21.780
 Like if you were to bet all your money on one

13:21.780 --> 13:23.300
 or the other, RISC 5 versus it?

13:23.300 --> 13:24.180
 No idea.

13:24.180 --> 13:25.020
 It's case dependent?

13:25.020 --> 13:27.660
 Well, x86, oddly enough, when Intel first started

13:27.660 --> 13:30.220
 developing it, they licensed like seven people.

13:30.220 --> 13:33.060
 So it was the open architecture.

13:33.060 --> 13:35.340
 And then they moved faster than others

13:35.340 --> 13:37.460
 and also bought one or two of them.

13:37.460 --> 13:40.260
 But there was seven different people making x86

13:40.260 --> 13:45.260
 because at the time there was 6502 and Z80s and 8086.

13:46.940 --> 13:49.060
 And you could argue everybody thought Z80

13:49.060 --> 13:50.940
 was the better instruction set,

13:50.940 --> 13:54.460
 but that was proprietary to one place.

13:54.460 --> 13:56.100
 Oh, and the 6800.

13:56.100 --> 13:59.420
 So there's like four or five different microprocessors.

13:59.420 --> 14:02.380
 Intel went open, got the market share

14:02.380 --> 14:04.700
 because people felt like they had multiple sources from it,

14:04.700 --> 14:07.620
 and then over time it narrowed down to two players.

14:07.620 --> 14:12.620
 So why, you as a historian, why did Intel win for so long

14:14.420 --> 14:17.260
 with their processors?

14:17.260 --> 14:18.100
 I mean, I mean.

14:18.100 --> 14:18.940
 They were great.

14:18.940 --> 14:21.020
 Their process development was great.

14:21.020 --> 14:23.700
 Oh, so it's just looking back to JavaScript

14:23.700 --> 14:26.540
 and what I like is Microsoft and Netscape

14:26.540 --> 14:28.940
 and all these internet browsers.

14:28.940 --> 14:31.740
 Microsoft won the browser game

14:31.740 --> 14:35.940
 because they aggressively stole other people's ideas

14:35.940 --> 14:37.820
 like right after they did it.

14:37.820 --> 14:39.100
 You know, I don't know

14:39.100 --> 14:41.180
 if Intel was stealing other people's ideas.

14:41.180 --> 14:42.020
 They started making.

14:42.020 --> 14:43.780
 In a good way, stealing in a good way just to clarify.

14:43.780 --> 14:48.260
 They started making RAMs, random access memories.

14:48.260 --> 14:50.300
 And then at the time

14:50.300 --> 14:52.940
 when the Japanese manufacturers came up,

14:52.940 --> 14:54.860
 you know, they were getting out competed on that

14:54.860 --> 14:56.580
 and they pivoted the microprocessors

14:56.580 --> 14:57.700
 and they made the first, you know,

14:57.700 --> 14:59.860
 integrated microprocessor grant programs.

14:59.860 --> 15:03.820
 It was the 4D04 or something.

15:03.820 --> 15:04.820
 Who was behind that pivot?

15:04.820 --> 15:05.860
 That's a hell of a pivot.

15:05.860 --> 15:07.740
 Andy Grove and he was great.

15:08.780 --> 15:10.140
 That's a hell of a pivot.

15:10.140 --> 15:13.860
 And then they led semiconductor industry.

15:13.860 --> 15:15.980
 Like they were just a little company, IBM,

15:15.980 --> 15:18.980
 all kinds of big companies had boatloads of money

15:18.980 --> 15:21.180
 and they out innovated everybody.

15:21.180 --> 15:22.420
 Out innovated, okay.

15:22.420 --> 15:23.260
 Yeah, yeah.

15:23.260 --> 15:26.260
 So it's not like marketing, it's not any of that stuff.

15:26.260 --> 15:28.420
 Their processor designs were pretty good.

15:29.340 --> 15:34.340
 I think the, you know, Core 2 was probably the first one

15:34.340 --> 15:36.180
 I thought was great.

15:36.180 --> 15:38.980
 It was a really fast processor and then Haswell was great.

15:40.180 --> 15:42.220
 What makes a great processor in that?

15:42.220 --> 15:43.300
 Oh, if you just look at it,

15:43.300 --> 15:45.580
 it's performance versus everybody else.

15:45.580 --> 15:49.860
 It's, you know, the size of it, the usability of it.

15:49.860 --> 15:50.940
 So it's not specific,

15:50.940 --> 15:52.620
 some kind of element that makes you beautiful.

15:52.620 --> 15:55.100
 It's just like literally just raw performance.

15:55.100 --> 15:57.140
 Is that how you think about processors?

15:57.140 --> 15:59.740
 It's just like raw performance?

15:59.740 --> 16:01.300
 Of course.

16:01.300 --> 16:02.300
 It's like a horse race.

16:02.300 --> 16:04.260
 The fastest one wins.

16:04.260 --> 16:05.100
 Now.

16:05.100 --> 16:05.940
 You don't care how.

16:05.940 --> 16:08.460
 Just as long as it wins.

16:08.460 --> 16:10.620
 Well, there's the fastest in the environment.

16:10.620 --> 16:13.060
 Like, you know, for years you made the fastest one you could

16:13.060 --> 16:14.940
 and then people started to have power limits.

16:14.940 --> 16:17.660
 So then you made the fastest at the right PowerPoint.

16:17.660 --> 16:20.460
 And then when we started doing multi processors,

16:20.460 --> 16:23.580
 like if you could scale your processors

16:23.580 --> 16:24.420
 more than the other guy,

16:24.420 --> 16:26.980
 you could be 10% faster on like a single thread,

16:26.980 --> 16:28.420
 but you have more threads.

16:28.420 --> 16:30.020
 So there's lots of variability.

16:30.020 --> 16:34.460
 And then ARM really explored,

16:34.460 --> 16:36.580
 like, you know, they have the A series

16:36.580 --> 16:38.900
 and the R series and the M series,

16:38.900 --> 16:40.340
 like a family of processors

16:40.340 --> 16:41.980
 for all these different design points

16:41.980 --> 16:44.580
 from like unbelievably small and simple.

16:44.580 --> 16:46.540
 And so then when you're doing the design,

16:46.540 --> 16:49.380
 it's sort of like this big pallet of CPUs.

16:49.380 --> 16:51.500
 Like they're the only ones with a credible,

16:51.500 --> 16:53.020
 you know, top to bottom pallet.

16:54.700 --> 16:56.900
 What do you mean a credible top to bottom?

16:56.900 --> 16:58.620
 Well, there's people who make microcontrollers

16:58.620 --> 17:00.500
 that are small, but they don't have a fast one.

17:00.500 --> 17:02.080
 There's people who make fast processors,

17:02.080 --> 17:04.900
 but don't have a medium one or a small one.

17:04.900 --> 17:07.420
 Is that hard to do that full pallet?

17:07.420 --> 17:08.260
 That seems like a...

17:08.260 --> 17:09.380
 Yeah, it's a lot of different.

17:09.380 --> 17:13.340
 So what's the difference in the ARM folks and Intel

17:13.340 --> 17:15.620
 in terms of the way they're approaching this problem?

17:15.620 --> 17:19.200
 Well, Intel, almost all their processor designs

17:19.200 --> 17:21.740
 were, you know, very custom high end,

17:21.740 --> 17:23.460
 you know, for the last 15, 20 years.

17:23.460 --> 17:24.900
 So the fastest horse possible.

17:24.900 --> 17:25.860
 Yeah.

17:25.860 --> 17:27.540
 In one horse race.

17:27.540 --> 17:30.420
 Yeah, and then architecturally they're really good,

17:30.420 --> 17:33.380
 but the company itself was fairly insular

17:33.380 --> 17:36.300
 to what's going on in the industry with CAD tools and stuff.

17:36.300 --> 17:38.200
 And there's this debate about custom design

17:38.200 --> 17:41.340
 versus synthesis and how do you approach that?

17:41.340 --> 17:45.700
 I'd say Intel was slow on getting to synthesize processors.

17:45.700 --> 17:49.100
 ARM came in from the bottom and they generated IP,

17:49.100 --> 17:50.860
 which went to all kinds of customers.

17:50.860 --> 17:52.020
 So they had very little say

17:52.020 --> 17:54.980
 on how the customer implemented their IP.

17:54.980 --> 17:59.420
 So ARM is super friendly to the synthesis IP environment.

17:59.420 --> 18:00.260
 Whereas Intel said,

18:00.260 --> 18:03.200
 we're gonna make this great client chip or server chip

18:03.200 --> 18:05.460
 with our own CAD tools, with our own process,

18:05.460 --> 18:08.140
 with our own, you know, other supporting IP

18:08.140 --> 18:10.140
 and everything only works with our stuff.

18:11.340 --> 18:16.340
 So is that, is ARM winning the mobile platform space

18:16.440 --> 18:17.280
 in terms of process?

18:17.280 --> 18:18.120
 Yeah.

18:18.120 --> 18:21.780
 And so in that, what you're describing

18:21.780 --> 18:22.860
 is why they're winning.

18:22.860 --> 18:24.940
 Well, they had lots of people doing lots

18:24.940 --> 18:26.420
 of different experiments.

18:26.420 --> 18:29.420
 So they controlled the processor architecture and IP,

18:29.420 --> 18:32.060
 but they let people put in lots of different chips.

18:32.060 --> 18:35.260
 And there was a lot of variability in what happened there.

18:35.260 --> 18:37.140
 Whereas Intel, when they made their mobile,

18:37.140 --> 18:38.460
 their foray into mobile,

18:38.460 --> 18:41.700
 they had one team doing one part, right?

18:41.700 --> 18:43.180
 So it wasn't 10 experiments.

18:43.180 --> 18:45.980
 And then their mindset was PC mindset,

18:45.980 --> 18:48.060
 Microsoft software mindset.

18:48.060 --> 18:49.940
 And that brought a whole bunch of things along

18:49.940 --> 18:52.580
 that the mobile world and the embedded world don't do.

18:52.580 --> 18:55.460
 Do you think it was possible for Intel to pivot hard

18:55.460 --> 18:58.260
 and win the mobile market?

18:58.260 --> 19:00.060
 That's a hell of a difficult thing to do, right?

19:00.060 --> 19:02.040
 For a huge company to just pivot.

19:03.420 --> 19:05.540
 I mean, it's so interesting to,

19:05.540 --> 19:07.420
 because we'll talk about your current work.

19:07.420 --> 19:11.100
 It's like, it's clear that PCs were dominating

19:11.100 --> 19:14.180
 for several decades, like desktop computers.

19:14.180 --> 19:17.940
 And then mobile, it's unclear.

19:17.940 --> 19:19.380
 It's a leadership question.

19:19.380 --> 19:23.060
 Like Apple under Steve Jobs, when he came back,

19:23.060 --> 19:24.760
 they pivoted multiple times.

19:25.660 --> 19:28.260
 You know, they built iPads and iTunes and phones

19:28.260 --> 19:30.060
 and tablets and great Macs.

19:30.060 --> 19:33.380
 Like who knew computers should be made out of aluminum?

19:33.380 --> 19:34.260
 Nobody knew that.

19:35.300 --> 19:36.140
 But they're great.

19:36.140 --> 19:37.160
 It's super fun.

19:37.160 --> 19:38.000
 That was Steve?

19:38.000 --> 19:38.820
 Yeah, Steve Jobs.

19:38.820 --> 19:40.540
 Like they pivoted multiple times.

19:41.400 --> 19:45.860
 And you know, the old Intel, they did that multiple times.

19:45.860 --> 19:48.420
 They made DRAMs and processors and processes

19:48.420 --> 19:50.900
 and I gotta ask this,

19:50.900 --> 19:53.060
 what was it like working with Steve Jobs?

19:53.060 --> 19:54.420
 I didn't work with him.

19:54.420 --> 19:55.700
 Did you interact with him?

19:55.700 --> 19:56.540
 Twice.

19:57.420 --> 19:59.860
 I said hi to him twice in the cafeteria.

19:59.860 --> 20:01.020
 What did he say?

20:01.020 --> 20:01.860
 Hi?

20:01.860 --> 20:02.700
 He said, hey fellas.

20:04.340 --> 20:05.940
 He was friendly.

20:05.940 --> 20:08.260
 He was wandering around and with somebody,

20:08.260 --> 20:12.300
 he couldn't find a table because the cafeteria was packed

20:12.300 --> 20:13.700
 and I gave him my table.

20:13.700 --> 20:16.060
 But I worked for Mike Colbert who talked to,

20:16.060 --> 20:19.260
 like Mike was the unofficial CTO of Apple

20:19.260 --> 20:22.140
 and a brilliant guy and he worked for Steve for 25 years,

20:22.140 --> 20:25.540
 maybe more and he talked to Steve multiple times a day

20:26.680 --> 20:29.380
 and he was one of the people who could put up with Steve's,

20:29.380 --> 20:31.740
 let's say, brilliance and intensity

20:31.740 --> 20:35.700
 and Steve really liked him and Steve trusted Mike

20:35.700 --> 20:39.060
 to translate the shit he thought up

20:39.060 --> 20:40.860
 into engineering products that work

20:40.860 --> 20:43.140
 and then Mike ran a group called Platform Architecture

20:43.140 --> 20:44.760
 and I was in that group.

20:44.760 --> 20:46.380
 So many times I'd be sitting with Mike

20:46.380 --> 20:48.680
 and the phone would ring and it'd be Steve

20:48.680 --> 20:50.420
 and Mike would hold the phone like this

20:50.420 --> 20:53.060
 because Steve would be yelling about something or other.

20:53.060 --> 20:54.120
 And then he would translate.

20:54.120 --> 20:55.900
 And he'd translate and then he would say,

20:55.900 --> 20:58.300
 Steve wants us to do this.

20:58.300 --> 20:59.460
 So.

20:59.460 --> 21:01.100
 Was Steve a good engineer or no?

21:01.100 --> 21:02.380
 I don't know.

21:02.380 --> 21:03.780
 He was a great idea guy.

21:03.780 --> 21:04.620
 Idea person.

21:04.620 --> 21:07.540
 And he's a really good selector for talent.

21:07.540 --> 21:09.580
 Yeah, that seems to be one of the key elements

21:09.580 --> 21:10.740
 of leadership, right?

21:10.740 --> 21:12.740
 And then he was a really good first principles guy.

21:12.740 --> 21:15.060
 Like somebody would say something couldn't be done

21:15.060 --> 21:20.060
 and he would just think, that's obviously wrong, right?

21:20.300 --> 21:23.020
 But you know, maybe it's hard to do.

21:23.020 --> 21:24.420
 Maybe it's expensive to do.

21:24.420 --> 21:25.860
 Maybe we need different people.

21:25.860 --> 21:27.260
 You know, there's like a whole bunch of,

21:27.260 --> 21:29.420
 if you want to do something hard,

21:29.420 --> 21:30.580
 you know, maybe it takes time.

21:30.580 --> 21:31.580
 Maybe you have to iterate.

21:31.580 --> 21:33.700
 There's a whole bunch of things you could think about

21:33.700 --> 21:36.340
 but saying it can't be done is stupid.

21:36.340 --> 21:38.060
 How would you compare?

21:38.060 --> 21:42.860
 So it seems like Elon Musk is more engineering centric

21:42.860 --> 21:45.660
 but is also, I think he considers himself a designer too.

21:45.660 --> 21:46.980
 He has a design mind.

21:46.980 --> 21:50.540
 Steve Jobs feels like he's much more idea space,

21:50.540 --> 21:52.740
 design space versus engineering.

21:52.740 --> 21:53.900
 Just make it happen.

21:53.900 --> 21:55.820
 Like the world should be this way.

21:55.820 --> 21:57.140
 Just figure it out.

21:57.140 --> 21:58.680
 But he used computers.

21:58.680 --> 22:01.840
 You know, he had computer people talk to him all the time.

22:01.840 --> 22:03.340
 Like Mike was a really good computer guy.

22:03.340 --> 22:04.820
 He knew computers could do.

22:04.820 --> 22:06.300
 Computer meaning computer hardware?

22:06.300 --> 22:09.100
 Like hardware, software, all the pieces.

22:09.100 --> 22:12.100
 And then he would have an idea about

22:12.100 --> 22:14.540
 what could we do with this next.

22:14.540 --> 22:16.060
 That was grounded in reality.

22:16.060 --> 22:19.220
 It wasn't like he was just finger painting on the wall

22:19.220 --> 22:21.380
 and wishing somebody would interpret it.

22:21.380 --> 22:23.420
 So he had this interesting connection

22:23.420 --> 22:28.320
 because he wasn't a computer architect or designer

22:28.320 --> 22:30.820
 but he had an intuition from the computers we had

22:30.820 --> 22:31.960
 to what could happen.

22:31.960 --> 22:35.280
 And it's interesting you say intuition

22:35.280 --> 22:39.980
 because it seems like he was pissing off a lot of engineers

22:39.980 --> 22:43.660
 in his intuition about what can and can't be done.

22:43.660 --> 22:46.840
 Those, like the, what is all these stories

22:46.840 --> 22:49.080
 about like floppy disks and all that kind of stuff.

22:49.080 --> 22:52.080
 Yeah, so in Steve, the first round,

22:52.080 --> 22:55.420
 like he'd go into a lab and look at what's going on

22:55.420 --> 22:59.920
 and hate it and fire people or ask somebody

22:59.920 --> 23:01.840
 in the elevator what they're doing for Apple.

23:01.840 --> 23:03.840
 And not be happy.

23:03.840 --> 23:06.520
 When he came back, my impression was

23:06.520 --> 23:08.000
 is he surrounded himself

23:08.000 --> 23:10.640
 with a relatively small group of people

23:10.640 --> 23:13.880
 and didn't really interact outside of that as much.

23:13.880 --> 23:16.320
 And then the joke was you'd see like somebody moving

23:16.320 --> 23:20.800
 a prototype through the quad with a black blanket over it.

23:20.800 --> 23:24.200
 And that was because it was secret, partly from Steve

23:24.200 --> 23:26.980
 because they didn't want Steve to see it until it was ready.

23:26.980 --> 23:31.420
 Yeah, the dynamic with Johnny Ive and Steve is interesting.

23:31.420 --> 23:32.920
 It's like you don't wanna,

23:34.200 --> 23:37.280
 he ruins as many ideas as he generates.

23:37.280 --> 23:38.800
 Yeah, yeah.

23:38.800 --> 23:42.080
 It's a dangerous kind of line to walk.

23:42.080 --> 23:43.480
 If you have a lot of ideas,

23:43.480 --> 23:47.260
 like Gordon Bell was famous for ideas, right?

23:47.260 --> 23:49.120
 And it wasn't that the percentage of good ideas

23:49.120 --> 23:51.420
 was way higher than anybody else.

23:51.420 --> 23:53.160
 It was, he had so many ideas

23:53.160 --> 23:55.840
 and he was also good at talking to people about it

23:55.840 --> 23:58.120
 and getting the filters right.

23:58.120 --> 24:00.200
 And seeing through stuff.

24:00.200 --> 24:03.360
 Whereas Elon was like, hey, I wanna build rockets.

24:03.360 --> 24:05.980
 So Steve would hire a bunch of rocket guys

24:05.980 --> 24:08.520
 and Elon would go read rocket manuals.

24:08.520 --> 24:11.440
 So Elon is a better engineer, a sense like,

24:11.440 --> 24:16.440
 or like more like a love and passion for the manuals.

24:16.880 --> 24:17.800
 And the details.

24:17.800 --> 24:20.800
 The details, the craftsmanship too, right?

24:20.800 --> 24:22.720
 Well, I guess Steve had craftsmanship too,

24:22.720 --> 24:24.240
 but of a different kind.

24:24.240 --> 24:26.200
 What do you make of the,

24:26.200 --> 24:27.920
 just to stay in there for just a little longer,

24:27.920 --> 24:29.200
 what do you make of like the anger

24:29.200 --> 24:30.640
 and the passion and all of that?

24:30.640 --> 24:35.080
 The firing and the mood swings and the madness,

24:35.080 --> 24:39.360
 the being emotional and all of that, that's Steve.

24:39.360 --> 24:40.680
 And I guess Elon too.

24:40.680 --> 24:43.680
 So what, is that a bug or a feature?

24:43.680 --> 24:45.020
 It's a feature.

24:45.020 --> 24:50.020
 So there's a graph, which is Y axis productivity,

24:50.240 --> 24:52.920
 X axis at zero is chaos,

24:52.920 --> 24:56.280
 and infinity is complete order, right?

24:56.280 --> 25:00.920
 So as you go from the origin,

25:00.920 --> 25:04.160
 as you improve order, you improve productivity.

25:04.160 --> 25:06.420
 And at some point, productivity peaks,

25:06.420 --> 25:08.340
 and then it goes back down again.

25:08.340 --> 25:09.800
 Too much order, nothing can happen.

25:09.800 --> 25:10.640
 Yes.

25:10.640 --> 25:13.680
 But the question is, how close to the chaos is that?

25:13.680 --> 25:15.000
 No, no, no, here's the thing,

25:15.000 --> 25:16.920
 is once you start moving in the direction of order,

25:16.920 --> 25:21.000
 the force vector to drive you towards order is unstoppable.

25:21.000 --> 25:22.240
 Oh, so it's a slippery slope.

25:22.240 --> 25:24.880
 And every organization will move to the place

25:24.880 --> 25:27.120
 where their productivity is stymied by order.

25:27.120 --> 25:28.160
 So you need a...

25:28.160 --> 25:30.360
 So the question is, who's the counter force?

25:31.880 --> 25:33.360
 Because it also feels really good.

25:33.360 --> 25:36.240
 As you get more organized, the productivity goes up.

25:36.240 --> 25:39.720
 The organization feels it, they orient towards it, right?

25:39.720 --> 25:41.080
 They hired more people.

25:41.080 --> 25:42.880
 They got more guys who couldn't run process,

25:42.880 --> 25:44.740
 you get bigger, right?

25:44.740 --> 25:49.120
 And then inevitably, the organization gets captured

25:49.120 --> 25:51.820
 by the bureaucracy that manages all the processes.

25:51.820 --> 25:53.660
 Yeah.

25:53.660 --> 25:55.540
 All right, and then humans really like that.

25:55.540 --> 25:57.840
 And so if you just walk into a room and say,

25:57.840 --> 26:00.140
 guys, love what you're doing,

26:00.980 --> 26:03.340
 but I need you to have less order.

26:04.980 --> 26:06.900
 If you don't have some force behind that,

26:06.900 --> 26:07.900
 nothing will happen.

26:09.080 --> 26:12.500
 I can't tell you on how many levels that's profound, so.

26:12.500 --> 26:14.080
 So that's why I'd say it's a feature.

26:14.080 --> 26:17.220
 Now, could you be nicer about it?

26:17.220 --> 26:18.940
 I don't know, I don't know any good examples

26:18.940 --> 26:20.140
 of being nicer about it.

26:20.140 --> 26:23.460
 Well, the funny thing is to get stuff done,

26:23.460 --> 26:25.940
 you need people who can manage stuff and manage people,

26:25.940 --> 26:26.900
 because humans are complicated.

26:26.900 --> 26:28.500
 They need lots of care and feeding that you need

26:28.500 --> 26:30.780
 to tell them they look nice and they're doing good stuff

26:30.780 --> 26:33.060
 and pat them on the back, right?

26:33.060 --> 26:35.940
 I don't know, you tell me, is that needed?

26:35.940 --> 26:36.780
 Oh yeah.

26:36.780 --> 26:37.600
 Do humans need that?

26:37.600 --> 26:39.660
 I had a friend, he started a magic group and he said,

26:39.660 --> 26:40.820
 I figured it out.

26:40.820 --> 26:43.380
 You have to praise them before they do anything.

26:43.380 --> 26:45.220
 I was waiting until they were done.

26:45.220 --> 26:46.520
 And they were always mad at me.

26:46.520 --> 26:48.140
 Now I tell them what a great job they're doing

26:48.140 --> 26:49.380
 while they're doing it.

26:49.380 --> 26:51.020
 But then you get stuck in that trap,

26:51.020 --> 26:52.180
 because then when they're not doing something,

26:52.180 --> 26:54.060
 how do you confront these people?

26:54.060 --> 26:55.900
 I think a lot of people that had trauma

26:55.900 --> 26:57.540
 in their childhood would disagree with you,

26:57.540 --> 27:00.640
 successful people, that you need to first do the rough stuff

27:00.640 --> 27:02.320
 and then be nice later.

27:02.320 --> 27:03.160
 I don't know.

27:03.160 --> 27:05.820
 Okay, but engineering companies are full of adults

27:05.820 --> 27:08.100
 who had all kinds of range of childhoods.

27:08.100 --> 27:11.400
 You know, most people had okay childhoods.

27:11.400 --> 27:12.900
 Well, I don't know if...

27:12.900 --> 27:15.620
 Lots of people only work for praise, which is weird.

27:15.620 --> 27:16.820
 You mean like everybody.

27:16.820 --> 27:21.140
 I'm not that interested in it, but...

27:21.140 --> 27:24.060
 Well, you're probably looking for somebody's approval.

27:25.420 --> 27:27.400
 Even still.

27:27.400 --> 27:28.240
 Yeah, maybe.

27:28.240 --> 27:29.540
 I should think about that.

27:29.540 --> 27:32.260
 Maybe somebody who's no longer with us kind of thing.

27:33.160 --> 27:34.100
 I don't know.

27:34.100 --> 27:36.340
 I used to call up my dad and tell him what I was doing.

27:36.340 --> 27:38.580
 He was very excited about engineering and stuff.

27:38.580 --> 27:40.140
 You got his approval?

27:40.140 --> 27:42.060
 Uh, yeah, a lot.

27:42.060 --> 27:43.340
 I was lucky.

27:43.340 --> 27:47.180
 Like, he decided I was smart and unusual as a kid

27:47.180 --> 27:49.280
 and that was okay when I was really young.

27:50.180 --> 27:52.520
 So when I did poorly in school, I was dyslexic.

27:52.520 --> 27:55.220
 I didn't read until I was third or fourth grade.

27:55.220 --> 27:56.060
 They didn't care.

27:56.060 --> 27:58.420
 My parents were like, oh, he'll be fine.

27:59.760 --> 28:01.520
 So I was lucky.

28:01.520 --> 28:02.480
 That was cool.

28:02.480 --> 28:04.020
 Is he still with us?

28:05.180 --> 28:06.020
 You miss him?

28:07.500 --> 28:08.340
 Sure, yeah.

28:08.340 --> 28:10.740
 He had Parkinson's and then cancer.

28:10.740 --> 28:15.740
 His last 10 years were tough and I killed him.

28:15.980 --> 28:18.280
 Killing a man like that's hard.

28:18.280 --> 28:19.420
 The mind?

28:19.420 --> 28:21.460
 Well, it's pretty good.

28:21.460 --> 28:23.780
 Parkinson's causes slow dementia

28:23.780 --> 28:27.740
 and the chemotherapy, I think, accelerated it.

28:29.060 --> 28:31.020
 But it was like hallucinogenic dementia.

28:31.020 --> 28:34.180
 So he was clever and funny and interesting

28:34.180 --> 28:37.920
 and it was pretty unusual.

28:37.920 --> 28:39.820
 Do you remember conversations?

28:39.820 --> 28:41.500
 From that time?

28:41.500 --> 28:43.940
 Like, do you have fond memories of the guy?

28:43.940 --> 28:45.220
 Yeah, oh yeah.

28:45.220 --> 28:46.280
 Anything come to mind?

28:48.020 --> 28:50.340
 A friend told me one time I could draw a computer

28:50.340 --> 28:52.500
 on the whiteboard faster than anybody he'd ever met.

28:52.500 --> 28:54.060
 I said, you should meet my dad.

28:54.920 --> 28:56.860
 Like, when I was a kid, he'd come home and say,

28:56.860 --> 28:58.820
 I was driving by this bridge and I was thinking about it

28:58.820 --> 28:59.780
 and he pulled out a piece of paper

28:59.780 --> 29:01.500
 and he'd draw the whole bridge.

29:01.500 --> 29:03.620
 He was a mechanical engineer.

29:03.620 --> 29:05.000
 And he would just draw the whole thing

29:05.000 --> 29:06.260
 and then he would tell me about it

29:06.260 --> 29:08.700
 and then tell me how he would have changed it.

29:08.700 --> 29:11.900
 And he had this idea that he could understand

29:11.900 --> 29:13.380
 and conceive anything.

29:13.380 --> 29:16.460
 And I just grew up with that, so that was natural.

29:16.460 --> 29:19.780
 So when I interview people, I ask them to draw a picture

29:19.780 --> 29:21.780
 of something they did on a whiteboard

29:21.780 --> 29:22.860
 and it's really interesting.

29:22.860 --> 29:24.700
 Like, some people draw a little box

29:25.900 --> 29:27.820
 and then they'll say, and then this talks to this

29:27.820 --> 29:30.220
 and I'll be like, oh, this is frustrating.

29:30.220 --> 29:32.620
 I had this other guy come in one time, he says,

29:32.620 --> 29:34.500
 well, I designed a floating point in this chip

29:34.500 --> 29:36.320
 but I'd really like to tell you how the whole thing works

29:36.320 --> 29:38.180
 and then tell you how the floating point works inside of it.

29:38.180 --> 29:39.080
 Do you mind if I do that?

29:39.080 --> 29:42.060
 And he covered two whiteboards in like 30 minutes

29:42.060 --> 29:42.900
 and I hired him.

29:42.900 --> 29:44.580
 Like, he was great.

29:44.580 --> 29:45.420
 This is craftsman.

29:45.420 --> 29:47.060
 I mean, that's the craftsmanship to that.

29:47.060 --> 29:49.500
 Yeah, but also the mental agility

29:49.500 --> 29:51.660
 to understand the whole thing,

29:51.660 --> 29:53.580
 put the pieces in context,

29:54.780 --> 29:57.700
 real view of the balance of how the design worked.

29:58.640 --> 30:01.020
 Because if you don't understand it properly,

30:01.020 --> 30:02.220
 when you start to draw it,

30:02.220 --> 30:03.820
 you'll fill up half the whiteboard

30:03.820 --> 30:05.220
 with like a little piece of it

30:05.220 --> 30:09.260
 and like your ability to lay it out in an understandable way

30:09.260 --> 30:11.500
 takes a lot of understanding, so.

30:11.500 --> 30:13.460
 And be able to, so zoom into the detail

30:13.460 --> 30:14.980
 and then zoom out to the big picture.

30:14.980 --> 30:16.420
 Zoom out really fast.

30:16.420 --> 30:17.620
 What about the impossible thing?

30:17.620 --> 30:21.880
 You see, your dad believed that you can do anything.

30:22.960 --> 30:25.500
 That's a weird feature for a craftsman.

30:25.500 --> 30:26.700
 Yeah.

30:26.700 --> 30:30.820
 It seems that that echoes in your own behavior.

30:30.820 --> 30:32.100
 Like that's the.

30:32.100 --> 30:36.500
 Well, it's not that anybody can do anything right now, right?

30:36.500 --> 30:39.660
 It's that if you work at it, you can get better at it

30:39.660 --> 30:41.220
 and there might not be a limit.

30:43.100 --> 30:44.620
 And they did funny things like,

30:44.620 --> 30:46.140
 like he always wanted to play piano.

30:46.140 --> 30:48.460
 So at the end of his life, he started playing the piano

30:48.460 --> 30:51.580
 when he had Parkinson's and he was terrible.

30:51.580 --> 30:53.540
 But he thought if he really worked out in this life,

30:53.540 --> 30:56.420
 maybe the next life he'd be better at it.

30:56.420 --> 30:57.620
 He might be onto something.

30:57.620 --> 31:00.940
 Yeah, he enjoyed doing it.

31:00.940 --> 31:01.780
 Yeah.

31:01.780 --> 31:02.620
 It's pretty funny.

31:02.620 --> 31:06.180
 Do you think the perfect is the enemy of the good

31:06.180 --> 31:08.180
 in hardware and software engineering?

31:08.180 --> 31:10.500
 It's like we were talking about JavaScript a little bit

31:10.500 --> 31:14.780
 and the messiness of the 10 day building process.

31:14.780 --> 31:17.140
 Yeah, you know, creative tension, right?

31:19.060 --> 31:21.460
 So creative tension is you have two different ideas

31:21.460 --> 31:24.380
 that you can't do both, right?

31:24.380 --> 31:27.660
 And, but the fact that you wanna do both

31:27.660 --> 31:29.980
 causes you to go try to solve that problem.

31:29.980 --> 31:32.020
 That's the creative part.

31:32.020 --> 31:35.140
 So if you're building computers,

31:35.140 --> 31:37.060
 like some people say we have the schedule

31:37.060 --> 31:40.220
 and anything that doesn't fit in the schedule we can't do.

31:40.220 --> 31:41.060
 Right?

31:41.060 --> 31:42.100
 And so they throw out the perfect

31:42.100 --> 31:44.300
 because they have a schedule.

31:44.300 --> 31:45.140
 I hate that.

31:46.620 --> 31:48.220
 Then there's other people who say

31:48.220 --> 31:50.540
 we need to get this perfectly right.

31:50.540 --> 31:53.980
 And no matter what, you know, more people, more money,

31:53.980 --> 31:55.500
 right?

31:55.500 --> 31:57.860
 And there's a really clear idea about what you want.

31:57.860 --> 32:00.740
 Some people are really good at articulating it, right?

32:00.740 --> 32:02.380
 So let's call that the perfect, yeah.

32:02.380 --> 32:03.300
 Yeah.

32:03.300 --> 32:04.780
 All right, but that's also terrible

32:04.780 --> 32:06.180
 because they never ship anything.

32:06.180 --> 32:07.420
 You never hit any goals.

32:07.420 --> 32:09.980
 So now you have your framework.

32:09.980 --> 32:10.820
 Yes.

32:10.820 --> 32:11.660
 You can't throw out stuff

32:11.660 --> 32:12.820
 because you can't get it done today

32:12.820 --> 32:14.020
 because maybe you'll get it done tomorrow

32:14.020 --> 32:15.860
 or the next project, right?

32:15.860 --> 32:18.340
 You can't, so you have to,

32:18.340 --> 32:20.620
 I work with a guy that I really like working with,

32:20.620 --> 32:23.140
 but he over filters his ideas.

32:23.140 --> 32:24.780
 Over filters?

32:24.780 --> 32:26.620
 He'd start thinking about something

32:26.620 --> 32:28.020
 and as soon as he figured out what was wrong with it,

32:28.020 --> 32:28.900
 he'd throw it out.

32:29.820 --> 32:31.260
 And then I start thinking about it

32:31.260 --> 32:32.700
 and you come up with an idea

32:32.700 --> 32:34.980
 and then you find out what's wrong with it.

32:34.980 --> 32:36.780
 And then you give it a little time to set

32:36.780 --> 32:39.260
 because sometimes you figure out how to tweak it

32:39.260 --> 32:41.380
 or maybe that idea helps some other idea.

32:42.620 --> 32:45.100
 So idea generation is really funny.

32:45.100 --> 32:46.940
 So you have to give your ideas space.

32:46.940 --> 32:49.780
 Like spaciousness of mind is key.

32:49.780 --> 32:53.420
 But you also have to execute programs and get shit done.

32:53.420 --> 32:55.540
 And then it turns out computer engineering is fun

32:55.540 --> 32:58.300
 because it takes 100 people to build a computer,

32:58.300 --> 33:00.620
 200 or 300, whatever the number is.

33:00.620 --> 33:05.260
 And people are so variable about temperament

33:05.260 --> 33:07.700
 and skill sets and stuff.

33:07.700 --> 33:09.460
 That in a big organization,

33:09.460 --> 33:11.860
 you find the people who love the perfect ideas

33:11.860 --> 33:13.780
 and the people that want to get stuffed on yesterday

33:13.780 --> 33:16.500
 and people like to come up with ideas

33:16.500 --> 33:19.300
 and people like to, let's say shoot down ideas.

33:19.300 --> 33:23.300
 And it takes the whole, it takes a large group of people.

33:23.300 --> 33:25.980
 Some are good at generating ideas, some are good at filtering ideas.

33:25.980 --> 33:30.980
 And then all in that giant mess, you're somehow,

33:30.980 --> 33:33.820
 I guess the goal is for that giant mess of people

33:33.820 --> 33:37.260
 to find the perfect path through the tension,

33:37.260 --> 33:38.460
 the creative tension.

33:38.460 --> 33:41.340
 But like, how do you know when you said

33:41.340 --> 33:42.940
 there's some people good at articulating

33:42.940 --> 33:44.740
 what perfect looks like, what a good design is?

33:44.740 --> 33:46.860
 Like if you're sitting in a room

33:48.060 --> 33:51.020
 and you have a set of ideas

33:51.020 --> 33:55.340
 about like how to design a better processor,

33:55.340 --> 33:58.820
 how do you know this is something special here?

33:58.820 --> 34:00.780
 This is a good idea, let's try this.

34:00.780 --> 34:02.220
 Have you ever brainstormed an idea

34:02.220 --> 34:04.540
 with a couple of people that were really smart?

34:04.540 --> 34:07.540
 And you kind of go into it and you don't quite understand it

34:07.540 --> 34:09.700
 and you're working on it.

34:09.700 --> 34:12.180
 And then you start talking about it,

34:12.180 --> 34:16.140
 putting it on the whiteboard, maybe it takes days or weeks.

34:16.140 --> 34:18.620
 And then your brain starts to kind of synchronize.

34:18.620 --> 34:19.540
 It's really weird.

34:19.540 --> 34:23.020
 Like you start to see what each other is thinking.

34:25.980 --> 34:28.460
 And it starts to work.

34:28.460 --> 34:29.380
 Like you can see work.

34:29.380 --> 34:30.980
 Like my talent in computer design

34:30.980 --> 34:35.340
 is I can see how computers work in my head, like really well.

34:35.340 --> 34:37.340
 And I know other people can do that too.

34:37.340 --> 34:40.460
 And when you're working with people that can do that,

34:40.460 --> 34:44.460
 like it is kind of an amazing experience.

34:45.380 --> 34:48.180
 And then every once in a while you get to that place

34:48.180 --> 34:50.220
 and then you find the flaw, which is kind of funny

34:50.220 --> 34:52.340
 because you can fool yourself.

34:53.740 --> 34:55.900
 The two of you kind of drifted along

34:55.900 --> 34:58.460
 in the direction that was useless.

34:58.460 --> 34:59.420
 That happens too.

34:59.420 --> 35:03.500
 Like you have to, because the nice thing

35:03.500 --> 35:05.580
 about computer design is always reduction in practice.

35:05.580 --> 35:08.100
 Like you come up with your good ideas

35:08.100 --> 35:10.980
 and I know some architects who really love ideas

35:10.980 --> 35:13.100
 and then they work on them and they put it on the shelf

35:13.100 --> 35:14.820
 and they go work on the next idea and put it on the shelf

35:14.820 --> 35:16.820
 and they never reduce it to practice.

35:16.820 --> 35:18.780
 So they find out what's good and bad.

35:18.780 --> 35:22.500
 Because almost every time I've done something really new,

35:22.500 --> 35:25.660
 by the time it's done, like the good parts are good,

35:25.660 --> 35:27.620
 but I know all the flaws, like.

35:27.620 --> 35:28.460
 Yeah.

35:28.460 --> 35:31.580
 Would you say your career, just your own experience,

35:31.580 --> 35:35.260
 is your career defined mostly by flaws or by successes?

35:35.260 --> 35:36.100
 Like if...

35:36.100 --> 35:38.020
 Again, there's great tension between those.

35:38.020 --> 35:42.580
 If you haven't tried hard, right?

35:42.580 --> 35:46.300
 And done something new, right?

35:46.300 --> 35:48.500
 Then you're not gonna be facing the challenges

35:48.500 --> 35:49.340
 when you build it.

35:49.340 --> 35:51.900
 Then you find out all the problems with it.

35:51.900 --> 35:52.740
 And...

35:52.740 --> 35:55.580
 But when you look back, do you see problems?

35:55.580 --> 35:56.420
 Okay.

35:56.420 --> 35:58.060
 Oh, when I look back?

35:58.060 --> 35:58.900
 What do you remember?

35:58.900 --> 36:00.460
 I think earlier in my career,

36:00.460 --> 36:02.680
 like EV5 was the second alpha chip.

36:04.100 --> 36:06.500
 I was so embarrassed about the mistakes,

36:06.500 --> 36:08.580
 I could barely talk about it.

36:08.580 --> 36:10.340
 And it was in the Guinness Book of World Records

36:10.340 --> 36:12.420
 and it was the fastest processor on the planet.

36:12.420 --> 36:13.740
 Yeah.

36:13.740 --> 36:15.780
 So it was, and at some point I realized

36:15.780 --> 36:18.540
 that was really a bad mental framework

36:18.540 --> 36:20.020
 to deal with doing something new.

36:20.020 --> 36:21.180
 We did a bunch of new things

36:21.180 --> 36:23.460
 and some worked out great and some were bad.

36:23.460 --> 36:24.660
 And we learned a lot from it.

36:24.660 --> 36:28.020
 And then the next one, we learned a lot.

36:28.020 --> 36:31.820
 That EV6 also had some really cool things in it.

36:31.820 --> 36:34.240
 I think the proportion of good stuff went up,

36:34.240 --> 36:38.660
 but it had a couple of fatal flaws in it that were painful.

36:39.580 --> 36:41.500
 And then, yeah.

36:41.500 --> 36:44.660
 You learned to channel the pain into like pride.

36:44.660 --> 36:45.740
 Not pride, really.

36:45.740 --> 36:50.060
 You know, just a realization about how the world works

36:50.060 --> 36:52.300
 or how that kind of idea set works.

36:52.300 --> 36:53.220
 Life is suffering.

36:53.220 --> 36:54.280
 That's the reality.

36:55.540 --> 36:57.140
 No, it's not.

36:57.140 --> 36:58.380
 Well, I know the Buddha said that

36:58.380 --> 37:00.480
 and a couple other people are stuck on it.

37:00.480 --> 37:03.820
 No, it's, you know, there's this kind of weird combination

37:03.820 --> 37:06.940
 of good and bad, you know, light and darkness

37:06.940 --> 37:10.260
 that you have to tolerate and, you know, deal with.

37:10.260 --> 37:12.620
 Yeah, there's definitely lots of suffering in the world.

37:12.620 --> 37:13.780
 Depends on the perspective.

37:13.780 --> 37:15.420
 It seems like there's way more darkness,

37:15.420 --> 37:18.620
 but that makes the light part really nice.

37:18.620 --> 37:23.620
 What computing hardware or just any kind,

37:24.780 --> 37:28.760
 even software design, do you find beautiful

37:28.760 --> 37:32.500
 from your own work, from other people's work?

37:32.500 --> 37:37.340
 You're just, we were just talking about the battleground

37:37.340 --> 37:39.260
 of flaws and mistakes and errors,

37:39.260 --> 37:42.540
 but things that were just beautifully done.

37:42.540 --> 37:44.500
 Is there something that pops to mind?

37:44.500 --> 37:47.900
 Well, when things are beautifully done,

37:47.900 --> 37:52.600
 usually there's a well thought out set of abstraction layers.

37:53.660 --> 37:56.420
 So the whole thing works in unison nicely.

37:56.420 --> 37:57.380
 Yes.

37:57.380 --> 37:59.380
 And when I say abstraction layer,

37:59.380 --> 38:01.180
 that means two different components

38:01.180 --> 38:04.940
 when they work together, they work independently.

38:04.940 --> 38:07.740
 They don't have to know what the other one is doing.

38:07.740 --> 38:08.660
 So that decoupling.

38:08.660 --> 38:09.500
 Yeah.

38:09.500 --> 38:11.500
 So the famous one was the network stack.

38:11.500 --> 38:13.100
 Like there's a seven layer network stack,

38:13.100 --> 38:16.380
 you know, data transport and protocol and all the layers.

38:16.380 --> 38:17.580
 And the innovation was,

38:17.580 --> 38:20.000
 is when they really wrote, got that right.

38:20.000 --> 38:22.940
 Cause networks before that didn't define those very well.

38:22.940 --> 38:26.220
 The layers could innovate independently.

38:26.220 --> 38:28.780
 And occasionally the layer boundary would,

38:28.780 --> 38:30.980
 the interface would be upgraded.

38:30.980 --> 38:34.780
 And that let the design space breathe.

38:34.780 --> 38:37.860
 And you could do something new in layer seven

38:37.860 --> 38:40.620
 without having to worry about how layer four worked.

38:40.620 --> 38:43.000
 And so good design does that.

38:43.000 --> 38:45.220
 And you see it in processor designs.

38:45.220 --> 38:48.580
 When we did the Zen design at AMD,

38:48.580 --> 38:51.940
 we made several components very modular.

38:51.940 --> 38:54.700
 And, you know, my insistence at the top was

38:54.700 --> 38:56.620
 I wanted all the interfaces defined

38:56.620 --> 38:59.320
 before we wrote the RTL for the pieces.

38:59.320 --> 39:01.060
 One of the verification leads said,

39:01.060 --> 39:02.220
 if we do this right,

39:02.220 --> 39:04.900
 I can test the pieces so well independently

39:04.900 --> 39:06.440
 when we put it together,

39:06.440 --> 39:08.140
 we won't find all these interaction bugs

39:08.140 --> 39:10.700
 cause the floating point knows how the cache works.

39:10.700 --> 39:12.020
 And I was a little skeptical,

39:12.020 --> 39:14.220
 but he was mostly right.

39:14.220 --> 39:16.700
 That the modularity of the design

39:16.700 --> 39:18.960
 greatly improved the quality.

39:18.960 --> 39:20.540
 Is that universally true in general?

39:20.540 --> 39:21.860
 Would you say about good designs,

39:21.860 --> 39:24.180
 the modularity is like usually modular?

39:24.180 --> 39:25.180
 Well, we talked about this before.

39:25.180 --> 39:26.420
 Humans are only so smart.

39:26.420 --> 39:29.460
 Like, and we're not getting any smarter, right?

39:29.460 --> 39:32.260
 But the complexity of things is going up.

39:32.260 --> 39:36.200
 So, you know, a beautiful design can't be bigger

39:36.200 --> 39:37.960
 than the person doing it.

39:37.960 --> 39:40.020
 It's just, you know, their piece of it.

39:40.020 --> 39:42.420
 Like the odds of you doing a really beautiful design

39:42.420 --> 39:46.560
 of something that's way too hard for you is low, right?

39:46.560 --> 39:48.000
 If it's way too simple for you,

39:48.000 --> 39:49.020
 it's not that interesting.

39:49.020 --> 39:50.600
 It's like, well, anybody could do that.

39:50.600 --> 39:54.720
 But when you get the right match of your expertise

39:54.720 --> 39:58.680
 and, you know, mental power to the right design size,

39:58.680 --> 40:00.400
 that's cool, but that's not big enough

40:00.400 --> 40:02.220
 to make a meaningful impact in the world.

40:02.220 --> 40:04.900
 So now you have to have some framework

40:04.900 --> 40:08.060
 to design the pieces so that the whole thing

40:08.060 --> 40:10.060
 is big and harmonious.

40:10.060 --> 40:13.520
 But, you know, when you put it together,

40:13.520 --> 40:18.520
 it's, you know, sufficiently interesting to be used.

40:18.900 --> 40:21.400
 And, you know, so that's what a beautiful design is.

40:23.300 --> 40:27.960
 Matching the limits of that human cognitive capacity

40:27.960 --> 40:30.300
 to the module that you can create

40:30.300 --> 40:33.100
 and creating a nice interface between those modules

40:33.100 --> 40:34.500
 and thereby, do you think there's a limit

40:34.500 --> 40:37.080
 to the kind of beautiful complex systems

40:37.080 --> 40:40.980
 we can build with this kind of modular design?

40:40.980 --> 40:45.900
 It's like, you know, if we build increasingly

40:45.900 --> 40:48.500
 more complicated, you can think of like the internet.

40:49.500 --> 40:50.900
 Okay, let's scale it down.

40:50.900 --> 40:52.300
 Or you can think of like social network,

40:52.300 --> 40:56.320
 like Twitter as one computing system.

40:57.740 --> 41:00.700
 But those are little modules, right?

41:00.700 --> 41:03.780
 But it's built on so many components

41:03.780 --> 41:05.980
 nobody at Twitter even understands.

41:05.980 --> 41:06.820
 Right.

41:06.820 --> 41:09.300
 So if an alien showed up and looked at Twitter,

41:09.300 --> 41:11.180
 he wouldn't just see Twitter as a beautiful,

41:11.180 --> 41:14.420
 simple thing that everybody uses, which is really big.

41:14.420 --> 41:18.180
 You would see the network, it runs on the fiber optics,

41:18.180 --> 41:19.880
 the data is transported to the computers.

41:19.880 --> 41:22.060
 The whole thing is so bloody complicated,

41:22.060 --> 41:23.760
 nobody at Twitter understands it.

41:23.760 --> 41:25.760
 And so that's what the alien would see.

41:25.760 --> 41:28.820
 So yeah, if an alien showed up and looked at Twitter

41:28.820 --> 41:32.060
 or looked at the various different network systems

41:32.060 --> 41:33.700
 that you could see on Earth.

41:33.700 --> 41:34.980
 So imagine they were really smart

41:34.980 --> 41:36.700
 and they could comprehend the whole thing.

41:36.700 --> 41:40.140
 And then they sort of evaluated the human

41:40.140 --> 41:41.540
 and thought, this is really interesting.

41:41.540 --> 41:44.620
 No human on this planet comprehends the system they built.

41:45.500 --> 41:48.900
 No individual, well, would they even see individual humans?

41:48.900 --> 41:52.720
 Like we humans are very human centric, entity centric.

41:52.720 --> 41:56.860
 And so we think of us as the central organism

41:56.860 --> 41:59.820
 and the networks as just the connection of organisms.

41:59.820 --> 42:02.500
 But from a perspective of an alien,

42:02.500 --> 42:05.380
 from an outside perspective, it seems like.

42:05.380 --> 42:06.980
 Yeah, I get it.

42:06.980 --> 42:08.940
 We're the ants and they'd see the ant colony.

42:08.940 --> 42:10.500
 The ant colony, yeah.

42:10.500 --> 42:12.780
 Or the result of production of the ant colony,

42:12.780 --> 42:16.020
 which is like cities and it's,

42:18.100 --> 42:19.880
 in that sense, humans are pretty impressive.

42:19.880 --> 42:21.860
 The modularity that we're able to,

42:23.120 --> 42:25.940
 and how robust we are to noise and mutation

42:25.940 --> 42:26.780
 and all that kind of stuff.

42:26.780 --> 42:28.540
 Well, that's because it's stress tested all the time.

42:28.540 --> 42:29.380
 Yeah.

42:29.380 --> 42:31.060
 You know, you build all these cities with buildings

42:31.060 --> 42:32.420
 and you get earthquakes occasionally

42:32.420 --> 42:35.540
 and, you know, wars, earthquakes.

42:35.540 --> 42:37.620
 Viruses every once in a while.

42:37.620 --> 42:39.500
 You know, changes in business plans

42:39.500 --> 42:41.620
 or, you know, like shipping or something.

42:41.620 --> 42:44.740
 Like as long as it's all stress tested,

42:44.740 --> 42:48.560
 then it keeps adapting to the situation.

42:48.560 --> 42:52.540
 So that's a curious phenomenon.

42:52.540 --> 42:55.060
 Well, let's go, let's talk about Moore's Law a little bit.

42:55.060 --> 43:00.060
 It's at the broad view of Moore's Law

43:00.060 --> 43:05.060
 was just exponential improvement of computing capability.

43:05.260 --> 43:08.380
 Like OpenAI, for example, recently published

43:08.380 --> 43:13.380
 this kind of papers looking at the exponential improvement

43:14.060 --> 43:17.020
 in the training efficiency of neural networks

43:17.020 --> 43:18.620
 for like ImageNet and all that kind of stuff.

43:18.620 --> 43:22.300
 We just got better on this purely software side,

43:22.300 --> 43:25.620
 just figuring out better tricks and algorithms

43:25.620 --> 43:26.980
 for training neural networks.

43:26.980 --> 43:30.620
 And that seems to be improving significantly faster

43:30.620 --> 43:33.100
 than the Moore's Law prediction, you know.

43:33.100 --> 43:35.300
 So that's in the software space.

43:35.300 --> 43:39.140
 What do you think if Moore's Law continues

43:39.140 --> 43:42.900
 or if the general version of Moore's Law continues,

43:42.900 --> 43:45.320
 do you think that comes mostly from the hardware,

43:45.320 --> 43:47.580
 from the software, some mix of the two,

43:47.580 --> 43:50.000
 some interesting, totally,

43:50.000 --> 43:52.800
 so not the reduction of the size of the transistor

43:52.800 --> 43:54.420
 kind of thing, but more in the,

43:54.420 --> 43:58.940
 in the totally interesting kinds of innovations

43:58.940 --> 44:01.260
 in the hardware space, all that kind of stuff.

44:01.260 --> 44:04.060
 Well, there's like a half a dozen things

44:04.060 --> 44:05.580
 going on in that graph.

44:05.580 --> 44:08.500
 So one is there's initial innovations

44:08.500 --> 44:11.660
 that had a lot of headroom to be exploited.

44:11.660 --> 44:13.980
 So, you know, the efficiency of the networks

44:13.980 --> 44:15.900
 has improved dramatically.

44:15.900 --> 44:19.660
 And then the decomposability of those and the use going,

44:19.660 --> 44:21.380
 you know, they started running on one computer,

44:21.380 --> 44:23.740
 then multiple computers, then multiple GPUs,

44:23.740 --> 44:27.100
 and then arrays of GPUs, and they're up to thousands.

44:27.100 --> 44:30.620
 And at some point, so it's sort of like

44:30.620 --> 44:32.300
 they were consumed, they were going from

44:32.300 --> 44:33.860
 like a single computer application

44:33.860 --> 44:36.240
 to a thousand computer application.

44:36.240 --> 44:38.200
 So that's not really a Moore's Law thing.

44:38.200 --> 44:39.520
 That's an independent vector.

44:39.520 --> 44:42.340
 How many computers can I put on this problem?

44:42.340 --> 44:44.220
 Because the computers themselves are getting better

44:44.220 --> 44:45.980
 on like a Moore's Law rate,

44:45.980 --> 44:47.900
 but their ability to go from one to 10

44:47.900 --> 44:51.180
 to 100 to a thousand, you know, was something.

44:51.180 --> 44:54.300
 And then multiplied by, you know, the amount of computes

44:54.300 --> 44:58.300
 it took to resolve like AlexNet to ResNet to transformers.

44:58.300 --> 45:01.700
 It's been quite, you know, steady improvements.

45:01.700 --> 45:03.300
 But those are like S curves, aren't they?

45:03.300 --> 45:04.940
 That's the exactly kind of S curves

45:04.940 --> 45:07.620
 that are underlying Moore's Law from the very beginning.

45:07.620 --> 45:12.620
 So what's the biggest, what's the most productive,

45:13.380 --> 45:16.740
 rich source of S curves in the future, do you think?

45:16.740 --> 45:18.780
 Is it hardware, is it software, or is it?

45:18.780 --> 45:23.660
 So hardware is going to move along relatively slowly.

45:23.660 --> 45:26.660
 Like, you know, double performance every two years.

45:26.660 --> 45:28.380
 There's still...

45:28.380 --> 45:29.620
 I like how you call that slowly.

45:29.620 --> 45:31.460
 Yeah, that's the slow version.

45:31.460 --> 45:33.220
 The snail's pace of Moore's Law.

45:33.220 --> 45:37.580
 Maybe we should trademark that one.

45:39.100 --> 45:41.980
 Whereas the scaling by number of computers, you know,

45:41.980 --> 45:44.020
 can go much faster, you know.

45:44.020 --> 45:46.380
 I'm sure at some point Google had a, you know,

45:46.380 --> 45:48.900
 their initial search engine was running on a laptop,

45:48.900 --> 45:50.140
 you know, like.

45:50.140 --> 45:52.580
 And at some point they really worked on scaling that.

45:52.580 --> 45:55.940
 And then they factored the indexer from, you know,

45:55.940 --> 45:57.500
 this piece and this piece and this piece,

45:57.500 --> 45:59.340
 and they spread the data on more and more things.

45:59.340 --> 46:02.820
 And, you know, they did a dozen innovations.

46:02.820 --> 46:05.420
 But as they scaled up the number of computers on that,

46:05.420 --> 46:07.500
 it kept breaking, finding new bottlenecks

46:07.500 --> 46:09.220
 in their software and their schedulers,

46:09.220 --> 46:11.780
 and made them rethink.

46:11.780 --> 46:13.980
 Like, it seems insane to do a scheduler

46:13.980 --> 46:16.700
 across 1,000 computers to schedule parts of it

46:16.700 --> 46:19.020
 and then send the results to one computer.

46:19.020 --> 46:21.380
 But if you want to schedule a million searches,

46:21.380 --> 46:23.180
 that makes perfect sense.

46:23.180 --> 46:26.860
 So there's the scaling by just quantity

46:26.860 --> 46:28.980
 is probably the richest thing.

46:28.980 --> 46:31.980
 But then as you scale quantity,

46:31.980 --> 46:34.660
 like a network that was great on 100 computers

46:34.660 --> 46:36.580
 may be completely the wrong one.

46:36.580 --> 46:39.620
 You may pick a network that's 10 times slower

46:39.620 --> 46:42.540
 on 10,000 computers, like per computer.

46:42.540 --> 46:45.820
 But if you go from 100 to 10,000, it's 100 times.

46:45.820 --> 46:47.220
 So that's one of the things that happened

46:47.220 --> 46:48.740
 when we did internet scaling.

46:48.740 --> 46:52.580
 This efficiency went down, not up.

46:52.580 --> 46:55.500
 The future of computing is inefficiency, not efficiency.

46:55.500 --> 46:57.620
 But scale, inefficient scale.

46:57.620 --> 47:01.860
 It's scaling faster than inefficiency bites you.

47:01.860 --> 47:03.860
 And as long as there's, you know, dollar value there,

47:03.860 --> 47:05.980
 like scaling costs lots of money.

47:05.980 --> 47:08.220
 But Google showed, Facebook showed, everybody showed

47:08.220 --> 47:10.740
 that the scale was where the money was at.

47:10.740 --> 47:13.780
 It was, and so it was worth the financial.

47:13.780 --> 47:17.780
 Do you think, is it possible that like basically

47:17.780 --> 47:21.800
 the entirety of Earth will be like a computing surface?

47:21.800 --> 47:24.460
 Like this table will be doing computing.

47:24.460 --> 47:26.140
 This hedgehog will be doing computing.

47:26.140 --> 47:28.180
 Like everything really inefficient,

47:28.180 --> 47:29.500
 dumb computing will be leveraged.

47:29.500 --> 47:31.820
 The science fiction books, they call it computronium.

47:31.820 --> 47:32.660
 Computronium?

47:32.660 --> 47:34.700
 We turn everything into computing.

47:34.700 --> 47:37.980
 Well, most of the elements aren't very good for anything.

47:37.980 --> 47:39.940
 Like you're not gonna make a computer out of iron.

47:39.940 --> 47:44.120
 Like, you know, silicon and carbon have like nice structures.

47:45.020 --> 47:48.060
 You know, we'll see what you can do with the rest of it.

47:48.060 --> 47:50.380
 Like people talk about, well, maybe we can turn the sun

47:50.380 --> 47:54.980
 into computer, but it's hydrogen and a little bit of helium.

47:54.980 --> 47:55.820
 So.

47:55.820 --> 47:59.060
 What I mean is more like actually just adding computers

47:59.060 --> 47:59.940
 to everything.

47:59.940 --> 48:00.780
 Oh, okay.

48:00.780 --> 48:03.100
 So you're just converting all the mass of the universe

48:03.100 --> 48:04.260
 into computer.

48:04.260 --> 48:05.100
 No, no, no.

48:05.100 --> 48:05.920
 So not using.

48:05.920 --> 48:07.580
 It'd be ironic from the simulation point of view.

48:07.580 --> 48:10.720
 It's like the simulator build mass, the simulates.

48:12.020 --> 48:12.860
 Yeah, I mean, yeah.

48:12.860 --> 48:14.940
 So, I mean, ultimately this is all heading

48:14.940 --> 48:15.780
 towards a simulation.

48:15.780 --> 48:18.460
 Yeah, well, I think I might've told you this story.

48:18.460 --> 48:20.280
 At Tesla, they were deciding,

48:20.280 --> 48:22.420
 so they wanna measure the current coming out of the battery

48:22.420 --> 48:25.900
 and they decided between putting the resistor in there

48:25.900 --> 48:29.460
 and putting a computer with a sensor in there.

48:29.460 --> 48:31.940
 And the computer was faster than the computer

48:31.940 --> 48:34.140
 I worked on in 1982.

48:34.140 --> 48:35.560
 And we chose the computer

48:35.560 --> 48:37.620
 because it was cheaper than the resistor.

48:38.660 --> 48:42.340
 So, sure, this hedgehog costs $13

48:42.340 --> 48:45.160
 and we can put an AI that's as smart as you

48:45.160 --> 48:46.060
 in there for five bucks.

48:46.060 --> 48:46.900
 It'll have one.

48:48.560 --> 48:51.780
 So computers will be everywhere.

48:51.780 --> 48:54.620
 I was hoping it wouldn't be smarter than me because.

48:54.620 --> 48:56.660
 Well, everything's gonna be smarter than you.

48:56.660 --> 48:58.060
 But you were saying it's inefficient.

48:58.060 --> 49:00.240
 I thought it was better to have a lot of dumb things.

49:00.240 --> 49:02.740
 Well, Moore's law will slowly compact that stuff.

49:02.740 --> 49:04.860
 So even the dumb things will be smarter than us.

49:04.860 --> 49:06.020
 The dumb things are gonna be smart

49:06.020 --> 49:08.020
 or they're gonna be smart enough to talk to something

49:08.020 --> 49:09.040
 that's really smart.

49:10.140 --> 49:11.140
 You know, it's like.

49:12.580 --> 49:15.220
 Well, just remember, like a big computer chip.

49:15.220 --> 49:16.060
 Yeah.

49:16.060 --> 49:17.620
 You know, it's like an inch by an inch

49:17.620 --> 49:20.120
 and, you know, 40 microns thick.

49:20.980 --> 49:23.340
 It doesn't take very much, very many atoms

49:23.340 --> 49:25.020
 to make a high power computer.

49:25.020 --> 49:25.860
 Yeah.

49:25.860 --> 49:27.800
 And 10,000 of them can fit in a shoebox.

49:29.060 --> 49:31.500
 But, you know, you have the cooling and power problems,

49:31.500 --> 49:33.540
 but, you know, people are working on that.

49:33.540 --> 49:37.660
 But they still can't write compelling poetry or music

49:37.660 --> 49:41.740
 or understand what love is or have a fear of mortality.

49:41.740 --> 49:43.500
 So we're still winning.

49:43.500 --> 49:46.180
 Neither can most of humanity, so.

49:46.180 --> 49:48.280
 Well, they can write books about it.

49:48.280 --> 49:52.460
 So, but speaking about this,

49:53.900 --> 49:56.860
 this walk along the path of innovation

49:56.860 --> 50:00.100
 towards the dumb things being smarter than humans,

50:00.100 --> 50:05.100
 you are now the CTO of 10storrent as of two months ago.

50:08.500 --> 50:12.020
 They build hardware for deep learning.

50:13.780 --> 50:16.140
 How do you build scalable and efficient deep learning?

50:16.140 --> 50:17.460
 This is such a fascinating space.

50:17.460 --> 50:18.740
 Yeah, yeah, so it's interesting.

50:18.740 --> 50:20.780
 So up until recently,

50:20.780 --> 50:22.340
 I thought there was two kinds of computers.

50:22.340 --> 50:25.380
 There are serial computers that run like C programs,

50:25.380 --> 50:27.100
 and then there's parallel computers.

50:27.100 --> 50:29.340
 So the way I think about it is, you know,

50:29.340 --> 50:31.900
 parallel computers have given parallelism.

50:31.900 --> 50:34.780
 Like, GPUs are great because you have a million pixels,

50:34.780 --> 50:37.500
 and modern GPUs run a program on every pixel.

50:37.500 --> 50:39.340
 They call it the shader program, right?

50:39.340 --> 50:42.460
 So, or like finite element analysis.

50:42.460 --> 50:43.900
 You build something, you know,

50:43.900 --> 50:45.540
 you make this into little tiny chunks,

50:45.540 --> 50:47.100
 you give each chunk to a computer,

50:47.100 --> 50:48.420
 so you're given all these chunks,

50:48.420 --> 50:50.160
 you have parallelism like that.

50:50.160 --> 50:53.520
 But most C programs, you write this linear narrative,

50:53.520 --> 50:55.540
 and you have to make it go fast.

50:55.540 --> 50:57.680
 To make it go fast, you predict all the branches,

50:57.680 --> 50:59.300
 all the data fetches, and you run that.

50:59.300 --> 51:01.920
 More parallel, but that's found parallelism.

51:04.260 --> 51:08.420
 AI is, I'm still trying to decide how fundamental this is.

51:08.420 --> 51:10.900
 It's a given parallelism problem.

51:10.900 --> 51:14.800
 But the way people describe the neural networks,

51:14.800 --> 51:17.900
 and then how they write them in PyTorch, it makes graphs.

51:17.900 --> 51:19.980
 Yeah, that might be fundamentally different

51:19.980 --> 51:21.660
 than the GPU kind of.

51:21.660 --> 51:23.280
 Parallelism, yeah, it might be.

51:23.280 --> 51:27.300
 Because when you run the GPU program on all the pixels,

51:27.300 --> 51:29.860
 you're running, you know, it depends,

51:29.860 --> 51:32.540
 this group of pixels say it's background blue,

51:32.540 --> 51:34.020
 and it runs a really simple program.

51:34.020 --> 51:36.900
 This pixel is, you know, some patch of your face,

51:36.900 --> 51:39.520
 so you have some really interesting shader program

51:39.520 --> 51:41.740
 to give you the impression of translucency.

51:41.740 --> 51:43.940
 But the pixels themselves don't talk to each other.

51:43.940 --> 51:46.620
 There's no graph, right?

51:46.620 --> 51:49.540
 So you do the image, and then you do the next image,

51:49.540 --> 51:51.300
 and you do the next image,

51:51.300 --> 51:53.860
 and you run eight million pixels,

51:53.860 --> 51:55.620
 eight million programs every time,

51:55.620 --> 51:59.580
 and modern GPUs have like 6,000 thread engines in them.

51:59.580 --> 52:02.100
 So, you know, to get eight million pixels,

52:02.100 --> 52:06.140
 each one runs a program on, you know, 10 or 20 pixels.

52:06.140 --> 52:09.380
 And that's how they work, but there's no graph.

52:09.380 --> 52:13.680
 But you think graph might be a totally new way

52:13.680 --> 52:14.900
 to think about hardware.

52:14.900 --> 52:18.140
 So Rajagat Dori and I have been having this conversation

52:18.140 --> 52:20.580
 about given versus found parallelism.

52:20.580 --> 52:22.540
 And then the kind of walk,

52:22.540 --> 52:23.860
 because we got more transistors,

52:23.860 --> 52:25.660
 like, you know, computers way back when

52:25.660 --> 52:27.820
 did stuff on scalar data.

52:27.820 --> 52:30.740
 Now we did it on vector data, famous vector machines.

52:30.740 --> 52:34.500
 Now we're making computers that operate on matrices, right?

52:34.500 --> 52:38.900
 And then the category we said that was next was spatial.

52:38.900 --> 52:40.580
 Like, imagine you have so much data

52:40.580 --> 52:43.420
 that, you know, you want to do the compute on this data,

52:43.420 --> 52:45.920
 and then when it's done, it says,

52:45.920 --> 52:49.260
 send the result to this pile of data on some software on that.

52:49.260 --> 52:53.060
 And it's better to think about it spatially

52:53.060 --> 52:56.140
 than to move all the data to a central processor

52:56.140 --> 52:57.580
 and do all the work.

52:57.580 --> 53:00.740
 So spatially, you mean moving in the space of data

53:00.740 --> 53:02.460
 as opposed to moving the data.

53:02.460 --> 53:05.340
 Yeah, you have a petabyte data space

53:05.340 --> 53:08.620
 spread across some huge array of computers.

53:08.620 --> 53:10.560
 And when you do a computation somewhere,

53:10.560 --> 53:12.300
 you send the result of that computation

53:12.300 --> 53:14.380
 or maybe a pointer to the next program

53:14.380 --> 53:16.660
 to some other piece of data and do it.

53:16.660 --> 53:18.800
 But I think a better word might be graph.

53:18.800 --> 53:21.700
 And all the AI neural networks are graphs.

53:21.700 --> 53:24.060
 Do some computations, send the result here,

53:24.060 --> 53:26.420
 do another computation, do a data transformation,

53:26.420 --> 53:30.340
 do a merging, do a pooling, do another computation.

53:30.340 --> 53:32.280
 Is it possible to compress and say

53:32.280 --> 53:34.580
 how we make this thing efficient,

53:34.580 --> 53:37.300
 this whole process efficient, this different?

53:37.300 --> 53:40.920
 So first, the fundamental elements in the graphs

53:40.920 --> 53:43.220
 are things like matrix multiplies, convolutions,

53:43.220 --> 53:46.140
 data manipulations, and data movements.

53:46.140 --> 53:49.660
 So GPUs emulate those things with their little singles,

53:49.660 --> 53:53.100
 you know, basically running a single threaded program.

53:53.100 --> 53:55.580
 And then there's, you know, and NVIDIA calls it a warp

53:55.580 --> 53:56.900
 where they group a bunch of programs

53:56.900 --> 53:58.420
 that are similar together.

53:58.420 --> 54:01.580
 So for efficiency and instruction use.

54:01.580 --> 54:04.020
 And then at a higher level, you kind of,

54:04.020 --> 54:06.100
 you take this graph and you say this part of the graph

54:06.100 --> 54:09.860
 is a matrix multiplier, which runs on these 32 threads.

54:09.860 --> 54:12.660
 But the model at the bottom was built

54:12.660 --> 54:17.180
 for running programs on pixels, not executing graphs.

54:17.180 --> 54:19.440
 So it's emulation, ultimately.

54:19.440 --> 54:21.120
 So is it possible to build something

54:21.120 --> 54:23.060
 that natively runs graphs?

54:23.060 --> 54:24.980
 Yes, so that's what 10storrent did.

54:26.260 --> 54:27.100
 So.

54:27.100 --> 54:28.220
 Where are we on that?

54:28.220 --> 54:30.920
 How, like, in the history of that effort,

54:30.920 --> 54:32.100
 are we in the early days?

54:32.100 --> 54:33.420
 Yeah, I think so.

54:33.420 --> 54:35.740
 10storrent started by a friend of mine,

54:35.740 --> 54:39.020
 Labisha Bajek, and I was his first investor.

54:39.020 --> 54:41.660
 So I've been, you know, kind of following him

54:41.660 --> 54:43.740
 and talking to him about it for years.

54:43.740 --> 54:47.000
 And in the fall when I was considering things to do,

54:47.000 --> 54:51.620
 I decided, you know, we held a conference last year

54:51.620 --> 54:53.020
 with a friend, organized it,

54:53.020 --> 54:56.180
 and we wanted to bring in thinkers.

54:56.180 --> 55:00.520
 And two of the people were Andre Carpassi and Chris Ladner.

55:00.520 --> 55:03.440
 And Andre gave this talk, it's on YouTube,

55:03.440 --> 55:06.860
 called Software 2.0, which I think is great.

55:06.860 --> 55:10.200
 Which is, we went from programmed computers,

55:10.200 --> 55:13.820
 where you write programs, to data program computers.

55:13.820 --> 55:18.180
 You know, like the future of software is data programs,

55:18.180 --> 55:19.380
 the networks.

55:19.380 --> 55:21.380
 And I think that's true.

55:21.380 --> 55:23.980
 And then Chris has been working,

55:23.980 --> 55:26.620
 he worked on LLVM, the low level virtual machine,

55:26.620 --> 55:29.100
 which became the intermediate representation

55:29.100 --> 55:31.380
 for all compilers.

55:31.380 --> 55:33.660
 And now he's working on another project called MLIR,

55:33.660 --> 55:36.460
 which is mid level intermediate representation,

55:36.460 --> 55:39.860
 which is essentially under the graph

55:39.860 --> 55:42.820
 about how do you represent that kind of computation

55:42.820 --> 55:44.360
 and then coordinate large numbers

55:44.360 --> 55:46.680
 of potentially heterogeneous computers.

55:47.880 --> 55:50.640
 And I would say technically, Tens Torrents,

55:51.500 --> 55:54.900
 you know, two pillars of those two ideas,

55:54.900 --> 55:58.300
 software 2.0 and mid level representation.

55:58.300 --> 56:01.900
 But it's in service of executing graph programs.

56:01.900 --> 56:03.820
 The hardware is designed to do that.

56:03.820 --> 56:05.580
 So it's including the hardware piece.

56:05.580 --> 56:06.480
 Yeah.

56:06.480 --> 56:08.500
 And then the other cool thing is,

56:08.500 --> 56:10.100
 for a relatively small amount of money,

56:10.100 --> 56:13.340
 they did a test chip and two production chips.

56:13.340 --> 56:15.380
 So it's like a super effective team.

56:15.380 --> 56:18.180
 And unlike some AI startups,

56:18.180 --> 56:20.180
 where if you don't build the hardware

56:20.180 --> 56:22.900
 to run the software that they really want to do,

56:22.900 --> 56:26.060
 then you have to fix it by writing lots more software.

56:26.060 --> 56:29.100
 So the hardware naturally does matrix multiply,

56:29.100 --> 56:31.820
 convolution, the data manipulations,

56:31.820 --> 56:35.340
 and the data movement between processing elements

56:35.340 --> 56:37.600
 that you can see in the graph,

56:37.600 --> 56:40.340
 which I think is all pretty clever.

56:40.340 --> 56:45.060
 And that's what I'm working on now.

56:45.060 --> 56:49.660
 So the, I think it's called the Grace Call Processor.

56:49.660 --> 56:51.260
 I introduced last year.

56:51.260 --> 56:53.780
 It's, you know, there's a bunch of measures of performance.

56:53.780 --> 56:55.480
 We're talking about horses.

56:55.480 --> 56:59.820
 It seems to outperform 368 trillion operations per second.

56:59.820 --> 57:03.180
 It seems to outperform NVIDIA's Tesla T4 system.

57:03.180 --> 57:04.620
 So these are just numbers.

57:04.620 --> 57:07.540
 What do they actually mean in real world performance?

57:07.540 --> 57:10.140
 Like what are the metrics for you

57:10.140 --> 57:12.380
 that you're chasing in your horse race?

57:12.380 --> 57:13.820
 Like what do you care about?

57:13.820 --> 57:17.700
 Well, first, so the native language of,

57:17.700 --> 57:20.340
 you know, people who write AI network programs

57:20.340 --> 57:22.500
 is PyTorch now, PyTorch, TensorFlow.

57:22.500 --> 57:24.020
 There's a couple others.

57:24.020 --> 57:25.820
 Do you think PyTorch is one over TensorFlow?

57:25.820 --> 57:26.640
 Or is it just?

57:26.640 --> 57:27.980
 I'm not an expert on that.

57:27.980 --> 57:29.780
 I know many people who have switched

57:29.780 --> 57:31.660
 from TensorFlow to PyTorch.

57:31.660 --> 57:33.820
 And there's technical reasons for it.

57:33.820 --> 57:34.740
 I use both.

57:34.740 --> 57:35.900
 Both are still awesome.

57:35.900 --> 57:37.160
 Both are still awesome.

57:37.160 --> 57:39.860
 But the deepest love is for PyTorch currently.

57:39.860 --> 57:41.360
 Yeah, there's more love for that.

57:41.360 --> 57:42.620
 And that may change.

57:42.620 --> 57:46.680
 So the first thing is when they write their programs,

57:46.680 --> 57:50.460
 can the hardware execute it pretty much as it was written?

57:50.460 --> 57:53.340
 Right, so PyTorch turns into a graph.

57:53.340 --> 57:55.580
 We have a graph compiler that makes that graph.

57:55.580 --> 57:57.480
 Then it fractions the graph down.

57:57.480 --> 57:58.820
 So if you have big matrix multiply,

57:58.820 --> 58:00.140
 we turn it into right size chunks

58:00.140 --> 58:02.180
 to run on the processing elements.

58:02.180 --> 58:03.300
 It hooks all the graph up.

58:03.300 --> 58:05.140
 It lays out all the data.

58:05.140 --> 58:08.020
 There's a couple of mid level representations of it

58:08.020 --> 58:09.420
 that are also simulatable.

58:09.420 --> 58:12.140
 So that if you're writing the code,

58:12.140 --> 58:15.100
 you can see how it's gonna go through the machine,

58:15.100 --> 58:15.940
 which is pretty cool.

58:15.940 --> 58:17.700
 And then at the bottom, it schedules kernels,

58:17.700 --> 58:21.780
 like math, data manipulation, data movement kernels,

58:21.780 --> 58:22.860
 which do this stuff.

58:22.860 --> 58:26.180
 So we don't have to write a little program

58:26.180 --> 58:27.300
 to do matrix multiply,

58:27.300 --> 58:29.140
 because we have a big matrix multiplier.

58:29.140 --> 58:31.240
 There's no SIMD program for that.

58:31.240 --> 58:36.000
 But there is scheduling for that, right?

58:36.000 --> 58:37.640
 So one of the goals is,

58:37.640 --> 58:40.200
 if you write a piece of PyTorch code

58:40.200 --> 58:41.240
 that looks pretty reasonable,

58:41.240 --> 58:43.480
 you should be able to compile it, run it on the hardware

58:43.480 --> 58:44.760
 without having to tweak it

58:44.760 --> 58:48.100
 and do all kinds of crazy things to get performance.

58:48.100 --> 58:50.120
 There's not a lot of intermediate steps.

58:50.120 --> 58:51.320
 It's running directly as written.

58:51.320 --> 58:54.640
 Like on a GPU, if you write a large matrix multiply naively,

58:54.640 --> 58:57.680
 you'll get five to 10% of the peak performance of the GPU.

58:58.680 --> 59:00.520
 Right, and then there's a bunch of people

59:00.520 --> 59:01.600
 who've published papers on this,

59:01.600 --> 59:04.080
 and I read them about what steps do you have to do.

59:04.080 --> 59:06.760
 And it goes from pretty reasonable,

59:06.760 --> 59:08.480
 well, transpose one of the matrices.

59:08.480 --> 59:10.560
 So you do row ordered, not column ordered,

59:11.680 --> 59:14.520
 block it so that you can put a block of the matrix

59:14.520 --> 59:17.740
 on different SMs, groups of threads.

59:19.340 --> 59:21.160
 But some of it gets into little details,

59:21.160 --> 59:23.000
 like you have to schedule it just so,

59:23.000 --> 59:25.040
 so you don't have register conflicts.

59:25.040 --> 59:28.240
 So they call them CUDA ninjas.

59:28.240 --> 59:31.080
 CUDA ninjas, I love it.

59:31.080 --> 59:32.320
 To get to the optimal point,

59:32.320 --> 59:36.080
 you either use a prewritten library,

59:36.080 --> 59:37.880
 which is a good strategy for some things,

59:37.880 --> 59:39.600
 or you have to be an expert

59:39.600 --> 59:42.200
 in micro architecture to program it.

59:42.200 --> 59:43.480
 Right, so the optimization step

59:43.480 --> 59:44.960
 is way more complicated with the GPU.

59:44.960 --> 59:47.880
 So our goal is if you write PyTorch,

59:47.880 --> 59:49.560
 that's good PyTorch, you can do it.

59:49.560 --> 59:53.080
 Now there's, as the networks are evolving,

59:53.080 --> 59:56.440
 they've changed from convolutional to matrix multiply.

59:56.440 --> 59:58.040
 People are talking about conditional graphs,

59:58.040 --> 59:59.800
 they're talking about very large matrices,

59:59.800 --> 1:00:01.680
 they're talking about sparsity,

1:00:01.680 --> 1:00:03.360
 they're talking about problems

1:00:03.360 --> 1:00:06.120
 that scale across many, many chips.

1:00:06.120 --> 1:00:11.120
 So the native data item is a packet.

1:00:11.720 --> 1:00:14.560
 So you send a packet to a processor, it gets processed,

1:00:14.560 --> 1:00:15.400
 it does a bunch of work,

1:00:15.400 --> 1:00:17.640
 and then it may send packets to other processors,

1:00:17.640 --> 1:00:20.520
 and they execute in like a data flow graph

1:00:20.520 --> 1:00:22.080
 kind of methodology.

1:00:22.080 --> 1:00:22.920
 Got it.

1:00:22.920 --> 1:00:24.400
 We have a big network on chip,

1:00:24.400 --> 1:00:27.760
 and then the second chip has 16 ethernet ports

1:00:27.760 --> 1:00:29.560
 to hook lots of them together,

1:00:29.560 --> 1:00:32.400
 and it's the same graph compiler across multiple chips.

1:00:32.400 --> 1:00:33.600
 So that's where the scale comes in.

1:00:33.600 --> 1:00:35.120
 So it's built to scale naturally.

1:00:35.120 --> 1:00:38.180
 Now, my experience with scaling is as you scale,

1:00:38.180 --> 1:00:40.760
 you run into lots of interesting problems.

1:00:40.760 --> 1:00:43.200
 So scaling is the mountain to climb.

1:00:43.200 --> 1:00:44.040
 Yeah.

1:00:44.040 --> 1:00:44.980
 So the hardware is built to do this,

1:00:44.980 --> 1:00:47.700
 and then we're in the process of.

1:00:47.700 --> 1:00:49.160
 Is there a software part to this

1:00:49.160 --> 1:00:51.640
 with ethernet and all that?

1:00:51.640 --> 1:00:54.760
 Well, the protocol at the bottom,

1:00:54.760 --> 1:00:57.640
 we sent, it's an ethernet PHY,

1:00:57.640 --> 1:00:59.760
 but the protocol basically says,

1:00:59.760 --> 1:01:01.440
 send the packet from here to there.

1:01:01.440 --> 1:01:03.120
 It's all point to point.

1:01:03.120 --> 1:01:05.840
 The header bit says which processor to send it to,

1:01:05.840 --> 1:01:09.560
 and we basically take a packet off our on chip network,

1:01:09.560 --> 1:01:11.200
 put an ethernet header on it,

1:01:11.200 --> 1:01:13.920
 send it to the other end to strip the header off,

1:01:13.920 --> 1:01:14.880
 and send it to the local thing.

1:01:14.880 --> 1:01:16.120
 It's pretty straightforward.

1:01:16.120 --> 1:01:18.160
 Human to human interaction is pretty straightforward too,

1:01:18.160 --> 1:01:19.360
 but when you get a million of us,

1:01:19.360 --> 1:01:21.440
 we could do some crazy stuff together.

1:01:21.440 --> 1:01:23.380
 Yeah, it's gonna be fun.

1:01:23.380 --> 1:01:25.860
 So is that the goal is scale?

1:01:25.860 --> 1:01:28.360
 So like, for example, I've been recently

1:01:28.360 --> 1:01:30.100
 doing a bunch of robots at home

1:01:30.100 --> 1:01:32.360
 for my own personal pleasure.

1:01:32.360 --> 1:01:35.780
 Am I going to ever use 10th Story, or is this more for?

1:01:35.780 --> 1:01:37.200
 There's all kinds of problems.

1:01:37.200 --> 1:01:38.720
 Like, there's small inference problems,

1:01:38.720 --> 1:01:41.440
 or small training problems, or big training problems.

1:01:41.440 --> 1:01:42.680
 What's the big goal?

1:01:42.680 --> 1:01:45.080
 Is it the big training problems,

1:01:45.080 --> 1:01:46.320
 or the small training problems?

1:01:46.320 --> 1:01:48.060
 Well, one of the goals is to scale

1:01:48.060 --> 1:01:51.720
 from 100 milliwatts to a megawatt, you know?

1:01:51.720 --> 1:01:54.840
 So like, really have some range on the problems,

1:01:54.840 --> 1:01:57.120
 and the same kind of AI programs

1:01:57.120 --> 1:01:59.320
 work at all different levels.

1:01:59.320 --> 1:02:00.600
 So that's the goal.

1:02:00.600 --> 1:02:02.960
 The natural, since the natural data item

1:02:02.960 --> 1:02:05.320
 is a packet that we can move around,

1:02:05.320 --> 1:02:10.320
 it's built to scale, but so many people have small problems.

1:02:11.560 --> 1:02:12.400
 Right, right.

1:02:12.400 --> 1:02:13.240
 But the, you know.

1:02:13.240 --> 1:02:16.400
 Like, inside that phone is a small problem to solve.

1:02:16.400 --> 1:02:19.960
 So do you see 10th Story potentially being inside a phone?

1:02:19.960 --> 1:02:22.600
 Well, the power efficiency of local memory,

1:02:22.600 --> 1:02:26.360
 local computation, and the way we built it is pretty good.

1:02:26.360 --> 1:02:28.520
 And then there's a lot of efficiency

1:02:28.520 --> 1:02:31.500
 on being able to do conditional graphs and sparsity.

1:02:31.500 --> 1:02:34.540
 I think it's, for complicated networks

1:02:34.540 --> 1:02:38.180
 that wanna go in a small factor, it's gonna be quite good.

1:02:38.180 --> 1:02:40.200
 But we have to prove that, that's all.

1:02:40.200 --> 1:02:41.040
 It's a fun problem.

1:02:41.040 --> 1:02:42.280
 And that's the early days of the company, right?

1:02:42.280 --> 1:02:44.600
 It's a couple years, you said?

1:02:44.600 --> 1:02:47.560
 But you think, you invested, you think they're legit.

1:02:47.560 --> 1:02:48.400
 Yeah.

1:02:48.400 --> 1:02:49.220
 And so you joined.

1:02:49.220 --> 1:02:50.060
 Yeah, I joined.

1:02:50.060 --> 1:02:50.900
 Well, that's.

1:02:50.900 --> 1:02:53.240
 That's a really interesting place to be.

1:02:53.240 --> 1:02:55.720
 Like, the AI world is exploding, you know.

1:02:55.720 --> 1:02:58.520
 And I looked at some other opportunities

1:02:58.520 --> 1:03:01.520
 like build a faster processor, which people want.

1:03:01.520 --> 1:03:03.760
 But that's more on an incremental path

1:03:03.760 --> 1:03:07.860
 than what's gonna happen in AI in the next 10 years.

1:03:07.860 --> 1:03:08.700
 Yeah.

1:03:08.700 --> 1:03:10.080
 So this is kind of, you know,

1:03:10.080 --> 1:03:12.240
 an exciting place to be part of.

1:03:12.240 --> 1:03:14.080
 Yeah, the revolutions will be happening

1:03:14.080 --> 1:03:15.280
 in the very space that Tesla is.

1:03:15.280 --> 1:03:16.680
 And then lots of people are working on it,

1:03:16.680 --> 1:03:18.900
 but there's lots of technical reasons why some of them,

1:03:18.900 --> 1:03:20.320
 you know, aren't gonna work out that well.

1:03:20.320 --> 1:03:23.640
 And, you know, that's interesting.

1:03:23.640 --> 1:03:25.860
 And there's also the same problem

1:03:25.860 --> 1:03:27.540
 about getting the basics right.

1:03:27.540 --> 1:03:30.000
 Like, we've talked to customers about exciting features.

1:03:30.000 --> 1:03:32.080
 And at some point we realized that,

1:03:32.080 --> 1:03:34.720
 Labish and I were realizing they want to hear first

1:03:34.720 --> 1:03:36.700
 about memory bandwidth, local bandwidth,

1:03:36.700 --> 1:03:39.240
 compute intensity, programmability.

1:03:39.240 --> 1:03:42.000
 They want to know the basics, power management,

1:03:42.000 --> 1:03:44.140
 how the network ports work, what are the basics,

1:03:44.140 --> 1:03:46.120
 do all the basics work.

1:03:46.120 --> 1:03:48.000
 Because it's easy to say, we've got this great idea,

1:03:48.000 --> 1:03:53.000
 you know, the crack GPT3, but the people we talked to

1:03:53.260 --> 1:03:57.520
 want to say, if I buy the, so we have a PCI Express card

1:03:57.520 --> 1:03:59.680
 with our chip on it, if you buy the card,

1:03:59.680 --> 1:04:01.960
 you plug it in your machine to download the driver,

1:04:01.960 --> 1:04:05.080
 how long does it take me to get my network to run?

1:04:05.080 --> 1:04:05.920
 Right, right.

1:04:05.920 --> 1:04:06.760
 You know, that's a real question.

1:04:06.760 --> 1:04:08.360
 It's a very basic question.

1:04:08.360 --> 1:04:09.360
 So, yeah.

1:04:09.360 --> 1:04:10.520
 Is there an answer to that yet,

1:04:10.520 --> 1:04:11.360
 or is it trying to get to that?

1:04:11.360 --> 1:04:13.400
 Our goal is like an hour.

1:04:13.400 --> 1:04:14.240
 Okay.

1:04:14.240 --> 1:04:15.520
 When can I buy a Tesla?

1:04:16.800 --> 1:04:17.640
 Pretty soon.

1:04:17.640 --> 1:04:19.640
 Or my, for the small case training.

1:04:19.640 --> 1:04:21.120
 Yeah, pretty soon.

1:04:21.120 --> 1:04:21.960
 Months.

1:04:21.960 --> 1:04:22.800
 Good.

1:04:22.800 --> 1:04:24.740
 I love the idea of you inside the room

1:04:24.740 --> 1:04:29.180
 with the Carpathi, Andre Carpathi and Chris Ladner.

1:04:31.440 --> 1:04:35.980
 Very, very interesting, very brilliant people,

1:04:35.980 --> 1:04:37.560
 very out of the box thinkers,

1:04:37.560 --> 1:04:39.960
 but also like first principles thinkers.

1:04:39.960 --> 1:04:42.640
 Well, they both get stuff done.

1:04:42.640 --> 1:04:44.920
 They only get stuff done to get their own projects done.

1:04:44.920 --> 1:04:47.000
 They talk about it clearly.

1:04:47.000 --> 1:04:48.720
 They educate large numbers of people,

1:04:48.720 --> 1:04:50.520
 and they've created platforms for other people

1:04:50.520 --> 1:04:52.000
 to go do their stuff on.

1:04:52.000 --> 1:04:55.520
 Yeah, the clear thinking that's able to be communicated

1:04:55.520 --> 1:04:57.200
 is kind of impressive.

1:04:57.200 --> 1:05:00.760
 It's kind of remarkable to, yeah, I'm a fan.

1:05:00.760 --> 1:05:02.000
 Well, let me ask,

1:05:02.000 --> 1:05:05.000
 because I talk to Chris actually a lot these days.

1:05:05.000 --> 1:05:08.880
 He's been one of the, just to give him a shout out,

1:05:08.880 --> 1:05:13.700
 he's been so supportive as a human being.

1:05:13.700 --> 1:05:16.280
 So everybody's quite different.

1:05:16.280 --> 1:05:17.640
 Like great engineers are different,

1:05:17.640 --> 1:05:20.760
 but he's been like sensitive to the human element

1:05:20.760 --> 1:05:22.240
 in a way that's been fascinating.

1:05:22.240 --> 1:05:23.960
 Like he was one of the early people

1:05:23.960 --> 1:05:27.880
 on this stupid podcast that I do to say like,

1:05:27.880 --> 1:05:29.640
 don't quit this thing,

1:05:29.640 --> 1:05:34.120
 and also talk to whoever the hell you want to talk to.

1:05:34.120 --> 1:05:38.040
 That kind of from a legit engineer to get like props

1:05:38.040 --> 1:05:39.960
 and be like, you can do this.

1:05:39.960 --> 1:05:42.240
 That was, I mean, that's what a good leader does, right?

1:05:42.240 --> 1:05:45.100
 To just kind of let a little kid do his thing,

1:05:45.100 --> 1:05:48.700
 like go do it, let's see what turns out.

1:05:48.700 --> 1:05:50.500
 That's a pretty powerful thing.

1:05:50.500 --> 1:05:54.440
 But what do you, what's your sense about,

1:05:54.440 --> 1:05:57.660
 he used to be, no, I think stepped away from Google, right?

1:05:58.800 --> 1:06:00.980
 He's at SciFive, I think.

1:06:02.400 --> 1:06:03.820
 What's really impressive to you

1:06:03.820 --> 1:06:05.720
 about the things that Chris has worked on?

1:06:05.720 --> 1:06:08.300
 Because we mentioned the optimization,

1:06:08.300 --> 1:06:10.840
 the compiler design stuff, the LLVM,

1:06:10.840 --> 1:06:15.200
 then there's, he's also at Google worked at the TPU stuff.

1:06:16.400 --> 1:06:19.360
 He's obviously worked on Swift,

1:06:19.360 --> 1:06:21.360
 so the programming language side.

1:06:21.360 --> 1:06:24.280
 Talking about people that work in the entirety of the stack.

1:06:24.280 --> 1:06:27.920
 What, from your time interacting with Chris

1:06:27.920 --> 1:06:30.760
 and knowing the guy, what's really impressive to you

1:06:30.760 --> 1:06:32.120
 that just inspires you?

1:06:32.120 --> 1:06:37.120
 Well, like LLVM became the defacto platform

1:06:37.120 --> 1:06:42.120
 for the defacto platform for compilers.

1:06:42.180 --> 1:06:43.840
 It's amazing.

1:06:43.840 --> 1:06:46.380
 And it was good code quality, good design choices.

1:06:46.380 --> 1:06:48.860
 He hit the right level of abstraction.

1:06:48.860 --> 1:06:52.060
 There's a little bit of the right time, the right place.

1:06:52.060 --> 1:06:55.460
 And then he built a new programming language called Swift,

1:06:55.460 --> 1:06:59.100
 which after, let's say some adoption resistance

1:06:59.100 --> 1:07:01.180
 became very successful.

1:07:01.180 --> 1:07:03.380
 I don't know that much about his work at Google,

1:07:03.380 --> 1:07:07.140
 although I know that that was a typical,

1:07:07.140 --> 1:07:11.580
 they started TensorFlow stuff and it was new.

1:07:11.580 --> 1:07:13.620
 They wrote a lot of code and then at some point

1:07:13.620 --> 1:07:16.140
 it needed to be refactored to be,

1:07:17.220 --> 1:07:19.100
 because its development slowed down,

1:07:19.100 --> 1:07:22.340
 why PyTorch started a little later and then passed it.

1:07:22.340 --> 1:07:23.940
 So he did a lot of work on that.

1:07:23.940 --> 1:07:25.980
 And then his idea about MLIR,

1:07:25.980 --> 1:07:28.260
 which is what people started to realize

1:07:28.260 --> 1:07:30.580
 is the complexity of the software stack above

1:07:30.580 --> 1:07:33.540
 the low level IR was getting so high

1:07:33.540 --> 1:07:36.580
 that forcing the features of that into the level

1:07:36.580 --> 1:07:38.740
 was putting too much of a burden on it.

1:07:38.740 --> 1:07:41.580
 So he's splitting that into multiple pieces.

1:07:41.580 --> 1:07:43.820
 And that was one of the inspirations for our software stack

1:07:43.820 --> 1:07:46.700
 where we have several intermediate representations

1:07:46.700 --> 1:07:49.700
 that are all executable and you can look at them

1:07:49.700 --> 1:07:53.940
 and do transformations on them before you lower the level.

1:07:53.940 --> 1:07:58.160
 So that was, I think we started before MLIR

1:07:58.160 --> 1:08:01.700
 really got far enough along to use,

1:08:01.700 --> 1:08:02.820
 but we're interested in that.

1:08:02.820 --> 1:08:04.660
 He's really excited about MLIR.

1:08:04.660 --> 1:08:06.660
 That's his like little baby.

1:08:06.660 --> 1:08:10.900
 So he, and there seems to be some profound ideas on that

1:08:10.900 --> 1:08:11.820
 that are really useful.

1:08:11.820 --> 1:08:14.960
 So each one of those things has been,

1:08:14.960 --> 1:08:17.780
 as the world of software gets more and more complicated,

1:08:17.780 --> 1:08:20.060
 how do we create the right abstraction levels

1:08:20.060 --> 1:08:23.340
 to simplify it in a way that people can now work independently

1:08:23.340 --> 1:08:25.140
 on different levels of it?

1:08:25.140 --> 1:08:27.200
 So I would say all three of those projects,

1:08:27.200 --> 1:08:31.620
 LLVM, Swift, and MLIR did that successfully.

1:08:31.620 --> 1:08:33.700
 So I'm interested in what he's gonna do next

1:08:33.700 --> 1:08:34.820
 in the same kind of way.

1:08:34.820 --> 1:08:36.220
 Yes.

1:08:36.220 --> 1:08:40.680
 On either the TPU or maybe the Nvidia GPU side,

1:08:41.820 --> 1:08:45.860
 how does 10th Story think, or the ideas underlying it,

1:08:45.860 --> 1:08:47.020
 does it have to be 10th Story?

1:08:47.020 --> 1:08:49.900
 Just this kind of graph focused,

1:08:51.580 --> 1:08:56.580
 graph centric hardware, deep learning centric hardware,

1:08:56.580 --> 1:09:00.180
 beat NVIDIAs, do you think it's possible

1:09:00.180 --> 1:09:02.280
 for it to basically overtake NVIDIA?

1:09:02.280 --> 1:09:03.500
 Sure.

1:09:03.500 --> 1:09:05.600
 What's that process look like?

1:09:05.600 --> 1:09:08.060
 What's that journey look like, you think?

1:09:08.060 --> 1:09:11.060
 Well, GPUs were built to run shader programs

1:09:11.060 --> 1:09:13.860
 on millions of pixels, not to run graphs.

1:09:13.860 --> 1:09:14.700
 Yes.

1:09:14.700 --> 1:09:17.380
 So there's a hypothesis that says

1:09:17.380 --> 1:09:20.300
 the way the graphs are built

1:09:20.300 --> 1:09:21.540
 is going to be really interesting

1:09:21.540 --> 1:09:24.080
 to be inefficient on computing this.

1:09:24.080 --> 1:09:27.520
 And then the primitives is not a SIMD program,

1:09:27.520 --> 1:09:30.080
 it's matrix multiply convolution.

1:09:30.080 --> 1:09:33.780
 And then the data manipulations are fairly extensive about,

1:09:33.780 --> 1:09:36.380
 like, how do you do a fast transpose with a program?

1:09:36.380 --> 1:09:38.780
 I don't know if you've ever written a transpose program.

1:09:38.780 --> 1:09:40.420
 They're ugly and slow, but in hardware,

1:09:40.420 --> 1:09:42.140
 you can do really well.

1:09:42.140 --> 1:09:43.300
 Like, I'll give you an example.

1:09:43.300 --> 1:09:47.800
 So when GPU accelerators first started doing triangles,

1:09:47.800 --> 1:09:49.020
 like, so you have a triangle

1:09:49.020 --> 1:09:51.180
 which maps on a set of pixels.

1:09:51.180 --> 1:09:52.580
 So you build, it's very easy,

1:09:52.580 --> 1:09:54.220
 straightforward to build a hardware engine

1:09:54.220 --> 1:09:55.860
 that'll find all those pixels.

1:09:55.860 --> 1:09:56.700
 And it's kind of weird

1:09:56.700 --> 1:09:59.260
 because you walk along the triangle to get to the edge,

1:09:59.260 --> 1:10:01.300
 and then you have to go back down to the next row

1:10:01.300 --> 1:10:04.080
 and walk along, and then you have to decide on the edge

1:10:04.080 --> 1:10:08.060
 if the line of the triangle is like half on the pixel,

1:10:08.060 --> 1:10:09.140
 what's the pixel color?

1:10:09.140 --> 1:10:11.100
 Because it's half of this pixel and half the next one.

1:10:11.100 --> 1:10:12.980
 That's called rasterization.

1:10:12.980 --> 1:10:15.900
 And you're saying that could be done in hardware?

1:10:15.900 --> 1:10:19.340
 No, that's an example of that operation

1:10:19.340 --> 1:10:22.100
 as a software program is really bad.

1:10:22.100 --> 1:10:24.420
 I've written a program that did rasterization.

1:10:24.420 --> 1:10:26.860
 The hardware that does it has actually less code

1:10:26.860 --> 1:10:28.980
 than the software program that does it,

1:10:28.980 --> 1:10:30.340
 and it's way faster.

1:10:31.640 --> 1:10:33.440
 Right, so there are certain times

1:10:33.440 --> 1:10:37.780
 when the abstraction you have, rasterize a triangle,

1:10:37.780 --> 1:10:41.300
 you know, execute a graph, you know, components of a graph.

1:10:41.300 --> 1:10:43.860
 But the right thing to do in the hardware software boundary

1:10:43.860 --> 1:10:45.780
 is for the hardware to naturally do it.

1:10:45.780 --> 1:10:47.940
 And so the GPU is really optimized

1:10:47.940 --> 1:10:50.100
 for the rasterization of triangles.

1:10:50.100 --> 1:10:52.860
 Well, you know, that's just, well, like in a modern,

1:10:52.860 --> 1:10:56.980
 you know, that's a small piece of modern GPUs.

1:10:56.980 --> 1:10:59.940
 What they did is that they still rasterize triangles

1:10:59.940 --> 1:11:02.460
 when you're running in a game, but for the most part,

1:11:02.460 --> 1:11:04.420
 most of the computation in the area of the GPU

1:11:04.420 --> 1:11:05.900
 is running shader programs.

1:11:05.900 --> 1:11:09.580
 But they're single threaded programs on pixels, not graphs.

1:11:09.580 --> 1:11:11.820
 I have to be honest, I'd say I don't actually know

1:11:11.820 --> 1:11:15.060
 the math behind shader, shading and lighting

1:11:15.060 --> 1:11:16.180
 and all that kind of stuff.

1:11:16.180 --> 1:11:17.780
 I don't know what.

1:11:17.780 --> 1:11:20.100
 They look like little simple floating point programs

1:11:20.100 --> 1:11:21.220
 or complicated ones.

1:11:21.220 --> 1:11:23.740
 You can have 8,000 instructions in a shader program.

1:11:23.740 --> 1:11:25.580
 But I don't have a good intuition

1:11:25.580 --> 1:11:27.980
 why it could be parallelized so easily.

1:11:27.980 --> 1:11:30.660
 No, it's because you have 8 million pixels in every single.

1:11:30.660 --> 1:11:34.660
 So when you have a light, right, that comes down,

1:11:34.660 --> 1:11:36.780
 the angle, you know, the amount of light,

1:11:36.780 --> 1:11:40.740
 like say this is a line of pixels across this table, right?

1:11:40.740 --> 1:11:43.620
 The amount of light on each pixel is subtly different.

1:11:43.620 --> 1:11:45.980
 And each pixel is responsible for figuring out what.

1:11:45.980 --> 1:11:46.820
 Figuring it out.

1:11:46.820 --> 1:11:48.580
 So that pixel says, I'm this pixel.

1:11:48.580 --> 1:11:49.940
 I know the angle of the light.

1:11:49.940 --> 1:11:50.900
 I know the occlusion.

1:11:50.900 --> 1:11:52.420
 I know the color I am.

1:11:52.420 --> 1:11:54.420
 Like every single pixel here is a different color.

1:11:54.420 --> 1:11:57.160
 Every single pixel gets a different amount of light.

1:11:57.160 --> 1:12:00.580
 Every single pixel has a subtly different translucency.

1:12:00.580 --> 1:12:02.140
 So to make it look realistic,

1:12:02.140 --> 1:12:05.140
 the solution was you run a separate program on every pixel.

1:12:05.140 --> 1:12:06.720
 See, but I thought there's like reflection

1:12:06.720 --> 1:12:08.060
 from all over the place.

1:12:08.060 --> 1:12:09.620
 Every pixel. Yeah, but there is.

1:12:09.620 --> 1:12:11.060
 So you build a reflection map,

1:12:11.060 --> 1:12:14.180
 which also has some pixelated thing.

1:12:14.180 --> 1:12:16.340
 And then when the pixel is looking at the reflection map,

1:12:16.340 --> 1:12:19.220
 it has to calculate what the normal of the surface is.

1:12:19.220 --> 1:12:20.900
 And it does it per pixel.

1:12:20.900 --> 1:12:22.780
 By the way, there's boatloads of hacks on that.

1:12:22.780 --> 1:12:25.660
 You know, like you may have a lower resolution light map,

1:12:25.660 --> 1:12:26.660
 your reflection map.

1:12:26.660 --> 1:12:29.220
 There's all these, you know, tax they do.

1:12:29.220 --> 1:12:32.940
 But at the end of the day, it's per pixel computation.

1:12:32.940 --> 1:12:35.540
 And it's so happening that you can map

1:12:35.540 --> 1:12:39.340
 graph like computation onto this pixel central computation.

1:12:39.340 --> 1:12:41.360
 You can do floating point programs

1:12:41.360 --> 1:12:43.460
 on convolutions and the matrices.

1:12:43.460 --> 1:12:46.220
 And Nvidia invested for years in CUDA.

1:12:46.220 --> 1:12:50.140
 First for HPC, and then they got lucky with the AI trend.

1:12:50.140 --> 1:12:52.300
 But do you think they're going to essentially

1:12:52.300 --> 1:12:55.440
 not be able to hardcore pivot out of their?

1:12:55.440 --> 1:12:56.280
 We'll see.

1:12:57.420 --> 1:12:59.460
 That's always interesting.

1:12:59.460 --> 1:13:01.260
 How often do big companies hardcore pivot?

1:13:01.260 --> 1:13:02.100
 Occasionally.

1:13:03.820 --> 1:13:06.340
 How much do you know about Nvidia, folks?

1:13:06.340 --> 1:13:08.140
 Some. Some?

1:13:08.140 --> 1:13:10.020
 Well, I'm curious as well.

1:13:10.020 --> 1:13:11.460
 Who's ultimately, as a...

1:13:11.460 --> 1:13:13.380
 Well, they've innovated several times.

1:13:13.380 --> 1:13:15.220
 But they've also worked really hard on mobile.

1:13:15.220 --> 1:13:17.340
 They've worked really hard on radios.

1:13:17.340 --> 1:13:20.680
 You know, they're fundamentally a GPU company.

1:13:20.680 --> 1:13:21.860
 Well, they tried to pivot.

1:13:21.860 --> 1:13:26.160
 There's an interesting little game and play

1:13:26.160 --> 1:13:27.660
 in autonomous vehicles, right?

1:13:27.660 --> 1:13:30.660
 With, or semi autonomous, like playing with Tesla

1:13:30.660 --> 1:13:34.020
 and so on and seeing that's dipping a toe

1:13:34.020 --> 1:13:35.700
 into that kind of pivot.

1:13:35.700 --> 1:13:37.100
 They came out with this platform,

1:13:37.100 --> 1:13:39.140
 which is interesting technically.

1:13:39.140 --> 1:13:42.700
 But it was like a 3000 watt, you know,

1:13:42.700 --> 1:13:46.220
 3000 watt, $3,000 GPU platform.

1:13:46.220 --> 1:13:47.540
 I don't know if it's interesting technically.

1:13:47.540 --> 1:13:49.920
 It's interesting philosophically.

1:13:49.920 --> 1:13:51.900
 Technically, I don't know if it's the execution

1:13:51.900 --> 1:13:53.440
 of the craftsmanship is there.

1:13:53.440 --> 1:13:54.580
 I'm not sure.

1:13:54.580 --> 1:13:55.420
 But I didn't get a sense.

1:13:55.420 --> 1:13:57.780
 I think they were repurposing GPUs

1:13:57.780 --> 1:13:59.140
 for an automotive solution.

1:13:59.140 --> 1:14:00.340
 Right, it's not a real pivot.

1:14:00.340 --> 1:14:03.140
 They didn't build a ground up solution.

1:14:03.140 --> 1:14:03.980
 Right.

1:14:03.980 --> 1:14:06.360
 Like the chips inside Tesla are pretty cheap.

1:14:06.360 --> 1:14:08.080
 Like Mobileye has been doing this.

1:14:08.080 --> 1:14:10.840
 They're doing the classic work from the simplest thing.

1:14:10.840 --> 1:14:11.680
 Yeah.

1:14:11.680 --> 1:14:14.260
 I mean, 40 square millimeter chips.

1:14:14.260 --> 1:14:17.500
 And Nvidia, their solution had 800 millimeter chips

1:14:17.500 --> 1:14:19.180
 and two 200 millimeter chips.

1:14:19.180 --> 1:14:22.540
 And, you know, like boatloads are really expensive DRAMs.

1:14:22.540 --> 1:14:27.020
 And, you know, it's a really different approach.

1:14:27.020 --> 1:14:28.900
 And Mobileye fit the, let's say,

1:14:28.900 --> 1:14:31.300
 automotive cost and form factor.

1:14:31.300 --> 1:14:34.140
 And then they added features as it was economically viable.

1:14:34.140 --> 1:14:36.300
 And Nvidia said, take the biggest thing

1:14:36.300 --> 1:14:38.780
 and we're gonna go make it work.

1:14:38.780 --> 1:14:41.420
 You know, and that's also influenced like Waymo.

1:14:41.420 --> 1:14:43.660
 There's a whole bunch of autonomous startups

1:14:43.660 --> 1:14:46.820
 where they have a 5,000 watt server in their trunk.

1:14:46.820 --> 1:14:47.860
 Right.

1:14:47.860 --> 1:14:50.580
 But that's because they think, well, 5,000 watts

1:14:50.580 --> 1:14:52.300
 and, you know, $10,000 is okay

1:14:52.300 --> 1:14:54.740
 because it's replacing a driver.

1:14:54.740 --> 1:14:58.100
 Elon's approach was that port has to be cheap enough

1:14:58.100 --> 1:14:59.540
 to put it in every single Tesla,

1:14:59.540 --> 1:15:02.300
 whether they turn on autonomous driving or not.

1:15:02.300 --> 1:15:04.740
 Which, and Mobileye was like,

1:15:04.740 --> 1:15:06.820
 we need to fit in the bomb and, you know,

1:15:06.820 --> 1:15:09.460
 cost structure that car companies do.

1:15:09.460 --> 1:15:12.460
 So they may sell you a GPS for 1500 bucks,

1:15:12.460 --> 1:15:15.140
 but the bomb for that, it's like $25.

1:15:16.460 --> 1:15:20.140
 Well, and for Mobileye, it seems like neural networks

1:15:20.140 --> 1:15:22.980
 were not first class citizens, like the computation.

1:15:22.980 --> 1:15:24.660
 They didn't start out as a...

1:15:24.660 --> 1:15:26.100
 Yeah, it was a CV problem.

1:15:26.100 --> 1:15:27.100
 Yeah.

1:15:27.100 --> 1:15:29.940
 And did classic CV and found stoplights and lines.

1:15:29.940 --> 1:15:31.220
 And they were really good at it.

1:15:31.220 --> 1:15:33.060
 Yeah, and they never, I mean,

1:15:33.060 --> 1:15:34.140
 I don't know what's happening now,

1:15:34.140 --> 1:15:35.820
 but they never fully pivoted.

1:15:35.820 --> 1:15:37.980
 I mean, it's like, it's the Nvidia thing.

1:15:37.980 --> 1:15:39.740
 And then as opposed to,

1:15:39.740 --> 1:15:41.980
 so if you look at the new Tesla work,

1:15:41.980 --> 1:15:45.540
 it's like neural networks from the ground up, right?

1:15:45.540 --> 1:15:48.100
 Yeah, and even Tesla started with a lot of CV stuff in it

1:15:48.100 --> 1:15:50.340
 and Andrei's basically been eliminating it.

1:15:51.740 --> 1:15:54.340
 Move everything into the network.

1:15:54.340 --> 1:15:57.940
 So without, this isn't like confidential stuff,

1:15:57.940 --> 1:16:01.620
 but you sitting on a porch, looking over the world,

1:16:01.620 --> 1:16:03.740
 looking at the work that Andrei's doing,

1:16:03.740 --> 1:16:06.420
 that Elon's doing with Tesla Autopilot,

1:16:06.420 --> 1:16:08.780
 do you like the trajectory of where things are going

1:16:08.780 --> 1:16:09.620
 on the hardware side?

1:16:09.620 --> 1:16:10.900
 Well, they're making serious progress.

1:16:10.900 --> 1:16:14.100
 I like the videos of people driving the beta stuff.

1:16:14.100 --> 1:16:16.500
 I guess taking some pretty complicated intersections

1:16:16.500 --> 1:16:19.580
 and all that, but it's still an intervention per drive.

1:16:20.780 --> 1:16:23.020
 I mean, I have autopilot, the current autopilot,

1:16:23.020 --> 1:16:24.540
 my Tesla, I use it every day.

1:16:24.540 --> 1:16:26.340
 Do you have full self driving beta or no?

1:16:26.340 --> 1:16:27.180
 No.

1:16:27.180 --> 1:16:28.700
 So you like where this is going?

1:16:28.700 --> 1:16:29.540
 They're making progress.

1:16:29.540 --> 1:16:32.220
 It's taking longer than anybody thought.

1:16:32.220 --> 1:16:37.220
 You know, my wonder is, you know, hardware three,

1:16:37.380 --> 1:16:40.620
 is it enough computing off by two, off by five,

1:16:40.620 --> 1:16:42.380
 off by 10, off by a hundred?

1:16:42.380 --> 1:16:43.220
 Yeah.

1:16:43.220 --> 1:16:47.180
 And I thought it probably wasn't enough,

1:16:47.180 --> 1:16:49.820
 but they're doing pretty well with it now.

1:16:49.820 --> 1:16:50.660
 Yeah.

1:16:50.660 --> 1:16:53.380
 And one thing is the data set gets bigger,

1:16:53.380 --> 1:16:55.060
 the training gets better.

1:16:55.060 --> 1:16:58.420
 And then there's this interesting thing is you sort of train

1:16:58.420 --> 1:17:01.380
 and build an arbitrary size network that solves the problem.

1:17:01.380 --> 1:17:03.720
 And then you refactor the network down to the thing

1:17:03.720 --> 1:17:06.780
 that you can afford to ship, right?

1:17:06.780 --> 1:17:10.740
 So the goal isn't to build a network that fits in the phone.

1:17:10.740 --> 1:17:13.760
 It's to build something that actually works.

1:17:14.860 --> 1:17:17.700
 And then how do you make that most effective

1:17:17.700 --> 1:17:19.860
 on the hardware you have?

1:17:19.860 --> 1:17:21.700
 And they seem to be doing that much better

1:17:21.700 --> 1:17:23.580
 than a couple of years ago.

1:17:23.580 --> 1:17:25.820
 Well, the one really important thing is also

1:17:25.820 --> 1:17:28.700
 what they're doing well is how to iterate that quickly,

1:17:28.700 --> 1:17:31.780
 which means like it's not just about one time deployment,

1:17:31.780 --> 1:17:34.220
 one building, it's constantly iterating the network

1:17:34.220 --> 1:17:37.540
 and trying to automate as many steps as possible, right?

1:17:37.540 --> 1:17:41.700
 And that's actually the principles of the Software 2.0,

1:17:41.700 --> 1:17:46.700
 like you mentioned with Andre is it's not just,

1:17:46.980 --> 1:17:48.300
 I mean, I don't know what the actual,

1:17:48.300 --> 1:17:50.900
 his description of Software 2.0 is.

1:17:50.900 --> 1:17:53.520
 If it's just high level philosophical or their specifics,

1:17:53.520 --> 1:17:57.100
 but the interesting thing about what that actually looks

1:17:57.100 --> 1:18:01.860
 in the real world is it's that what I think Andre calls

1:18:01.860 --> 1:18:05.740
 the data engine, it's like it's the iterative improvement

1:18:05.740 --> 1:18:06.580
 of the thing.

1:18:06.580 --> 1:18:10.500
 You have a neural network that does stuff,

1:18:10.500 --> 1:18:12.740
 fails on a bunch of things and learns from it

1:18:12.740 --> 1:18:13.620
 over and over and over.

1:18:13.620 --> 1:18:15.900
 So you're constantly discovering edge cases.

1:18:15.900 --> 1:18:19.920
 So it's very much about like data engineering,

1:18:19.920 --> 1:18:23.060
 like figuring out, it's kind of what you were talking about

1:18:23.060 --> 1:18:25.740
 with TestTorrent is you have the data landscape.

1:18:25.740 --> 1:18:27.580
 And you have to walk along that data landscape

1:18:27.580 --> 1:18:32.580
 in a way that is constantly improving the neural network.

1:18:32.600 --> 1:18:35.820
 And that feels like that's the central piece of it.

1:18:35.820 --> 1:18:37.140
 And there's two pieces of it.

1:18:37.140 --> 1:18:40.900
 Like you find edge cases that don't work

1:18:40.900 --> 1:18:42.340
 and then you define something that goes,

1:18:42.340 --> 1:18:44.220
 get your data for that.

1:18:44.220 --> 1:18:45.820
 But then the other constraint is whether you have

1:18:45.820 --> 1:18:46.940
 to label it or not.

1:18:46.940 --> 1:18:49.860
 Like the amazing thing about like the GPT3 stuff

1:18:49.860 --> 1:18:51.540
 is it's unsupervised.

1:18:51.540 --> 1:18:53.300
 So there's essentially infinite amount of data.

1:18:53.300 --> 1:18:56.260
 Now there's obviously infinite amount of data available

1:18:56.260 --> 1:18:59.220
 from cars of people successfully driving.

1:18:59.220 --> 1:19:02.060
 But the current pipelines are mostly running

1:19:02.060 --> 1:19:04.660
 on labeled data, which is human limited.

1:19:04.660 --> 1:19:07.540
 So when that becomes unsupervised,

1:19:09.040 --> 1:19:12.620
 it'll create unlimited amount of data,

1:19:12.620 --> 1:19:14.240
 which then they'll scale.

1:19:14.240 --> 1:19:16.220
 Now the networks that may use that data

1:19:16.220 --> 1:19:18.260
 might be way too big for cars,

1:19:18.260 --> 1:19:20.020
 but then there'll be the transformation from now

1:19:20.020 --> 1:19:22.360
 we have unlimited data, I know exactly what I want.

1:19:22.360 --> 1:19:25.820
 Now can I turn that into something that fits in the car?

1:19:25.820 --> 1:19:29.220
 And that process is gonna happen all over the place.

1:19:29.220 --> 1:19:30.700
 Every time you get to the place where you have

1:19:30.700 --> 1:19:34.100
 unlimited data, and that's what software 2.0 is about,

1:19:34.100 --> 1:19:36.780
 unlimited data training networks to do stuff

1:19:37.980 --> 1:19:40.700
 without humans writing code to do it.

1:19:40.700 --> 1:19:42.980
 And ultimately also trying to discover,

1:19:42.980 --> 1:19:46.540
 like you're saying, the self supervised formulation

1:19:46.540 --> 1:19:47.380
 of the problem.

1:19:47.380 --> 1:19:49.660
 So the unsupervised formulation of the problem.

1:19:49.660 --> 1:19:53.540
 Like in driving, there's this really interesting thing,

1:19:53.540 --> 1:19:58.140
 which is you look at a scene that's before you,

1:19:58.140 --> 1:20:01.900
 and you have data about what a successful human driver did

1:20:01.900 --> 1:20:04.460
 in that scene one second later.

1:20:04.460 --> 1:20:06.620
 It's a little piece of data that you can use

1:20:06.620 --> 1:20:09.380
 just like with GPT3 as training.

1:20:09.380 --> 1:20:12.380
 Currently, even though Tesla says they're using that,

1:20:12.380 --> 1:20:15.980
 it's an open question to me, how far can you,

1:20:15.980 --> 1:20:17.420
 can you solve all of the driving

1:20:17.420 --> 1:20:20.940
 with just that self supervised piece of data?

1:20:20.940 --> 1:20:23.380
 And like, I think.

1:20:23.380 --> 1:20:25.540
 Well, that's what Common AI is doing.

1:20:25.540 --> 1:20:26.860
 That's what Common AI is doing,

1:20:26.860 --> 1:20:29.980
 but the question is how much data.

1:20:29.980 --> 1:20:33.580
 So what Common AI doesn't have is as good

1:20:33.580 --> 1:20:35.940
 of a data engine, for example, as Tesla does.

1:20:35.940 --> 1:20:38.580
 That's where the, like the organization of the data.

1:20:39.820 --> 1:20:41.900
 I mean, as far as I know, I haven't talked to George,

1:20:41.900 --> 1:20:44.580
 but they do have the data.

1:20:44.580 --> 1:20:47.860
 The question is how much data is needed,

1:20:47.860 --> 1:20:49.940
 because we say infinite very loosely here.

1:20:51.420 --> 1:20:54.380
 And then the other question, which you said,

1:20:54.380 --> 1:20:57.700
 I don't know if you think it's still an open question is,

1:20:57.700 --> 1:20:59.420
 are we on the right order of magnitude

1:20:59.420 --> 1:21:02.020
 for the compute necessary?

1:21:02.020 --> 1:21:04.940
 That is this, is it like what Elon said,

1:21:04.940 --> 1:21:07.140
 this chip that's in there now is enough

1:21:07.140 --> 1:21:08.620
 to do full self driving,

1:21:08.620 --> 1:21:10.820
 or do we need another order of magnitude?

1:21:10.820 --> 1:21:13.300
 I think nobody actually knows the answer to that question.

1:21:13.300 --> 1:21:15.300
 I like the confidence that Elon has, but.

1:21:16.260 --> 1:21:17.820
 Yeah, we'll see.

1:21:17.820 --> 1:21:20.180
 There's another funny thing is you don't learn to drive

1:21:20.180 --> 1:21:22.260
 with infinite amounts of data.

1:21:22.260 --> 1:21:24.300
 You learn to drive with an intellectual framework

1:21:24.300 --> 1:21:28.060
 that understands physics and color and horizontal surfaces

1:21:28.060 --> 1:21:32.060
 and laws and roads and all your experience

1:21:33.980 --> 1:21:36.700
 from manipulating your environment.

1:21:36.700 --> 1:21:39.020
 Like, look, there's so many factors go into that.

1:21:39.020 --> 1:21:40.660
 So then when you learn to drive,

1:21:40.660 --> 1:21:44.380
 like driving is a subset of this conceptual framework

1:21:44.380 --> 1:21:46.300
 that you have, right?

1:21:46.300 --> 1:21:48.580
 And so with self driving cars right now,

1:21:48.580 --> 1:21:51.540
 we're teaching them to drive with driving data.

1:21:51.540 --> 1:21:53.580
 You never teach a human to do that.

1:21:53.580 --> 1:21:55.780
 You teach a human all kinds of interesting things,

1:21:55.780 --> 1:21:59.340
 like language, like don't do that, watch out.

1:21:59.340 --> 1:22:01.020
 There's all kinds of stuff going on.

1:22:01.020 --> 1:22:02.900
 Well, this is where you, I think previous time

1:22:02.900 --> 1:22:07.300
 we talked about where you poetically disagreed

1:22:07.300 --> 1:22:10.300
 with my naive notion about humans.

1:22:10.300 --> 1:22:13.700
 I just think that humans will make

1:22:13.700 --> 1:22:15.700
 this whole driving thing really difficult.

1:22:15.700 --> 1:22:17.180
 Yeah, all right.

1:22:17.180 --> 1:22:19.460
 I said, humans don't move that slow.

1:22:19.460 --> 1:22:20.820
 It's a ballistics problem.

1:22:20.820 --> 1:22:22.700
 It's a ballistics, humans are a ballistics problem,

1:22:22.700 --> 1:22:24.060
 which is like poetry to me.

1:22:24.060 --> 1:22:26.180
 It's very possible that in driving

1:22:26.180 --> 1:22:28.460
 they're indeed purely a ballistics problem.

1:22:28.460 --> 1:22:30.860
 And I think that's probably the right way to think about it.

1:22:30.860 --> 1:22:34.420
 But I still, they still continue to surprise me,

1:22:34.420 --> 1:22:36.940
 those damn pedestrians, the cyclists,

1:22:36.940 --> 1:22:39.340
 other humans in other cars and.

1:22:39.340 --> 1:22:41.180
 Yeah, but it's gonna be one of these compensating things.

1:22:41.180 --> 1:22:43.980
 So like when you're driving,

1:22:43.980 --> 1:22:46.860
 you have an intuition about what humans are going to do,

1:22:46.860 --> 1:22:49.660
 but you don't have 360 cameras and radars

1:22:49.660 --> 1:22:51.140
 and you have an attention problem.

1:22:51.140 --> 1:22:55.100
 So the self driving car comes in with no attention problem,

1:22:55.100 --> 1:22:58.780
 360 cameras right now, a bunch of other features.

1:22:58.780 --> 1:23:01.980
 So they'll wipe out a whole class of accidents, right?

1:23:01.980 --> 1:23:05.780
 And emergency braking with radar

1:23:05.780 --> 1:23:07.980
 and especially as it gets AI enhanced

1:23:07.980 --> 1:23:10.940
 will eliminate collisions, right?

1:23:10.940 --> 1:23:12.060
 But then you have the other problems

1:23:12.060 --> 1:23:13.860
 of these unexpected things where

1:23:13.860 --> 1:23:15.600
 you think your human intuition is helping,

1:23:15.600 --> 1:23:19.580
 but then the cars also have a set of hardware features

1:23:19.580 --> 1:23:21.500
 that you're not even close to.

1:23:21.500 --> 1:23:25.380
 And the key thing of course is if you wipe out

1:23:25.380 --> 1:23:27.020
 a huge number of kind of accidents,

1:23:27.020 --> 1:23:30.240
 then it might be just way safer than a human driver,

1:23:30.240 --> 1:23:32.980
 even though, even if humans are still a problem,

1:23:32.980 --> 1:23:34.740
 that's hard to figure out.

1:23:34.740 --> 1:23:36.180
 Yeah, that's probably what will happen.

1:23:36.180 --> 1:23:38.820
 Those autonomous cars will have a small number of accidents

1:23:38.820 --> 1:23:41.060
 humans would have avoided, but they'll wipe,

1:23:41.060 --> 1:23:43.840
 they'll get rid of the bulk of them.

1:23:43.840 --> 1:23:48.660
 What do you think about like Tesla's dojo efforts

1:23:48.660 --> 1:23:51.140
 or it can be bigger than Tesla in general.

1:23:51.140 --> 1:23:55.140
 It's kind of like the tense torrent trying to innovate,

1:23:55.140 --> 1:23:58.160
 like this is the dichotomy, like should a company

1:23:58.160 --> 1:24:00.380
 try to from scratch build its own

1:24:00.380 --> 1:24:03.180
 neural network training hardware?

1:24:03.180 --> 1:24:04.260
 Well, first of all, I think it's great.

1:24:04.260 --> 1:24:06.840
 So we need lots of experiments, right?

1:24:06.840 --> 1:24:09.460
 And there's lots of startups working on this

1:24:09.460 --> 1:24:11.580
 and they're pursuing different things.

1:24:11.580 --> 1:24:14.580
 I was there when we started dojo and it was sort of like,

1:24:14.580 --> 1:24:17.980
 what's the unconstrained computer solution

1:24:17.980 --> 1:24:21.760
 to go do very large training problems?

1:24:21.760 --> 1:24:24.520
 And then there's fun stuff like, we said,

1:24:24.520 --> 1:24:27.220
 well, we have this 10,000 watt board to cool.

1:24:27.220 --> 1:24:29.140
 Well, you go talk to guys at SpaceX

1:24:29.140 --> 1:24:31.200
 and they think 10,000 watts is a really small number,

1:24:31.200 --> 1:24:32.740
 not a big number.

1:24:32.740 --> 1:24:35.300
 And there's brilliant people working on it.

1:24:35.300 --> 1:24:37.300
 I'm curious to see how it'll come out.

1:24:37.300 --> 1:24:39.840
 I couldn't tell you, I know it pivoted

1:24:39.840 --> 1:24:41.660
 a few times since I left, so.

1:24:41.660 --> 1:24:44.540
 So the cooling does seem to be a big problem.

1:24:44.540 --> 1:24:47.640
 I do like what Elon said about it, which is like,

1:24:47.640 --> 1:24:50.380
 we don't wanna do the thing unless it's way better

1:24:50.380 --> 1:24:52.980
 than the alternative, whatever the alternative is.

1:24:52.980 --> 1:24:57.620
 So it has to be way better than like racks or GPUs.

1:24:57.620 --> 1:25:00.100
 Yeah, and the other thing is just like,

1:25:00.100 --> 1:25:03.900
 you know, the Tesla autonomous driving hardware,

1:25:03.900 --> 1:25:06.620
 it was only serving one software stack.

1:25:06.620 --> 1:25:08.040
 And the hardware team and the software team

1:25:08.040 --> 1:25:09.880
 were tightly coupled.

1:25:09.880 --> 1:25:12.160
 You know, if you're building a general purpose AI solution,

1:25:12.160 --> 1:25:14.280
 then you know, there's so many different customers

1:25:14.280 --> 1:25:16.420
 with so many different needs.

1:25:16.420 --> 1:25:19.780
 Now, something Andre said is, I think this is amazing.

1:25:19.780 --> 1:25:24.660
 10 years ago, like vision, recommendation, language,

1:25:24.660 --> 1:25:27.140
 were completely different disciplines.

1:25:27.140 --> 1:25:29.740
 He said, the people literally couldn't talk to each other.

1:25:29.740 --> 1:25:32.580
 And three years ago, it was all neural networks,

1:25:32.580 --> 1:25:34.860
 but the very different neural networks.

1:25:34.860 --> 1:25:37.740
 And recently, it's converging on one set of networks.

1:25:37.740 --> 1:25:40.460
 They vary a lot in size, obviously, they vary in data,

1:25:40.460 --> 1:25:43.820
 vary in outputs, but the technology has converged

1:25:43.820 --> 1:25:44.780
 a good bit.

1:25:44.780 --> 1:25:47.420
 Yeah, these transformers behind GPT3,

1:25:47.420 --> 1:25:48.980
 it seems like they could be applied to video,

1:25:48.980 --> 1:25:51.020
 they could be applied to a lot of, and it's like,

1:25:51.020 --> 1:25:52.500
 and they're all really simple.

1:25:52.500 --> 1:25:56.380
 And it was like they literally replace letters with pixels.

1:25:56.380 --> 1:25:58.780
 It does vision, it's amazing.

1:25:58.780 --> 1:26:02.100
 And then size actually improves the thing.

1:26:02.100 --> 1:26:04.420
 So the bigger it gets, the more compute you throw at it,

1:26:04.420 --> 1:26:05.660
 the better it gets.

1:26:05.660 --> 1:26:08.320
 And the more data you have, the better it gets.

1:26:08.320 --> 1:26:11.220
 So then you start to wonder, well,

1:26:11.220 --> 1:26:12.540
 is that a fundamental thing?

1:26:12.540 --> 1:26:16.580
 Or is this just another step to some fundamental understanding

1:26:16.580 --> 1:26:18.820
 about this kind of computation?

1:26:18.820 --> 1:26:20.300
 Which is really interesting.

1:26:20.300 --> 1:26:22.260
 Us humans don't want to believe that that kind of thing

1:26:22.260 --> 1:26:24.420
 will achieve conceptual understandings, you were saying,

1:26:24.420 --> 1:26:27.000
 like you'll figure out physics, but maybe it will.

1:26:27.000 --> 1:26:27.840
 Maybe.

1:26:27.840 --> 1:26:29.360
 Maybe it will.

1:26:29.360 --> 1:26:31.060
 Well, it's worse than that.

1:26:31.060 --> 1:26:33.780
 It'll understand physics in ways that we can't understand.

1:26:33.780 --> 1:26:36.340
 I like your Stephen Wolfram talk where he said,

1:26:36.340 --> 1:26:38.020
 you know, there's three generations of physics.

1:26:38.020 --> 1:26:40.100
 There was physics by reasoning.

1:26:40.100 --> 1:26:42.620
 Well, big things should fall faster than small things,

1:26:42.620 --> 1:26:43.460
 right?

1:26:43.460 --> 1:26:44.280
 That's reasoning.

1:26:44.280 --> 1:26:46.940
 And then there's physics by equations.

1:26:46.940 --> 1:26:49.620
 Like, you know, but the number of programs in the world

1:26:49.620 --> 1:26:51.980
 that are solved with a single equation is relatively low.

1:26:51.980 --> 1:26:53.660
 Almost all programs have, you know,

1:26:53.660 --> 1:26:56.860
 more than one line of code, maybe 100 million lines of code.

1:26:56.860 --> 1:26:59.980
 So he said, then now we're going to physics by equation,

1:26:59.980 --> 1:27:02.580
 which is his project, which is cool.

1:27:02.580 --> 1:27:07.260
 I might point out there was two generations of physics

1:27:07.260 --> 1:27:10.240
 before reasoning habit.

1:27:10.240 --> 1:27:12.360
 Like all animals, you know, know things fall

1:27:12.360 --> 1:27:15.300
 and, you know, birds fly and, you know, predators know

1:27:15.300 --> 1:27:17.360
 how to, you know, solve a differential equation

1:27:17.360 --> 1:27:22.360
 to cut off a accelerating, you know, curving animal path.

1:27:22.360 --> 1:27:27.360
 And then there was, you know, the gods did it, right?

1:27:28.400 --> 1:27:29.560
 So, right.

1:27:29.560 --> 1:27:31.620
 So there was, you know, there's five generations.

1:27:31.620 --> 1:27:35.960
 Now, software 2.0 says programming things

1:27:35.960 --> 1:27:37.320
 is not the last step.

1:27:38.320 --> 1:27:39.160
 Data.

1:27:39.160 --> 1:27:44.060
 So there's going to be a physics past Stephen Wolfram's con.

1:27:44.060 --> 1:27:47.520
 That's not explainable to us humans.

1:27:47.520 --> 1:27:51.060
 And actually there's no reason that I can see

1:27:51.060 --> 1:27:53.280
 well that even that's the limit.

1:27:53.280 --> 1:27:55.600
 Like, there's something beyond that.

1:27:55.600 --> 1:27:57.080
 I mean, they're usually, like, usually when you have

1:27:57.080 --> 1:27:59.620
 this hierarchy, it's not like, well, if you have this step

1:27:59.620 --> 1:28:01.840
 and this step and this step and they're all qualitatively

1:28:01.840 --> 1:28:05.100
 different and conceptually different, it's not obvious why,

1:28:05.100 --> 1:28:07.360
 you know, six is the right number of hierarchy steps

1:28:07.360 --> 1:28:09.200
 and not seven or eight or.

1:28:09.200 --> 1:28:12.120
 Well, then it's probably impossible for us to,

1:28:12.120 --> 1:28:15.920
 to comprehend something that's beyond the thing

1:28:15.920 --> 1:28:17.120
 that's not explainable.

1:28:18.280 --> 1:28:19.800
 Yeah.

1:28:19.800 --> 1:28:21.760
 But the thing that, you know, understands the thing

1:28:21.760 --> 1:28:25.120
 that's not explainable to us will conceive the next one.

1:28:25.120 --> 1:28:28.300
 And like, I'm not sure why there's a limit to it.

1:28:30.920 --> 1:28:31.760
 Click your brain hurts.

1:28:31.760 --> 1:28:32.700
 That's a sad story.

1:28:34.840 --> 1:28:37.600
 If we look at our own brain, which is an interesting

1:28:38.560 --> 1:28:42.600
 illustrative example in your work with test story

1:28:42.600 --> 1:28:46.160
 and trying to design deep learning architectures,

1:28:46.160 --> 1:28:50.080
 do you think about the brain at all?

1:28:50.080 --> 1:28:53.500
 Maybe from a hardware designer perspective,

1:28:53.500 --> 1:28:56.240
 if you could change something about the brain,

1:28:56.240 --> 1:28:58.200
 what would you change or do?

1:28:58.200 --> 1:28:59.040
 Funny question.

1:29:00.120 --> 1:29:00.960
 Like, how would you do it?

1:29:00.960 --> 1:29:02.380
 So your brain is really weird.

1:29:02.380 --> 1:29:04.440
 Like, you know, your cerebral cortex where we think

1:29:04.440 --> 1:29:06.400
 we do most of our thinking is what,

1:29:06.400 --> 1:29:08.660
 like six or seven neurons thick?

1:29:08.660 --> 1:29:09.500
 Yeah.

1:29:09.500 --> 1:29:10.320
 Like, that's weird.

1:29:10.320 --> 1:29:13.240
 Like all the big networks are way bigger than that.

1:29:13.240 --> 1:29:14.360
 Like way deeper.

1:29:14.360 --> 1:29:16.200
 So that seems odd.

1:29:16.200 --> 1:29:19.200
 And then, you know, when you're thinking if it's,

1:29:19.200 --> 1:29:21.840
 if the input generates a result you can lose,

1:29:21.840 --> 1:29:22.840
 it goes really fast.

1:29:22.840 --> 1:29:25.280
 But if it can't, that generates an output

1:29:25.280 --> 1:29:27.120
 that's interesting, which turns into an input

1:29:27.120 --> 1:29:29.840
 and then your brain to the point where you mold things

1:29:29.840 --> 1:29:31.560
 over for days and how many trips

1:29:31.560 --> 1:29:33.440
 through your brain is that, right?

1:29:33.440 --> 1:29:36.120
 Like it's, you know, 300 milliseconds or something

1:29:36.120 --> 1:29:37.880
 to get through seven levels of neurons.

1:29:37.880 --> 1:29:39.880
 I forget the number exactly.

1:29:39.880 --> 1:29:43.320
 But then it does it over and over and over as it searches.

1:29:43.320 --> 1:29:46.160
 And the brain clearly looks like some kind of graph

1:29:46.160 --> 1:29:48.200
 because you have a neuron with connections

1:29:48.200 --> 1:29:49.240
 and it talks to other ones

1:29:49.240 --> 1:29:52.400
 and it's locally very computationally intense,

1:29:52.400 --> 1:29:55.520
 but it's also does sparse computations

1:29:55.520 --> 1:29:57.840
 across a pretty big area.

1:29:57.840 --> 1:30:00.680
 There's a lot of messy biological type of things

1:30:00.680 --> 1:30:03.760
 and it's meaning like, first of all,

1:30:03.760 --> 1:30:06.040
 there's mechanical, chemical and electrical signals.

1:30:06.040 --> 1:30:07.480
 It's all that's going on.

1:30:07.480 --> 1:30:12.400
 Then there's the asynchronicity of signals.

1:30:12.400 --> 1:30:14.720
 And there's like, there's just a lot of variability

1:30:14.720 --> 1:30:16.520
 that seems continuous and messy

1:30:16.520 --> 1:30:18.600
 and just the mess of biology.

1:30:18.600 --> 1:30:22.640
 And it's unclear whether that's a good thing

1:30:22.640 --> 1:30:26.320
 or it's a bad thing, because if it's a good thing

1:30:26.320 --> 1:30:29.240
 that we need to run the entirety of the evolution,

1:30:29.240 --> 1:30:31.560
 well, we're gonna have to start with basic bacteria

1:30:31.560 --> 1:30:32.400
 to create something.

1:30:32.400 --> 1:30:34.000
 So imagine we could control,

1:30:34.000 --> 1:30:35.640
 you could build a brain with 10 layers.

1:30:35.640 --> 1:30:37.360
 Would that be better or worse?

1:30:37.360 --> 1:30:39.800
 Or more connections or less connections,

1:30:39.800 --> 1:30:43.400
 or we don't know to what level our brains are optimized.

1:30:44.240 --> 1:30:45.480
 But if I was changing things,

1:30:45.480 --> 1:30:49.360
 like you can only hold like seven numbers in your head.

1:30:49.360 --> 1:30:51.840
 Like why not a hundred or a million?

1:30:51.840 --> 1:30:53.680
 Never thought of that.

1:30:53.680 --> 1:30:56.800
 And why can't we have like a floating point processor

1:30:56.800 --> 1:30:58.640
 that can compute anything we want

1:30:59.560 --> 1:31:01.240
 and see it all properly?

1:31:01.240 --> 1:31:03.120
 Like that would be kind of fun.

1:31:03.120 --> 1:31:05.760
 And why can't we see in four or eight dimensions?

1:31:05.760 --> 1:31:10.040
 Because 3D is kind of a drag.

1:31:10.040 --> 1:31:11.600
 Like all the hard mass transforms

1:31:11.600 --> 1:31:13.960
 are up in multiple dimensions.

1:31:13.960 --> 1:31:16.560
 So you could imagine a brain architecture

1:31:16.560 --> 1:31:21.120
 that you could enhance with a whole bunch of features

1:31:21.120 --> 1:31:24.440
 that would be really useful for thinking about things.

1:31:24.440 --> 1:31:26.880
 It's possible that the limitations you're describing

1:31:26.880 --> 1:31:29.880
 are actually essential for like the constraints

1:31:29.880 --> 1:31:34.000
 are essential for creating like the depth of intelligence.

1:31:34.000 --> 1:31:37.200
 Like that, the ability to reason.

1:31:38.360 --> 1:31:39.200
 It's hard to say

1:31:39.200 --> 1:31:42.840
 because like your brain is clearly a parallel processor.

1:31:44.360 --> 1:31:46.200
 10 billion neurons talking to each other

1:31:46.200 --> 1:31:48.440
 at a relatively low clock rate.

1:31:48.440 --> 1:31:50.480
 But it produces something

1:31:50.480 --> 1:31:52.640
 that looks like a serial thought process.

1:31:52.640 --> 1:31:54.720
 It's a serial narrative in your head.

1:31:54.720 --> 1:31:55.560
 That's true.

1:31:55.560 --> 1:31:59.040
 But then there are people famously who are visual thinkers.

1:31:59.040 --> 1:32:02.320
 Like I think I'm a relatively visual thinker.

1:32:02.320 --> 1:32:05.120
 I can imagine any object and rotate it in my head

1:32:05.120 --> 1:32:06.440
 and look at it.

1:32:06.440 --> 1:32:07.360
 And there are people who say

1:32:07.360 --> 1:32:09.640
 they don't think that way at all.

1:32:09.640 --> 1:32:12.440
 And recently I read an article about people

1:32:12.440 --> 1:32:16.240
 who say they don't have a voice in their head.

1:32:16.240 --> 1:32:18.520
 They can talk.

1:32:18.520 --> 1:32:19.880
 But when they, you know, it's like,

1:32:19.880 --> 1:32:21.040
 well, what are you thinking?

1:32:21.040 --> 1:32:23.280
 No, they'll describe something that's visual.

1:32:24.400 --> 1:32:26.480
 So that's curious.

1:32:26.480 --> 1:32:31.480
 Now, if you're saying,

1:32:31.760 --> 1:32:34.960
 if we dedicated more hardware to holding information,

1:32:34.960 --> 1:32:37.960
 like, you know, 10 numbers or a million numbers,

1:32:37.960 --> 1:32:41.680
 like would that distract us from our ability

1:32:41.680 --> 1:32:44.760
 to form this kind of singular identity?

1:32:44.760 --> 1:32:46.960
 Like it dissipates somehow.

1:32:46.960 --> 1:32:49.400
 But maybe, you know, future humans

1:32:49.400 --> 1:32:50.720
 will have many identities

1:32:50.720 --> 1:32:53.120
 that have some higher level organization

1:32:53.120 --> 1:32:55.620
 but can actually do lots more things in parallel.

1:32:55.620 --> 1:32:57.880
 Yeah, there's no reason, if we're thinking modularly,

1:32:57.880 --> 1:33:00.280
 there's no reason we can't have multiple consciousnesses

1:33:00.280 --> 1:33:01.520
 in one brain.

1:33:01.520 --> 1:33:03.720
 Yeah, and maybe there's some way to make it faster

1:33:03.720 --> 1:33:07.920
 so that the, you know, the area of the computation

1:33:07.920 --> 1:33:12.920
 could still have a unified feel to it

1:33:13.240 --> 1:33:15.720
 while still having way more ability

1:33:15.720 --> 1:33:17.600
 to do parallel stuff at the same time.

1:33:17.600 --> 1:33:19.040
 Could definitely be improved.

1:33:19.040 --> 1:33:20.040
 Could be improved?

1:33:20.040 --> 1:33:20.860
 Yeah.

1:33:20.860 --> 1:33:22.920
 Okay, well, it's pretty good right now.

1:33:22.920 --> 1:33:24.680
 Actually, people don't give it enough credit.

1:33:24.680 --> 1:33:25.880
 The thing is pretty nice.

1:33:25.880 --> 1:33:29.240
 The, you know, the fact that the right ends

1:33:29.240 --> 1:33:32.920
 seem to be, give a nice, like,

1:33:32.920 --> 1:33:37.920
 spark of beauty to the whole experience.

1:33:37.920 --> 1:33:38.760
 I don't know.

1:33:38.760 --> 1:33:40.280
 I don't know if it can be improved easily.

1:33:40.280 --> 1:33:42.480
 It could be more beautiful.

1:33:42.480 --> 1:33:44.320
 I don't know how, I, what?

1:33:44.320 --> 1:33:46.280
 What do you mean, what do you mean how?

1:33:46.280 --> 1:33:48.280
 All the ways you can't imagine.

1:33:48.280 --> 1:33:49.500
 No, but that's the whole point.

1:33:49.500 --> 1:33:51.080
 I wouldn't be able to,

1:33:51.080 --> 1:33:53.200
 the fact that I can imagine ways

1:33:53.200 --> 1:33:55.880
 in which it could be more beautiful means.

1:33:55.880 --> 1:33:59.400
 So do you know, you know, Ian Banks, his stories?

1:33:59.400 --> 1:34:03.600
 So the super smart AIs there live,

1:34:03.600 --> 1:34:06.240
 mostly live in the world of what they call infinite fun

1:34:07.540 --> 1:34:12.200
 because they can create arbitrary worlds.

1:34:12.200 --> 1:34:14.480
 So they interact in, you know, the story has it.

1:34:14.480 --> 1:34:16.720
 They interact in the normal world and they're very smart

1:34:16.720 --> 1:34:18.560
 and they can do all kinds of stuff.

1:34:18.560 --> 1:34:20.420
 And, you know, a given mind can, you know,

1:34:20.420 --> 1:34:22.040
 talk to a million humans at the same time

1:34:22.040 --> 1:34:24.680
 because we're very slow and for reasons,

1:34:24.680 --> 1:34:26.280
 you know, artificial, the story,

1:34:26.280 --> 1:34:28.240
 they're interested in people and doing stuff,

1:34:28.240 --> 1:34:33.000
 but they mostly live in this other land of thinking.

1:34:33.000 --> 1:34:36.520
 My inclination is to think that the ability

1:34:36.520 --> 1:34:40.120
 to create infinite fun will not be so fun.

1:34:41.200 --> 1:34:42.400
 That's sad.

1:34:42.400 --> 1:34:43.800
 Well, there are so many things to do.

1:34:43.800 --> 1:34:47.600
 Imagine being able to make a star move planets around.

1:34:47.600 --> 1:34:50.080
 Yeah, yeah, but because we can imagine that

1:34:50.080 --> 1:34:53.360
 is why life is fun, if we actually were able to do it,

1:34:53.360 --> 1:34:55.040
 it would be a slippery slope

1:34:55.040 --> 1:34:56.720
 where fun wouldn't even have a meaning

1:34:56.720 --> 1:35:00.320
 because we just consistently desensitize ourselves

1:35:00.320 --> 1:35:03.280
 by the infinite amounts of fun we're having.

1:35:04.120 --> 1:35:07.480
 And the sadness, the dark stuff is what makes it fun.

1:35:07.480 --> 1:35:10.440
 I think that could be the Russian.

1:35:10.440 --> 1:35:12.400
 It could be the fun makes it fun

1:35:12.400 --> 1:35:14.700
 and the sadness makes it bittersweet.

1:35:16.560 --> 1:35:17.400
 Yeah, that's true.

1:35:17.400 --> 1:35:20.560
 Fun could be the thing that makes it fun.

1:35:20.560 --> 1:35:22.560
 So what do you think about the expansion,

1:35:22.560 --> 1:35:23.920
 not through the biology side,

1:35:23.920 --> 1:35:27.220
 but through the BCI, the brain computer interfaces?

1:35:27.220 --> 1:35:30.120
 Yeah, you got a chance to check out the Neuralink stuff.

1:35:30.120 --> 1:35:31.520
 It's super interesting.

1:35:31.520 --> 1:35:36.520
 Like humans like our thoughts to manifest as action.

1:35:37.600 --> 1:35:39.560
 You know, like as a kid, you know,

1:35:39.560 --> 1:35:41.720
 like shooting a rifle was super fun,

1:35:41.720 --> 1:35:44.320
 driving a mini bike, doing things.

1:35:44.320 --> 1:35:46.160
 And then computer games, I think,

1:35:46.160 --> 1:35:47.920
 for a lot of kids became the thing

1:35:47.920 --> 1:35:50.360
 where they can do what they want.

1:35:50.360 --> 1:35:53.600
 They can fly a plane, they can do this, they can do this.

1:35:53.600 --> 1:35:55.860
 But you have to have this physical interaction.

1:35:55.860 --> 1:36:00.860
 Now imagine, you could just imagine stuff and it happens.

1:36:03.280 --> 1:36:06.620
 Like really richly and interestingly.

1:36:06.620 --> 1:36:08.080
 Like we kind of do that when we dream.

1:36:08.080 --> 1:36:12.040
 Like dreams are funny because like if you have some control

1:36:12.040 --> 1:36:13.520
 or awareness in your dreams,

1:36:13.520 --> 1:36:16.380
 like it's very realistic looking,

1:36:16.380 --> 1:36:19.420
 or not realistic looking, it depends on the dream.

1:36:19.420 --> 1:36:21.240
 But you can also manipulate that.

1:36:22.500 --> 1:36:26.220
 And you know, what's possible there is odd.

1:36:26.220 --> 1:36:29.860
 And the fact that nobody understands it, it's hilarious, but.

1:36:29.860 --> 1:36:31.780
 Do you think it's possible to expand

1:36:31.780 --> 1:36:34.060
 that capability through computing?

1:36:34.060 --> 1:36:35.340
 Sure.

1:36:35.340 --> 1:36:36.500
 Is there some interesting,

1:36:36.500 --> 1:36:38.420
 so from a hardware designer perspective,

1:36:38.420 --> 1:36:41.660
 is there, do you think it'll present totally new challenges

1:36:41.660 --> 1:36:44.100
 in the kind of hardware required that like,

1:36:44.100 --> 1:36:47.740
 so this hardware isn't standalone computing.

1:36:47.740 --> 1:36:49.540
 Well, this is not working with the brain.

1:36:49.540 --> 1:36:52.860
 So today, computer games are rendered by GPUs.

1:36:52.860 --> 1:36:53.700
 Right.

1:36:53.700 --> 1:36:56.840
 Right, so, but you've seen the GAN stuff, right?

1:36:56.840 --> 1:37:00.900
 Where trained neural networks render realistic images,

1:37:00.900 --> 1:37:03.740
 but there's no pixels, no triangles, no shaders,

1:37:03.740 --> 1:37:05.400
 no light maps, no nothing.

1:37:05.400 --> 1:37:09.540
 So the future of graphics is probably AI, right?

1:37:09.540 --> 1:37:10.380
 Yes.

1:37:10.380 --> 1:37:14.820
 AI is heavily trained by lots of real data, right?

1:37:14.820 --> 1:37:19.820
 So if you have an interface with a AI renderer, right?

1:37:20.340 --> 1:37:23.420
 So if you say render a cat, it won't say,

1:37:23.420 --> 1:37:25.060
 well, how tall's the cat and how big it,

1:37:25.060 --> 1:37:26.260
 you know, it'll render a cat.

1:37:26.260 --> 1:37:28.220
 And you might say, oh, a little bigger, a little smaller,

1:37:28.220 --> 1:37:31.060
 you know, make it a tabby, shorter hair.

1:37:31.060 --> 1:37:32.900
 You know, like you could tweak it.

1:37:32.900 --> 1:37:36.500
 Like the amount of data you'll have to send

1:37:36.500 --> 1:37:40.120
 to interact with a very powerful AI renderer

1:37:40.120 --> 1:37:41.420
 could be low.

1:37:41.420 --> 1:37:44.780
 But the question is brain computer interfaces

1:37:44.780 --> 1:37:47.860
 would need to render not onto a screen,

1:37:47.860 --> 1:37:51.980
 but render onto the brain and like directly

1:37:51.980 --> 1:37:52.820
 so that there's a bandwidth.

1:37:52.820 --> 1:37:53.880
 Well, it could do it both ways.

1:37:53.880 --> 1:37:56.020
 I mean, our eyes are really good sensors.

1:37:56.020 --> 1:37:58.580
 They could render onto a screen

1:37:58.580 --> 1:38:01.100
 and we could feel like we're participating in it.

1:38:01.100 --> 1:38:03.360
 You know, they're gonna have, you know,

1:38:03.360 --> 1:38:04.860
 like the Oculus kind of stuff.

1:38:04.860 --> 1:38:07.020
 It's gonna be so good when a projection to your eyes,

1:38:07.020 --> 1:38:08.040
 you think it's real.

1:38:08.040 --> 1:38:11.620
 You know, they're slowly solving those problems.

1:38:12.520 --> 1:38:17.240
 And I suspect when the renderer of that information

1:38:17.240 --> 1:38:19.760
 into your head is also AI mediated,

1:38:19.760 --> 1:38:23.520
 they'll be able to give you the cues that, you know,

1:38:23.520 --> 1:38:26.200
 you really want for depth and all kinds of stuff.

1:38:27.280 --> 1:38:30.920
 Like your brain is partly faking your visual field, right?

1:38:30.920 --> 1:38:32.680
 Like your eyes are twitching around,

1:38:32.680 --> 1:38:33.800
 but you don't notice that.

1:38:33.800 --> 1:38:36.520
 Occasionally they blank, you don't notice that.

1:38:36.520 --> 1:38:37.800
 You know, there's all kinds of things.

1:38:37.800 --> 1:38:39.160
 Like you think you see over here,

1:38:39.160 --> 1:38:40.840
 but you don't really see there.

1:38:40.840 --> 1:38:42.200
 It's all fabricated.

1:38:42.200 --> 1:38:45.520
 Yeah, peripheral vision is fascinating.

1:38:45.520 --> 1:38:48.560
 So if you have an AI renderer that's trained

1:38:48.560 --> 1:38:51.700
 to understand exactly how you see

1:38:51.700 --> 1:38:54.760
 and the kind of things that enhance the realism

1:38:54.760 --> 1:38:57.660
 of the experience, it could be super real actually.

1:39:01.160 --> 1:39:03.520
 So I don't know what the limits to that are,

1:39:03.520 --> 1:39:06.960
 but obviously if we have a brain interface

1:39:06.960 --> 1:39:10.480
 that goes inside your visual cortex

1:39:10.480 --> 1:39:13.480
 in a better way than your eyes do, which is possible,

1:39:13.480 --> 1:39:18.480
 it's a lot of neurons, maybe that'll be even cooler.

1:39:19.800 --> 1:39:21.600
 Well, the really cool thing is that it has to do

1:39:21.600 --> 1:39:24.240
 with the infinite fun that you were referring to,

1:39:24.240 --> 1:39:26.640
 which is our brains seem to be very limited.

1:39:26.640 --> 1:39:28.360
 And like you said, computations.

1:39:28.360 --> 1:39:29.920
 It's also very plastic.

1:39:29.920 --> 1:39:30.920
 Very plastic, yeah.

1:39:30.920 --> 1:39:33.640
 Yeah, so it's a interesting combination.

1:39:33.640 --> 1:39:37.480
 The interesting open question is the limits

1:39:37.480 --> 1:39:42.320
 of that neuroplasticity, like how flexible is that thing?

1:39:42.320 --> 1:39:44.880
 Because we haven't really tested it.

1:39:44.880 --> 1:39:46.240
 We know about that at the experiments

1:39:46.240 --> 1:39:49.120
 where they put like a pressure pad on somebody's head

1:39:49.120 --> 1:39:51.520
 and had a visual transducer pressurize it

1:39:51.520 --> 1:39:53.440
 and somebody slowly learned to see.

1:39:53.440 --> 1:39:54.280
 Yep.

1:39:55.880 --> 1:39:58.720
 Especially at a young age, if you throw a lot at it,

1:39:58.720 --> 1:40:03.720
 like what can it, so can you like arbitrarily expand it

1:40:05.920 --> 1:40:06.880
 with computing power?

1:40:06.880 --> 1:40:09.880
 So connected to the internet directly somehow?

1:40:09.880 --> 1:40:11.960
 Yeah, the answer's probably yes.

1:40:11.960 --> 1:40:13.840
 So the problem with biology and ethics

1:40:13.840 --> 1:40:15.560
 is like there's a mess there.

1:40:15.560 --> 1:40:20.560
 Like us humans are perhaps unwilling to take risks

1:40:21.840 --> 1:40:25.600
 into directions that are full of uncertainty.

1:40:25.600 --> 1:40:26.440
 So it's like. No, no.

1:40:26.440 --> 1:40:28.880
 90% of the population's unwilling to take risks.

1:40:28.880 --> 1:40:31.360
 The other 10% is rushing into the risks

1:40:31.360 --> 1:40:34.400
 unaided by any infrastructure whatsoever.

1:40:34.400 --> 1:40:38.960
 And that's where all the fun happens in society.

1:40:38.960 --> 1:40:41.160
 There's been huge transformations

1:40:41.160 --> 1:40:43.600
 in the last couple thousand years.

1:40:43.600 --> 1:40:44.560
 Yeah, it's funny.

1:40:44.560 --> 1:40:48.200
 I got a chance to interact with this Matthew Johnson

1:40:48.200 --> 1:40:49.360
 from Johns Hopkins.

1:40:49.360 --> 1:40:52.520
 He's doing this large scale study of psychedelics.

1:40:52.520 --> 1:40:54.240
 It's becoming more and more,

1:40:54.240 --> 1:40:55.240
 I've gotten a chance to interact

1:40:55.240 --> 1:40:57.760
 with that community of scientists working on psychedelics.

1:40:57.760 --> 1:41:00.080
 But because of that, that opened the door to me

1:41:00.080 --> 1:41:02.740
 to all these, what do they call it?

1:41:02.740 --> 1:41:05.340
 Psychonauts, the people who, like you said,

1:41:05.340 --> 1:41:08.000
 the 10% who are like, I don't care.

1:41:08.000 --> 1:41:09.840
 I don't know if there's a science behind this.

1:41:09.840 --> 1:41:12.040
 I'm taking this spaceship to,

1:41:12.040 --> 1:41:14.180
 if I'm being the first on Mars, I'll be.

1:41:15.760 --> 1:41:17.440
 Psychedelics are interesting in the sense

1:41:17.440 --> 1:41:21.400
 that in another dimension, like you said,

1:41:21.400 --> 1:41:25.440
 it's a way to explore the limits of the human mind.

1:41:25.440 --> 1:41:28.240
 Like, what is this thing capable of doing?

1:41:28.240 --> 1:41:31.440
 Because you kind of, like when you dream, you detach it.

1:41:31.440 --> 1:41:33.080
 I don't know exactly the neuroscience of it,

1:41:33.080 --> 1:41:38.080
 but you detach your reality from what your mind,

1:41:39.000 --> 1:41:40.800
 the images your mind is able to conjure up

1:41:40.800 --> 1:41:44.960
 and your mind goes into weird places and entities appear.

1:41:44.960 --> 1:41:48.800
 Somehow Freudian type of trauma

1:41:48.800 --> 1:41:50.320
 is probably connected in there somehow,

1:41:50.320 --> 1:41:54.040
 but you start to have these weird, vivid worlds that like.

1:41:54.040 --> 1:41:55.540
 So do you actively dream?

1:41:56.400 --> 1:41:58.220
 Do you, why not?

1:41:59.060 --> 1:42:01.360
 I have like six hours of dreams a night.

1:42:01.360 --> 1:42:03.140
 It's like really useful time.

1:42:03.140 --> 1:42:06.160
 I know, I haven't, I don't for some reason.

1:42:06.160 --> 1:42:11.040
 I just knock out and I have sometimes anxiety inducing

1:42:11.040 --> 1:42:16.040
 kind of like very pragmatic nightmare type of dreams,

1:42:16.680 --> 1:42:18.480
 but nothing fun, nothing.

1:42:18.480 --> 1:42:19.320
 Nothing fun?

1:42:19.320 --> 1:42:20.640
 Nothing fun.

1:42:20.640 --> 1:42:24.640
 I try, I unfortunately have mostly have fun

1:42:24.640 --> 1:42:27.760
 in the waking world, which is very limited

1:42:27.760 --> 1:42:30.040
 in the amount of fun you can have.

1:42:30.040 --> 1:42:31.240
 It's not that limited either.

1:42:31.240 --> 1:42:32.600
 Yeah, that's why.

1:42:32.600 --> 1:42:33.440
 We'll have to talk.

1:42:33.440 --> 1:42:36.840
 Yeah, I need instructions.

1:42:36.840 --> 1:42:37.680
 Yeah.

1:42:37.680 --> 1:42:38.680
 There's like a manual for that.

1:42:38.680 --> 1:42:39.520
 You might wanna.

1:42:41.040 --> 1:42:41.860
 I'll look it up.

1:42:41.860 --> 1:42:42.700
 I'll ask Elon.

1:42:42.700 --> 1:42:44.720
 What would you dream?

1:42:44.720 --> 1:42:47.120
 You know, years ago when I read about, you know,

1:42:47.120 --> 1:42:51.360
 like, you know, a book about how to have, you know,

1:42:51.360 --> 1:42:53.080
 become aware of your dreams.

1:42:53.080 --> 1:42:54.320
 I worked on it for a while.

1:42:54.320 --> 1:42:55.980
 Like there's this trick about, you know,

1:42:55.980 --> 1:42:58.280
 imagine you can see your hands and look out

1:42:58.280 --> 1:43:00.640
 and I got somewhat good at it.

1:43:00.640 --> 1:43:04.400
 Like, but my mostly, when I'm thinking about things

1:43:04.400 --> 1:43:09.040
 or working on problems, I prep myself before I go to sleep.

1:43:09.040 --> 1:43:13.160
 It's like, I pull into my mind all the things

1:43:13.160 --> 1:43:15.440
 I wanna work on or think about.

1:43:15.440 --> 1:43:19.840
 And then that, let's say, greatly improves the chances

1:43:19.840 --> 1:43:22.140
 that I'll work on that while I'm sleeping.

1:43:23.400 --> 1:43:28.400
 And then I also, you know, basically ask to remember it.

1:43:30.320 --> 1:43:33.180
 And I often remember very detailed.

1:43:33.180 --> 1:43:34.120
 Within the dream.

1:43:34.120 --> 1:43:34.960
 Yeah.

1:43:34.960 --> 1:43:35.780
 Or outside the dream.

1:43:35.780 --> 1:43:37.840
 Well, to bring it up in my dreaming

1:43:37.840 --> 1:43:39.740
 and then to remember it when I wake up.

1:43:41.020 --> 1:43:43.360
 It's just, it's more of a meditative practice.

1:43:43.360 --> 1:43:47.920
 You say, you know, to prepare yourself to do that.

1:43:48.920 --> 1:43:50.600
 Like if you go to, you know, to sleep,

1:43:50.600 --> 1:43:52.960
 still gnashing your teeth about some random thing

1:43:52.960 --> 1:43:55.800
 that happened that you're not that really interested in,

1:43:55.800 --> 1:43:56.960
 you'll dream about it.

1:43:57.960 --> 1:43:58.840
 That's really interesting.

1:43:58.840 --> 1:43:59.680
 Maybe.

1:43:59.680 --> 1:44:03.460
 But you can direct your dreams somewhat by prepping.

1:44:04.440 --> 1:44:05.480
 Yeah, I'm gonna have to try that.

1:44:05.480 --> 1:44:06.400
 It's really interesting.

1:44:06.400 --> 1:44:08.480
 Like the most important, the interesting,

1:44:08.480 --> 1:44:12.240
 not like what did this guy send in an email

1:44:12.240 --> 1:44:14.080
 kind of like stupid worry stuff,

1:44:14.080 --> 1:44:15.240
 but like fundamental problems

1:44:15.240 --> 1:44:16.320
 you're actually concerned about.

1:44:16.320 --> 1:44:17.160
 Yeah.

1:44:17.160 --> 1:44:18.200
 And interesting things you're worried about.

1:44:18.200 --> 1:44:20.040
 Or books you're reading or, you know,

1:44:20.040 --> 1:44:21.360
 some great conversation you had

1:44:21.360 --> 1:44:23.480
 or some adventure you want to have.

1:44:23.480 --> 1:44:26.720
 Like there's a lot of space there.

1:44:28.880 --> 1:44:32.520
 And it seems to work that, you know,

1:44:32.520 --> 1:44:36.440
 my percentage of interesting dreams and memories went up.

1:44:36.440 --> 1:44:40.440
 Is there, is that the source of,

1:44:40.440 --> 1:44:42.280
 if you were able to deconstruct like

1:44:42.280 --> 1:44:44.560
 where some of your best ideas came from,

1:44:45.720 --> 1:44:49.400
 is there a process that's at the core of that?

1:44:49.400 --> 1:44:52.420
 Like, so some people, you know, walk and think,

1:44:52.420 --> 1:44:55.160
 some people like in the shower, the best ideas hit them.

1:44:55.160 --> 1:44:58.560
 If you talk about like Newton, Apple hitting them on the head.

1:44:58.560 --> 1:45:01.080
 No, I found out a long time ago,

1:45:01.080 --> 1:45:03.200
 I process things somewhat slowly.

1:45:03.200 --> 1:45:05.680
 So like in college, I had friends who could study

1:45:05.680 --> 1:45:07.520
 at the last minute, get an A the next day.

1:45:07.520 --> 1:45:09.060
 I can't do that at all.

1:45:09.060 --> 1:45:10.920
 So I always front loaded all the work.

1:45:10.920 --> 1:45:14.160
 Like I do all the problems early, you know,

1:45:14.160 --> 1:45:15.800
 for finals, like the last three days,

1:45:15.800 --> 1:45:18.840
 I wouldn't look at a book because I want, you know,

1:45:18.840 --> 1:45:22.200
 cause like a new fact day before finals may screw up

1:45:22.200 --> 1:45:23.880
 my understanding of what I thought I knew.

1:45:23.880 --> 1:45:28.880
 So my goal was to always get it in and give it time to soak.

1:45:29.880 --> 1:45:32.060
 And I used to, you know,

1:45:32.060 --> 1:45:33.780
 I remember when we were doing like 3D calculus,

1:45:33.780 --> 1:45:36.280
 I would have these amazing dreams of 3D surfaces

1:45:36.280 --> 1:45:38.560
 with normal, you know, calculating the gradient.

1:45:38.560 --> 1:45:40.160
 And it's just like all come up.

1:45:40.160 --> 1:45:43.920
 So it was like really fun, like very visual.

1:45:43.920 --> 1:45:47.440
 And if I got cycles of that, that was useful.

1:45:48.520 --> 1:45:50.960
 And the other is, is don't over filter your ideas.

1:45:50.960 --> 1:45:54.520
 Like I like that process of brainstorming

1:45:54.520 --> 1:45:55.640
 where lots of ideas can happen.

1:45:55.640 --> 1:45:57.360
 I like people who have lots of ideas.

1:45:57.360 --> 1:46:00.240
 But then there's a, yeah, I'll let them sit

1:46:00.240 --> 1:46:02.560
 and let it breathe a little bit

1:46:02.560 --> 1:46:04.960
 and then reduce it to practice.

1:46:04.960 --> 1:46:09.920
 Like at some point you really have to, does it really work?

1:46:09.920 --> 1:46:13.360
 Like, you know, is this real or not, right?

1:46:13.360 --> 1:46:15.020
 But you have to do both.

1:46:15.020 --> 1:46:16.160
 There's creative tension there.

1:46:16.160 --> 1:46:20.480
 Like how do you be both open and, you know, precise?

1:46:20.480 --> 1:46:22.280
 Have you had ideas that you just,

1:46:22.280 --> 1:46:26.120
 that sit in your mind for like years before the?

1:46:26.120 --> 1:46:27.640
 Sure.

1:46:27.640 --> 1:46:31.760
 It's an interesting way to just generate ideas

1:46:31.760 --> 1:46:35.080
 and just let them sit, let them sit there for a while.

1:46:35.080 --> 1:46:38.480
 I think I have a few of those ideas.

1:46:38.480 --> 1:46:40.160
 You know, that was so funny.

1:46:40.160 --> 1:46:42.440
 Yeah, I think that's, you know,

1:46:42.440 --> 1:46:44.740
 creativity this one or something.

1:46:45.740 --> 1:46:49.380
 For the slow thinkers in the room, I suppose.

1:46:49.380 --> 1:46:53.300
 As I, some people, like you said, are just like, like the.

1:46:53.300 --> 1:46:54.840
 Yeah, it's really interesting.

1:46:54.840 --> 1:46:57.680
 There's so much diversity in how people think.

1:46:57.680 --> 1:46:59.320
 You know, how fast or slow they are,

1:46:59.320 --> 1:47:01.660
 how well they remember or don't.

1:47:01.660 --> 1:47:04.040
 Like, you know, I'm not super good at remembering facts,

1:47:04.040 --> 1:47:06.440
 but processes and methods.

1:47:06.440 --> 1:47:08.040
 Like in our engineering, I went to Penn State

1:47:08.040 --> 1:47:11.860
 and almost all our engineering tests were open book.

1:47:11.860 --> 1:47:14.800
 I could remember the page and not the formula.

1:47:14.800 --> 1:47:15.920
 But as soon as I saw the formula,

1:47:15.920 --> 1:47:19.720
 I could remember the whole method if I'd learned it.

1:47:19.720 --> 1:47:20.560
 Yeah.

1:47:20.560 --> 1:47:23.480
 So it's just a funny, where some people could, you know,

1:47:23.480 --> 1:47:25.580
 I'd watch friends like flipping through the book,

1:47:25.580 --> 1:47:27.440
 trying to find the formula,

1:47:27.440 --> 1:47:30.080
 even knowing that they'd done just as much work.

1:47:30.080 --> 1:47:31.240
 And I would just open the book

1:47:31.240 --> 1:47:33.680
 and I was on page 27, about half,

1:47:33.680 --> 1:47:35.960
 I could see the whole thing visually.

1:47:35.960 --> 1:47:36.800
 Yeah.

1:47:36.800 --> 1:47:37.640
 And, you know.

1:47:37.640 --> 1:47:39.040
 And you have to learn that about yourself

1:47:39.040 --> 1:47:41.480
 and figure out what would function optimally.

1:47:41.480 --> 1:47:43.320
 I had a friend who was always concerned

1:47:43.320 --> 1:47:45.760
 he didn't know how he came up with ideas.

1:47:45.760 --> 1:47:49.160
 He had lots of ideas, but he said they just sort of popped up.

1:47:49.160 --> 1:47:51.080
 Like, you'd be working on something, you have this idea,

1:47:51.080 --> 1:47:53.360
 like, where does it come from?

1:47:53.360 --> 1:47:54.840
 But you can have more awareness of it.

1:47:54.840 --> 1:47:59.760
 Like, how your brain works is a little murky

1:47:59.760 --> 1:48:01.600
 as you go down from the voice in your head

1:48:01.600 --> 1:48:03.920
 or the obvious visualizations.

1:48:03.920 --> 1:48:06.580
 Like, when you visualize something, how does that happen?

1:48:06.580 --> 1:48:07.420
 Yeah, that's right.

1:48:07.420 --> 1:48:09.080
 You know, if I say, you know, visualize a volcano,

1:48:09.080 --> 1:48:10.320
 it's easy to do, right?

1:48:10.320 --> 1:48:12.560
 And what does it actually look like when you visualize it?

1:48:12.560 --> 1:48:14.880
 I can visualize to the point where I don't see very much

1:48:14.880 --> 1:48:16.280
 out of my eyes and I see the colors

1:48:16.280 --> 1:48:18.280
 of the thing I'm visualizing.

1:48:18.280 --> 1:48:20.600
 Yeah, but there's a shape, there's a texture,

1:48:20.600 --> 1:48:23.160
 there's a color, but there's also conceptual visualization.

1:48:23.160 --> 1:48:25.720
 Like, what are you actually visualizing

1:48:25.720 --> 1:48:27.240
 when you're visualizing a volcano?

1:48:27.240 --> 1:48:28.480
 Just like with peripheral vision,

1:48:28.480 --> 1:48:29.720
 you think you see the whole thing.

1:48:29.720 --> 1:48:31.840
 Yeah, yeah, yeah, that's a good way to say it.

1:48:31.840 --> 1:48:34.860
 You know, you have this kind of almost peripheral vision

1:48:34.860 --> 1:48:37.440
 of your visualizations, they're like these ghosts.

1:48:38.440 --> 1:48:40.200
 But if, you know, if you work on it,

1:48:40.200 --> 1:48:42.320
 you can get a pretty high level of detail.

1:48:42.320 --> 1:48:44.400
 And somehow you can walk along those visualizations

1:48:44.400 --> 1:48:47.240
 and come up with an idea, which is weird.

1:48:47.240 --> 1:48:50.940
 But when you're thinking about solving problems,

1:48:50.940 --> 1:48:53.000
 like, you're putting information in,

1:48:53.000 --> 1:48:55.760
 you're exercising the stuff you do know,

1:48:55.760 --> 1:48:59.400
 you're sort of teasing the area that you don't understand

1:48:59.400 --> 1:49:02.240
 and don't know, but you can almost, you know,

1:49:02.240 --> 1:49:06.600
 feel, you know, that process happening.

1:49:06.600 --> 1:49:08.360
 You know, that's how I, like,

1:49:10.080 --> 1:49:12.040
 like, I know sometimes when I'm working really hard

1:49:12.040 --> 1:49:14.920
 on something, like, I get really hot when I'm sleeping.

1:49:14.920 --> 1:49:17.320
 And, you know, it's like, we got the blank throw,

1:49:17.320 --> 1:49:20.080
 I wake up, all the blanks are on the floor.

1:49:20.080 --> 1:49:21.920
 And, you know, every time it's, well,

1:49:21.920 --> 1:49:24.880
 I wake up and think, wow, that was great.

1:49:24.880 --> 1:49:25.720
 You know?

1:49:25.720 --> 1:49:27.600
 Are you able to reverse engineer

1:49:27.600 --> 1:49:28.960
 what the hell happened there?

1:49:28.960 --> 1:49:30.360
 Well, sometimes it's vivid dreams

1:49:30.360 --> 1:49:32.500
 and sometimes it's just kind of, like you say,

1:49:32.500 --> 1:49:35.120
 like shadow thinking that you sort of have this feeling

1:49:35.120 --> 1:49:38.720
 you're going through this stuff, but it's not that obvious.

1:49:38.720 --> 1:49:40.320
 Isn't that so amazing that the mind

1:49:40.320 --> 1:49:42.880
 just does all these little experiments?

1:49:42.880 --> 1:49:46.040
 I never, you know, I always thought it's like a river

1:49:46.040 --> 1:49:48.160
 that you can't, you're just there for the ride,

1:49:48.160 --> 1:49:50.360
 but you're right, if you prep it.

1:49:50.360 --> 1:49:52.400
 No, it's all understandable.

1:49:52.400 --> 1:49:53.720
 Meditation really helps.

1:49:53.720 --> 1:49:55.160
 You gotta start figuring out,

1:49:55.160 --> 1:49:59.320
 you need to learn language of your own mind.

1:49:59.320 --> 1:50:02.600
 And there's multiple levels of it, but.

1:50:02.600 --> 1:50:04.040
 The abstractions again, right?

1:50:04.040 --> 1:50:06.700
 It's somewhat comprehensible and observable

1:50:06.700 --> 1:50:10.000
 and feelable or whatever the right word is.

1:50:11.960 --> 1:50:13.680
 You know, you're not alone for the ride.

1:50:13.680 --> 1:50:15.600
 You are the ride.

1:50:15.600 --> 1:50:17.960
 I have to ask you, hardware engineer,

1:50:17.960 --> 1:50:21.420
 working on neural networks now, what's consciousness?

1:50:21.420 --> 1:50:22.840
 What the hell is that thing?

1:50:22.840 --> 1:50:25.960
 Is that just some little weird quirk

1:50:25.960 --> 1:50:29.280
 of our particular computing device?

1:50:29.280 --> 1:50:30.560
 Or is it something fundamental

1:50:30.560 --> 1:50:32.040
 that we really need to crack open

1:50:32.040 --> 1:50:36.560
 if we're to build good computers?

1:50:36.560 --> 1:50:37.940
 Do you ever think about consciousness?

1:50:37.940 --> 1:50:39.960
 Like why it feels like something to be?

1:50:39.960 --> 1:50:41.760
 I know, it's really weird.

1:50:42.640 --> 1:50:43.680
 So.

1:50:43.680 --> 1:50:44.520
 Yeah.

1:50:45.560 --> 1:50:48.000
 I mean, everything about it's weird.

1:50:48.000 --> 1:50:51.340
 First, it's a half a second behind reality, right?

1:50:51.340 --> 1:50:53.780
 It's a post hoc narrative about what happened.

1:50:53.780 --> 1:50:55.640
 You've already done stuff

1:50:56.520 --> 1:50:58.880
 by the time you're conscious of it.

1:50:58.880 --> 1:51:00.160
 And your consciousness generally

1:51:00.160 --> 1:51:01.240
 is a single threaded thing,

1:51:01.240 --> 1:51:03.680
 but we know your brain is 10 billion neurons

1:51:03.680 --> 1:51:07.980
 running some crazy parallel thing.

1:51:07.980 --> 1:51:11.200
 And there's a really big sorting thing going on there.

1:51:11.200 --> 1:51:13.040
 It also seems to be really reflective

1:51:13.040 --> 1:51:18.000
 in the sense that you create a space in your head.

1:51:18.000 --> 1:51:19.640
 Like we don't really see anything, right?

1:51:19.640 --> 1:51:21.600
 Like photons hit your eyes,

1:51:21.600 --> 1:51:22.840
 it gets turned into signals,

1:51:22.840 --> 1:51:25.000
 it goes through multiple layers of neurons.

1:51:26.600 --> 1:51:29.160
 I'm so curious that that looks glassy

1:51:29.160 --> 1:51:30.480
 and that looks not glassy.

1:51:30.480 --> 1:51:33.520
 Like how the resolution of your vision is so high

1:51:33.520 --> 1:51:36.080
 you have to go through all this processing.

1:51:36.080 --> 1:51:38.760
 Where for most of it, it looks nothing like vision.

1:51:39.680 --> 1:51:43.640
 Like there's no theater in your mind, right?

1:51:43.640 --> 1:51:46.820
 So we have a world in our heads.

1:51:46.820 --> 1:51:51.740
 We're literally just isolated behind our sensors.

1:51:51.740 --> 1:51:55.580
 But we can look at it, speculate about it,

1:51:55.580 --> 1:52:00.240
 speculate about alternatives, problem solve, what if.

1:52:00.240 --> 1:52:02.880
 There's so many things going on

1:52:02.880 --> 1:52:06.200
 and that process is lagging reality.

1:52:06.200 --> 1:52:07.580
 And it's single threaded

1:52:07.580 --> 1:52:10.460
 even though the underlying thing is like massively parallel.

1:52:10.460 --> 1:52:12.780
 So it's so curious.

1:52:12.780 --> 1:52:14.520
 So imagine you're building an AI computer.

1:52:14.520 --> 1:52:16.380
 If you wanted to replicate humans,

1:52:16.380 --> 1:52:18.380
 well, you'd have huge arrays of neural networks

1:52:18.380 --> 1:52:22.420
 and apparently only six or seven deep, which is hilarious.

1:52:22.420 --> 1:52:23.780
 They don't even remember seven numbers,

1:52:23.780 --> 1:52:26.220
 but I think we can upgrade that a lot, right?

1:52:26.220 --> 1:52:28.240
 And then somewhere in there,

1:52:28.240 --> 1:52:30.020
 you would train the network to create

1:52:30.020 --> 1:52:32.860
 basically the world that you live in, right?

1:52:32.860 --> 1:52:34.860
 So like tell stories to itself

1:52:34.860 --> 1:52:36.800
 about the world that it's perceiving.

1:52:36.800 --> 1:52:40.820
 Well, create the world, tell stories in the world

1:52:40.820 --> 1:52:45.820
 and then have many dimensions of like side shows to it.

1:52:47.660 --> 1:52:49.340
 Like we have an emotional structure,

1:52:49.340 --> 1:52:51.500
 like we have a biological structure.

1:52:51.500 --> 1:52:52.740
 And that seems hierarchical too.

1:52:52.740 --> 1:52:55.620
 Like if you're hungry, it dominates your thinking.

1:52:55.620 --> 1:52:57.660
 If you're mad, it dominates your thinking.

1:52:59.220 --> 1:53:00.380
 And we don't know if that's important

1:53:00.380 --> 1:53:01.300
 to consciousness or not,

1:53:01.300 --> 1:53:05.740
 but it certainly disrupts, intrudes in the consciousness.

1:53:05.740 --> 1:53:08.160
 Like so there's lots of structure to that.

1:53:08.160 --> 1:53:09.880
 And we like to dwell on the past.

1:53:09.880 --> 1:53:11.280
 We like to think about the future.

1:53:11.280 --> 1:53:14.740
 We like to imagine, we like to fantasize, right?

1:53:14.740 --> 1:53:18.580
 And the somewhat circular observation of that

1:53:18.580 --> 1:53:20.620
 is the thing we call consciousness.

1:53:21.760 --> 1:53:23.340
 Now, if you created a computer system

1:53:23.340 --> 1:53:24.900
 and did all things, create worldviews,

1:53:24.900 --> 1:53:27.620
 create the future alternate histories,

1:53:27.620 --> 1:53:31.340
 dwelled on past events, accurately or semi accurately.

1:53:33.020 --> 1:53:35.380
 Well, consciousness just spring up like naturally.

1:53:35.380 --> 1:53:38.100
 Well, would that look and feel conscious to you?

1:53:38.100 --> 1:53:39.940
 Like you seem conscious to me, but I don't know.

1:53:39.940 --> 1:53:41.780
 Off of the external observer sense.

1:53:41.780 --> 1:53:44.940
 Do you think a thing that looks conscious is conscious?

1:53:44.940 --> 1:53:48.220
 Like do you, again, this is like an engineering

1:53:48.220 --> 1:53:50.220
 kind of question, I think, because like.

1:53:53.900 --> 1:53:54.860
 I don't know.

1:53:54.860 --> 1:53:56.840
 If we want to engineer consciousness,

1:53:56.840 --> 1:53:58.300
 is it okay to engineer something

1:53:58.300 --> 1:53:59.740
 that just looks conscious?

1:54:00.740 --> 1:54:02.660
 Or is there a difference between something that is?

1:54:02.660 --> 1:54:04.060
 Well, we evolve consciousness

1:54:04.060 --> 1:54:07.140
 because it's a super effective way to manage our affairs.

1:54:07.140 --> 1:54:09.020
 Yeah, this is a social element, yeah.

1:54:09.020 --> 1:54:11.540
 Well, it gives us a planning system.

1:54:11.540 --> 1:54:13.280
 We have a huge amount of stuff.

1:54:13.280 --> 1:54:15.220
 Like when we're talking, like the reason

1:54:15.220 --> 1:54:17.260
 we can talk really fast is we're modeling each other

1:54:17.260 --> 1:54:19.100
 at a really high level of detail.

1:54:19.100 --> 1:54:21.340
 And consciousness is required for that.

1:54:21.340 --> 1:54:23.740
 Well, all those components together

1:54:23.740 --> 1:54:26.740
 manifest consciousness, right?

1:54:26.740 --> 1:54:28.460
 So if we make intelligent beings

1:54:28.460 --> 1:54:30.820
 that we want to interact with that we're like

1:54:30.820 --> 1:54:32.860
 wondering what they're thinking,

1:54:32.860 --> 1:54:35.140
 looking forward to seeing them,

1:54:35.140 --> 1:54:37.280
 when they interact with them, they're interesting,

1:54:37.280 --> 1:54:41.460
 surprising, you know, fascinating, you know,

1:54:41.460 --> 1:54:43.500
 they will probably feel conscious like we do

1:54:43.500 --> 1:54:45.400
 and we'll perceive them as conscious.

1:54:47.180 --> 1:54:49.980
 I don't know why not, but you never know.

1:54:49.980 --> 1:54:51.460
 Another fun question on this,

1:54:51.460 --> 1:54:55.020
 because from a computing perspective,

1:54:55.020 --> 1:54:55.980
 we're trying to create something

1:54:55.980 --> 1:54:57.820
 that's humanlike or superhumanlike.

1:54:59.740 --> 1:55:01.280
 Let me ask you about aliens.

1:55:01.280 --> 1:55:02.120
 Aliens.

1:55:02.120 --> 1:55:07.120
 Do you think there's intelligent alien civilizations

1:55:08.440 --> 1:55:13.160
 out there and do you think their technology,

1:55:13.160 --> 1:55:16.480
 their computing, their AI bots,

1:55:16.480 --> 1:55:21.280
 their chips are of the same nature as ours?

1:55:21.280 --> 1:55:23.120
 Yeah, I've got no idea.

1:55:23.120 --> 1:55:25.000
 I mean, if there's lots of aliens out there

1:55:25.000 --> 1:55:26.440
 that have been awfully quiet,

1:55:27.320 --> 1:55:29.620
 you know, there's speculation about why.

1:55:29.620 --> 1:55:34.620
 There seems to be more than enough planets out there.

1:55:34.940 --> 1:55:35.780
 There's a lot.

1:55:37.460 --> 1:55:38.980
 There's intelligent life on this planet

1:55:38.980 --> 1:55:40.500
 that seems quite different, you know,

1:55:40.500 --> 1:55:44.580
 like dolphins seem like plausibly understandable,

1:55:44.580 --> 1:55:47.620
 octopuses don't seem understandable at all.

1:55:47.620 --> 1:55:48.820
 If they lived longer than a year,

1:55:48.820 --> 1:55:50.980
 maybe they would be running the planet.

1:55:50.980 --> 1:55:52.700
 They seem really smart.

1:55:52.700 --> 1:55:54.260
 And their neural architecture

1:55:54.260 --> 1:55:56.540
 is completely different than ours.

1:55:56.540 --> 1:55:58.700
 Now, who knows how they perceive things.

1:55:58.700 --> 1:56:01.180
 I mean, that's the question is for us intelligent beings,

1:56:01.180 --> 1:56:03.620
 we might not be able to perceive other kinds of intelligence

1:56:03.620 --> 1:56:05.580
 if they become sufficiently different than us.

1:56:05.580 --> 1:56:08.940
 Yeah, like we live in the current constrained world,

1:56:08.940 --> 1:56:10.660
 you know, it's three dimensional geometry

1:56:10.660 --> 1:56:14.500
 and the geometry defines a certain amount of physics.

1:56:14.500 --> 1:56:18.560
 And, you know, there's like how time works seems to work.

1:56:18.560 --> 1:56:21.100
 There's so many things that seem like

1:56:21.100 --> 1:56:23.500
 a whole bunch of the input parameters to the, you know,

1:56:23.500 --> 1:56:25.540
 another conscious being are the same.

1:56:25.540 --> 1:56:28.180
 Yes, like if it's biological,

1:56:28.180 --> 1:56:30.020
 biological things seem to be

1:56:30.020 --> 1:56:32.940
 in a relatively narrow temperature range, right?

1:56:32.940 --> 1:56:35.620
 Because, you know, organics aren't stable,

1:56:35.620 --> 1:56:37.740
 too cold or too hot.

1:56:37.740 --> 1:56:42.740
 Now, so if you specify the list of things that input to that,

1:56:45.260 --> 1:56:49.620
 but as soon as we make really smart, you know, beings

1:56:49.620 --> 1:56:51.140
 and they go solve about how to think

1:56:51.140 --> 1:56:52.940
 about a billion numbers at the same time

1:56:52.940 --> 1:56:56.060
 and how to think in end dimensions.

1:56:56.060 --> 1:56:57.340
 There's a funny science fiction book

1:56:57.340 --> 1:57:01.620
 where all the society had uploaded into this matrix.

1:57:01.620 --> 1:57:05.340
 And at some point, some of the beings in the matrix thought,

1:57:05.340 --> 1:57:07.900
 I wonder if there's intelligent life out there.

1:57:07.900 --> 1:57:09.940
 So they had to do a whole bunch of work to figure out

1:57:09.940 --> 1:57:12.380
 like how to make a physical thing

1:57:12.380 --> 1:57:15.000
 because their matrix was self sustaining

1:57:15.000 --> 1:57:16.140
 and they made a little spaceship

1:57:16.140 --> 1:57:18.540
 and they traveled to another planet when they got there,

1:57:18.540 --> 1:57:20.660
 there was like life running around,

1:57:20.660 --> 1:57:22.700
 but there was no intelligent life.

1:57:22.700 --> 1:57:25.340
 And then they figured out that there was these huge,

1:57:26.260 --> 1:57:28.780
 you know, organic matrix all over the planet

1:57:28.780 --> 1:57:30.540
 inside there where intelligent beings

1:57:30.540 --> 1:57:33.720
 had uploaded themselves into that matrix.

1:57:34.960 --> 1:57:38.220
 So everywhere intelligent life was,

1:57:38.220 --> 1:57:42.180
 soon as it got smart, it upleveled itself

1:57:42.180 --> 1:57:45.180
 into something way more interesting than 3D geometry.

1:57:45.180 --> 1:57:47.100
 Yeah, it escaped whatever this,

1:57:47.100 --> 1:57:49.780
 not escaped, uplevel is better.

1:57:49.780 --> 1:57:53.180
 The essence of what we think of as an intelligent being,

1:57:53.180 --> 1:57:58.100
 I tend to like the thought experiment of the organism,

1:57:58.100 --> 1:58:00.340
 like humans aren't the organisms.

1:58:00.340 --> 1:58:03.700
 I like the notion of like Richard Dawkins and memes

1:58:03.700 --> 1:58:07.980
 that ideas themselves are the organisms,

1:58:07.980 --> 1:58:11.460
 like that are just using our minds to evolve.

1:58:11.460 --> 1:58:15.180
 So like we're just like meat receptacles

1:58:15.180 --> 1:58:18.140
 for ideas to breed and multiply and so on.

1:58:18.140 --> 1:58:20.980
 And maybe those are the aliens.

1:58:20.980 --> 1:58:25.980
 Yeah, so Jordan Peterson has a line that says,

1:58:26.300 --> 1:58:29.180
 you know, you think you have ideas, but ideas have you.

1:58:29.180 --> 1:58:30.620
 Yeah, good line.

1:58:30.620 --> 1:58:34.220
 Which, and then we know about the phenomenon of groupthink

1:58:34.220 --> 1:58:37.940
 and there's so many things that constrain us.

1:58:37.940 --> 1:58:39.920
 But I think you can examine all that

1:58:39.920 --> 1:58:43.300
 and not be completely owned by the ideas

1:58:43.300 --> 1:58:46.120
 and completely sucked into groupthink.

1:58:46.120 --> 1:58:48.940
 And part of your responsibility as a human

1:58:49.820 --> 1:58:51.740
 is to escape that kind of phenomenon,

1:58:51.740 --> 1:58:55.940
 which isn't, it's one of the creative tension things again,

1:58:55.940 --> 1:58:59.500
 you're constructed by it, but you can still observe it

1:58:59.500 --> 1:59:01.820
 and you can think about it and you can make choices

1:59:01.820 --> 1:59:05.660
 about to some level, how constrained you are by it.

1:59:06.940 --> 1:59:09.780
 And it's useful to do that.

1:59:09.780 --> 1:59:14.780
 And, but at the same time, and it could be by doing that,

1:59:17.380 --> 1:59:21.460
 you know, the group and society you're part of

1:59:21.460 --> 1:59:24.140
 becomes collectively even more interesting.

1:59:24.140 --> 1:59:27.020
 So, you know, so the outside observer will think,

1:59:27.020 --> 1:59:30.060
 wow, you know, all these Lexus running around

1:59:30.060 --> 1:59:31.540
 with all these really independent ideas

1:59:31.540 --> 1:59:33.700
 have created something even more interesting

1:59:33.700 --> 1:59:35.700
 in the aggregate.

1:59:35.700 --> 1:59:40.700
 So, I don't know, those are lenses to look at the situation

1:59:41.860 --> 1:59:43.500
 that'll give you some inspiration,

1:59:43.500 --> 1:59:45.460
 but I don't think they're constrained.

1:59:45.460 --> 1:59:46.660
 Right.

1:59:46.660 --> 1:59:49.340
 As a small little quirk of history,

1:59:49.340 --> 1:59:53.540
 it seems like you're related to Jordan Peterson,

1:59:53.540 --> 1:59:54.740
 like you mentioned.

1:59:54.740 --> 1:59:57.620
 He's going through some rough stuff now.

1:59:57.620 --> 1:59:59.180
 Is there some comment you can make

1:59:59.180 --> 2:00:04.180
 about the roughness of the human journey, the ups and downs?

2:00:04.180 --> 2:00:09.180
 Well, I became an expert in Benza withdrawal,

2:00:10.700 --> 2:00:13.540
 like, which is, you took Benza to Aspen's,

2:00:13.540 --> 2:00:18.540
 and at some point they interact with GABA circuits,

2:00:18.940 --> 2:00:21.860
 you know, to reduce anxiety and do a hundred other things.

2:00:21.860 --> 2:00:25.100
 Like there's actually no known list of everything they do

2:00:25.100 --> 2:00:28.180
 because they interact with so many parts of your body.

2:00:28.180 --> 2:00:30.460
 And then once you're on them, you habituate to them

2:00:30.460 --> 2:00:32.580
 and you have a dependency.

2:00:32.580 --> 2:00:34.180
 It's not like you're a drug dependency

2:00:34.180 --> 2:00:35.020
 where you're trying to get high.

2:00:35.020 --> 2:00:38.820
 It's a metabolic dependency.

2:00:38.820 --> 2:00:41.020
 And then if you discontinue them,

2:00:42.580 --> 2:00:44.420
 there's a funny thing called kindling,

2:00:45.340 --> 2:00:47.540
 which is if you stop them and then go,

2:00:47.540 --> 2:00:49.900
 you know, you'll have a horrible withdrawal symptoms.

2:00:49.900 --> 2:00:51.460
 And if you go back on them at the same level,

2:00:51.460 --> 2:00:53.260
 you won't be stable.

2:00:53.260 --> 2:00:55.820
 And that unfortunately happened to him.

2:00:55.820 --> 2:00:57.140
 Because it's so deeply integrated

2:00:57.140 --> 2:00:58.860
 into all the kinds of systems in the body.

2:00:58.860 --> 2:01:00.780
 It literally changes the size and numbers

2:01:00.780 --> 2:01:03.820
 of neurotransmitter sites in your brain.

2:01:03.820 --> 2:01:07.340
 So there's a process called the Ashton protocol

2:01:07.340 --> 2:01:10.300
 where you taper it down slowly over two years

2:01:10.300 --> 2:01:13.660
 to people go through that goes through unbelievable hell.

2:01:13.660 --> 2:01:15.620
 And what Jordan went through seemed to be worse

2:01:15.620 --> 2:01:18.460
 because on advice of doctors, you know,

2:01:18.460 --> 2:01:20.260
 we'll stop taking these and take this.

2:01:20.260 --> 2:01:21.340
 It was the disaster.

2:01:21.340 --> 2:01:24.860
 And he got some, yeah, it was pretty tough.

2:01:26.620 --> 2:01:29.180
 He seems to be doing quite a bit better intellectually.

2:01:29.180 --> 2:01:32.020
 You can see his brain clicking back together.

2:01:32.020 --> 2:01:32.940
 I spent a lot of time with him.

2:01:32.940 --> 2:01:34.940
 I've never seen anybody suffer so much.

2:01:34.940 --> 2:01:37.740
 Well, his brain is also like this powerhouse, right?

2:01:37.740 --> 2:01:42.500
 So I wonder, does a brain that's able to think deeply

2:01:42.500 --> 2:01:44.740
 about the world suffer more through these kinds

2:01:44.740 --> 2:01:46.220
 of withdrawals, like?

2:01:46.220 --> 2:01:47.060
 I don't know.

2:01:47.060 --> 2:01:49.260
 I've watched videos of people going through withdrawal.

2:01:49.260 --> 2:01:52.700
 They all seem to suffer unbelievably.

2:01:54.060 --> 2:01:57.580
 And, you know, my heart goes out to everybody.

2:01:57.580 --> 2:01:59.300
 And there's some funny math about this.

2:01:59.300 --> 2:02:01.980
 Some doctor said, as best he can tell, you know,

2:02:01.980 --> 2:02:03.620
 there's the standard recommendations.

2:02:03.620 --> 2:02:04.820
 Don't take them for more than a month

2:02:04.820 --> 2:02:07.220
 and then taper over a couple of weeks.

2:02:07.220 --> 2:02:09.380
 Many doctors prescribe them endlessly,

2:02:09.380 --> 2:02:13.180
 which is against the protocol, but it's common, right?

2:02:13.180 --> 2:02:17.500
 And then something like 75% of people, when they taper,

2:02:17.500 --> 2:02:19.900
 it's, you know, half the people have difficulty,

2:02:19.900 --> 2:02:22.140
 but 75% get off okay.

2:02:22.140 --> 2:02:24.020
 20% have severe difficulty

2:02:24.020 --> 2:02:27.300
 and 5% have life threatening difficulty.

2:02:27.300 --> 2:02:29.580
 And if you're one of those, it's really bad.

2:02:29.580 --> 2:02:31.580
 And the stories that people have on this

2:02:31.580 --> 2:02:34.980
 is heartbreaking and tough.

2:02:34.980 --> 2:02:36.860
 So you put some of the fault at the doctors.

2:02:36.860 --> 2:02:38.660
 They just not know what the hell they're doing.

2:02:38.660 --> 2:02:40.580
 No, no, it's hard to say.

2:02:40.580 --> 2:02:43.140
 It's one of those commonly prescribed things.

2:02:43.140 --> 2:02:46.140
 Like one doctor said, what happens is,

2:02:46.140 --> 2:02:47.820
 if you're prescribed them for a reason

2:02:47.820 --> 2:02:49.900
 and then you have a hard time getting off,

2:02:49.900 --> 2:02:52.420
 the protocol basically says you're either crazy

2:02:52.420 --> 2:02:55.500
 or dependent and you get kind of pushed

2:02:55.500 --> 2:02:58.380
 into a different treatment regime.

2:02:58.380 --> 2:03:01.820
 You're a drug addict or a psychiatric patient.

2:03:01.820 --> 2:03:04.100
 And so like one doctor said, you know,

2:03:04.100 --> 2:03:05.500
 I prescribed them for 10 years thinking

2:03:05.500 --> 2:03:06.580
 I was helping my patients

2:03:06.580 --> 2:03:08.620
 and I realized I was really harming them.

2:03:09.620 --> 2:03:12.860
 And you know, the awareness of that is slowly coming up.

2:03:14.420 --> 2:03:18.180
 The fact that they're casually prescribed to people

2:03:18.180 --> 2:03:22.260
 is horrible and it's bloody scary.

2:03:23.780 --> 2:03:25.020
 And some people are stable on them,

2:03:25.020 --> 2:03:26.260
 but they're on them for life.

2:03:26.260 --> 2:03:29.260
 Like once you, you know, it's another one of those drugs.

2:03:29.260 --> 2:03:32.540
 But benzos long range have real impacts on your personality.

2:03:32.540 --> 2:03:34.140
 People talk about the benzo bubble

2:03:34.140 --> 2:03:36.300
 where you get disassociated from reality

2:03:36.300 --> 2:03:38.180
 and your friends a little bit.

2:03:38.180 --> 2:03:40.340
 It's really terrible.

2:03:40.340 --> 2:03:41.700
 The mind is terrifying.

2:03:41.700 --> 2:03:45.460
 We were talking about how the infinite possibility of fun,

2:03:45.460 --> 2:03:48.660
 but like it's the infinite possibility of suffering too,

2:03:48.660 --> 2:03:52.340
 which is one of the dangers of like expansion

2:03:52.340 --> 2:03:53.500
 of the human mind.

2:03:53.500 --> 2:03:58.260
 It's like, I wonder if all the possible experiences

2:03:58.260 --> 2:04:01.740
 that an intelligent computer can have,

2:04:01.740 --> 2:04:05.860
 is it mostly fun or is it mostly suffering?

2:04:05.860 --> 2:04:10.860
 So like if you brute force expand the set of possibilities,

2:04:11.380 --> 2:04:13.980
 like are you going to run into some trouble

2:04:13.980 --> 2:04:16.580
 in terms of like torture and suffering and so on?

2:04:16.580 --> 2:04:18.900
 Maybe our human brain is just protecting us

2:04:18.900 --> 2:04:22.300
 from much more possible pain and suffering.

2:04:22.300 --> 2:04:25.980
 Maybe the space of pain is like much larger

2:04:25.980 --> 2:04:27.540
 than we could possibly imagine.

2:04:27.540 --> 2:04:28.380
 And that.

2:04:28.380 --> 2:04:29.580
 The world's in a balance.

2:04:30.780 --> 2:04:34.260
 You know, all the literature on religion and stuff is,

2:04:34.260 --> 2:04:36.340
 you know, the struggle between good and evil

2:04:36.340 --> 2:04:39.420
 is balanced for very finely tuned

2:04:39.420 --> 2:04:41.660
 for reasons that are complicated.

2:04:41.660 --> 2:04:44.900
 But that's a long philosophical conversation.

2:04:44.900 --> 2:04:46.700
 Speaking of balance that's complicated,

2:04:46.700 --> 2:04:48.460
 I wonder because we're living through

2:04:48.460 --> 2:04:51.620
 one of the more important moments in human history

2:04:51.620 --> 2:04:53.780
 with this particular virus.

2:04:53.780 --> 2:04:56.980
 It seems like pandemics have at least the ability

2:04:56.980 --> 2:05:01.980
 to kill off most of the human population at their worst.

2:05:03.060 --> 2:05:04.300
 And there's just fascinating

2:05:04.300 --> 2:05:06.180
 because there's so many viruses in this world.

2:05:06.180 --> 2:05:08.620
 There's so many, I mean, viruses basically run the world

2:05:08.620 --> 2:05:12.260
 in the sense that they've been around very long time.

2:05:12.260 --> 2:05:13.700
 They're everywhere.

2:05:13.700 --> 2:05:15.340
 They seem to be extremely powerful

2:05:15.340 --> 2:05:17.300
 in the distributed kind of way.

2:05:17.300 --> 2:05:19.620
 But at the same time, they're not intelligent

2:05:19.620 --> 2:05:21.260
 and they're not even living.

2:05:21.260 --> 2:05:23.820
 Do you have like high level thoughts about this virus

2:05:23.820 --> 2:05:28.260
 that like in terms of you being fascinated or terrified

2:05:28.260 --> 2:05:30.420
 or somewhere in between?

2:05:30.420 --> 2:05:32.500
 So I believe in frameworks, right?

2:05:32.500 --> 2:05:35.420
 So like one of them is evolution.

2:05:36.300 --> 2:05:37.900
 Like we're evolved creatures, right?

2:05:37.900 --> 2:05:38.980
 Yes.

2:05:38.980 --> 2:05:40.900
 And one of the things about evolution

2:05:40.900 --> 2:05:42.740
 is it's hyper competitive.

2:05:42.740 --> 2:05:44.900
 And it's not competitive out of a sense of evil.

2:05:44.900 --> 2:05:47.820
 It's competitive as a sense of there's endless variation

2:05:47.820 --> 2:05:50.380
 and variations that work better when.

2:05:50.380 --> 2:05:52.980
 And then over time, there's so many levels

2:05:52.980 --> 2:05:54.140
 of that competition.

2:05:55.260 --> 2:05:57.740
 Like multicellular life partly exists

2:05:57.740 --> 2:06:01.140
 because of the competition

2:06:01.140 --> 2:06:04.260
 between different kinds of life forms.

2:06:04.260 --> 2:06:06.900
 And we know sex partly exists to scramble our genes

2:06:06.900 --> 2:06:09.900
 so that we have genetic variation

2:06:09.900 --> 2:06:14.220
 against the invasion of the bacteria and the viruses.

2:06:14.220 --> 2:06:16.020
 And it's endless.

2:06:16.020 --> 2:06:18.020
 Like I read some funny statistic,

2:06:18.020 --> 2:06:20.780
 like the density of viruses and bacteria in the ocean

2:06:20.780 --> 2:06:22.020
 is really high.

2:06:22.020 --> 2:06:23.900
 And one third of the bacteria die every day

2:06:23.900 --> 2:06:26.220
 because a virus is invading them.

2:06:26.220 --> 2:06:27.940
 Like one third of them.

2:06:27.940 --> 2:06:29.020
 Wow.

2:06:29.020 --> 2:06:31.020
 Like I don't know if that number is true,

2:06:31.020 --> 2:06:34.900
 but it was like the amount of competition

2:06:34.900 --> 2:06:36.460
 and what's going on is stunning.

2:06:37.380 --> 2:06:38.660
 And there's a theory as we age,

2:06:38.660 --> 2:06:41.780
 we slowly accumulate bacterias and viruses

2:06:41.780 --> 2:06:45.620
 and as our immune system kind of goes down,

2:06:45.620 --> 2:06:47.740
 that's what slowly kills us.

2:06:47.740 --> 2:06:50.220
 It just feels so peaceful from a human perspective

2:06:50.220 --> 2:06:51.420
 when we sit back and are able

2:06:51.420 --> 2:06:53.180
 to have a relaxed conversation.

2:06:54.220 --> 2:06:56.780
 And there's wars going on out there.

2:06:56.780 --> 2:07:00.900
 Like right now, you're harboring how many bacteria?

2:07:00.900 --> 2:07:04.860
 And the ones, many of them are parasites on you

2:07:04.860 --> 2:07:06.060
 and some of them are helpful

2:07:06.060 --> 2:07:07.780
 and some of them are modifying your behavior

2:07:07.780 --> 2:07:12.220
 and some of them are, it's just really wild.

2:07:12.220 --> 2:07:16.460
 But this particular manifestation is unusual

2:07:16.460 --> 2:07:18.420
 in the demographic, how it hit

2:07:18.420 --> 2:07:21.380
 and the political response that it engendered

2:07:21.380 --> 2:07:23.860
 and the healthcare response it engendered

2:07:23.860 --> 2:07:27.100
 and the technology it engendered, it's kind of wild.

2:07:27.100 --> 2:07:30.500
 Yeah, the communication on Twitter that it led to,

2:07:30.500 --> 2:07:32.980
 all that kind of stuff, at every single level, yeah.

2:07:32.980 --> 2:07:34.620
 But what usually kills life,

2:07:34.620 --> 2:07:39.460
 the big extinctions are caused by meteors and volcanoes.

2:07:39.460 --> 2:07:40.820
 That's the one you're worried about

2:07:40.820 --> 2:07:44.500
 as opposed to human created bombs that we launch.

2:07:44.500 --> 2:07:46.100
 Solar flares are another good one.

2:07:46.100 --> 2:07:48.580
 Occasionally, solar flares hit the planet.

2:07:48.580 --> 2:07:49.540
 So it's nature.

2:07:51.100 --> 2:07:52.700
 Yeah, it's all pretty wild.

2:07:53.540 --> 2:07:57.500
 On another historic moment, this is perhaps outside

2:07:57.500 --> 2:08:02.460
 but perhaps within your space of frameworks

2:08:02.460 --> 2:08:04.540
 that you think about that just happened,

2:08:04.540 --> 2:08:06.620
 I guess a couple of weeks ago is,

2:08:06.620 --> 2:08:08.020
 I don't know if you're paying attention at all,

2:08:08.020 --> 2:08:12.540
 is the GameStop and Wall Street bets.

2:08:12.540 --> 2:08:14.100
 It's super fun.

2:08:14.100 --> 2:08:16.580
 So it's really fascinating.

2:08:16.580 --> 2:08:19.180
 There's kind of a theme to this conversation today

2:08:19.180 --> 2:08:20.780
 because it's like neural networks,

2:08:21.980 --> 2:08:25.020
 it's cool how there's a large number of people

2:08:25.020 --> 2:08:30.020
 in a distributed way, almost having a kind of fun,

2:08:30.340 --> 2:08:33.340
 we're able to take on the powerful elites,

2:08:34.620 --> 2:08:39.060
 elite hedge funds, centralized powers and overpower them.

2:08:39.980 --> 2:08:43.340
 Do you have thoughts on this whole saga?

2:08:43.340 --> 2:08:45.020
 I don't know enough about finance,

2:08:45.020 --> 2:08:49.260
 but it was like the Elon, Robinhood guy when they talked.

2:08:49.260 --> 2:08:51.580
 Yeah, what'd you think about that?

2:08:51.580 --> 2:08:52.660
 Well, Robinhood guy didn't know

2:08:52.660 --> 2:08:54.300
 how the finance system worked.

2:08:54.300 --> 2:08:55.540
 That was clear, right?

2:08:55.540 --> 2:08:57.340
 He was treating like the people

2:08:57.340 --> 2:09:00.020
 who settled the transactions as a black box.

2:09:00.020 --> 2:09:01.620
 And suddenly somebody called him up and say,

2:09:01.620 --> 2:09:04.740
 hey, black box calling you, your transaction volume

2:09:04.740 --> 2:09:06.940
 means you need to put out $3 billion right now.

2:09:06.940 --> 2:09:08.940
 And he's like, I don't have $3 billion.

2:09:08.940 --> 2:09:10.540
 Like I don't even make any money on these trades.

2:09:10.540 --> 2:09:13.220
 Why do I owe $3 billion while you're sponsoring the trade?

2:09:13.220 --> 2:09:15.620
 So there was a set of abstractions

2:09:15.620 --> 2:09:19.540
 that I don't think either, like now we understand it.

2:09:19.540 --> 2:09:21.100
 Like this happens in chip design.

2:09:21.100 --> 2:09:25.660
 Like you buy wafers from TSMC or Samsung or Intel,

2:09:25.660 --> 2:09:27.460
 and they say it works like this

2:09:27.460 --> 2:09:29.020
 and you do your design based on that.

2:09:29.020 --> 2:09:31.260
 And then chip comes back and doesn't work.

2:09:31.260 --> 2:09:34.260
 And then suddenly you started having to open the black boxes.

2:09:34.260 --> 2:09:36.380
 The transistors really work like they said,

2:09:36.380 --> 2:09:37.620
 what's the real issue?

2:09:37.620 --> 2:09:42.620
 So there's a whole set of things

2:09:43.260 --> 2:09:46.220
 that created this opportunity and somebody spotted it.

2:09:46.220 --> 2:09:49.900
 Now, people spot these kinds of opportunities all the time.

2:09:49.900 --> 2:09:51.380
 So there's been flash crashes,

2:09:51.380 --> 2:09:55.340
 there's always short squeezes are fairly regular.

2:09:55.340 --> 2:09:58.500
 Every CEO I know hates the shorts

2:09:58.500 --> 2:10:01.860
 because they're trying to manipulate their stock

2:10:01.860 --> 2:10:03.860
 in a way that they make money

2:10:03.860 --> 2:10:07.420
 and deprive value from both the company

2:10:07.420 --> 2:10:08.900
 and the investors.

2:10:08.900 --> 2:10:13.700
 So the fact that some of these stocks were so short,

2:10:13.700 --> 2:10:17.340
 it's hilarious that this hasn't happened before.

2:10:17.340 --> 2:10:19.900
 I don't know why, and I don't actually know why

2:10:19.900 --> 2:10:23.460
 some serious hedge funds didn't do it to other hedge funds.

2:10:23.460 --> 2:10:24.380
 And some of the hedge funds

2:10:24.380 --> 2:10:26.580
 actually made a lot of money on this.

2:10:26.580 --> 2:10:31.580
 So my guess is we know 5% of what really happened

2:10:32.140 --> 2:10:34.420
 and that a lot of the players don't know what happened.

2:10:34.420 --> 2:10:37.420
 And the people who probably made the most money

2:10:37.420 --> 2:10:39.500
 aren't the people that they're talking about.

2:10:39.500 --> 2:10:41.060
 That's.

2:10:41.060 --> 2:10:42.660
 Do you think there was something,

2:10:42.660 --> 2:10:46.700
 I mean, this is the cool kind of Elon,

2:10:47.940 --> 2:10:50.660
 you're the same kind of conversationalist,

2:10:50.660 --> 2:10:53.860
 which is like first principles questions of like,

2:10:53.860 --> 2:10:56.260
 what the hell happened?

2:10:56.260 --> 2:10:57.900
 Just very basic questions of like,

2:10:57.900 --> 2:10:59.860
 was there something shady going on?

2:11:00.780 --> 2:11:03.660
 What, who are the parties involved?

2:11:03.660 --> 2:11:06.340
 It's the basic questions everybody wants to know about.

2:11:06.340 --> 2:11:10.340
 Yeah, so like we're in a very hyper competitive world,

2:11:10.340 --> 2:11:12.180
 but transactions like buying and selling stock

2:11:12.180 --> 2:11:13.780
 is a trust event.

2:11:13.780 --> 2:11:16.980
 I trust the company, represented themselves properly.

2:11:16.980 --> 2:11:19.660
 I bought the stock because I think it's gonna go up.

2:11:19.660 --> 2:11:22.660
 I trust that the regulations are solid.

2:11:22.660 --> 2:11:26.140
 Now, inside of that, there's all kinds of places

2:11:26.140 --> 2:11:31.140
 where humans over trust and this exposed,

2:11:31.140 --> 2:11:34.580
 let's say some weak points in the system.

2:11:34.580 --> 2:11:37.340
 I don't know if it's gonna get corrected.

2:11:37.340 --> 2:11:40.860
 I don't know if we have close to the real story.

2:11:41.740 --> 2:11:44.460
 Yeah, my suspicion is we don't.

2:11:44.460 --> 2:11:47.300
 And listen to that guy, he was like a little wide eyed

2:11:47.300 --> 2:11:49.060
 about and then he did this and then he did that.

2:11:49.060 --> 2:11:51.820
 And I was like, I think you should know more

2:11:51.820 --> 2:11:54.180
 about your business than that.

2:11:54.180 --> 2:11:56.140
 But again, there's many businesses

2:11:56.140 --> 2:11:58.780
 when like this layer is really stable,

2:11:58.780 --> 2:12:00.700
 you stop paying attention to it.

2:12:00.700 --> 2:12:04.500
 You pay attention to the stuff that's bugging you or new.

2:12:04.500 --> 2:12:05.780
 You don't pay attention to the stuff

2:12:05.780 --> 2:12:07.060
 that just seems to work all the time.

2:12:07.060 --> 2:12:11.100
 You just, sky's blue every day, California.

2:12:11.100 --> 2:12:12.740
 And every once in a while it rains

2:12:12.740 --> 2:12:15.300
 and everybody's like, what do we do?

2:12:15.300 --> 2:12:17.940
 Somebody go bring in the lawn furniture.

2:12:17.940 --> 2:12:18.780
 It's getting wet.

2:12:18.780 --> 2:12:19.980
 You don't know why it's getting wet.

2:12:19.980 --> 2:12:20.820
 Yeah, it doesn't always work.

2:12:20.820 --> 2:12:24.580
 I was blue for like a hundred days and now it's, so.

2:12:24.580 --> 2:12:27.020
 But part of the problem here with Vlad,

2:12:27.020 --> 2:12:29.540
 the CEO of Robinhood is the scaling

2:12:29.540 --> 2:12:32.540
 that we've been talking about is there's a lot

2:12:32.540 --> 2:12:36.020
 of unexpected things that happen with the scaling

2:12:36.020 --> 2:12:39.660
 and you have to be, I think the scaling forces you

2:12:39.660 --> 2:12:41.780
 to then return to the fundamentals.

2:12:41.780 --> 2:12:44.460
 Well, it's interesting because when you buy and sell stocks,

2:12:44.460 --> 2:12:46.460
 the scaling is, the stocks don't only move

2:12:46.460 --> 2:12:48.180
 in a certain range and if you buy a stock,

2:12:48.180 --> 2:12:50.020
 you can only lose that amount of money.

2:12:50.020 --> 2:12:52.420
 On the short market, you can lose a lot more

2:12:52.420 --> 2:12:53.860
 than you can benefit.

2:12:53.860 --> 2:12:57.220
 Like it has a weird cost function

2:12:57.220 --> 2:12:59.260
 or whatever the right word for that is.

2:12:59.260 --> 2:13:01.140
 So he was trading in a market

2:13:01.140 --> 2:13:04.220
 where he wasn't actually capitalized for the downside.

2:13:04.220 --> 2:13:06.260
 If it got outside a certain range.

2:13:07.380 --> 2:13:09.780
 Now, whether something nefarious has happened,

2:13:09.780 --> 2:13:12.580
 I have no idea, but at some point,

2:13:12.580 --> 2:13:16.540
 the financial risk to both him and his customers

2:13:16.540 --> 2:13:19.140
 was way outside of his financial capacity

2:13:19.140 --> 2:13:23.380
 and his understanding how the system work was clearly weak

2:13:23.380 --> 2:13:25.140
 or he didn't represent himself.

2:13:25.140 --> 2:13:28.780
 I don't know the person and when I listened to him,

2:13:28.780 --> 2:13:30.500
 it could have been the surprise question was like,

2:13:30.500 --> 2:13:34.020
 and then these guys called and it sounded like

2:13:34.020 --> 2:13:36.260
 he was treating stuff as a black box.

2:13:36.260 --> 2:13:38.540
 Maybe he shouldn't have, but maybe he has a whole pile

2:13:38.540 --> 2:13:40.060
 of experts somewhere else and it was going on.

2:13:40.060 --> 2:13:41.220
 I don't know.

2:13:41.220 --> 2:13:45.180
 Yeah, I mean, this is one of the qualities

2:13:45.180 --> 2:13:49.060
 of a good leader is under fire, you have to perform.

2:13:49.060 --> 2:13:53.020
 And that means to think clearly and to speak clearly.

2:13:53.020 --> 2:13:55.260
 And he dropped the ball on those things

2:13:55.260 --> 2:13:58.060
 because and understand the problem quickly,

2:13:58.060 --> 2:14:03.060
 learn and understand the problem at this basic level.

2:14:03.380 --> 2:14:05.100
 What the hell happened?

2:14:05.100 --> 2:14:09.820
 And my guess is, at some level it was amateurs trading

2:14:09.820 --> 2:14:12.940
 against experts slash insiders slash people

2:14:12.940 --> 2:14:14.900
 with special information.

2:14:14.900 --> 2:14:16.900
 Outsiders versus insiders.

2:14:16.900 --> 2:14:20.700
 Yeah, and the insiders, my guess is the next time

2:14:20.700 --> 2:14:22.980
 this happens, we'll make money on it.

2:14:22.980 --> 2:14:25.100
 The insiders always win?

2:14:25.100 --> 2:14:27.140
 Well, they have more tools and more incentive.

2:14:27.140 --> 2:14:28.460
 I mean, this always happens.

2:14:28.460 --> 2:14:30.820
 Like the outsiders are doing this for fun.

2:14:30.820 --> 2:14:33.340
 The insiders are doing this 24 seven.

2:14:33.340 --> 2:14:35.740
 But there's numbers in the outsiders.

2:14:35.740 --> 2:14:37.540
 This is the interesting thing is it could be

2:14:37.540 --> 2:14:38.380
 a new chapter. There's numbers

2:14:38.380 --> 2:14:39.380
 on the insiders too.

2:14:41.100 --> 2:14:44.020
 Different kind of numbers, yeah.

2:14:44.020 --> 2:14:46.100
 But this could be a new era because, I don't know,

2:14:46.100 --> 2:14:49.460
 at least I didn't expect that a bunch of Redditors could,

2:14:49.460 --> 2:14:51.580
 there's millions of people who can get together.

2:14:51.580 --> 2:14:52.420
 It was a surprise attack.

2:14:52.420 --> 2:14:54.220
 The next one will be a surprise.

2:14:54.220 --> 2:14:57.540
 But don't you think the crowd, the people are planning

2:14:57.540 --> 2:14:59.260
 the next attack?

2:14:59.260 --> 2:15:00.500
 We'll see.

2:15:00.500 --> 2:15:01.420
 But it has to be a surprise.

2:15:01.420 --> 2:15:02.700
 It can't be the same game.

2:15:04.620 --> 2:15:05.460
 And so the insiders.

2:15:05.460 --> 2:15:07.980
 It's like, it could be there's a very large number

2:15:07.980 --> 2:15:10.540
 of games to play and they can be agile about it.

2:15:10.540 --> 2:15:11.380
 I don't know.

2:15:11.380 --> 2:15:12.220
 I'm not an expert.

2:15:12.220 --> 2:15:13.780
 Right, that's a good question.

2:15:13.780 --> 2:15:16.500
 The space of games, how restricted is it?

2:15:18.020 --> 2:15:20.220
 Yeah, and the system is so complicated

2:15:20.220 --> 2:15:22.740
 it could be relatively unrestricted.

2:15:22.740 --> 2:15:26.260
 And also during the last couple of financial crashes,

2:15:27.180 --> 2:15:30.180
 what set it off was sets of derivative events

2:15:30.180 --> 2:15:35.180
 where Nassim Taleb's thing is they're trying

2:15:35.980 --> 2:15:39.420
 to lower volatility in the short run

2:15:39.420 --> 2:15:41.660
 by creating tail events.

2:15:41.660 --> 2:15:43.700
 And the system's always evolved towards that

2:15:43.700 --> 2:15:45.620
 and then they always crash.

2:15:45.620 --> 2:15:50.620
 The S curve is the start low, ramp, plateau, crash.

2:15:50.620 --> 2:15:53.020
 It's 100% effective.

2:15:54.540 --> 2:15:55.860
 In the long run.

2:15:55.860 --> 2:15:59.820
 Let me ask you some advice to put on your profound hat.

2:16:01.660 --> 2:16:04.620
 There's a bunch of young folks who listen to this thing

2:16:04.620 --> 2:16:07.460
 for no good reason whatsoever.

2:16:07.460 --> 2:16:10.620
 Undergraduate students, maybe high school students,

2:16:10.620 --> 2:16:13.020
 maybe just young folks, a young at heart

2:16:13.020 --> 2:16:16.860
 looking for the next steps to take in life.

2:16:16.860 --> 2:16:19.300
 What advice would you give to a young person today

2:16:19.300 --> 2:16:23.860
 about life, maybe career, but also life in general?

2:16:23.860 --> 2:16:25.100
 Get good at some stuff.

2:16:26.100 --> 2:16:28.220
 Well, get to know yourself, right?

2:16:28.220 --> 2:16:30.660
 Get good at something that you're actually interested in.

2:16:30.660 --> 2:16:33.500
 You have to love what you're doing to get good at it.

2:16:33.500 --> 2:16:34.420
 You really gotta find that.

2:16:34.420 --> 2:16:35.800
 Don't waste all your time doing stuff

2:16:35.800 --> 2:16:40.140
 that's just boring or bland or numbing, right?

2:16:40.140 --> 2:16:42.380
 Don't let old people screw you.

2:16:42.380 --> 2:16:46.740
 Well, people get talked into doing all kinds of shit

2:16:46.740 --> 2:16:49.300
 and racking up huge student debts

2:16:49.300 --> 2:16:52.580
 and there's so much crap going on.

2:16:52.580 --> 2:16:54.700
 And then drains your time and drains your energy.

2:16:54.700 --> 2:16:58.100
 The Eric Weinstein thesis that the older generation

2:16:58.100 --> 2:17:01.100
 won't let go and they're trapping all the young people.

2:17:01.100 --> 2:17:02.460
 Do you think there's some truth to that?

2:17:02.460 --> 2:17:03.300
 Yeah, sure.

2:17:04.940 --> 2:17:06.940
 Just because you're old doesn't mean you stop thinking.

2:17:06.940 --> 2:17:10.380
 I know lots of really original old people.

2:17:10.380 --> 2:17:11.300
 I'm an old person.

2:17:14.260 --> 2:17:15.660
 But you have to be conscious about it.

2:17:15.660 --> 2:17:18.940
 You can fall into the ruts and then do that.

2:17:18.940 --> 2:17:22.060
 I mean, when I hear young people spouting opinions

2:17:22.060 --> 2:17:24.380
 that sounds like they come from Fox News or CNN,

2:17:24.380 --> 2:17:27.980
 I think they've been captured by groupthink and memes.

2:17:27.980 --> 2:17:29.780
 They're supposed to think on their own.

2:17:29.780 --> 2:17:31.420
 So if you find yourself repeating

2:17:31.420 --> 2:17:33.420
 what everybody else is saying,

2:17:33.420 --> 2:17:35.120
 you're not gonna have a good life.

2:17:36.260 --> 2:17:38.460
 Like, that's not how the world works.

2:17:38.460 --> 2:17:41.040
 It seems safe, but it puts you at great jeopardy

2:17:41.040 --> 2:17:45.900
 for being boring or unhappy.

2:17:45.900 --> 2:17:47.780
 How long did it take you to find the thing

2:17:47.780 --> 2:17:50.620
 that you have fun with?

2:17:50.620 --> 2:17:52.140
 Oh, I don't know.

2:17:52.140 --> 2:17:54.300
 I've been a fun person since I was pretty little.

2:17:54.300 --> 2:17:55.140
 So everything.

2:17:55.140 --> 2:17:58.100
 I've gone through a couple periods of depression in my life.

2:17:58.100 --> 2:18:00.180
 For a good reason or for a reason

2:18:00.180 --> 2:18:02.620
 that doesn't make any sense?

2:18:02.620 --> 2:18:05.980
 Yeah, like some things are hard.

2:18:05.980 --> 2:18:08.900
 Like you go through mental transitions in high school.

2:18:08.900 --> 2:18:10.700
 I was really depressed for a year

2:18:10.700 --> 2:18:15.140
 and I think I had my first midlife crisis at 26.

2:18:15.140 --> 2:18:16.660
 I kind of thought, is this all there is?

2:18:16.660 --> 2:18:19.340
 Like I was working at a job that I loved,

2:18:20.500 --> 2:18:23.420
 but I was going to work and all my time was consumed.

2:18:23.420 --> 2:18:25.820
 What's the escape out of that depression?

2:18:25.820 --> 2:18:29.220
 What's the answer to is this all there is?

2:18:29.220 --> 2:18:31.820
 Well, a friend of mine, I asked him,

2:18:31.820 --> 2:18:32.900
 because he was working his ass off,

2:18:32.900 --> 2:18:34.540
 I said, what's your work life balance?

2:18:34.540 --> 2:18:39.540
 Like there's work, friends, family, personal time.

2:18:39.540 --> 2:18:41.380
 Are you balancing any of that?

2:18:41.380 --> 2:18:43.580
 And he said, work 80%, family 20%.

2:18:43.580 --> 2:18:47.540
 And I tried to find some time to sleep.

2:18:47.540 --> 2:18:49.220
 Like there's no personal time.

2:18:49.220 --> 2:18:51.820
 There's no passionate time.

2:18:51.820 --> 2:18:54.580
 Like the young people are often passionate about work.

2:18:54.580 --> 2:18:56.980
 So I was certainly like that.

2:18:56.980 --> 2:18:59.940
 But you need to have some space in your life

2:18:59.940 --> 2:19:01.820
 for different things.

2:19:01.820 --> 2:19:05.860
 And that creates, that makes you resistant

2:19:05.860 --> 2:19:10.860
 to the whole, the deep dips into depression kind of thing.

2:19:11.260 --> 2:19:13.060
 Yeah, well, you have to get to know yourself too.

2:19:13.060 --> 2:19:14.460
 Meditation helps.

2:19:14.460 --> 2:19:18.540
 Some physical, something physically intense helps.

2:19:18.540 --> 2:19:21.940
 Like the weird places your mind goes kind of thing.

2:19:21.940 --> 2:19:23.780
 Like, and why does it happen?

2:19:23.780 --> 2:19:24.860
 Why do you do what you do?

2:19:24.860 --> 2:19:27.660
 Like triggers, like the things that cause your mind

2:19:27.660 --> 2:19:29.460
 to go to different places kind of thing,

2:19:29.460 --> 2:19:32.180
 or like events like.

2:19:32.180 --> 2:19:33.740
 Your upbringing for better or worse,

2:19:33.740 --> 2:19:35.700
 whether your parents are great people or not,

2:19:35.700 --> 2:19:40.700
 you come into adulthood with all kinds of emotional burdens.

2:19:42.780 --> 2:19:45.060
 And you can see some people are so bloody stiff

2:19:45.060 --> 2:19:47.180
 and restrained, and they think the world's

2:19:47.180 --> 2:19:50.660
 fundamentally negative, like you maybe.

2:19:50.660 --> 2:19:53.020
 You have unexplored territory.

2:19:53.020 --> 2:19:53.980
 Yeah.

2:19:53.980 --> 2:19:56.300
 Or you're afraid of something.

2:19:56.300 --> 2:19:58.780
 Definitely afraid of quite a few things.

2:19:58.780 --> 2:20:00.180
 Then you gotta go face them.

2:20:00.180 --> 2:20:03.460
 Like what's the worst thing that can happen?

2:20:03.460 --> 2:20:05.180
 You're gonna die, right?

2:20:05.180 --> 2:20:06.340
 Like that's inevitable.

2:20:06.340 --> 2:20:07.380
 You might as well get over that.

2:20:07.380 --> 2:20:08.860
 Like 100%, that's right.

2:20:09.780 --> 2:20:11.060
 Like people are worried about the virus,

2:20:11.060 --> 2:20:14.460
 but you know, the human condition is pretty deadly.

2:20:14.460 --> 2:20:16.300
 There's something about embarrassment

2:20:16.300 --> 2:20:18.220
 that's, I've competed a lot in my life,

2:20:18.220 --> 2:20:21.980
 and I think the, if I'm to introspect it,

2:20:21.980 --> 2:20:26.100
 the thing I'm most afraid of is being like humiliated,

2:20:26.100 --> 2:20:26.940
 I think.

2:20:26.940 --> 2:20:28.020
 Yeah, nobody cares about that.

2:20:28.020 --> 2:20:29.980
 Like you're the only person on the planet

2:20:29.980 --> 2:20:31.620
 that cares about you being humiliated.

2:20:31.620 --> 2:20:32.460
 Exactly.

2:20:32.460 --> 2:20:34.740
 It's like a really useless thought.

2:20:34.740 --> 2:20:35.580
 It is.

2:20:35.580 --> 2:20:39.540
 It's like, you're all humiliated.

2:20:39.540 --> 2:20:41.140
 Something happened in a room full of people,

2:20:41.140 --> 2:20:42.660
 and they walk out, and they didn't think about it

2:20:42.660 --> 2:20:43.780
 one more second.

2:20:43.780 --> 2:20:45.900
 Or maybe somebody told a funny story to somebody else.

2:20:45.900 --> 2:20:48.580
 And then it dissipates it throughout, yeah.

2:20:48.580 --> 2:20:49.420
 Yeah.

2:20:49.420 --> 2:20:50.260
 No, I know it too.

2:20:50.260 --> 2:20:53.340
 I mean, I've been really embarrassed about shit

2:20:53.340 --> 2:20:55.500
 that nobody cared about myself.

2:20:55.500 --> 2:20:56.340
 Yeah.

2:20:56.340 --> 2:20:57.180
 It's a funny thing.

2:20:57.180 --> 2:20:59.620
 So the worst thing ultimately is just.

2:20:59.620 --> 2:21:01.020
 Yeah, but that's a cage,

2:21:01.020 --> 2:21:02.060
 and then you have to get out of it.

2:21:02.060 --> 2:21:02.900
 Yeah.

2:21:02.900 --> 2:21:03.860
 Like once you, here's the thing.

2:21:03.860 --> 2:21:05.740
 Once you find something like that,

2:21:05.740 --> 2:21:08.160
 you have to be determined to break it.

2:21:09.060 --> 2:21:10.260
 Because otherwise you'll just,

2:21:10.260 --> 2:21:11.740
 so you accumulate that kind of junk,

2:21:11.740 --> 2:21:15.420
 and then you die as a mess.

2:21:15.420 --> 2:21:18.420
 So the goal, I guess it's like a cage within a cage.

2:21:18.420 --> 2:21:21.980
 I guess the goal is to die in the biggest possible cage.

2:21:21.980 --> 2:21:23.760
 Well, ideally you'd have no cage.

2:21:25.460 --> 2:21:26.500
 People do get enlightened.

2:21:26.500 --> 2:21:27.460
 I've met a few.

2:21:27.460 --> 2:21:28.500
 It's great.

2:21:28.500 --> 2:21:29.340
 You've found a few?

2:21:29.340 --> 2:21:30.460
 There's a few out there?

2:21:30.460 --> 2:21:31.280
 I don't know.

2:21:31.280 --> 2:21:32.120
 Of course there are.

2:21:32.120 --> 2:21:33.360
 I don't know.

2:21:33.360 --> 2:21:35.520
 Either that or it's a great sales pitch.

2:21:35.520 --> 2:21:37.080
 There's enlightened people writing books

2:21:37.080 --> 2:21:38.280
 and doing all kinds of stuff.

2:21:38.280 --> 2:21:39.520
 It's a good way to sell a book.

2:21:39.520 --> 2:21:40.840
 I'll give you that.

2:21:40.840 --> 2:21:42.880
 You've never met somebody you just thought,

2:21:42.880 --> 2:21:43.840
 they just kill me.

2:21:43.840 --> 2:21:47.880
 Like they just, like mental clarity, humor.

2:21:47.880 --> 2:21:49.560
 No, 100%, but I just feel like

2:21:49.560 --> 2:21:50.960
 they're living in a bigger cage.

2:21:50.960 --> 2:21:52.040
 They have their own.

2:21:52.040 --> 2:21:53.360
 You still think there's a cage?

2:21:53.360 --> 2:21:54.400
 There's still a cage.

2:21:54.400 --> 2:21:56.600
 You secretly suspect there's always a cage.

2:21:57.560 --> 2:21:59.880
 There's nothing outside the universe.

2:21:59.880 --> 2:22:02.280
 There's nothing outside the cage.

2:22:02.280 --> 2:22:07.280
 You work in a bunch of companies,

2:22:10.160 --> 2:22:12.640
 you lead a lot of amazing teams.

2:22:15.320 --> 2:22:16.580
 I'm not sure if you've ever been

2:22:16.580 --> 2:22:19.440
 like in the early stages of a startup,

2:22:19.440 --> 2:22:22.760
 but do you have advice for somebody

2:22:24.560 --> 2:22:28.320
 that wants to do a startup or build a company,

2:22:28.320 --> 2:22:31.160
 like build a strong team of engineers that are passionate

2:22:31.160 --> 2:22:35.000
 and just want to solve a big problem?

2:22:35.000 --> 2:22:39.080
 Like, is there a more specifically on that point?

2:22:39.080 --> 2:22:41.360
 Well, you have to be really good at stuff.

2:22:41.360 --> 2:22:43.040
 If you're going to lead and build a team,

2:22:43.040 --> 2:22:44.520
 you better be really interested

2:22:44.520 --> 2:22:46.000
 in how people work and think.

2:22:46.960 --> 2:22:49.040
 The people or the solution to the problem.

2:22:49.040 --> 2:22:50.160
 So there's two things, right?

2:22:50.160 --> 2:22:52.920
 One is how people work and the other is the...

2:22:52.920 --> 2:22:55.640
 Well, actually there's quite a few successful startups.

2:22:55.640 --> 2:22:56.800
 It's pretty clear the founders

2:22:56.800 --> 2:22:58.360
 don't know anything about people.

2:22:58.360 --> 2:23:01.480
 Like the idea was so powerful that it propelled them.

2:23:01.480 --> 2:23:03.760
 But I suspect somewhere early,

2:23:03.760 --> 2:23:06.980
 they hired some people who understood people

2:23:06.980 --> 2:23:08.960
 because people really need a lot of care and feeding

2:23:08.960 --> 2:23:10.480
 to collaborate and work together

2:23:10.480 --> 2:23:12.760
 and feel engaged and work hard.

2:23:13.800 --> 2:23:17.000
 Like startups are all about out producing other people.

2:23:17.000 --> 2:23:19.820
 Like you're nimble because you don't have any legacy.

2:23:19.820 --> 2:23:22.320
 You don't have a bunch of people

2:23:22.320 --> 2:23:24.720
 who are depressed about life just showing up.

2:23:24.720 --> 2:23:28.000
 So startups have a lot of advantages that way.

2:23:29.720 --> 2:23:32.960
 Do you like the, Steve Jobs talked about this idea

2:23:32.960 --> 2:23:34.940
 of A players and B players.

2:23:34.940 --> 2:23:37.240
 I don't know if you know this formulation.

2:23:37.240 --> 2:23:38.080
 Yeah, no.

2:23:39.940 --> 2:23:43.400
 Organizations that get taken over by B player leaders

2:23:44.680 --> 2:23:48.120
 often really underperform their C players.

2:23:48.120 --> 2:23:50.720
 That said, in big organizations,

2:23:50.720 --> 2:23:52.600
 there's so much work to do.

2:23:52.600 --> 2:23:54.040
 And there's so many people who are happy

2:23:54.040 --> 2:23:57.480
 to do what the leadership or the big idea people

2:23:57.480 --> 2:24:00.320
 would consider menial jobs.

2:24:00.320 --> 2:24:01.880
 And you need a place for them,

2:24:01.880 --> 2:24:05.680
 but you need an organization that both values and rewards

2:24:05.680 --> 2:24:08.460
 them but doesn't let them take over the leadership of it.

2:24:08.460 --> 2:24:09.300
 Got it.

2:24:09.300 --> 2:24:11.040
 So you need to have an organization

2:24:11.040 --> 2:24:11.960
 that's resistant to that.

2:24:11.960 --> 2:24:16.720
 But in the early days, the notion with Steve

2:24:16.720 --> 2:24:20.680
 was that like one B player in a room of A players

2:24:20.680 --> 2:24:23.040
 will be like destructive to the whole.

2:24:23.040 --> 2:24:24.360
 I've seen that happen.

2:24:24.360 --> 2:24:26.560
 I don't know if it's like always true.

2:24:28.200 --> 2:24:30.320
 You run into people who are clearly B players

2:24:30.320 --> 2:24:31.520
 but they think they're A players

2:24:31.520 --> 2:24:33.200
 and so they have a loud voice at the table

2:24:33.200 --> 2:24:35.160
 and they make lots of demands for that.

2:24:35.160 --> 2:24:37.520
 But there's other people who are like, I know who I am.

2:24:37.520 --> 2:24:39.720
 I just wanna work with cool people on cool shit

2:24:39.720 --> 2:24:42.560
 and just tell me what to do and I'll go get it done.

2:24:42.560 --> 2:24:45.840
 So you have to, again, this is like people skills.

2:24:45.840 --> 2:24:47.960
 What kind of person is it?

2:24:47.960 --> 2:24:51.040
 I've met some really great people I love working with

2:24:51.040 --> 2:24:53.600
 that weren't the biggest ID people or the most productive

2:24:53.600 --> 2:24:56.200
 ever but they show up, they get it done.

2:24:56.200 --> 2:24:59.880
 They create connection and community that people value.

2:24:59.880 --> 2:25:02.360
 It's pretty diverse so I don't think

2:25:02.360 --> 2:25:03.660
 there's a recipe for that.

2:25:05.120 --> 2:25:07.000
 I gotta ask you about love.

2:25:07.000 --> 2:25:08.700
 I heard you're into this now.

2:25:08.700 --> 2:25:09.560
 Into this love thing?

2:25:09.560 --> 2:25:11.720
 Yeah, is this, do you think this is your solution

2:25:11.720 --> 2:25:13.320
 to your depression?

2:25:13.320 --> 2:25:14.880
 No, I'm just trying to, like you said,

2:25:14.880 --> 2:25:16.960
 delighting people and occasionally trying to sell a book.

2:25:16.960 --> 2:25:18.120
 I'm writing a book about love.

2:25:18.120 --> 2:25:18.960
 You're writing a book about love?

2:25:18.960 --> 2:25:21.080
 No, I'm not, I'm not.

2:25:21.080 --> 2:25:25.080
 I have a friend of mine, he's gonna,

2:25:25.080 --> 2:25:27.240
 he said you should really write a book

2:25:27.240 --> 2:25:29.080
 about your management philosophy.

2:25:29.080 --> 2:25:30.620
 He said it'd be a short book.

2:25:35.000 --> 2:25:37.800
 Well, that one was thought pretty well.

2:25:37.800 --> 2:25:40.440
 What role do you think love, family, friendship,

2:25:40.440 --> 2:25:44.400
 all that kind of human stuff play in a successful life?

2:25:44.400 --> 2:25:46.360
 You've been exceptionally successful in the space

2:25:46.360 --> 2:25:51.160
 of running teams, building cool shit in this world,

2:25:51.160 --> 2:25:53.160
 creating some amazing things.

2:25:53.160 --> 2:25:54.720
 What, did love get in the way?

2:25:54.720 --> 2:25:57.720
 Did love help the family get in the way?

2:25:57.720 --> 2:25:59.760
 Did family help friendship?

2:25:59.760 --> 2:26:02.120
 You want the engineer's answer?

2:26:02.120 --> 2:26:03.120
 Please.

2:26:03.120 --> 2:26:05.800
 But first, love is functional, right?

2:26:05.800 --> 2:26:07.280
 It's functional in what way?

2:26:07.280 --> 2:26:11.000
 So we habituate ourselves to the environment.

2:26:11.000 --> 2:26:13.920
 And actually, Jordan Peterson told me this line.

2:26:13.920 --> 2:26:16.440
 So you go through life and you just get used to everything,

2:26:16.440 --> 2:26:17.800
 except for the things you love.

2:26:17.800 --> 2:26:20.080
 They remain new.

2:26:20.080 --> 2:26:22.440
 Like, this is really useful for, you know,

2:26:22.440 --> 2:26:26.080
 like other people's children and dogs and trees.

2:26:26.080 --> 2:26:27.700
 You just don't pay that much attention to them.

2:26:27.700 --> 2:26:31.000
 Your own kids, you monitor them really closely.

2:26:31.000 --> 2:26:32.720
 Like, and if they go off a little bit,

2:26:32.720 --> 2:26:35.280
 because you love them, if you're smart,

2:26:35.280 --> 2:26:37.480
 if you're gonna be a successful parent,

2:26:37.480 --> 2:26:38.920
 you notice it right away.

2:26:38.920 --> 2:26:43.920
 You don't habituate to just things you love.

2:26:44.320 --> 2:26:46.160
 And if you want to be successful at work,

2:26:46.160 --> 2:26:47.560
 if you don't love it,

2:26:47.560 --> 2:26:50.400
 you're not gonna put the time in somebody else.

2:26:50.400 --> 2:26:51.600
 It's somebody else that loves it.

2:26:51.600 --> 2:26:53.760
 Like, because it's new and interesting,

2:26:53.760 --> 2:26:56.060
 and that lets you go to the next level.

2:26:57.560 --> 2:26:59.120
 So it's the thing, it's just a function

2:26:59.120 --> 2:27:01.680
 that generates newness and novelty

2:27:01.680 --> 2:27:04.680
 and surprises, you know, all those kinds of things.

2:27:04.680 --> 2:27:06.360
 It's really interesting.

2:27:06.360 --> 2:27:09.840
 There's people who figured out lots of frameworks for this.

2:27:09.840 --> 2:27:11.600
 Like, humans seem to go,

2:27:11.600 --> 2:27:13.880
 in partnership, go through interests.

2:27:13.880 --> 2:27:16.640
 Like, suddenly somebody's interesting,

2:27:16.640 --> 2:27:18.200
 and then you're infatuated with them,

2:27:18.200 --> 2:27:20.080
 and then you're in love with them.

2:27:20.080 --> 2:27:22.600
 And then you, you know, different people have ideas

2:27:22.600 --> 2:27:24.520
 about parental love or mature love.

2:27:24.520 --> 2:27:26.600
 Like, you go through a cycle of that,

2:27:26.600 --> 2:27:27.840
 which keeps us together,

2:27:27.840 --> 2:27:30.600
 and it's super functional for creating families

2:27:30.600 --> 2:27:34.560
 and creating communities and making you support somebody

2:27:34.560 --> 2:27:36.960
 despite the fact that you don't love them.

2:27:36.960 --> 2:27:41.960
 Like, and it can be really enriching.

2:27:44.260 --> 2:27:47.480
 You know, now, in the work life balance scheme,

2:27:47.480 --> 2:27:49.760
 if alls you do is work,

2:27:49.760 --> 2:27:52.320
 you think you may be optimizing your work potential,

2:27:52.320 --> 2:27:53.840
 but if you don't love your work

2:27:53.840 --> 2:27:56.960
 or you don't have family and friends

2:27:56.960 --> 2:27:58.420
 and things you care about,

2:27:59.280 --> 2:28:02.000
 your brain isn't well balanced.

2:28:02.000 --> 2:28:03.440
 Like, everybody knows the experience of,

2:28:03.440 --> 2:28:04.680
 he works on something all week.

2:28:04.680 --> 2:28:07.720
 He went home, took two days off, and he came back in.

2:28:07.720 --> 2:28:09.360
 The odds of you working on the thing,

2:28:09.360 --> 2:28:12.760
 you picking up right where you left off is zero.

2:28:12.760 --> 2:28:14.260
 Your brain refactored it.

2:28:17.040 --> 2:28:19.200
 But being in love is great.

2:28:19.200 --> 2:28:22.440
 It's like changes the color of the light in the room.

2:28:22.440 --> 2:28:25.600
 It creates a spaciousness that's different.

2:28:25.600 --> 2:28:26.680
 It helps you think.

2:28:27.900 --> 2:28:29.560
 It makes you strong.

2:28:29.560 --> 2:28:32.520
 Bukowski had this line about love being a fog

2:28:32.520 --> 2:28:36.240
 that dissipates with the first light of reality

2:28:36.240 --> 2:28:37.080
 in the morning.

2:28:37.080 --> 2:28:38.000
 That's depressing.

2:28:38.000 --> 2:28:39.560
 I think it's the other way around.

2:28:39.560 --> 2:28:40.400
 It lasts.

2:28:40.400 --> 2:28:42.100
 Well, like you said, it's a function.

2:28:42.100 --> 2:28:42.940
 It's a thing that generates.

2:28:42.940 --> 2:28:45.640
 It can be the light that actually enlivens your world

2:28:45.640 --> 2:28:49.320
 and creates the interest and the power and the strength

2:28:49.320 --> 2:28:50.500
 to go do something.

2:28:51.720 --> 2:28:54.360
 Well, it's like, that sounds like,

2:28:54.360 --> 2:28:56.200
 you know, there's like physical love, emotional love,

2:28:56.200 --> 2:28:58.240
 intellectual love, spiritual love, right?

2:28:58.240 --> 2:28:59.840
 Isn't it all the same thing, kind of?

2:28:59.840 --> 2:29:01.080
 Nope.

2:29:01.080 --> 2:29:02.160
 You should differentiate that.

2:29:02.160 --> 2:29:04.040
 Maybe that's your problem.

2:29:04.040 --> 2:29:06.080
 In your book, you should refine that a little bit.

2:29:06.080 --> 2:29:07.280
 Is it different chapters?

2:29:07.280 --> 2:29:08.560
 Yeah, there's different chapters.

2:29:08.560 --> 2:29:11.600
 What's these, aren't these just different layers

2:29:11.600 --> 2:29:14.360
 of the same thing, the stack of physical?

2:29:14.360 --> 2:29:17.400
 People, some people are addicted to physical love

2:29:17.400 --> 2:29:20.400
 and they have no idea about emotional or intellectual love.

2:29:21.880 --> 2:29:22.960
 I don't know if they're the same things.

2:29:22.960 --> 2:29:23.920
 I think they're different.

2:29:23.920 --> 2:29:24.760
 That's true.

2:29:24.760 --> 2:29:25.580
 They could be different.

2:29:25.580 --> 2:29:28.200
 I guess the ultimate goal is for it to be the same.

2:29:28.200 --> 2:29:30.200
 Well, if you want something to be bigger and interesting,

2:29:30.200 --> 2:29:32.560
 you should find all its components and differentiate them,

2:29:32.560 --> 2:29:34.520
 not clump it together.

2:29:34.520 --> 2:29:36.360
 Like, people do this all the time.

2:29:36.360 --> 2:29:38.120
 Yeah, the modularity.

2:29:38.120 --> 2:29:39.440
 Get your abstraction layers right

2:29:39.440 --> 2:29:41.600
 and then you have room to breathe.

2:29:41.600 --> 2:29:43.480
 Well, maybe you can write the forward to my book

2:29:43.480 --> 2:29:44.320
 about love.

2:29:44.320 --> 2:29:45.960
 Or the afterwards.

2:29:45.960 --> 2:29:46.800
 And the after.

2:29:46.800 --> 2:29:47.740
 You really tried.

2:29:49.320 --> 2:29:51.920
 I feel like Lex has made a lot of progress in this book.

2:29:53.920 --> 2:29:55.880
 Well, you have things in your life that you love.

2:29:55.880 --> 2:29:57.680
 Yeah, yeah.

2:29:57.680 --> 2:29:59.800
 And they are, you're right, they're modular.

2:29:59.800 --> 2:30:01.280
 It's quality.

2:30:01.280 --> 2:30:04.560
 And you can have multiple things with the same person

2:30:04.560 --> 2:30:06.320
 or the same thing.

2:30:06.320 --> 2:30:08.520
 But, yeah.

2:30:08.520 --> 2:30:09.720
 Depending on the moment of the day.

2:30:09.720 --> 2:30:13.160
 Yeah, there's, like what Bukowski described

2:30:13.160 --> 2:30:15.420
 is that moment when you go from being in love

2:30:15.420 --> 2:30:17.320
 to having a different kind of love.

2:30:17.320 --> 2:30:18.360
 Yeah.

2:30:18.360 --> 2:30:19.480
 And that's a transition.

2:30:19.480 --> 2:30:21.720
 But when it happens, if you read the owner's manual

2:30:21.720 --> 2:30:23.620
 and you believed it, you would have said,

2:30:23.620 --> 2:30:25.200
 oh, this happened.

2:30:25.200 --> 2:30:26.460
 It doesn't mean it's not love.

2:30:26.460 --> 2:30:27.920
 It's a different kind of love.

2:30:27.920 --> 2:30:32.320
 But maybe there's something better about that.

2:30:32.320 --> 2:30:36.760
 As you grow old, all you do is regret how you used to be.

2:30:36.760 --> 2:30:37.600
 It's sad.

2:30:38.560 --> 2:30:39.400
 Right?

2:30:39.400 --> 2:30:40.720
 You should have learned a lot of things

2:30:40.720 --> 2:30:43.280
 because like who you can be in your future self

2:30:43.280 --> 2:30:46.720
 is actually more interesting and possibly delightful

2:30:46.720 --> 2:30:51.720
 than being a mad kid in love with the next person.

2:30:52.000 --> 2:30:54.440
 Like, that's super fun when it happens.

2:30:54.440 --> 2:30:59.440
 But that's, you know, 5% of the possibility.

2:30:59.840 --> 2:31:02.280
 Yeah, that's right.

2:31:02.280 --> 2:31:05.320
 There's a lot more fun to be had in the long lasting stuff.

2:31:05.320 --> 2:31:07.640
 Yeah, or meaning, you know, if that's your thing.

2:31:07.640 --> 2:31:09.280
 Which is a kind of fun.

2:31:09.280 --> 2:31:10.640
 It's a deeper kind of fun.

2:31:10.640 --> 2:31:11.560
 And it's surprising.

2:31:11.560 --> 2:31:14.960
 You know, that's, like the thing I like is surprises.

2:31:15.920 --> 2:31:19.440
 You know, and you just never know what's gonna happen.

2:31:19.440 --> 2:31:21.400
 But you have to look carefully and you have to work at it

2:31:21.400 --> 2:31:24.000
 and you have to think about it and you know, it's.

2:31:24.000 --> 2:31:26.480
 Yeah, you have to see the surprises when they happen, right?

2:31:26.480 --> 2:31:28.320
 You have to be looking for it.

2:31:28.320 --> 2:31:31.280
 From the branching perspective, you mentioned regrets.

2:31:33.360 --> 2:31:36.200
 Do you have regrets about your own trajectory?

2:31:36.200 --> 2:31:37.240
 Oh yeah, of course.

2:31:38.200 --> 2:31:39.440
 Yeah, some of it's painful,

2:31:39.440 --> 2:31:41.320
 but you wanna hear the painful stuff?

2:31:41.320 --> 2:31:42.160
 No.

2:31:42.160 --> 2:31:46.960
 I would say, like in terms of working with people,

2:31:46.960 --> 2:31:48.760
 when people did stuff I didn't like,

2:31:48.760 --> 2:31:50.760
 especially if it was a bit nefarious,

2:31:50.760 --> 2:31:54.520
 I took it personally and I also felt it was personal

2:31:54.520 --> 2:31:56.000
 about them.

2:31:56.000 --> 2:31:57.760
 But a lot of times, like humans are,

2:31:57.760 --> 2:31:59.840
 you know, most humans are a mess, right?

2:31:59.840 --> 2:32:02.120
 And then they act out and they do stuff.

2:32:02.120 --> 2:32:06.000
 And the psychologist I heard a long time ago said,

2:32:06.000 --> 2:32:09.240
 you tend to think somebody does something to you.

2:32:09.240 --> 2:32:10.880
 But really what they're doing is they're doing

2:32:10.880 --> 2:32:13.360
 what they're doing while they're in front of you.

2:32:13.360 --> 2:32:16.240
 It's not that much about you, right?

2:32:16.240 --> 2:32:20.400
 And as I got more interested in,

2:32:20.400 --> 2:32:21.720
 you know, when I work with people,

2:32:21.720 --> 2:32:25.080
 I think about them and probably analyze them

2:32:25.080 --> 2:32:26.600
 and understand them a little bit.

2:32:26.600 --> 2:32:29.080
 And then when they do stuff, I'm way less surprised.

2:32:29.080 --> 2:32:32.320
 And if it's bad, I'm way less hurt.

2:32:32.320 --> 2:32:34.160
 And I react way less.

2:32:34.160 --> 2:32:37.080
 Like I sort of expect everybody's got their shit.

2:32:37.080 --> 2:32:38.920
 Yeah, and it's not about you as much.

2:32:38.920 --> 2:32:41.000
 It's not about me that much.

2:32:41.000 --> 2:32:42.760
 It's like, you know, you do something

2:32:42.760 --> 2:32:45.280
 and you think you're embarrassed, but nobody cares.

2:32:45.280 --> 2:32:46.920
 Like, and somebody's really mad at you,

2:32:46.920 --> 2:32:49.680
 the odds of it being about you.

2:32:49.680 --> 2:32:51.360
 No, they're getting mad the way they're doing that

2:32:51.360 --> 2:32:53.160
 because of some pattern they learned.

2:32:53.160 --> 2:32:55.560
 And you know, and maybe you can help them

2:32:55.560 --> 2:32:56.840
 if you care enough about it.

2:32:56.840 --> 2:33:00.560
 But, or you could see it coming and step out of the way.

2:33:00.560 --> 2:33:02.860
 Like, I wish I was way better at that.

2:33:02.860 --> 2:33:04.740
 I'm a bit of a hothead.

2:33:04.740 --> 2:33:06.000
 And in support of that.

2:33:06.000 --> 2:33:08.880
 You said with Steve, that was a feature, not a bug.

2:33:08.880 --> 2:33:11.640
 Yeah, well, he was using it as the counter force

2:33:11.640 --> 2:33:13.480
 to orderliness that would crush his work.

2:33:13.480 --> 2:33:15.080
 Well, you were doing the same.

2:33:15.080 --> 2:33:15.920
 Yeah, maybe.

2:33:15.920 --> 2:33:18.960
 I don't think I, I don't think my vision was big enough.

2:33:18.960 --> 2:33:22.560
 It was more like I just got pissed off and did stuff.

2:33:22.560 --> 2:33:27.280
 I'm sure that's the, yeah, you're telling me.

2:33:27.280 --> 2:33:29.080
 I don't know if it had the,

2:33:29.080 --> 2:33:30.920
 it didn't have the amazing effect

2:33:30.920 --> 2:33:32.440
 of creating the trillion dollar company.

2:33:32.440 --> 2:33:35.320
 It was more like I just got pissed off and left

2:33:35.320 --> 2:33:38.400
 and, or made enemies that I shouldn't have.

2:33:38.400 --> 2:33:40.520
 And yeah, it's hard.

2:33:40.520 --> 2:33:42.080
 Like, I didn't really understand politics

2:33:42.080 --> 2:33:44.320
 until I worked at Apple where, you know,

2:33:44.320 --> 2:33:46.120
 Steve was a master player of politics

2:33:46.120 --> 2:33:48.840
 and his staff had to be, or they wouldn't survive him.

2:33:48.840 --> 2:33:51.400
 And it was definitely part of the culture.

2:33:51.400 --> 2:33:52.640
 And then I've been in companies where they say

2:33:52.640 --> 2:33:54.880
 it's political, but it's all, you know,

2:33:54.880 --> 2:33:56.920
 fun and games compared to Apple.

2:33:56.920 --> 2:34:00.320
 And it's not that the people at Apple are bad people.

2:34:00.320 --> 2:34:03.560
 It's just, they operate politically at a higher level.

2:34:04.680 --> 2:34:06.920
 You know, it's not like, oh, somebody said something bad

2:34:06.920 --> 2:34:10.840
 about somebody, somebody else, which is most politics.

2:34:10.840 --> 2:34:13.520
 It's, you know, they had strategies

2:34:13.520 --> 2:34:15.680
 about accomplishing their goals.

2:34:15.680 --> 2:34:19.920
 Sometimes, you know, over the dead bodies of their enemies.

2:34:19.920 --> 2:34:23.080
 You know, with sophistication, yeah,

2:34:23.080 --> 2:34:25.440
 more Game of Thrones than sophistication

2:34:25.440 --> 2:34:29.000
 and like a big time factor rather than a, you know.

2:34:29.000 --> 2:34:31.280
 Wow, that requires a lot of control over your emotions,

2:34:31.280 --> 2:34:35.600
 I think, to have a bigger strategy in the way you behave.

2:34:35.600 --> 2:34:38.800
 Yeah, and it's effective in the sense

2:34:38.800 --> 2:34:40.760
 that coordinating thousands of people

2:34:40.760 --> 2:34:44.280
 to do really hard things where many of the people

2:34:44.280 --> 2:34:45.920
 in there don't understand themselves,

2:34:45.920 --> 2:34:47.960
 much less how they're participating,

2:34:47.960 --> 2:34:52.600
 creates all kinds of, you know, drama and problems

2:34:52.600 --> 2:34:55.800
 that, you know, our solution is political in nature.

2:34:55.800 --> 2:34:57.040
 Like how do you convince people?

2:34:57.040 --> 2:34:57.880
 How do you leverage them?

2:34:57.880 --> 2:34:59.040
 How do you motivate them?

2:34:59.040 --> 2:35:00.040
 How do you get rid of them?

2:35:00.040 --> 2:35:02.400
 How do you, you know, like there's so many layers

2:35:02.400 --> 2:35:04.440
 of that that are interesting.

2:35:04.440 --> 2:35:08.480
 And even though some of it, let's say, may be tough,

2:35:08.480 --> 2:35:13.480
 it's not evil unless, you know, you use that skill

2:35:13.480 --> 2:35:16.240
 to evil purposes, which some people obviously do.

2:35:16.240 --> 2:35:19.480
 But it's a skill set that operates, you know.

2:35:19.480 --> 2:35:22.320
 And I wish I'd, you know, I was interested in it,

2:35:22.320 --> 2:35:24.080
 but I, you know, it was sort of like,

2:35:24.080 --> 2:35:25.720
 I'm an engineer, I do my thing.

2:35:26.640 --> 2:35:28.360
 And, you know, there's times

2:35:28.360 --> 2:35:30.320
 when I could have had a way bigger impact

2:35:31.320 --> 2:35:33.160
 if I, you know, knew how to,

2:35:33.160 --> 2:35:35.760
 if I paid more attention and knew more about that.

2:35:36.640 --> 2:35:38.800
 Yeah, about the human layer of the stack.

2:35:38.800 --> 2:35:41.560
 Yeah, that human political power, you know,

2:35:41.560 --> 2:35:43.240
 expression layer of the stack.

2:35:43.240 --> 2:35:44.720
 Just complicated.

2:35:44.720 --> 2:35:45.960
 And there's lots to know about it.

2:35:45.960 --> 2:35:48.360
 I mean, people are good at it, are just amazing.

2:35:49.440 --> 2:35:50.480
 And when they're good at it,

2:35:50.480 --> 2:35:55.360
 and let's say, relatively kind and oriented

2:35:55.360 --> 2:35:58.640
 in a good direction, you can really feel,

2:35:58.640 --> 2:36:00.520
 you can get lots of stuff done and coordinate things

2:36:00.520 --> 2:36:02.120
 that you never thought possible.

2:36:03.560 --> 2:36:06.680
 But all people like that also have some pretty hard edges

2:36:06.680 --> 2:36:09.600
 because, you know, it's a heavy lift.

2:36:09.600 --> 2:36:13.160
 And I wish I'd spent more time like that when I was younger.

2:36:13.160 --> 2:36:14.120
 But maybe I wasn't ready.

2:36:14.120 --> 2:36:16.480
 You know, I was a wide eyed kid for 30 years.

2:36:17.720 --> 2:36:18.680
 Still a bit of a kid.

2:36:18.680 --> 2:36:19.960
 Yeah, I know.

2:36:19.960 --> 2:36:23.480
 What do you hope your legacy is

2:36:23.480 --> 2:36:28.000
 when there's a book like Hitchhiker's Guide to the Galaxy,

2:36:28.000 --> 2:36:31.120
 and this is like a one sentence entry by Jim Waller

2:36:31.120 --> 2:36:34.200
 from like that guy lived at some point.

2:36:34.200 --> 2:36:35.600
 There's not many, you know,

2:36:35.600 --> 2:36:37.720
 not many people would be remembered.

2:36:37.720 --> 2:36:42.360
 You're one of the sparkling little human creatures

2:36:42.360 --> 2:36:44.760
 that had a big impact on the world.

2:36:44.760 --> 2:36:46.360
 How do you hope you'll be remembered?

2:36:46.360 --> 2:36:48.520
 My daughter was trying to get,

2:36:48.520 --> 2:36:49.960
 she edited my Wikipedia page

2:36:49.960 --> 2:36:52.360
 to say that I was a legend and a guru.

2:36:53.840 --> 2:36:55.600
 But they took it out, so she put it back in.

2:36:55.600 --> 2:36:56.520
 She's 15.

2:36:58.720 --> 2:37:01.320
 I think that was probably the best part of my legacy.

2:37:02.720 --> 2:37:04.560
 She got her sister, and they were all excited.

2:37:04.560 --> 2:37:06.600
 They were like trying to put it in the references

2:37:06.600 --> 2:37:09.360
 because there's articles and that on the title.

2:37:09.360 --> 2:37:13.080
 So in the eyes of your kids, you're a legend.

2:37:13.080 --> 2:37:14.320
 Well, they're pretty skeptical

2:37:14.320 --> 2:37:15.960
 because they don't be better than that.

2:37:15.960 --> 2:37:17.320
 They're like dad.

2:37:18.400 --> 2:37:21.600
 So yeah, that kind of stuff is super fun.

2:37:21.600 --> 2:37:24.360
 In terms of the big legends stuff, I don't care.

2:37:24.360 --> 2:37:25.200
 You don't care.

2:37:25.200 --> 2:37:26.680
 I don't really care.

2:37:26.680 --> 2:37:28.560
 You're just an engineer.

2:37:28.560 --> 2:37:31.080
 Yeah, I've been thinking about building a big pyramid.

2:37:32.080 --> 2:37:33.560
 So I had a debate with a friend

2:37:33.560 --> 2:37:36.840
 about whether pyramids or craters are cooler.

2:37:36.840 --> 2:37:39.240
 And he realized that there's craters everywhere,

2:37:39.240 --> 2:37:42.040
 but they built a couple of pyramids 5,000 years ago.

2:37:42.040 --> 2:37:43.240
 And they remember you for a while.

2:37:43.240 --> 2:37:45.080
 We're still talking about it.

2:37:45.080 --> 2:37:47.280
 So I think that would be cool.

2:37:47.280 --> 2:37:48.680
 Those aren't easy to build.

2:37:48.680 --> 2:37:49.520
 Oh, I know.

2:37:50.360 --> 2:37:51.960
 And they don't actually know how they built them,

2:37:51.960 --> 2:37:52.880
 which is great.

2:37:54.400 --> 2:37:58.480
 It's either AGI or aliens could be involved.

2:37:58.480 --> 2:38:01.680
 So I think you're gonna have to figure out

2:38:01.680 --> 2:38:03.640
 quite a few more things than just

2:38:03.640 --> 2:38:05.400
 the basics of civil engineering.

2:38:05.400 --> 2:38:09.120
 So I guess you hope your legacy is pyramids.

2:38:10.000 --> 2:38:12.400
 That would be cool.

2:38:12.400 --> 2:38:13.880
 And my Wikipedia page, you know,

2:38:13.880 --> 2:38:16.240
 getting updated by my daughter periodically.

2:38:16.240 --> 2:38:18.640
 Like those two things would pretty much make it.

2:38:18.640 --> 2:38:20.600
 Jim, it's a huge honor talking to you again.

2:38:20.600 --> 2:38:22.720
 I hope we talk many more times in the future.

2:38:22.720 --> 2:38:26.160
 I can't wait to see what you do with Tense Torrent.

2:38:26.160 --> 2:38:27.800
 I can't wait to use it.

2:38:27.800 --> 2:38:30.040
 I can't wait for you to revolutionize

2:38:30.040 --> 2:38:33.400
 yet another space in computing.

2:38:33.400 --> 2:38:34.760
 It's a huge honor to talk to you.

2:38:34.760 --> 2:38:35.600
 Thanks for talking to me.

2:38:35.600 --> 2:39:05.320
 This was fun.

2:39:05.600 --> 2:39:06.600
 See you next time.

