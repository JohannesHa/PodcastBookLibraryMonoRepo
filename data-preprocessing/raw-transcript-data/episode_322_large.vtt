WEBVTT

00:00.000 --> 00:07.040
 there's a broader question here, right? As we build socially and emotionally intelligent machines,

00:07.920 --> 00:12.640
 what does that mean about our relationship with them and then more broadly our relationship with

00:12.640 --> 00:18.240
 one another, right? Because this machine is going to be programmed to be amazing at empathy,

00:18.240 --> 00:22.560
 by definition, right? It's going to always be there for you. It's not going to get bored.

00:23.440 --> 00:25.680
 I don't know how I feel about that. I think about that a lot.

00:25.680 --> 00:30.320
 TITO The following is a conversation with Rana

00:30.320 --> 00:36.080
 L. Kliubi, a pioneer in the field of emotion recognition and human centric artificial

00:36.080 --> 00:43.920
 intelligence. She is the founder of Effectiva, deputy CEO of SmartEye, author of Girl Decoded,

00:43.920 --> 00:49.200
 and one of the most brilliant, kind, inspiring, and fun human beings I've gotten the chance to

00:49.200 --> 00:54.800
 talk to. This is the Lex Friedman podcast. To support it, please check out our sponsors in

00:54.800 --> 01:02.400
 the description. And now, dear friends, here's Rana L. Kliubi. You grew up in the Middle East,

01:02.400 --> 01:08.000
 in Egypt. What is the memory from that time that makes you smile? Or maybe a memory that stands out

01:08.000 --> 01:12.320
 as helping your mind take shape and helping you define yourself in this world?

01:12.320 --> 01:15.440
 RANA L. KLIUBI So the memory that stands out is we used to

01:15.440 --> 01:21.680
 live in my grandma's house. She used to have these mango trees in her garden. And in the summer,

01:21.680 --> 01:26.640
 and so mango season was like July and August. And so in the summer, she would invite all my aunts

01:26.640 --> 01:31.680
 and uncles and cousins. And it was just like maybe there were like 20 or 30 people in the house,

01:31.680 --> 01:38.080
 and she would cook all this amazing food. And us, the kids, we would go down the garden,

01:38.080 --> 01:43.920
 and we would pick all these mangoes. And I don't know, I think it's just the bringing people

01:43.920 --> 01:47.920
 together that always stuck with me, the warmth. TITO Around the mango tree.

01:47.920 --> 01:52.800
 RANA L. KLIUBI Yeah, around the mango tree. And there's just like the joy, the joy of being

01:52.800 --> 02:00.880
 together around food. And I'm a terrible cook. So I guess that didn't, that memory didn't translate

02:00.880 --> 02:05.520
 to me kind of doing the same. I love hosting people. TITO Do you remember colors, smells?

02:05.520 --> 02:10.560
 Is that what, like what, how does memory work? Like what do you visualize? Do you visualize

02:10.560 --> 02:19.360
 people's faces, smiles? Do you, is there colors? Is there like a theme to the colors? Is it smells

02:19.360 --> 02:23.360
 because of food involved? RANA L. KLIUBI Yeah, I think that's a great question. So the,

02:23.360 --> 02:28.800
 those Egyptian mangoes, there's a particular type that I love, and it's called Darwasi mangoes. And

02:28.800 --> 02:33.680
 they're kind of, you know, they're oval, and they have a little red in them. So I kind of,

02:33.680 --> 02:39.600
 they're red and mango colored on the outside. So I remember that. TITO Does red indicate like

02:39.600 --> 02:45.520
 extra sweetness? Is that, is that, that means like it's nicely, yeah, it's nice and ripe and stuff.

02:45.520 --> 02:52.640
 Yeah. What, what's like a definitive food of Egypt? You know, there's like these almost

02:52.640 --> 02:58.800
 stereotypical foods in different parts of the world, like Ukraine invented borscht.

02:59.600 --> 03:04.800
 Borscht is this beet soup with, that you put sour cream on. See, it's not, I can't see if you,

03:04.800 --> 03:10.880
 if you know, if you know what it is, I think, you know, is delicious. But if I explain it,

03:10.880 --> 03:15.280
 it's just not going to sound delicious. I feel like beet soup. This doesn't make any sense,

03:15.280 --> 03:19.600
 but that's kind of, and you probably have actually seen pictures of it because it's one of the

03:19.600 --> 03:26.800
 traditional foods in Ukraine, in Russia, in different parts of the Slavic world. So that's,

03:26.800 --> 03:31.520
 but it's become so cliche and stereotypical that you almost don't mention it, but it's still

03:31.520 --> 03:35.440
 delicious. Like I visited Ukraine, I eat that every single day, so.

03:35.440 --> 03:38.480
 Do you, do you make it yourself? How hard is it to make?

03:38.480 --> 03:43.600
 No, I don't know. I think to make it well, like anything, like Italians, they say, well,

03:44.320 --> 03:51.760
 tomato sauce is easy to make, but to make it right, that's like a generational skill. So anyway,

03:51.760 --> 03:55.200
 is there something like that in Egypt? Is there a culture of food?

03:55.200 --> 04:02.880
 There is. And actually, we have a similar kind of soup. It's called molokhia, and it's, it's made

04:02.880 --> 04:07.520
 of this green plant. It's like, it's somewhere between spinach and kale, and you mince it,

04:07.520 --> 04:13.360
 and then you cook it in like chicken broth. And my grandma used to make, and my mom makes it really

04:13.360 --> 04:18.080
 well, and I try to make it, but it's not as great. So we used to have that. And then we used to have

04:18.080 --> 04:23.520
 it alongside stuffed pigeons. I'm pescetarian now, so I don't eat that anymore, but.

04:23.520 --> 04:24.480
 Stuffed pigeons.

04:24.480 --> 04:27.600
 Yeah, it's like, it was really yummy. It's the one thing I miss about,

04:28.480 --> 04:32.080
 you know, now that I'm pescetarian and I don't eat.

04:32.080 --> 04:33.040
 The stuffed pigeons?

04:33.040 --> 04:34.240
 Yeah, the stuffed pigeons.

04:35.440 --> 04:39.920
 Is it, what are they stuffed with? If that doesn't bother you too much to describe.

04:39.920 --> 04:46.000
 No, no, it's stuffed with a lot of like just rice and, yeah, it's just rice. Yeah, so.

04:46.000 --> 04:51.120
 And you also, you said that your first, in your book, that your first computer

04:51.120 --> 04:54.880
 was an Atari, and Space Invaders was your favorite game.

04:56.000 --> 04:58.800
 Is that when you first fell in love with computers, would you say?

04:58.800 --> 05:00.160
 Yeah, I would say so.

05:00.160 --> 05:04.160
 Video games, or just the computer itself? Just something about the machine.

05:04.160 --> 05:07.840
 Ooh, this thing, there's magic in here.

05:07.840 --> 05:12.080
 Yeah, I think the magical moment is definitely like playing video games with my,

05:12.080 --> 05:17.120
 I have two younger sisters, and we would just like had fun together, like playing games.

05:17.120 --> 05:22.240
 But the other memory I have is my first code, the first code I wrote.

05:22.240 --> 05:26.720
 I wrote, I drew a Christmas tree, and I'm Muslim, right?

05:26.720 --> 05:32.000
 So it's kind of, it was kind of funny that the first thing I did was like this Christmas tree.

05:32.000 --> 05:38.320
 So, yeah, and that's when I realized, wow, you can write code to do all sorts of like

05:38.320 --> 05:42.720
 really cool stuff. I must have been like six or seven at the time.

05:42.720 --> 05:48.560
 So you can write programs, and the programs do stuff for you. That's power.

05:48.560 --> 05:50.880
 That's, if you think about it, that's empowering.

05:50.880 --> 05:51.600
 It's AI.

05:51.600 --> 05:55.120
 Yeah, I know what it is. I don't know if that, you see like,

05:56.400 --> 05:59.520
 I don't know if many people think of it that way when they first learned to program.

05:59.520 --> 06:02.880
 They just love the puzzle of it. Like, ooh, this is cool. This is pretty.

06:02.880 --> 06:05.600
 It's a Christmas tree, but like, it's power.

06:05.600 --> 06:06.960
 It is power.

06:06.960 --> 06:11.040
 Eventually, I guess you couldn't at the time, but eventually this thing,

06:11.040 --> 06:14.640
 if it's interesting enough, if it's a pretty enough Christmas tree,

06:14.640 --> 06:19.280
 it can be run by millions of people and bring them joy, like that little thing.

06:19.280 --> 06:21.760
 And then because it's digital, it's easy to spread.

06:22.400 --> 06:26.560
 So like you just created something that's easily spreadable to millions of people.

06:26.560 --> 06:27.120
 Totally.

06:28.160 --> 06:29.840
 It's hard to think that way when you're six.

06:30.800 --> 06:37.040
 In the book, you write, I am who I am because I was raised by a particular set of parents,

06:37.040 --> 06:41.200
 both modern and conservative, forward thinking, yet locked in tradition.

06:41.760 --> 06:46.000
 I'm a Muslim and I feel I'm stronger, more centered for it.

06:46.000 --> 06:50.960
 I adhere to the values of my religion, even if I'm not as dutiful as I once was.

06:50.960 --> 06:55.040
 And I am a new American and I'm thriving on the energy,

06:55.040 --> 06:58.720
 vitality and entrepreneurial spirit of this great country.

06:59.840 --> 07:01.520
 So let me ask you about your parents.

07:01.520 --> 07:05.280
 What have you learned about life from them, especially when you were young?

07:05.280 --> 07:09.920
 So both my parents, they're Egyptian, but they moved to Kuwait right out.

07:09.920 --> 07:11.680
 Actually, there's a cute story about how they met.

07:11.680 --> 07:14.960
 So my dad taught COBOL in the 70s.

07:14.960 --> 07:15.680
 Nice.

07:15.680 --> 07:18.240
 And my mom decided to learn programming.

07:18.240 --> 07:21.120
 So she signed up to take his COBOL programming class.

07:22.400 --> 07:26.640
 And he tried to date her and she was like, no, no, no, I don't date.

07:26.640 --> 07:28.240
 And so he's like, okay, I'll propose.

07:28.240 --> 07:29.680
 And that's how they got married.

07:29.680 --> 07:30.960
 Whoa, strong move.

07:30.960 --> 07:32.240
 Right, exactly, right.

07:32.240 --> 07:34.640
 That's really impressive.

07:35.760 --> 07:38.800
 Those COBOL guys know how to impress a lady.

07:40.640 --> 07:43.520
 So yeah, so what have you learned from them?

07:43.520 --> 07:44.720
 So definitely grit.

07:44.720 --> 07:47.360
 One of the core values in our family is just hard work.

07:48.320 --> 07:50.080
 There were no slackers in our family.

07:50.720 --> 07:54.160
 And that's something that's definitely stayed with me,

07:55.920 --> 07:58.480
 both as a professional, but also in my personal life.

07:58.480 --> 08:06.160
 But I also think my mom, my mom always used to like, I don't know, it was like unconditional

08:06.160 --> 08:06.720
 love.

08:06.720 --> 08:12.160
 Like I just knew my parents would be there for me kind of regardless of what I chose to do.

08:14.240 --> 08:15.520
 And I think that's very powerful.

08:15.520 --> 08:21.600
 And they got tested on it because I kind of challenged cultural norms and I kind of took

08:21.600 --> 08:27.360
 a different path, I guess, than what's expected of a woman in the Middle East.

08:27.360 --> 08:32.480
 And they still love me, which I'm so grateful for that.

08:32.480 --> 08:35.440
 When was like a moment that was the most challenging for them?

08:35.440 --> 08:42.240
 Which moment where they kind of had to come face to face with the fact that you're a bit

08:42.240 --> 08:42.880
 of a rebel?

08:44.080 --> 08:52.080
 I think the first big moment was when I had just gotten married, but I decided to go do

08:52.080 --> 08:53.840
 my PhD at Cambridge University.

08:53.840 --> 08:59.440
 And because my husband at the time, he's now my ex, ran a company in Cairo, he was going

08:59.440 --> 09:00.240
 to stay in Egypt.

09:00.240 --> 09:02.560
 So it was going to be a long distance relationship.

09:03.120 --> 09:09.040
 And that's very unusual in the Middle East for a woman to just head out and kind of pursue

09:09.040 --> 09:09.840
 her career.

09:09.840 --> 09:18.720
 And so my dad and my parents in law both said, you know, we do not approve of you doing this,

09:18.720 --> 09:22.480
 but now you're under the jurisdiction of your husband so he can make the call.

09:22.480 --> 09:26.240
 And luckily for me, he was supportive.

09:26.800 --> 09:29.600
 He said, you know, this is your dream come true.

09:29.600 --> 09:30.960
 You've always wanted to do a PhD.

09:30.960 --> 09:31.840
 I'm going to support you.

09:33.200 --> 09:39.120
 So I think that was the first time where, you know, I challenged the cultural norms.

09:39.120 --> 09:40.080
 Was that scary?

09:40.080 --> 09:41.360
 Oh, my God, yes.

09:41.360 --> 09:42.720
 It was totally scary.

09:42.720 --> 09:50.320
 What's the biggest culture shock from there to Cambridge, to London?

09:50.320 --> 09:56.480
 Well, that was also during right around September 11th.

09:56.480 --> 10:01.040
 So everyone thought that there was going to be a third world war.

10:01.040 --> 10:07.120
 It was really like, and I, at the time I used to wear the hijab, so I was very visibly Muslim.

10:07.680 --> 10:11.840
 And so my parents just were, they were afraid for my safety.

10:11.840 --> 10:15.600
 But anyways, when I got to Cambridge, because I was so scared, I decided to take off my

10:15.600 --> 10:17.440
 headscarf and wear a hat instead.

10:17.440 --> 10:22.320
 So I just went to class wearing these like British hats, which was, in my opinion, actually

10:22.320 --> 10:25.920
 worse than just showing up in a headscarf because it was just so awkward, right?

10:25.920 --> 10:27.680
 Like sitting in class with like all these.

10:27.680 --> 10:28.320
 Trying to fit in.

10:29.040 --> 10:29.360
 Yeah.

10:29.360 --> 10:30.320
 Like a spy.

10:30.320 --> 10:31.280
 Yeah, yeah, yeah.

10:31.280 --> 10:34.640
 So after a few weeks of doing that, I was like, to heck with that.

10:34.640 --> 10:37.120
 I'm just going to go back to wearing my headscarf.

10:37.120 --> 10:43.840
 Yeah, you wore the hijab, so starting in 2000 and for 12 years after.

10:43.840 --> 10:47.280
 So it's always, whenever you're in public, you have to wear the head covering.

10:47.280 --> 10:52.480
 Can you speak to that, to the hijab, maybe your mixed feelings about it?

10:52.480 --> 10:55.120
 Like what does it represent in its best case?

10:55.120 --> 10:56.960
 What does it represent in the worst case?

10:56.960 --> 11:03.040
 Yeah, you know, I think there's a lot of, I guess I'll first start by saying I wore

11:03.040 --> 11:04.000
 it voluntarily.

11:04.000 --> 11:05.360
 I was not forced to wear it.

11:05.360 --> 11:09.920
 And in fact, I was one of the very first women in my family to decide to put on the hijab.

11:09.920 --> 11:13.040
 And my family thought it was really odd, right?

11:13.040 --> 11:15.840
 Like there was, they were like, why do you want to put this on?

11:15.840 --> 11:20.480
 And at its best, it's a sign of modesty, humility.

11:20.480 --> 11:20.980
 Yeah.

11:22.160 --> 11:25.280
 It's like me wearing a suit, people are like, why are you wearing a suit?

11:25.280 --> 11:30.880
 It's a step back into some kind of tradition, a respect for tradition of sorts.

11:30.880 --> 11:36.000
 So you said, because it's by choice, you're kind of free to make that choice to celebrate

11:36.000 --> 11:37.760
 a tradition of modesty.

11:37.760 --> 11:40.960
 Exactly. And I actually like made it my own.

11:40.960 --> 11:45.680
 I remember I would really match the color of my headscarf with what I was wearing.

11:45.680 --> 11:51.360
 Like it was a form of self expression and at its best, I loved wearing it.

11:52.080 --> 11:56.480
 You know, I have a lot of questions around how we practice religion and religion and,

11:56.480 --> 12:02.160
 you know, and I think also it was a time where I was spending a lot of time going back and

12:02.160 --> 12:04.480
 forth between the US and Egypt.

12:04.480 --> 12:09.920
 And I started meeting a lot of people in the US who are just amazing people, very purpose

12:09.920 --> 12:14.240
 driven, people who have very strong core values, but they're not Muslim.

12:14.720 --> 12:15.760
 That's okay, right?

12:15.760 --> 12:18.720
 And so that was when I just had a lot of questions.

12:19.920 --> 12:25.120
 And politically, also the situation in Egypt was when the Muslim Brotherhood ran the country

12:25.120 --> 12:27.840
 and I didn't agree with their ideology.

12:29.520 --> 12:31.360
 It was at a time when I was going through a divorce.

12:31.360 --> 12:37.040
 Like it was like, it was like just the perfect storm of like political, personal conditions

12:37.040 --> 12:39.200
 where I was like, this doesn't feel like me anymore.

12:40.160 --> 12:44.960
 And it took a lot of courage to take it off because culturally it's not, it's okay if

12:44.960 --> 12:48.960
 you don't wear it, but it's really not okay to wear it and then take it off.

12:50.000 --> 12:56.480
 But you're still, so you have to do that while still maintaining a deep core and pride in

12:56.480 --> 13:01.680
 the origins, in your origin story.

13:02.400 --> 13:02.960
 Totally.

13:02.960 --> 13:06.640
 So still being Egyptian, still being a Muslim.

13:06.640 --> 13:07.200
 Right.

13:07.200 --> 13:14.240
 And being, I think generally like faith driven, but yeah.

13:14.240 --> 13:17.440
 But what that means changes year by year for you.

13:17.440 --> 13:18.800
 It's like a personal journey.

13:18.800 --> 13:20.080
 Yeah, exactly.

13:20.080 --> 13:23.120
 What would you say is the role of faith in that part of the world?

13:23.120 --> 13:26.480
 Like, how do you see, you mentioned it a bit in the book too.

13:26.480 --> 13:27.040
 Yeah.

13:27.600 --> 13:34.480
 I mean, I think, I think there is something really powerful about just believing that

13:34.480 --> 13:39.200
 there's a bigger force, you know, there's a kind of surrendering, I guess, that comes

13:39.200 --> 13:43.360
 with religion and you surrender and you have this deep conviction that it's going to be

13:43.360 --> 13:44.480
 okay, right?

13:44.480 --> 13:48.320
 Like the universe is out to like do amazing things for you and it's going to be okay.

13:48.880 --> 13:50.080
 And there's strength to that.

13:50.080 --> 13:57.040
 Like even when you're going through adversity, you just know that it's going to work out.

13:57.040 --> 13:59.440
 Yeah, it gives you like an inner peace, a calmness.

13:59.440 --> 14:00.640
 Exactly, exactly.

14:00.640 --> 14:04.880
 Yeah, that's, it's faith in all the meanings of that word.

14:04.880 --> 14:05.440
 Right.

14:05.440 --> 14:07.200
 Faith that everything is going to be okay.

14:07.200 --> 14:12.320
 And it is because time passes and time cures all things.

14:12.320 --> 14:15.200
 It's like a calmness with the chaos of the world.

14:15.200 --> 14:15.680
 Yeah.

14:15.680 --> 14:22.320
 And also there's like a silver, I'm a true believer of this, that something at the specific

14:22.320 --> 14:27.120
 moment in time can look like it's catastrophic and it's not what you wanted in life.

14:28.800 --> 14:32.880
 But then time passes and then you look back and there's the silver lining, right?

14:32.880 --> 14:36.320
 It maybe closed the door, but it opened a new door for you.

14:37.120 --> 14:42.880
 And so I'm a true believer in that, that, you know, there's a silver lining in almost

14:42.880 --> 14:47.120
 anything in life, you just have to have this like, yeah, faith or conviction that it's

14:47.120 --> 14:47.760
 going to work out.

14:47.760 --> 14:50.720
 Yeah, it's such a beautiful way to see a shady feeling.

14:50.720 --> 14:56.320
 So if you feel shady about a current situation, I mean, it almost is always true.

14:57.840 --> 15:04.800
 Unless it's the cliche thing of if it doesn't kill you, whatever doesn't kill you makes

15:04.800 --> 15:05.360
 you stronger.

15:05.360 --> 15:13.200
 It's, it does seem that over time when you take a perspective on things that the hardest

15:13.200 --> 15:16.960
 moments and periods of your life are the most meaningful.

15:18.400 --> 15:19.280
 Yeah, yeah.

15:19.280 --> 15:21.520
 So over time you get to have that perspective.

15:21.520 --> 15:22.020
 Right.

15:23.760 --> 15:30.000
 What about, because you mentioned Kuwait, what about, let me ask you about war.

15:30.000 --> 15:35.680
 What's the role of war and peace, maybe even the big love and hate in that part of

15:35.680 --> 15:39.920
 the world, because it does seem to be a part of the world where there's turmoil.

15:40.720 --> 15:42.720
 There was turmoil, there's still turmoil.

15:44.560 --> 15:46.480
 It is so unfortunate, honestly.

15:46.480 --> 15:53.760
 It's, it's such a waste of human resources and, and, and yeah, and human mindshare.

15:53.760 --> 15:57.280
 I mean, and at the end of the day, we all kind of want the same things.

15:57.280 --> 16:02.560
 We want, you know, we want a human connection, we want joy, we want to feel fulfilled, we

16:02.560 --> 16:05.760
 want to feel, you know, a life of purpose.

16:05.760 --> 16:12.160
 And I just, I just find it baffling, honestly, that we are still having to grapple with that.

16:14.160 --> 16:15.840
 I have a story to share about this.

16:15.840 --> 16:21.840
 You know, I grew up, I'm Egyptian, American now, but, but, you know, originally from Egypt.

16:21.840 --> 16:28.080
 And when I first got to Cambridge, it turned out my officemate, like my PhD kind of, you

16:28.080 --> 16:31.920
 know, she ended up, you know, we ended up becoming friends, but she was from Israel.

16:32.960 --> 16:36.400
 And we didn't know, yeah, we didn't know how it was going to be like.

16:37.760 --> 16:40.000
 Did you guys sit there just staring at each other for a bit?

16:41.040 --> 16:44.560
 Actually, she, because I arrived before she did.

16:44.560 --> 16:50.320
 And it turns out she emailed our PhD advisor and asked him if she thought it was going

16:50.320 --> 16:52.320
 to be okay.

16:52.320 --> 16:52.800
 Yeah.

16:52.800 --> 16:55.040
 And this is around 9 11 too.

16:55.040 --> 16:55.680
 Yeah.

16:55.680 --> 17:01.280
 And, and Peter, Peter Robinson, our PhD advisor was like, yeah, like, this is an academic

17:01.280 --> 17:02.720
 institution, just show up.

17:02.720 --> 17:04.480
 And we became super good friends.

17:04.480 --> 17:07.200
 We were both new moms.

17:07.200 --> 17:09.200
 Like we both had our kids during our PhD.

17:09.200 --> 17:11.360
 We were both doing artificial emotional intelligence.

17:11.360 --> 17:12.320
 She was looking at speech.

17:12.320 --> 17:13.680
 I was looking at the face.

17:13.680 --> 17:16.480
 We just had so the culture was so similar.

17:17.040 --> 17:18.320
 Our jokes were similar.

17:18.320 --> 17:24.080
 It was just, I was like, why on earth are our countries, why is there all this like

17:24.080 --> 17:25.200
 war and tension?

17:25.200 --> 17:27.360
 And I think it falls back to the narrative, right?

17:27.360 --> 17:30.720
 If you change the narrative, like whoever creates this narrative of war.

17:31.840 --> 17:32.400
 I don't know.

17:32.400 --> 17:33.920
 We should have women run the world.

17:34.640 --> 17:36.480
 Yeah, that's one solution.

17:37.520 --> 17:40.640
 The good women, because there's also evil women in the world.

17:40.640 --> 17:41.360
 True, okay.

17:43.920 --> 17:47.280
 But yes, yes, there could be less war if women ran the world.

17:47.280 --> 17:52.960
 The other aspect is, it doesn't matter the gender, the people in power.

17:54.560 --> 17:59.280
 I get to see this with Ukraine and Russia and different parts of the world around that

17:59.280 --> 18:00.000
 conflict now.

18:00.800 --> 18:04.320
 And that's happening in Yemen as well and everywhere else.

18:05.200 --> 18:09.280
 There's these narratives told by the leaders to the populace.

18:09.840 --> 18:12.400
 And those narratives take hold and everybody believes that.

18:12.400 --> 18:17.360
 And they have a distorted view of the humanity on the other side.

18:17.920 --> 18:25.120
 In fact, especially during war, you don't even see the people on the other side as human

18:25.120 --> 18:30.960
 or as equal intelligence or worth or value as you.

18:30.960 --> 18:40.000
 You tell all kinds of narratives about them being Nazis or dumb or whatever narrative

18:40.000 --> 18:42.400
 you want to weave around that or evil.

18:44.720 --> 18:49.120
 But I think when you actually meet them face to face, you realize they're like the same.

18:49.120 --> 18:50.400
 Exactly, right?

18:50.400 --> 18:58.960
 It's actually a big shock for people to realize that they've been essentially lied to within

18:58.960 --> 19:00.000
 their country.

19:00.000 --> 19:05.840
 And I kind of have faith that social media, as ridiculous as it is to say, or any kind

19:05.840 --> 19:13.520
 of technology, is able to bypass the walls that governments put up and connect people

19:13.520 --> 19:14.000
 directly.

19:14.000 --> 19:20.880
 And then you get to realize, oh, people fall in love across different nations and religions

19:20.880 --> 19:21.440
 and so on.

19:21.440 --> 19:25.920
 And that, I think, ultimately can cure a lot of our ills, especially in person.

19:26.640 --> 19:32.400
 I also think that if leaders met in person, they'd have a conversation that could cure

19:32.400 --> 19:37.680
 a lot of the ills of the world, especially in private.

19:37.680 --> 19:41.280
 Let me ask you about the women running the world.

19:42.320 --> 19:49.440
 So gender does, in part, perhaps shape the landscape of just our human experience.

19:51.040 --> 19:57.760
 So in what ways was it limiting and in what ways was it empowering for you to be a woman

19:57.760 --> 19:58.560
 in the Middle East?

19:58.560 --> 20:03.920
 I think, just kind of going back to my comment on women running the world, I think it comes

20:03.920 --> 20:08.800
 back to empathy, which has been a common thread throughout my entire career.

20:08.800 --> 20:11.120
 And it's this idea of human connection.

20:12.320 --> 20:16.800
 Once you build common ground with a person or a group of people, you build trust, you

20:16.800 --> 20:20.160
 build loyalty, you build friendship.

20:20.160 --> 20:24.480
 And then you can turn that into behavior change and motivation and persuasion.

20:24.480 --> 20:29.520
 So it's like, empathy and emotions are just at the center of everything we do.

20:30.720 --> 20:38.080
 And I think being from the Middle East, kind of this human connection is very strong.

20:38.080 --> 20:44.640
 We have this running joke that if you come to Egypt for a visit, people will know everything

20:44.640 --> 20:46.000
 about your life right away, right?

20:46.000 --> 20:48.400
 I have no problems asking you about your personal life.

20:48.400 --> 20:53.200
 There's no boundaries, really, no personal boundaries in terms of getting to know people.

20:53.200 --> 20:55.680
 We get emotionally intimate very, very quickly.

20:56.400 --> 21:00.880
 But I think people just get to know each other authentically, I guess.

21:01.680 --> 21:05.040
 There isn't this superficial level of getting to know people.

21:05.040 --> 21:06.880
 You just try to get to know people really deeply.

21:06.880 --> 21:08.080
 Empathy is a part of that.

21:08.080 --> 21:08.640
 Totally.

21:08.640 --> 21:15.680
 Because you can put yourself in this person's shoe and kind of, yeah, imagine what challenges

21:15.680 --> 21:20.320
 they're going through, and so I think I've definitely taken that with me.

21:21.760 --> 21:26.960
 Generosity is another one too, like just being generous with your time and love and attention

21:26.960 --> 21:30.480
 and even with your wealth, right?

21:30.480 --> 21:32.800
 Even if you don't have a lot of it, you're still very generous.

21:32.800 --> 21:33.840
 And I think that's another...

21:34.720 --> 21:38.000
 Enjoying the humanity of other people.

21:38.000 --> 21:44.720
 And so do you think there's a useful difference between men and women in that?

21:44.720 --> 21:47.200
 In that aspect and empathy?

21:48.880 --> 21:56.160
 Or is doing these kind of big general groups, does that hinder progress?

21:56.880 --> 21:59.760
 Yeah, I actually don't want to overgeneralize.

21:59.760 --> 22:03.520
 I mean, some of the men I know are like the most empathetic humans.

22:03.520 --> 22:05.200
 Yeah, I strive to be empathetic.

22:05.200 --> 22:07.120
 Yeah, you're actually very empathetic.

22:10.640 --> 22:13.360
 Yeah, so I don't want to overgeneralize.

22:13.360 --> 22:18.400
 Although one of the researchers I worked with when I was at Cambridge, Professor Simon Baron Cohen,

22:18.400 --> 22:25.120
 he's Sacha Baron Cohen's cousin, and he runs the Autism Research Center at Cambridge,

22:25.120 --> 22:29.600
 and he's written multiple books on autism.

22:29.600 --> 22:35.040
 And one of his theories is the empathy scale, like the systemizers and the empathizers,

22:35.040 --> 22:42.800
 and there's a disproportionate amount of computer scientists and engineers who are

22:42.800 --> 22:51.760
 systemizers and perhaps not great empathizers, and then there's more men in that bucket,

22:51.760 --> 22:56.000
 I guess, than women, and then there's more women in the empathizers bucket.

22:56.000 --> 22:58.160
 So again, not to overgeneralize.

22:58.160 --> 22:59.520
 I sometimes wonder about that.

22:59.520 --> 23:04.560
 It's been frustrating to me how many, I guess, systemizers there are in the field of robotics.

23:05.200 --> 23:05.700
 Yeah.

23:06.240 --> 23:10.000
 It's actually encouraging to me because I care about, obviously, social robotics,

23:10.000 --> 23:18.720
 and because there's more opportunity for people that are empathic.

23:18.720 --> 23:19.200
 Exactly.

23:19.200 --> 23:20.400
 I totally agree.

23:20.400 --> 23:20.960
 Well, right?

23:20.960 --> 23:21.760
 So it's nice.

23:21.760 --> 23:22.160
 Yes.

23:22.160 --> 23:29.200
 So every robotics I talk to, they don't see the human as interesting, as it's not exciting.

23:29.200 --> 23:32.160
 You want to avoid the human at all costs.

23:32.160 --> 23:39.200
 It's a safety concern to be touching the human, which it is, but it is also an opportunity

23:39.200 --> 23:43.360
 for deep connection or collaboration or all that kind of stuff.

23:43.360 --> 23:48.160
 And because most brilliant roboticists don't care about the human, it's an opportunity,

23:49.280 --> 23:53.840
 in your case, it's a business opportunity too, but in general, an opportunity to explore

23:53.840 --> 23:54.640
 those ideas.

23:54.640 --> 24:03.760
 So in this beautiful journey to Cambridge, to UK, and then to America, what's the moment

24:03.760 --> 24:09.760
 or moments that were most transformational for you as a scientist and as a leader?

24:09.760 --> 24:16.320
 So you became an exceptionally successful CEO, founder, researcher, scientist, and so on.

24:18.320 --> 24:25.040
 Was there a face shift there where, like, I can be somebody, I can really do something

24:25.040 --> 24:25.600
 in this world?

24:26.640 --> 24:26.880
 Yeah.

24:26.880 --> 24:29.680
 So actually, just kind of a little bit of background.

24:29.680 --> 24:36.960
 So the reason why I moved from Cairo to Cambridge, UK to do my PhD is because I had a very clear

24:36.960 --> 24:37.920
 career plan.

24:37.920 --> 24:43.280
 I was like, okay, I'll go abroad, get my PhD, going to crush it in three or four years,

24:43.280 --> 24:44.720
 come back to Egypt and teach.

24:45.360 --> 24:47.520
 It was very clear, very well laid out.

24:47.520 --> 24:49.280
 Was topic clear or no?

24:49.280 --> 24:54.400
 The topic, well, I did my PhD around building artificial emotional intelligence and looking

24:54.400 --> 24:54.400
 at...

24:54.400 --> 24:58.880
 But in your master plan ahead of time, when you're sitting by the mango tree, did you

24:58.880 --> 25:00.480
 know it's going to be artificial intelligence?

25:00.480 --> 25:02.880
 No, no, no, that I did not know.

25:02.880 --> 25:07.840
 Although I think I kind of knew that I was going to be doing computer science, but I

25:07.840 --> 25:09.600
 didn't know the specific area.

25:10.160 --> 25:11.120
 But I love teaching.

25:11.120 --> 25:12.320
 I mean, I still love teaching.

25:13.120 --> 25:17.760
 So I just, yeah, I just wanted to go abroad, get a PhD, come back, teach.

25:18.960 --> 25:19.760
 Why computer science?

25:19.760 --> 25:21.200
 Can we just linger on that?

25:21.200 --> 25:21.440
 What?

25:21.440 --> 25:25.520
 Because you're such an empathic person who cares about emotion, humans and so on.

25:25.520 --> 25:31.440
 Isn't, aren't computers cold and emotionless and just...

25:31.440 --> 25:32.480
 We're changing that.

25:32.480 --> 25:38.400
 Yeah, I know, but like, isn't that the, or did you see computers as the, having the

25:38.400 --> 25:42.400
 capability to actually connect with humans?

25:42.400 --> 25:46.720
 I think that was like my takeaway from my experience just growing up, like computers

25:46.720 --> 25:49.840
 sit at the center of how we connect and communicate with one another, right?

25:50.320 --> 25:51.760
 Or technology in general.

25:51.760 --> 25:54.640
 Like I remember my first experience being away from my parents.

25:54.640 --> 25:58.800
 We communicated with a fax machine, but thank goodness for the fax machine, because we

25:58.800 --> 26:00.800
 could send letters back and forth to each other.

26:00.800 --> 26:02.480
 This was pre emails and stuff.

26:04.080 --> 26:09.600
 So I think, I think there's, I think technology can be not just transformative in terms of

26:09.600 --> 26:10.960
 productivity, et cetera.

26:10.960 --> 26:14.080
 It actually does change how we connect with one another.

26:14.960 --> 26:16.720
 Can I just defend the fax machine?

26:16.720 --> 26:22.720
 There's something like the haptic feel because the email is all digital.

26:22.720 --> 26:23.760
 There's something really nice.

26:23.760 --> 26:25.600
 I still write letters to people.

26:26.400 --> 26:30.160
 There's something nice about the haptic aspect of the fax machine, because you still have

26:30.160 --> 26:35.200
 to press, you still have to do something in the physical world to make this thing a reality.

26:35.200 --> 26:39.760
 Right, and then it like comes out as a printout and you can actually touch it and read it.

26:39.760 --> 26:40.240
 Yeah.

26:40.240 --> 26:43.600
 There's something, there's something lost when it's just an email.

26:44.960 --> 26:51.440
 Obviously I wonder how we can regain some of that in the digital world, which goes to

26:51.440 --> 26:53.760
 the metaverse and all those kinds of things.

26:53.760 --> 26:54.800
 We'll talk about it anyway.

26:54.800 --> 26:57.680
 So, actually do you question on that one?

26:57.680 --> 27:00.400
 Do you still, do you have photo albums anymore?

27:00.400 --> 27:02.000
 Do you still print photos?

27:03.600 --> 27:06.320
 No, no, but I'm a minimalist.

27:06.320 --> 27:06.640
 Okay.

27:06.640 --> 27:12.400
 So it was one of the, one of the painful steps in my life was to scan all the photos and

27:12.400 --> 27:16.320
 let go of them and then let go of all my books.

27:16.320 --> 27:17.600
 You let go of your books?

27:17.600 --> 27:18.000
 Yeah.

27:18.000 --> 27:19.920
 Switch to Kindle, everything Kindle.

27:19.920 --> 27:20.800
 Yeah.

27:20.800 --> 27:29.440
 So I thought, I thought, okay, think 30 years from now, nobody's going to have books anymore.

27:29.440 --> 27:32.160
 The technology of digital books is going to get better and better and better.

27:32.160 --> 27:36.240
 Are you really going to be the guy that's still romanticizing physical books?

27:36.240 --> 27:39.440
 Are you going to be the old man on the porch who's like kids?

27:39.440 --> 27:39.760
 Yes.

27:40.480 --> 27:45.040
 So just get used to it because it was, it felt, it still feels a little bit uncomfortable

27:45.040 --> 27:48.560
 to read on a Kindle, but get used to it.

27:48.560 --> 27:53.200
 Like you always, I mean, I'm trying to learn new programming language is always,

27:53.200 --> 27:56.720
 like with technology, you have to kind of challenge yourself to adapt to it.

27:56.720 --> 27:58.880
 You know, I forced myself to use TikTok.

27:58.880 --> 28:01.440
 No, that thing doesn't need much forcing.

28:01.440 --> 28:05.920
 It pulls you in like a, like the worst kind of, or the best kind of drug.

28:05.920 --> 28:08.560
 Anyway, yeah.

28:08.560 --> 28:11.200
 So yeah, but I do love haptic things.

28:11.760 --> 28:13.440
 There's a magic to the haptic.

28:13.440 --> 28:19.520
 Even like touchscreens, it's tricky to get right, to get the experience of a button.

28:19.520 --> 28:19.760
 Yeah.

28:22.400 --> 28:23.760
 Anyway, what were we talking about?

28:23.760 --> 28:29.680
 So AI, so the journey, your whole plan was to come back to Cairo and teach.

28:30.560 --> 28:30.800
 Right.

28:31.840 --> 28:32.560
 And then.

28:32.560 --> 28:33.840
 What did the plan go wrong?

28:33.840 --> 28:34.720
 Yeah, exactly.

28:34.720 --> 28:35.120
 Right.

28:35.120 --> 28:39.120
 And then I get to Cambridge and I fall in love with the idea of research.

28:39.120 --> 28:39.440
 Right.

28:39.440 --> 28:41.520
 And kind of embarking on a path.

28:41.520 --> 28:43.680
 Nobody's explored this path before.

28:43.680 --> 28:45.440
 You're building stuff that nobody's built before.

28:45.440 --> 28:46.960
 And it's challenging and it's hard.

28:46.960 --> 28:48.720
 And there's a lot of nonbelievers.

28:49.280 --> 28:50.960
 I just totally love that.

28:50.960 --> 28:56.320
 And at the end of my PhD, I think it's the meeting that changed the trajectory of my life.

28:56.880 --> 29:02.160
 Professor Roslyn Picard, who's, she runs the Affective Computing Group at the MIT Media Lab.

29:02.160 --> 29:03.040
 I had read her book.

29:03.040 --> 29:07.520
 I, you know, I was like following, following, following all her research.

29:07.520 --> 29:08.880
 AKA Ros.

29:08.880 --> 29:10.080
 Yes, AKA Ros.

29:10.080 --> 29:10.880
 Yes.

29:10.880 --> 29:15.680
 And she was giving a talk at a pattern recognition conference in Cambridge.

29:16.320 --> 29:18.000
 And she had a couple of hours to kill.

29:18.000 --> 29:22.560
 So she emailed the lab and she said, you know, if any students want to meet with me, like,

29:22.560 --> 29:23.840
 just, you know, sign up here.

29:24.640 --> 29:29.920
 And so I signed up for slot and I spent like the weeks leading up to it preparing for this

29:29.920 --> 29:33.680
 meeting and I want to show her a demo of my research and everything.

29:34.400 --> 29:36.640
 And we met and we ended up hitting it off.

29:36.640 --> 29:38.080
 Like we totally clicked.

29:38.080 --> 29:42.480
 And at the end of the meeting, she said, do you want to come work with me as a postdoc

29:42.480 --> 29:43.040
 at MIT?

29:44.720 --> 29:45.600
 And this is what I told her.

29:45.600 --> 29:49.280
 I was like, okay, this would be a dream come true, but there's a husband waiting for me

29:49.280 --> 29:49.840
 in Cairo.

29:49.840 --> 29:51.120
 I kind of have to go back.

29:51.120 --> 29:51.360
 Yeah.

29:52.080 --> 29:52.960
 She said, it's fine.

29:52.960 --> 29:53.600
 Just commute.

29:54.560 --> 29:57.600
 And I literally started commuting between Cairo and Boston.

29:59.200 --> 30:01.200
 Yeah, it was, it was a long commute.

30:01.200 --> 30:05.520
 And I didn't, I did that like every few weeks I would, you know, hop on a plane and go to

30:05.520 --> 30:06.400
 Boston.

30:06.400 --> 30:08.480
 But that, that changed the trajectory of my life.

30:08.480 --> 30:12.880
 There was no, I kind of outgrew my dreams, right?

30:12.880 --> 30:16.720
 I didn't want to go back to Egypt anymore and be faculty.

30:16.720 --> 30:18.320
 Like that was no longer my dream.

30:18.320 --> 30:19.200
 I had a dream.

30:19.200 --> 30:21.920
 What was the, what was it like to be at MIT?

30:22.560 --> 30:24.080
 What was that culture shock?

30:25.040 --> 30:31.040
 You mean America in general, but also, I mean, Cambridge has its own culture, right?

30:31.040 --> 30:34.000
 So what was MIT like and what was America like?

30:34.000 --> 30:37.600
 I think, I wonder if that's similar to your experience at MIT.

30:37.600 --> 30:45.200
 I was just, at the Media Lab in particular, I was just really, impressed is not the right

30:45.200 --> 30:45.700
 word.

30:46.240 --> 30:54.800
 I didn't expect the openness to like innovation and the acceptance of taking a risk and failing.

30:54.800 --> 30:58.320
 Like failure isn't really accepted back in Egypt, right?

30:58.320 --> 30:59.040
 You don't want to fail.

30:59.040 --> 31:03.200
 Like there's a fear of failure, which I think has been hardwired in my brain.

31:03.200 --> 31:05.840
 But you get to MIT and it's okay to start things.

31:05.840 --> 31:08.000
 And if they don't work out, like it's okay.

31:08.000 --> 31:09.120
 You pivot to another idea.

31:09.840 --> 31:12.640
 And that kind of thinking was just very new to me.

31:12.640 --> 31:13.600
 That's liberating.

31:13.600 --> 31:18.880
 Well, Media Lab, for people who don't know, MIT Media Lab is its own beautiful thing because

31:19.840 --> 31:24.000
 they, I think more than other places at MIT, reach for big ideas.

31:24.000 --> 31:28.480
 And like they try, I mean, I think, I mean, depending of course on who, but certainly

31:28.480 --> 31:36.160
 with Roslyn, you try wild stuff, you try big things and crazy things and also try to take

31:36.160 --> 31:38.240
 things to completion so you can demo them.

31:38.240 --> 31:42.240
 So always, always, always have a demo.

31:42.240 --> 31:46.880
 Like if you go, one of the sad things to me about robotics labs at MIT, and there's like

31:46.880 --> 31:53.680
 over 30, I think, is like, usually when you show up to a robotics lab, there's not a single

31:53.680 --> 31:55.200
 working robot, they're all broken.

31:55.760 --> 31:57.280
 All the robots are broken.

31:57.280 --> 32:01.600
 The robots are broken, which is like the normal state of things because you're working on

32:01.600 --> 32:02.080
 them.

32:02.080 --> 32:08.880
 But it would be nice if we lived in a world where robotics labs had some robots functioning.

32:08.880 --> 32:13.360
 One of my like favorite moments that just sticks with me, I visited Boston Dynamics

32:13.360 --> 32:19.680
 and there was a, first of all, seeing so many spots, so many legged robots in one place.

32:20.240 --> 32:21.280
 I'm like, I'm home.

32:22.720 --> 32:24.880
 But the, yeah.

32:24.880 --> 32:26.160
 This is where I was built.

32:27.200 --> 32:33.360
 The cool thing was just to see there was a random robot spot was walking down the hall.

32:33.360 --> 32:37.120
 It's probably doing mapping, but it looked like he wasn't doing anything and he was wearing

32:37.120 --> 32:39.120
 he or she, I don't know.

32:39.120 --> 32:44.640
 But it, well, I like, in my mind, there are people, they have a backstory, but this one

32:44.640 --> 32:48.640
 in particular definitely has a backstory because he was wearing a cowboy hat.

32:48.640 --> 32:54.160
 So I just saw a spot robot with a cowboy hat walking down the hall and there was just this

32:54.160 --> 32:58.880
 feeling like there's a life, like he has a life.

32:58.880 --> 33:02.240
 He probably has to commute back to his family at night.

33:02.240 --> 33:07.440
 Like there's a, there's a feeling like there's life instilled in this robot and it's magical.

33:07.440 --> 33:07.760
 I don't know.

33:07.760 --> 33:09.520
 It was, it was kind of inspiring to see.

33:09.520 --> 33:12.000
 Did it say hello to, did he say hello to you?

33:12.000 --> 33:15.360
 No, it's very, there's a focus nature to the robot.

33:15.360 --> 33:16.400
 No, no, listen.

33:16.400 --> 33:18.960
 I love competence and focus and great.

33:18.960 --> 33:25.200
 Like he was not going to get distracted by the, the shallowness of small talk.

33:25.200 --> 33:27.520
 There's a job to be done and he was doing it.

33:27.520 --> 33:30.560
 So anyway, the fact that it was working is a beautiful thing.

33:30.560 --> 33:35.440
 And I think Media Lab really prides itself on trying to always have a thing that's working

33:35.440 --> 33:36.480
 that you could show off.

33:36.480 --> 33:36.800
 Yes.

33:36.800 --> 33:38.960
 We used to call it a demo or die.

33:38.960 --> 33:43.520
 You, you could not, yeah, you could not like show up with like PowerPoint or something.

33:43.520 --> 33:48.080
 You actually had to have a working, you know what, my son who is now 13, I don't know if

33:48.080 --> 33:52.880
 this is still his life long goal or not, but when he was a little younger, his dream is

33:52.880 --> 33:57.280
 to build an island that's just inhabited by robots, like no humans.

33:57.280 --> 34:01.920
 He just wants all these robots to be connecting and having fun and there you go.

34:01.920 --> 34:06.480
 Does he have human, does he have an idea of which robots he loves most?

34:06.480 --> 34:09.280
 Is it, is it Roomba like robots?

34:09.280 --> 34:10.800
 Is it humanoid robots?

34:10.800 --> 34:13.920
 Robot dogs, or it's not clear yet.

34:13.920 --> 34:19.280
 We used to have a Jibo, which was one of the MIT Media Lab spin outs and he used to love

34:19.280 --> 34:30.400
 the giant head that spins and rotate and it's an eye or like not glowing like Cal 9000,

34:30.400 --> 34:31.760
 but the friendly version.

34:31.760 --> 34:34.080
 He loved that.

34:34.080 --> 34:38.240
 And then he just loves, uh, um,

34:38.240 --> 34:44.160
 yeah, he just, he, I think he loves all forms of robots actually.

34:44.160 --> 34:46.800
 So embodied intelligence.

34:46.800 --> 34:47.760
 Yes.

34:47.760 --> 34:54.320
 I like, I personally like legged robots, especially, uh, anything that can wiggle its butt.

34:55.120 --> 35:00.960
 No, that's not the definition of what I love, but that's just technically what I've been

35:00.960 --> 35:01.760
 working on recently.

35:01.760 --> 35:06.480
 Except I have a bunch of legged robots now in Austin and I've been doing, I was, I've

35:06.480 --> 35:12.400
 been trying to, uh, have them communicate affection with their body in different ways

35:12.400 --> 35:15.120
 just for art, for art really.

35:15.120 --> 35:20.080
 Cause I love the idea of walking around with the robots, like, uh, as you would with a

35:20.080 --> 35:20.400
 dog.

35:20.400 --> 35:23.120
 I think it's inspiring to a lot of people, especially young people.

35:23.120 --> 35:24.960
 Like kids love, kids love it.

35:25.760 --> 35:31.600
 Parents like adults are scared of robots, but kids don't have this kind of weird construction

35:31.600 --> 35:32.880
 of the world that's full of evil.

35:32.880 --> 35:34.480
 They love cool things.

35:34.480 --> 35:35.040
 Yeah.

35:35.040 --> 35:40.080
 I remember when Adam was in first grade, so he must have been like seven or so.

35:40.080 --> 35:44.960
 I went in to his class with a whole bunch of robots and like the emotion AI demo and

35:44.960 --> 35:45.680
 da da.

35:45.680 --> 35:51.120
 And I asked the kids, I was like, do you, would you kids want to have a robot, you know,

35:52.000 --> 35:53.600
 robot friend or robot companion?

35:53.600 --> 35:54.560
 Everybody said yes.

35:54.560 --> 35:58.880
 And they wanted it for all sorts of things, like to help them with their math homework

35:58.880 --> 36:00.160
 and to like be a friend.

36:00.160 --> 36:07.520
 So there's, it just struck me how there was no fear of robots was a lot of adults have

36:07.520 --> 36:09.280
 that like us versus them.

36:10.720 --> 36:11.920
 Yeah, none of that.

36:11.920 --> 36:16.320
 Of course you want to be very careful because you still have to look at the lessons of history

36:16.960 --> 36:21.920
 and how robots can be used by the power centers of the world to abuse your rights and all

36:21.920 --> 36:22.480
 that kind of stuff.

36:22.480 --> 36:30.480
 But mostly it's good to enter anything new with an excitement and an optimism.

36:30.480 --> 36:35.200
 Speaking of Roz, what have you learned about science and life from Rosalind Picard?

36:35.200 --> 36:39.920
 Oh my God, I've learned so many things about life from Roz.

36:41.200 --> 36:44.880
 I think the thing I learned the most is perseverance.

36:47.600 --> 36:51.280
 When I first met Roz, we applied and she invited me to be her postdoc.

36:51.280 --> 36:57.040
 We applied for a grant to the National Science Foundation to apply some of our research to

36:57.040 --> 36:57.760
 autism.

36:57.760 --> 36:59.360
 And we got back.

37:00.800 --> 37:01.520
 We were rejected.

37:01.520 --> 37:02.240
 Rejected.

37:02.240 --> 37:02.480
 Yeah.

37:02.480 --> 37:03.120
 And the reasoning was...

37:03.120 --> 37:06.000
 The first time you were rejected for fun, yeah.

37:06.000 --> 37:10.320
 Yeah, it was, and I basically, I just took the rejection to mean, okay, we're rejected.

37:10.320 --> 37:12.720
 It's done, like end of story, right?

37:12.720 --> 37:15.120
 And Roz was like, it's great news.

37:15.120 --> 37:16.080
 They love the idea.

37:16.080 --> 37:18.160
 They just don't think we can do it.

37:18.160 --> 37:21.360
 So let's build it, show them, and then reapply.

37:22.400 --> 37:25.520
 And it was that, oh my God, that story totally stuck with me.

37:26.320 --> 37:29.760
 And she's like that in every aspect of her life.

37:29.760 --> 37:32.080
 She just does not take no for an answer.

37:32.080 --> 37:34.080
 To reframe all negative feedback.

37:35.360 --> 37:36.400
 As a challenge.

37:36.400 --> 37:37.280
 As a challenge.

37:37.280 --> 37:38.560
 As a challenge.

37:38.560 --> 37:40.000
 Yes, they liked this.

37:40.000 --> 37:40.720
 Yeah, yeah, yeah.

37:40.720 --> 37:42.160
 It was a riot.

37:43.200 --> 37:45.040
 What else about science in general?

37:45.040 --> 37:51.680
 About how you see computers and also business and just everything about the world.

37:51.680 --> 37:54.800
 She's a very powerful, brilliant woman like yourself.

37:54.800 --> 37:56.400
 So is there some aspect of that too?

37:57.280 --> 38:00.320
 Yeah, I think Roz is actually also very faith driven.

38:00.320 --> 38:02.720
 She has this like deep belief in conviction.

38:04.240 --> 38:07.200
 Yeah, and in the good in the world and humanity.

38:07.200 --> 38:13.520
 And I think that was meeting her and her family was definitely like a defining moment for me

38:13.520 --> 38:17.520
 because that was when I was like, wow, like you can be of a different background and

38:18.080 --> 38:22.720
 religion and whatever and you can still have the same core values.

38:23.760 --> 38:25.200
 So that was, that was, yeah.

38:26.800 --> 38:27.680
 I'm grateful to her.

38:28.560 --> 38:30.240
 Roz, if you're listening, thank you.

38:30.240 --> 38:31.280
 Yeah, she's great.

38:31.280 --> 38:32.560
 She's been on this podcast before.

38:33.600 --> 38:36.320
 I hope she'll be on, I'm sure she'll be on again.

38:36.320 --> 38:44.720
 And you were the founder and CEO of Effektiva, which is a big company that was acquired by

38:44.720 --> 38:46.400
 another big company, SmartEye.

38:46.960 --> 38:49.120
 And you're now the deputy CEO of SmartEye.

38:49.120 --> 38:51.040
 So you're a powerful leader.

38:51.040 --> 38:51.760
 You're brilliant.

38:51.760 --> 38:52.800
 You're a brilliant scientist.

38:53.360 --> 38:55.040
 A lot of people are inspired by you.

38:55.040 --> 39:00.160
 What advice would you give, especially to young women, but people in general who dream

39:00.160 --> 39:08.080
 of becoming powerful leaders like yourself in a world where perhaps, in a world that

39:09.520 --> 39:17.440
 perhaps doesn't give them a clear, easy path to do so, whether we're talking about Egypt

39:17.440 --> 39:18.080
 or elsewhere?

39:19.920 --> 39:27.680
 You know, hearing you kind of describe me that way, kind of encapsulates, I think what

39:27.680 --> 39:31.200
 I think is the biggest challenge of all, which is believing in yourself, right?

39:32.160 --> 39:37.440
 I have had to like grapple with this, what I call now the Debbie Downer voice in my head.

39:39.360 --> 39:42.720
 The kind of basically, it's just chattering all the time.

39:42.720 --> 39:45.040
 It's basically saying, oh, no, no, no, no, you can't do this.

39:45.040 --> 39:46.320
 Like you're not going to raise money.

39:46.320 --> 39:47.280
 You can't start a company.

39:47.280 --> 39:50.720
 Like what business do you have, like starting a company or running a company or selling

39:50.720 --> 39:51.200
 a company?

39:51.200 --> 39:52.080
 Like you name it.

39:52.080 --> 39:53.120
 It's always like.

39:53.120 --> 40:02.160
 And I think my biggest advice to not just women, but people who are taking a new path

40:02.160 --> 40:07.200
 and, you know, they're not sure, is to not let yourself and let your thoughts be the

40:07.200 --> 40:08.880
 biggest obstacle in your way.

40:09.920 --> 40:16.880
 And I've had to like really work on myself to not be my own biggest obstacle.

40:17.520 --> 40:18.880
 So you got that negative voice.

40:18.880 --> 40:19.380
 Yeah.

40:20.640 --> 40:21.200
 So is that?

40:21.200 --> 40:21.920
 Am I the only one?

40:21.920 --> 40:23.280
 I don't think I'm the only one.

40:23.280 --> 40:25.040
 No, I have that negative voice.

40:25.040 --> 40:29.840
 I'm not exactly sure if it's a bad thing or a good thing.

40:29.840 --> 40:35.440
 I've been really torn about it because it's been a lifelong companions.

40:35.440 --> 40:36.240
 It's hard to know.

40:37.840 --> 40:44.800
 It's kind of, it drives productivity and progress, but it can hold you back from taking

40:44.800 --> 40:45.520
 big leaps.

40:45.520 --> 40:53.120
 I think the best I can say is probably you have to somehow be able to control it, to

40:53.120 --> 40:56.880
 turn it off when it's not useful and turn it on when it's useful.

40:57.680 --> 41:00.400
 Like I have from almost like a third person perspective.

41:00.400 --> 41:00.900
 Right.

41:00.900 --> 41:02.400
 Somebody who's sitting there like.

41:02.400 --> 41:02.900
 Yeah.

41:02.960 --> 41:06.720
 Like, because it is useful to be critical.

41:07.520 --> 41:12.480
 Like after, I just gave a talk yesterday.

41:12.480 --> 41:19.120
 At MIT and I was just, there's so much love and it was such an incredible experience.

41:19.120 --> 41:25.760
 So many amazing people I got a chance to talk to, but afterwards when I went home and just

41:25.760 --> 41:29.680
 took this long walk, it was mostly just negative thoughts about me.

41:29.680 --> 41:34.720
 I don't like one basic stuff like I don't deserve any of it.

41:34.720 --> 41:39.200
 And second is like, like, why did you, that was so bad.

41:39.200 --> 41:44.880
 Second is like, like, why did you, that was so dumb that you said this, that's so dumb.

41:44.880 --> 41:46.960
 Like you should have prepared that better.

41:47.520 --> 41:48.560
 Why did you say this?

41:50.240 --> 41:53.440
 But I think it's good to hear that voice out.

41:54.160 --> 41:54.560
 All right.

41:54.560 --> 41:56.240
 And like sit in that.

41:56.240 --> 41:58.560
 And ultimately I think you grow from that.

41:58.560 --> 42:03.680
 Now, when you're making really big decisions about funding or starting a company or taking

42:03.680 --> 42:10.960
 a leap to go to the UK or take a leap to go to America to work in Media Lab though.

42:10.960 --> 42:11.200
 Yeah.

42:11.200 --> 42:22.160
 There's, that's, you should be able to shut that off then because you should have like

42:22.160 --> 42:26.080
 this weird confidence, almost like faith that you said before that everything's going to

42:26.080 --> 42:26.720
 work out.

42:26.720 --> 42:28.480
 So take the leap of faith.

42:28.480 --> 42:29.520
 Take the leap of faith.

42:30.160 --> 42:32.400
 Despite all the negativity.

42:32.400 --> 42:34.240
 I mean, there's, there's, there's some of that.

42:34.240 --> 42:38.560
 You, you actually tweeted a really nice tweet thread.

42:39.760 --> 42:45.840
 It says, quote, a year ago, a friend recommended I do daily affirmations and I was skeptical,

42:46.800 --> 42:49.360
 but I was going through major transitions in my life.

42:49.360 --> 42:53.520
 So I gave it a shot and it set me on a journey of self acceptance and self love.

42:54.080 --> 42:55.680
 So what was that like?

42:55.680 --> 43:01.360
 Can you maybe talk through this idea of affirmations and how that helped you?

43:01.360 --> 43:02.320
 Yeah.

43:02.320 --> 43:07.200
 Because really like I'm just like me, I'm a kind, I'd like to think of myself as a kind

43:07.200 --> 43:10.320
 person in general, but I'm kind of mean to myself sometimes.

43:10.320 --> 43:10.640
 Yeah.

43:11.280 --> 43:15.440
 And so I've been doing journaling for almost 10 years now.

43:16.720 --> 43:18.880
 I use an app called Day One and it's awesome.

43:18.880 --> 43:22.880
 I just journal and I use it as an opportunity to almost have a conversation with the Debbie

43:22.880 --> 43:24.800
 Downer voice in my, it's like a rebuttal, right?

43:25.520 --> 43:29.120
 Like Debbie Downer says, oh my God, like you, you know, you won't be able to raise this

43:29.120 --> 43:29.680
 round of funding.

43:29.680 --> 43:31.760
 I'm like, okay, let's talk about it.

43:33.120 --> 43:35.520
 I have a track record of doing X, Y, and Z.

43:35.520 --> 43:37.120
 I think I can do this.

43:37.120 --> 43:42.240
 And it's literally like, so I wouldn't, I don't know that I can shut off the voice,

43:42.240 --> 43:44.240
 but I can have a conversation with it.

43:44.240 --> 43:47.760
 And it just, it just, and I bring data to the table, right?

43:49.840 --> 43:50.320
 Nice.

43:50.320 --> 43:53.120
 So that was the journaling part, which I found very helpful.

43:53.760 --> 43:57.600
 But the affirmation took it to a whole next level and I just love it.

43:57.600 --> 44:02.720
 I'm a year into doing this and you literally wake up in the morning and the first thing

44:02.720 --> 44:09.440
 you do, I meditate first and then I write my affirmations and it's the energy I want

44:09.440 --> 44:12.160
 to put out in the world that hopefully will come right back to me.

44:12.160 --> 44:16.560
 So I will say, I always start with my smile lights up the whole world.

44:17.200 --> 44:20.720
 And I kid you not, like people in the street will stop me and say, oh my God, like we love

44:20.720 --> 44:21.360
 your smile.

44:21.360 --> 44:22.320
 Like, yes.

44:22.320 --> 44:28.880
 So, so my affirmations will change depending on, you know, what's happening this day.

44:28.880 --> 44:29.520
 Is it funny?

44:29.520 --> 44:29.840
 I know.

44:29.840 --> 44:31.360
 Don't judge, don't judge.

44:31.360 --> 44:33.840
 No, that's not, laughter's not judgment.

44:33.840 --> 44:35.040
 It's just awesome.

44:35.040 --> 44:42.480
 I mean, it's true, but you're saying affirmations somehow help kind of, I mean, what is it that

44:42.480 --> 44:48.400
 they do work to like remind you of the kind of person you are and the kind of person you

44:48.400 --> 44:53.760
 want to be, which actually may be in reverse order, the kind of person you want to be.

44:53.760 --> 44:56.320
 And that helps you become the kind of person you actually are.

44:56.960 --> 45:01.280
 It's just, it's, it brings intentionality to like what you're doing.

45:01.280 --> 45:01.680
 Right.

45:01.680 --> 45:07.200
 And so, by the way, I was laughing because my affirmations, which I also do are the

45:07.200 --> 45:07.760
 opposite.

45:07.760 --> 45:08.320
 Oh, you do?

45:08.320 --> 45:09.040
 Oh, what do you do?

45:09.040 --> 45:11.920
 I don't, I don't have a, my smile lights up the world.

45:11.920 --> 45:22.240
 Maybe I should add that because like, I, I have, I just, I have, oh boy, it's, it's much

45:22.240 --> 45:30.400
 more stoic, like about focus, about this kind of stuff, but the joy, the emotion that you're

45:30.400 --> 45:32.960
 just in that little affirmation is beautiful.

45:32.960 --> 45:34.160
 So maybe I should add that.

45:35.120 --> 45:38.080
 I have some, I have some like focused stuff, but that's usually.

45:38.080 --> 45:39.120
 But that's a cool start.

45:39.120 --> 45:43.760
 It's after all the like smiling and playful and joyful and all that.

45:43.760 --> 45:45.440
 And then it's like, okay, I kick butt.

45:45.440 --> 45:46.560
 Let's get shit done.

45:46.560 --> 45:46.960
 Right.

45:46.960 --> 45:48.640
 Let's get shit done affirmation.

45:48.640 --> 45:49.280
 Okay, cool.

45:49.280 --> 45:51.040
 So like what else is on there?

45:52.640 --> 45:53.600
 What else is on there?

45:54.320 --> 46:00.000
 Well, I, I have, I'm also, I'm, I'm a magnet for all sorts of things.

46:00.000 --> 46:02.160
 So I'm an amazing people magnet.

46:02.160 --> 46:04.400
 I attract like awesome people into my universe.

46:05.520 --> 46:06.960
 That's an actual affirmation.

46:06.960 --> 46:07.200
 Yes.

46:07.840 --> 46:08.880
 That's great.

46:08.880 --> 46:09.280
 Yeah.

46:09.280 --> 46:10.640
 So that, that's, and that, yeah.

46:10.640 --> 46:13.920
 And that somehow manifests itself into like in working.

46:13.920 --> 46:14.720
 I think so.

46:15.440 --> 46:15.680
 Yeah.

46:15.680 --> 46:18.960
 Like, can you speak to like why it feels good to do the affirmations?

46:19.760 --> 46:23.280
 I honestly think it just grounds the day.

46:24.080 --> 46:30.000
 And then it allows me to, instead of just like being pulled back and forth, like throughout

46:30.000 --> 46:31.680
 the day, it just like grounds me.

46:31.680 --> 46:34.560
 I'm like, okay, like this thing happened.

46:34.560 --> 46:37.360
 It's not exactly what I wanted it to be, but I'm patient.

46:37.360 --> 46:42.960
 Or I'm, you know, I'm, I trust that the universe will do amazing things for me, which is one

46:42.960 --> 46:45.440
 of my other consistent affirmations.

46:45.440 --> 46:46.720
 Or I'm an amazing mom.

46:46.720 --> 46:47.040
 Right.

46:47.040 --> 46:51.040
 And so I can grapple with all the feelings of mom guilt that I have all the time.

46:52.240 --> 46:53.760
 Or here's another one.

46:53.760 --> 46:55.040
 I'm a love magnet.

46:55.040 --> 46:59.040
 And I literally say, I will kind of picture the person that I'd love to end up with.

46:59.040 --> 47:02.480
 And I write it all down and it hasn't happened yet, but it.

47:02.480 --> 47:03.920
 What are you, what are you picturing?

47:03.920 --> 47:04.720
 This is Brad Pitt.

47:06.000 --> 47:07.040
 Because that's what I picture.

47:07.040 --> 47:07.440
 Okay.

47:07.440 --> 47:08.240
 That's what you picture?

47:08.240 --> 47:08.480
 Yeah.

47:08.480 --> 47:08.880
 Okay.

47:08.880 --> 47:11.760
 On the, on the running, holding hands, running together.

47:11.760 --> 47:12.080
 Okay.

47:14.160 --> 47:18.800
 No, more like fight club that the fight club, Brad Pitt, where he's like standing.

47:18.800 --> 47:19.120
 All right.

47:19.120 --> 47:20.000
 People will know.

47:20.000 --> 47:20.720
 Anyway, I'm sorry.

47:20.720 --> 47:21.920
 I'll get off on that.

47:21.920 --> 47:27.360
 Do you have a, like when you're thinking about the being a love magnet in that way, are you

47:27.360 --> 47:35.120
 picturing specific people or is this almost like in the space of like energy?

47:36.000 --> 47:36.320
 Right.

47:36.320 --> 47:44.240
 It's somebody who is smart and well accomplished and successful in their life, but they're

47:44.240 --> 47:48.000
 generous and they're well traveled and they want to travel the world.

47:48.960 --> 47:49.760
 Things like that.

47:49.760 --> 47:51.360
 Like their head over heels into me.

47:51.360 --> 47:54.560
 It's like, I know it sounds super silly, but it's literally what I write.

47:54.560 --> 47:54.800
 Yeah.

47:54.800 --> 47:56.320
 And I believe it'll happen one day.

47:56.320 --> 47:58.000
 Oh, you actually write, so you don't say it out loud?

47:58.000 --> 47:58.240
 You write.

47:58.240 --> 47:58.960
 No, I write it.

47:58.960 --> 48:00.320
 I write all my affirmations.

48:01.200 --> 48:01.920
 I do the opposite.

48:01.920 --> 48:02.640
 I say it out loud.

48:02.640 --> 48:03.440
 Oh, you say it out loud?

48:03.440 --> 48:04.320
 Interesting.

48:04.320 --> 48:06.320
 Yeah, if I'm alone, I'll say it out loud.

48:06.320 --> 48:07.360
 Interesting.

48:07.360 --> 48:08.240
 I should try that.

48:10.000 --> 48:15.600
 I think it's what feels more powerful to you.

48:15.600 --> 48:16.960
 To me, more powerful.

48:18.240 --> 48:20.320
 Saying stuff feels more powerful.

48:20.320 --> 48:20.820
 Yeah.

48:21.520 --> 48:32.320
 Writing is, writing feels like I'm losing the words, like losing the power of the words

48:32.320 --> 48:33.520
 maybe because I write slow.

48:33.520 --> 48:34.320
 Do you handwrite?

48:34.960 --> 48:36.320
 No, I type.

48:36.320 --> 48:37.520
 It's on this app.

48:37.520 --> 48:38.800
 It's day one, basically.

48:38.800 --> 48:44.320
 And I just, I can look, the best thing about it is I can look back and see like a year ago,

48:44.320 --> 48:45.760
 what was I affirming, right?

48:46.560 --> 48:47.200
 So it's...

48:47.200 --> 48:48.320
 Oh, so it changes over time.

48:50.000 --> 48:54.640
 It hasn't like changed a lot, but the focus kind of changes over time.

48:54.640 --> 48:55.440
 I got it.

48:55.440 --> 48:57.840
 Yeah, I say the same exact thing over and over and over.

48:57.840 --> 48:58.400
 Oh, you do?

48:58.400 --> 48:58.560
 Okay.

48:58.560 --> 49:00.880
 There's a comfort in the sameness of it.

49:00.880 --> 49:05.440
 Well, actually, let me jump around because let me ask you about, because all this talk

49:05.440 --> 49:10.800
 about Brad Pitt, or maybe it's just going on inside my head, let me ask you about dating

49:10.800 --> 49:11.440
 in general.

49:12.960 --> 49:16.800
 You tweeted, are you based in Boston and single?

49:16.800 --> 49:22.880
 And then you pointed to a startup Singles Night sponsored by Smile Dating app.

49:23.440 --> 49:27.280
 I mean, this is jumping around a little bit, but since you mentioned...

49:27.280 --> 49:33.840
 Since you mentioned, can AI help solve this dating love problem?

49:34.400 --> 49:34.960
 What do you think?

49:34.960 --> 49:41.600
 This problem of connection that is part of the human condition, can AI help that you

49:41.600 --> 49:43.520
 yourself are in the search affirming?

49:44.960 --> 49:48.160
 Maybe that's what I should affirm, like build an AI.

49:48.160 --> 49:49.520
 Build an AI that finds love?

49:49.520 --> 50:00.400
 I think there must be a science behind that first moment you meet a person and you either

50:00.400 --> 50:02.240
 have chemistry or you don't, right?

50:02.800 --> 50:06.960
 I guess that was the question I was asking, would you put it brilliantly, is that a science

50:06.960 --> 50:07.440
 or an art?

50:09.680 --> 50:15.200
 I think there are like, there's actual chemicals that get exchanged when two people meet.

50:15.200 --> 50:16.240
 I don't know about that.

50:16.240 --> 50:22.880
 I like how you're changing, yeah, changing your mind as we're describing it, but it feels

50:22.880 --> 50:23.360
 that way.

50:23.920 --> 50:29.040
 But it's what science shows us is sometimes we can explain with the rigor, the things

50:29.040 --> 50:30.320
 that feel like magic.

50:31.760 --> 50:33.680
 So maybe we can remove all the magic.

50:34.320 --> 50:39.760
 Maybe it's like, I honestly think, like I said, like Goodreads should be a dating app,

50:39.760 --> 50:41.680
 which like books.

50:41.680 --> 50:46.960
 I wonder if you look at just like books or content you've consumed.

50:46.960 --> 50:50.640
 I mean, that's essentially what YouTube does when it does a recommendation.

50:50.640 --> 50:56.400
 If you just look at your footprint of content consumed, if there's an overlap, but maybe

50:56.400 --> 51:01.280
 interesting difference with an overlap that some, I'm sure this is a machine learning

51:01.280 --> 51:02.160
 problem that's solvable.

51:03.520 --> 51:10.560
 Like this person is very likely to be not only there to be chemistry in the short term,

51:10.560 --> 51:13.920
 but a good lifelong partner to grow together.

51:13.920 --> 51:15.600
 I bet you it's a good machine learning problem.

51:15.600 --> 51:16.480
 You just need the data.

51:16.480 --> 51:17.360
 Let's do it.

51:17.360 --> 51:22.080
 Well, actually, I do think there's so much data about each of us that there ought to

51:22.080 --> 51:26.320
 be a machine learning algorithm that can ingest all this data and basically say, I think the

51:26.320 --> 51:30.720
 following 10 people would be interesting connections for you, right?

51:32.080 --> 51:36.640
 And so Smile dating app kind of took one particular angle, which is humor.

51:36.640 --> 51:41.920
 It matches people based on their humor styles, which is one of the main ingredients of a

51:41.920 --> 51:43.120
 successful relationship.

51:43.120 --> 51:46.640
 Like if you meet somebody and they can make you laugh, like that's a good thing.

51:47.200 --> 51:53.040
 And if you develop like internal jokes, like inside jokes and you're bantering, like that's

51:53.040 --> 51:53.280
 fun.

51:54.320 --> 51:56.640
 So I think.

51:56.640 --> 51:57.520
 Yeah, definitely.

51:57.520 --> 51:58.320
 Definitely.

51:58.320 --> 52:04.880
 But yeah, that's the number of and the rate of inside joke generation.

52:04.880 --> 52:08.160
 You could probably measure that and then optimize it over the first few days.

52:08.160 --> 52:11.360
 You could say, we're just turning this into a machine learning problem.

52:11.360 --> 52:11.920
 I love it.

52:13.360 --> 52:23.120
 But for somebody like you, who's exceptionally successful and busy, is there, is there signs

52:23.120 --> 52:24.880
 to that aspect of dating?

52:24.880 --> 52:25.600
 Is it tricky?

52:26.320 --> 52:27.600
 Is there advice you can give?

52:27.600 --> 52:29.440
 Oh, my God, I give the worst advice.

52:29.440 --> 52:31.440
 Well, I can tell you like I have a spreadsheet.

52:31.440 --> 52:34.640
 Is that a good or a bad thing?

52:34.640 --> 52:36.160
 Do you regret the spreadsheet?

52:37.040 --> 52:38.240
 Well, I don't know.

52:38.240 --> 52:39.440
 What's the name of the spreadsheet?

52:39.440 --> 52:40.000
 Is it love?

52:40.800 --> 52:42.880
 It's the date track, dating tracker.

52:42.880 --> 52:43.840
 Dating tracker.

52:43.840 --> 52:44.560
 It's very like.

52:44.560 --> 52:45.280
 Love tracker.

52:45.280 --> 52:45.840
 Yeah.

52:46.320 --> 52:47.760
 And there's a rating system, I'm sure.

52:47.760 --> 52:48.080
 Yeah.

52:48.080 --> 52:49.920
 There's like weights and stuff.

52:49.920 --> 52:51.440
 It's too close to home.

52:51.440 --> 52:52.000
 Oh, is it?

52:52.000 --> 52:52.800
 Do you also have.

52:52.800 --> 52:56.640
 Well, I don't have a spreadsheet, but I would, now that you say it, it seems like a good

52:56.640 --> 52:57.200
 idea.

52:57.200 --> 52:58.160
 Oh, no.

52:58.160 --> 52:58.720
 Okay.

52:58.720 --> 53:03.600
 Turning it into data.

53:05.760 --> 53:09.280
 I do wish that somebody else had a spreadsheet about me.

53:11.200 --> 53:17.120
 You know, if it was like, like I said, like you said, convert, collect a lot of data about

53:17.120 --> 53:21.840
 us in a way that's privacy preserving, that I own the data, I can control it and then

53:21.840 --> 53:28.400
 use that data to find, I mean, not just romantic love, but collaborators, friends, all that

53:28.400 --> 53:28.960
 kind of stuff.

53:28.960 --> 53:30.240
 It seems like the data is there.

53:30.240 --> 53:30.740
 Right.

53:32.080 --> 53:35.280
 That's the problem social networks are trying to solve, but I think they're doing a really

53:35.280 --> 53:36.240
 poor job.

53:36.240 --> 53:39.600
 Even Facebook tried to get into a dating app business.

53:39.600 --> 53:44.400
 And I think there's so many components to running a successful company that connects

53:44.400 --> 53:45.360
 human beings.

53:45.360 --> 53:53.920
 And part of that is, you know, having engineers that care about the human side, right, as

53:53.920 --> 53:57.760
 you know, extremely well, it's not, it's not easy to find those.

53:57.760 --> 54:00.640
 But you also don't want just people that care about the human.

54:00.640 --> 54:02.240
 They also have to be good engineers.

54:02.240 --> 54:05.760
 So it's like, you have to find this beautiful mix.

54:05.760 --> 54:12.560
 And for some reason, just empirically speaking, people have not done a good job of that, of

54:12.560 --> 54:13.680
 building companies like that.

54:13.680 --> 54:16.480
 And it must mean that it's a difficult problem to solve.

54:17.040 --> 54:19.920
 Dating apps, it seems difficult.

54:19.920 --> 54:22.080
 Okay, Cupid, Tinder, all those kinds of stuff.

54:22.080 --> 54:32.080
 They seem to find, of course they work, but they seem to not work as well as I would imagine

54:32.080 --> 54:32.880
 is possible.

54:32.880 --> 54:36.960
 Like, with data, wouldn't you be able to find better human connection?

54:36.960 --> 54:39.520
 It's like arranged marriages on steroids, essentially.

54:39.520 --> 54:40.480
 Right, right.

54:40.480 --> 54:42.560
 Arranged by machine learning algorithm.

54:42.560 --> 54:45.600
 Arranged by machine learning algorithm, but not a superficial one.

54:45.600 --> 54:48.640
 I think a lot of the dating apps out there are just so superficial.

54:48.640 --> 54:55.680
 They're just matching on like high level criteria that aren't ingredients for successful partnership.

54:55.680 --> 54:58.480
 But you know what's missing, though, too?

54:58.480 --> 55:01.440
 I don't know how to fix that, the serendipity piece of it.

55:01.440 --> 55:03.760
 Like, how do you engineer serendipity?

55:03.760 --> 55:07.680
 Like this random, like, chance encounter, and then you fall in love with the person.

55:07.680 --> 55:10.080
 Like, I don't know how a dating app can do that.

55:10.080 --> 55:12.080
 So there has to be a little bit of randomness.

55:12.080 --> 55:21.680
 Maybe every 10th match is just a, you know, yeah, somebody that the algorithm wouldn't

55:21.680 --> 55:25.840
 have necessarily recommended, but it allows for a little bit of...

55:25.840 --> 55:33.440
 Well, it can also, you know, it can also trick you into thinking of serendipity by like somehow

55:33.440 --> 55:39.200
 showing you a tweet of a person that he thinks you'll match well with, but do it accidentally

55:39.200 --> 55:40.560
 as part of another search.

55:40.560 --> 55:41.040
 Right.

55:41.040 --> 55:46.080
 And like you just notice it, like, and then you get, you go down a rabbit hole and you

55:46.080 --> 55:51.360
 connect them outside the app to like, you connect with this person outside the app somehow.

55:51.360 --> 55:53.520
 So it's just, it creates that moment of meeting.

55:54.240 --> 55:57.600
 Of course, you have to think of, from an app perspective, how you can turn that into a

55:57.600 --> 55:58.240
 business.

55:58.240 --> 56:03.600
 But I think ultimately a business that helps people find love in any way.

56:04.400 --> 56:07.440
 Like that's what Apple was about, create products that people love.

56:07.440 --> 56:08.240
 That's beautiful.

56:08.240 --> 56:11.520
 I mean, you got to make money somehow.

56:11.520 --> 56:18.160
 If you help people fall in love personally with the product, find self love or love another

56:18.160 --> 56:19.840
 human being, you're going to make money.

56:19.840 --> 56:21.440
 You're going to figure out a way to make money.

56:22.960 --> 56:27.840
 I just feel like the dating apps often will optimize for something else than love.

56:28.560 --> 56:30.000
 It's the same with social networks.

56:30.000 --> 56:35.280
 They optimize for engagement as opposed to like a deep, meaningful connection that's

56:35.280 --> 56:39.760
 ultimately grounded in like personal growth, you as a human being growing and all that

56:39.760 --> 56:40.240
 kind of stuff.

56:41.520 --> 56:46.800
 Let me do like a pivot to a dark topic, which you opened the book with.

56:48.560 --> 56:56.080
 A story, because I'd like to talk to you about just emotion and artificial intelligence.

56:56.080 --> 56:59.680
 I think this is a good story to start to think about emotional intelligence.

56:59.680 --> 57:05.120
 You opened the book with a story of a central Florida man, Jamel Dunn, who was drowning

57:05.120 --> 57:10.000
 and drowned while five teenagers watched and laughed, saying things like, you're going

57:10.000 --> 57:10.800
 to die.

57:10.800 --> 57:15.840
 And when Jamel disappeared below the surface of the water, one of them said he just died

57:15.840 --> 57:16.960
 and the others laughed.

57:17.440 --> 57:23.360
 What does this incident teach you about human nature and the response to it perhaps?

57:23.360 --> 57:23.840
 Yeah.

57:24.320 --> 57:28.480
 I mean, I think this is a really, really, really sad story.

57:28.480 --> 57:34.480
 And it and it and it highlights what I believe is a it's a real problem in our world today.

57:34.480 --> 57:36.000
 It's it's an empathy crisis.

57:36.720 --> 57:39.840
 Yeah, we're living through an empathy crisis and crisis.

57:39.840 --> 57:40.400
 Yeah.

57:40.400 --> 57:40.900
 Yeah.

57:42.240 --> 57:45.360
 And I mean, we've we've talked about this throughout our conversation.

57:45.360 --> 57:47.040
 We dehumanize each other.

57:47.040 --> 57:51.920
 And unfortunately, yes, technology is bringing us together.

57:51.920 --> 57:53.840
 But in a way, it's just dehumanized.

57:53.840 --> 57:58.640
 It's creating this like, yeah, dehumanizing of the other.

57:58.640 --> 58:00.960
 And I think that's a huge problem.

58:01.840 --> 58:05.840
 The good news is I think solution, the solution could be technology based.

58:05.840 --> 58:11.520
 Like, I think if we rethink the way we design and deploy our technologies, we can solve

58:11.520 --> 58:12.560
 parts of this problem.

58:12.560 --> 58:13.200
 But I worry about it.

58:13.200 --> 58:19.200
 I mean, even with my son, a lot of his interactions are computer mediated.

58:19.200 --> 58:25.280
 And I just question what that's doing to his empathy skills and, you know, his ability

58:25.280 --> 58:26.560
 to really connect with people.

58:26.560 --> 58:34.560
 So that you think you think it's not possible to form empathy through the digital medium.

58:36.320 --> 58:37.360
 I think it is.

58:38.560 --> 58:44.000
 But we have to be thoughtful about because the way the way we engage face to face, which

58:44.000 --> 58:45.840
 is what we're doing right now, right?

58:45.840 --> 58:49.360
 There's the nonverbal signals, which are a majority of how we communicate.

58:49.360 --> 58:52.960
 It's like 90% of how we communicate is your facial expressions.

58:54.000 --> 58:57.680
 You know, I'm saying something and you're nodding your head now, and that creates a

58:57.680 --> 58:58.480
 feedback loop.

58:58.480 --> 59:02.160
 And and if you break that, and now I have anxiety about it.

59:04.160 --> 59:04.880
 Poor Lex.

59:06.000 --> 59:06.560
 Oh, boy.

59:06.560 --> 59:09.680
 I am not scrutinizing your facial expressions during this interview.

59:09.680 --> 59:10.160
 I am.

59:12.160 --> 59:12.800
 Look normal.

59:12.800 --> 59:13.360
 Look human.

59:13.360 --> 59:13.860
 Yeah.

59:13.860 --> 59:17.720
 Look normal, look human.

59:17.720 --> 59:18.680
 Nod head.

59:18.680 --> 59:19.400
 Yeah, nod head.

59:20.920 --> 59:21.560
 In agreement.

59:21.560 --> 59:24.600
 If Rana says yes, then nod head else.

59:25.720 --> 59:29.640
 Don't do it too much because it might be at the wrong time and then it will send the

59:29.640 --> 59:30.760
 wrong signal.

59:30.760 --> 59:31.400
 Oh, God.

59:31.400 --> 59:34.760
 And make eye contact sometimes because humans appreciate that.

59:35.320 --> 59:35.640
 All right.

59:35.640 --> 59:36.440
 Anyway, okay.

59:38.520 --> 59:42.920
 Yeah, but something about the especially when you say mean things in person, you get to

59:42.920 --> 59:44.280
 see the pain of the other person.

59:44.280 --> 59:44.600
 Exactly.

59:44.600 --> 59:48.120
 But if you're tweeting it at a person and you have no idea how it's going to land, you're

59:48.120 --> 59:52.040
 more likely to do that on social media than you are in face to face conversations.

59:52.040 --> 59:52.540
 So.

59:54.520 --> 59:55.880
 What do you think is more important?

59:59.000 --> 1:00:00.360
 EQ or IQ?

1:00:00.360 --> 1:00:02.200
 EQ being emotional intelligence.

1:00:03.880 --> 1:00:06.360
 In terms of in what makes us human.

1:00:08.120 --> 1:00:11.240
 I think emotional intelligence is what makes us human.

1:00:11.240 --> 1:00:14.760
 It's how we connect with one another.

1:00:14.760 --> 1:00:16.440
 It's how we build trust.

1:00:16.440 --> 1:00:19.560
 It's how we make decisions, right?

1:00:19.560 --> 1:00:25.720
 Like your emotions drive kind of what you had for breakfast, but also where you decide

1:00:25.720 --> 1:00:28.600
 to live and what you want to do for the rest of your life.

1:00:28.600 --> 1:00:31.320
 So I think emotions are underrated.

1:00:33.160 --> 1:00:39.080
 So emotional intelligence isn't just about the effective expression of your own emotions.

1:00:39.080 --> 1:00:44.440
 It's about a sensitivity and empathy to other people's emotions and that sort of being

1:00:44.440 --> 1:00:48.280
 able to effectively engage in the dance of emotions with other people.

1:00:48.840 --> 1:00:51.240
 Yeah, I like that explanation.

1:00:51.240 --> 1:00:52.360
 I like that kind of.

1:00:53.720 --> 1:00:56.680
 Yeah, thinking about it as a dance because it is really about that.

1:00:56.680 --> 1:01:01.800
 It's about sensing what state the other person's in and using that information to decide on

1:01:01.800 --> 1:01:02.920
 how you're going to react.

1:01:05.400 --> 1:01:06.760
 And I think it can be very powerful.

1:01:06.760 --> 1:01:15.160
 Like people who are the best, most persuasive leaders in the world tap into, you know, they

1:01:15.160 --> 1:01:20.360
 have, if you have higher EQ, you're more likely to be able to motivate people to change

1:01:20.360 --> 1:01:21.160
 their behaviors.

1:01:21.880 --> 1:01:24.360
 So it can be very powerful.

1:01:24.920 --> 1:01:31.800
 On a more kind of technical, maybe philosophical level, you've written that emotion is universal.

1:01:31.800 --> 1:01:36.920
 It seems that, sort of like Chomsky says, language is universal.

1:01:36.920 --> 1:01:39.960
 There's a bunch of other stuff like cognition, consciousness.

1:01:39.960 --> 1:01:43.560
 It seems a lot of us have these aspects.

1:01:43.560 --> 1:01:46.520
 So the human mind generates all this.

1:01:46.520 --> 1:01:52.040
 And so what do you think is the, they all seem to be like echoes of the same thing.

1:01:52.840 --> 1:01:56.280
 What do you think emotion is exactly?

1:01:56.280 --> 1:01:57.720
 Like how deep does it run?

1:01:57.720 --> 1:02:01.800
 Is it a surface level thing that we display to each other?

1:02:01.800 --> 1:02:04.760
 Is it just another form of language or something deep within?

1:02:05.640 --> 1:02:07.480
 I think it's really deep.

1:02:07.480 --> 1:02:09.320
 It's how, you know, we started with memory.

1:02:09.880 --> 1:02:12.600
 I think emotions play a really important role.

1:02:14.040 --> 1:02:18.040
 Yeah, emotions play a very important role in how we encode memories, right?

1:02:18.040 --> 1:02:21.640
 Our memories are often encoded, almost indexed by emotions.

1:02:21.640 --> 1:02:22.120
 Yeah.

1:02:22.120 --> 1:02:28.520
 Yeah, it's at the core of how, you know, our decision making engine is also heavily

1:02:28.520 --> 1:02:30.040
 influenced by our emotions.

1:02:30.040 --> 1:02:31.960
 So emotions is part of cognition.

1:02:31.960 --> 1:02:32.680
 Totally.

1:02:32.680 --> 1:02:34.680
 It's intermixed into the whole thing.

1:02:34.680 --> 1:02:35.960
 Yes, absolutely.

1:02:35.960 --> 1:02:39.960
 And in fact, when you take it away, people are unable to make decisions.

1:02:39.960 --> 1:02:41.000
 They're really paralyzed.

1:02:41.000 --> 1:02:45.240
 Like they can't go about their daily or their, you know, personal or professional lives.

1:02:45.240 --> 1:02:45.740
 So.

1:02:45.740 --> 1:02:53.740
 It does seem like there's probably some interesting interweaving of emotion and consciousness.

1:02:53.740 --> 1:02:58.940
 I wonder if it's possible to have, like if they're next door neighbors somehow, or if

1:02:58.940 --> 1:03:01.740
 they're actually flat mates.

1:03:01.740 --> 1:03:08.940
 I don't, it feels like the hard problem of consciousness where it's some, it feels like

1:03:08.940 --> 1:03:10.780
 something to experience the thing.

1:03:10.780 --> 1:03:16.780
 Like red feels like red, and it's, you know, when you eat a mango, it's sweet.

1:03:16.780 --> 1:03:24.620
 The taste, the sweetness, that it feels like something to experience that sweetness, that

1:03:24.620 --> 1:03:26.380
 whatever generates emotions.

1:03:28.060 --> 1:03:31.740
 But then like, see, I feel like emotion is part of communication.

1:03:31.740 --> 1:03:34.300
 It's very much about communication.

1:03:34.300 --> 1:03:39.420
 And then, you know, it's like, you know, it's like, you know, it's like, you know, it's

1:03:39.420 --> 1:03:44.540
 and then that means it's also deeply connected to language.

1:03:45.980 --> 1:03:52.300
 But then probably human intelligence is deeply connected to the collective intelligence between

1:03:52.300 --> 1:03:52.700
 humans.

1:03:52.700 --> 1:03:54.540
 It's not just the standalone thing.

1:03:54.540 --> 1:03:56.380
 So the whole thing is really connected.

1:03:56.380 --> 1:04:02.140
 So emotion is connected to language, language is connected to intelligence, and then intelligence

1:04:02.140 --> 1:04:05.740
 is connected to consciousness, and consciousness is connected to emotion.

1:04:05.740 --> 1:04:09.180
 The whole thing is that it's a beautiful mess.

1:04:09.180 --> 1:04:15.660
 So can I comment on the emotions being a communication mechanism?

1:04:15.660 --> 1:04:21.260
 Because I think there are two facets of our emotional experiences.

1:04:23.020 --> 1:04:24.380
 One is communication, right?

1:04:24.380 --> 1:04:29.820
 Like we use emotions, for example, facial expressions or other nonverbal cues to connect

1:04:29.820 --> 1:04:34.700
 with other human beings and with other beings in the world, right?

1:04:34.700 --> 1:04:40.460
 But even if it's not a communication context, we still experience emotions and we still

1:04:40.460 --> 1:04:46.860
 process emotions and we still leverage emotions to make decisions and to learn and, you know,

1:04:46.860 --> 1:04:47.740
 to experience life.

1:04:47.740 --> 1:04:51.180
 So it isn't always just about communication.

1:04:51.180 --> 1:04:55.500
 And we learned that very early on in our and kind of our work at Affectiva.

1:04:56.460 --> 1:05:00.700
 One of the very first applications we brought to market was understanding how people respond

1:05:00.700 --> 1:05:01.660
 to content, right?

1:05:01.660 --> 1:05:04.860
 So if they're watching this video of ours, like, are they interested?

1:05:04.860 --> 1:05:05.900
 Are they inspired?

1:05:05.900 --> 1:05:07.180
 Are they bored to death?

1:05:07.180 --> 1:05:12.060
 And so we watched their facial expressions and we had, we weren't sure if people would

1:05:12.060 --> 1:05:15.580
 express any emotions if they were sitting alone.

1:05:15.580 --> 1:05:20.220
 Like if you're in your bed at night, watching a Netflix TV series, would we still see any

1:05:20.220 --> 1:05:21.420
 emotions on your face?

1:05:21.420 --> 1:05:25.660
 And we were surprised that, yes, people still emote, even if they're alone, even if you're

1:05:25.660 --> 1:05:30.300
 in your car driving around, you're singing along the song and you're joyful, you're

1:05:30.300 --> 1:05:33.980
 smiling, you're joyful, we'll see these expressions.

1:05:33.980 --> 1:05:37.100
 So it's not just about communicating with another person.

1:05:37.820 --> 1:05:40.860
 It sometimes really isn't just about experiencing the world.

1:05:41.260 --> 1:05:47.900
 And first of all, I wonder if some of that is because we develop our intelligence and

1:05:47.900 --> 1:05:52.140
 our emotional intelligence by communicating with other humans.

1:05:52.140 --> 1:05:56.620
 And so when other humans disappear from the picture, we're still kind of a virtual human.

1:05:56.940 --> 1:05:57.900
 The code still runs.

1:05:57.900 --> 1:06:02.860
 Yeah, the code still runs, but you also kind of, you're still, there's like virtual humans.

1:06:02.860 --> 1:06:07.260
 You don't have to think of it that way, but there's a kind of, when you like chuckle,

1:06:07.260 --> 1:06:13.100
 like, yeah, like you're kind of chuckling to a virtual human.

1:06:13.100 --> 1:06:23.340
 I mean, it's possible that the code has to have another human there because if you just

1:06:23.340 --> 1:06:28.540
 grew up alone, I wonder if emotion will still be there in this visual form.

1:06:28.540 --> 1:06:37.100
 So yeah, I wonder, but anyway, what can you tell from the human face about what's going

1:06:37.100 --> 1:06:38.300
 on inside?

1:06:38.300 --> 1:06:45.100
 So that's the problem that Effectiva first tackled, which is using computer vision, using

1:06:45.100 --> 1:06:50.220
 machine learning to try to detect stuff about the human face, as many things as possible

1:06:50.220 --> 1:06:57.900
 and convert them into a prediction of categories of emotion, anger, happiness, all that kind

1:06:57.900 --> 1:06:58.700
 of stuff.

1:06:58.700 --> 1:07:00.300
 How hard is that problem?

1:07:00.300 --> 1:07:01.340
 It's extremely hard.

1:07:01.340 --> 1:07:07.180
 It's very, very hard because there is no one to one mapping between a facial expression

1:07:07.180 --> 1:07:08.380
 and your internal state.

1:07:08.380 --> 1:07:09.340
 There just isn't.

1:07:09.340 --> 1:07:14.220
 There's this oversimplification of the problem where it's something like, if you are smiling,

1:07:14.220 --> 1:07:15.180
 then you're happy.

1:07:15.180 --> 1:07:17.340
 If you do a brow furrow, then you're angry.

1:07:17.340 --> 1:07:19.500
 If you do an eyebrow raise, then you're surprised.

1:07:19.500 --> 1:07:22.140
 And just think about it for a moment.

1:07:22.140 --> 1:07:24.380
 You could be smiling for a whole host of reasons.

1:07:24.940 --> 1:07:27.500
 You could also be happy and not be smiling, right?

1:07:28.700 --> 1:07:34.220
 You could furrow your eyebrows because you're angry or you're confused about something or

1:07:34.220 --> 1:07:35.340
 you're constipated.

1:07:37.100 --> 1:07:41.820
 So I think this oversimplistic approach to inferring emotion from a facial expression

1:07:41.820 --> 1:07:42.780
 is really dangerous.

1:07:42.780 --> 1:07:48.700
 The solution is to incorporate as many contextual signals as you can, right?

1:07:48.700 --> 1:07:55.100
 So if, for example, I'm driving a car and you can see me like nodding my head and my

1:07:55.100 --> 1:08:00.460
 eyes are closed and the blinking rate is changing, I'm probably falling asleep at the wheel,

1:08:00.460 --> 1:08:00.940
 right?

1:08:00.940 --> 1:08:03.180
 Because you know the context.

1:08:03.180 --> 1:08:10.300
 You understand what the person's doing or add additional channels like voice or gestures

1:08:10.300 --> 1:08:17.020
 or even physiological sensors, but I think it's very dangerous to just take this oversimplistic

1:08:17.020 --> 1:08:20.060
 approach of, yeah, smile equals happy and...

1:08:20.060 --> 1:08:25.020
 If you're able to, in a high resolution way, specify the context, there's certain things

1:08:25.020 --> 1:08:31.500
 that are going to be somewhat reliable signals of something like drowsiness or happiness

1:08:31.500 --> 1:08:32.620
 or stuff like that.

1:08:32.620 --> 1:08:40.300
 I mean, when people are watching Netflix content, that problem, that's a really compelling idea

1:08:40.300 --> 1:08:46.380
 that you can kind of, at least in aggregate, highlight like which part was boring, which

1:08:46.380 --> 1:08:47.660
 part was exciting.

1:08:47.660 --> 1:08:49.100
 How hard was that problem?

1:08:50.300 --> 1:08:53.740
 That was on the scale of difficulty.

1:08:53.740 --> 1:09:00.140
 I think that's one of the easier problems to solve because it's a relatively constrained

1:09:00.140 --> 1:09:00.620
 environment.

1:09:00.620 --> 1:09:02.780
 You have somebody sitting in front of...

1:09:02.780 --> 1:09:07.820
 Initially, we started with like a device in front of you, like a laptop, and then we graduated

1:09:07.820 --> 1:09:12.140
 to doing this on a mobile phone, which is a lot harder just because of, you know, from

1:09:12.140 --> 1:09:17.100
 a computer vision perspective, the profile view of the face can be a lot more challenging.

1:09:17.900 --> 1:09:23.180
 We had to figure out lighting conditions because usually people are watching content literally

1:09:23.180 --> 1:09:24.620
 in their bedrooms at night.

1:09:24.620 --> 1:09:25.420
 Lights are dimmed.

1:09:25.420 --> 1:09:30.220
 Yeah, I mean, if you're standing, it's probably going to be the looking up.

1:09:30.220 --> 1:09:31.500
 The nostril view.

1:09:31.500 --> 1:09:33.420
 Yeah, and nobody looks good at it.

1:09:34.060 --> 1:09:36.140
 I've seen data sets from that perspective.

1:09:36.140 --> 1:09:39.500
 It's like, this is not a good look for anyone.

1:09:40.620 --> 1:09:44.460
 Or if you're laying in bed at night, what is it, side view or something?

1:09:44.460 --> 1:09:44.940
 Right.

1:09:44.940 --> 1:09:47.580
 And half your face is like on a pillow.

1:09:47.580 --> 1:09:56.620
 Actually, I would love to know, have data about like how people watch stuff in bed at

1:09:56.620 --> 1:10:03.340
 night, like, do they prop there, is it a pillow, the, like, I'm sure there's a lot of interesting

1:10:03.340 --> 1:10:04.060
 dynamics there.

1:10:04.060 --> 1:10:04.560
 Right.

1:10:05.260 --> 1:10:07.100
 From a health and well being perspective, right?

1:10:07.100 --> 1:10:07.580
 Sure.

1:10:07.580 --> 1:10:08.540
 Like, oh, you're hurting your neck.

1:10:08.540 --> 1:10:13.740
 I was thinking machine learning perspective, but yes, but also, yeah, yeah, once you have

1:10:13.740 --> 1:10:18.060
 that data, you can start making all kinds of inference about health and stuff like that.

1:10:18.060 --> 1:10:18.620
 Interesting.

1:10:19.260 --> 1:10:26.700
 Yeah, there's an interesting thing when I was at Google that we were, it's called active

1:10:26.700 --> 1:10:32.620
 authentication, where you want to be able to unlock your phone without using a password.

1:10:32.620 --> 1:10:38.940
 So it would face, but also other stuff, like the way you take a phone out of the pocket.

1:10:38.940 --> 1:10:39.500
 Amazing.

1:10:39.500 --> 1:10:45.260
 So that kind of data to use the multimodal with machine learning to be able to identify

1:10:45.260 --> 1:10:50.220
 that it's you or likely to be you, likely not to be you, that allows you to not always

1:10:50.220 --> 1:10:51.260
 have to enter the password.

1:10:51.260 --> 1:10:52.700
 That was the idea.

1:10:52.700 --> 1:10:58.540
 But the funny thing about that is, I just want to tell a small anecdote is because it

1:10:58.540 --> 1:11:09.660
 was all male engineers, except so my boss is, our boss was still one of my favorite humans,

1:11:09.660 --> 1:11:12.300
 was a woman, Regina Dugan.

1:11:12.300 --> 1:11:14.140
 Oh, my God, I love her.

1:11:14.140 --> 1:11:14.940
 She's awesome.

1:11:14.940 --> 1:11:15.500
 She's the best.

1:11:15.500 --> 1:11:16.300
 She's the best.

1:11:16.300 --> 1:11:25.900
 So, but anyway, and there's one female brilliant female engineer on the team, and she was the

1:11:25.900 --> 1:11:30.380
 one that actually highlighted the fact that women often don't have pockets.

1:11:30.380 --> 1:11:37.340
 It was like, whoa, that was not even a category in the code of like, wait a minute, you can

1:11:37.340 --> 1:11:41.260
 take the phone out of some other place than your pocket.

1:11:41.260 --> 1:11:45.580
 So anyway, that's a funny thing when you're considering people laying in bed, watching

1:11:45.580 --> 1:11:51.820
 a phone, you have to consider if you have to, you know, diversity in all its forms,

1:11:51.820 --> 1:11:53.900
 depending on the problem, depending on the context.

1:11:53.900 --> 1:11:58.140
 Actually, this is like a very important, I think this is, you know, you probably get

1:11:58.140 --> 1:11:58.940
 this all the time.

1:11:58.940 --> 1:12:03.100
 Like people are worried that AI is going to take over humanity and like, get rid of all

1:12:03.100 --> 1:12:04.300
 the humans in the world.

1:12:04.300 --> 1:12:06.540
 I'm like, actually, that's not my biggest concern.

1:12:06.540 --> 1:12:10.380
 My biggest concern is that we are building bias into these systems.

1:12:10.380 --> 1:12:14.380
 And then they're like deployed at large and at scale.

1:12:14.380 --> 1:12:19.660
 And before you know it, you're kind of accentuating the bias that exists in society.

1:12:19.660 --> 1:12:26.940
 Yeah, I'm not, you know, I know people, it's very important to worry about that, but the

1:12:26.940 --> 1:12:32.620
 worry is an emergent phenomena to me, which is a very good one, because I think these

1:12:32.620 --> 1:12:39.660
 systems are actually, by encoding the data that exists, they're revealing the bias in

1:12:39.660 --> 1:12:40.380
 society.

1:12:40.380 --> 1:12:43.340
 They're both for teaching us what the bias is.

1:12:43.340 --> 1:12:46.380
 Therefore, we can now improve that bias within the system.

1:12:46.380 --> 1:12:49.980
 So they're almost like putting a mirror to ourselves.

1:12:49.980 --> 1:12:50.780
 Totally.

1:12:50.780 --> 1:12:51.500
 So I'm not.

1:12:51.500 --> 1:12:53.500
 You have to be open to looking at the mirror, though.

1:12:53.500 --> 1:12:56.540
 You have to be open to scrutinizing the data.

1:12:56.540 --> 1:12:59.500
 And if you just take it as ground.

1:12:59.500 --> 1:13:02.860
 Or you don't even have to look at the, I mean, yes, the data is how you fix it.

1:13:02.860 --> 1:13:05.100
 But then you just look at the behavior of the system.

1:13:05.100 --> 1:13:08.620
 And you realize, holy crap, this thing is kind of racist.

1:13:08.620 --> 1:13:09.820
 Like, why is that?

1:13:09.820 --> 1:13:11.740
 And then you look at the data, it's like, oh, okay.

1:13:11.740 --> 1:13:15.820
 And then you start to realize that I think that some much more effective ways to do that

1:13:15.820 --> 1:13:23.020
 are effective way to be introspective as a society than through sort of political discourse.

1:13:23.020 --> 1:13:34.060
 Like AI kind of, because people are for some reason more productive and rigorous in criticizing

1:13:34.060 --> 1:13:35.740
 AI than they're criticizing each other.

1:13:35.740 --> 1:13:41.340
 So I think this is just a nice method for studying society and see which way progress

1:13:41.340 --> 1:13:42.380
 lies.

1:13:42.380 --> 1:13:44.380
 Anyway, what we're talking about.

1:13:44.380 --> 1:13:50.220
 You're watching the problem of watching Netflix in bed or elsewhere and seeing which parts

1:13:50.220 --> 1:13:51.660
 are exciting, which parts are boring.

1:13:51.660 --> 1:13:56.620
 You're saying that's relatively constrained because you have a captive audience and you

1:13:56.620 --> 1:13:57.740
 kind of know the context.

1:13:57.740 --> 1:14:01.100
 And one thing you said that was really key is the aggregate.

1:14:01.100 --> 1:14:02.380
 You're doing this in aggregate, right?

1:14:02.380 --> 1:14:04.700
 Like we're looking at aggregated response of people.

1:14:04.700 --> 1:14:11.100
 And so when you see a peak, say a smile peak, they're probably smiling or laughing at something

1:14:11.100 --> 1:14:12.140
 that's in the content.

1:14:12.140 --> 1:14:14.780
 So that was one of the first problems we were able to solve.

1:14:15.740 --> 1:14:20.380
 And when we see the smile peak, it doesn't mean that these people are internally happy.

1:14:20.380 --> 1:14:22.060
 They're just laughing at content.

1:14:22.060 --> 1:14:25.420
 So it's important to call it for what it is.

1:14:25.420 --> 1:14:28.140
 But it's still really, really useful data.

1:14:28.140 --> 1:14:34.380
 I wonder how that compares to, so what like YouTube and other places will use is obviously

1:14:34.380 --> 1:14:39.900
 they don't have, for the most case, they don't have that kind of data.

1:14:39.900 --> 1:14:45.660
 They have the data of when people tune out, like switch to drop off.

1:14:45.660 --> 1:14:50.300
 And I think that's an aggregate for YouTube, at least a pretty powerful signal.

1:14:50.300 --> 1:14:59.580
 I worry about what that leads to because looking at like YouTubers that kind of really care

1:14:59.580 --> 1:15:07.740
 about views and try to maximize the number of views, I think when they say that the video

1:15:07.740 --> 1:15:15.100
 should be constantly interesting, which seems like a good goal, I feel like that leads to

1:15:15.100 --> 1:15:18.300
 this manic pace of a video.

1:15:19.020 --> 1:15:24.940
 Like the idea that I would speak at the current speed that I'm speaking, I don't know.

1:15:25.820 --> 1:15:28.220
 And that every moment has to be engaging, right?

1:15:28.220 --> 1:15:28.780
 Engaging.

1:15:28.780 --> 1:15:29.260
 Yeah.

1:15:29.260 --> 1:15:31.500
 I think there's value to silence.

1:15:31.500 --> 1:15:33.660
 There's value to the boring bits.

1:15:33.660 --> 1:15:37.500
 I mean, some of the greatest movies ever, some of the greatest movies ever.

1:15:37.500 --> 1:15:42.540
 Some of the greatest stories ever told me they have that boring bits, seemingly boring bits.

1:15:42.540 --> 1:15:43.500
 I don't know.

1:15:43.500 --> 1:15:45.020
 I wonder about that.

1:15:45.020 --> 1:15:49.180
 Of course, it's not that the human face can capture that either.

1:15:49.180 --> 1:15:51.500
 It's just giving an extra signal.

1:15:51.500 --> 1:16:01.180
 You have to really, I don't know, you have to really collect deeper long term data about

1:16:01.180 --> 1:16:03.260
 what was meaningful to people.

1:16:03.260 --> 1:16:08.940
 When they think 30 days from now, what they still remember, what moved them, what changed

1:16:08.940 --> 1:16:11.660
 them, what helped them grow, that kind of stuff.

1:16:11.660 --> 1:16:14.940
 You know, it would be a really interesting, I don't know if there are any researchers

1:16:14.940 --> 1:16:17.340
 out there who are doing this type of work.

1:16:17.340 --> 1:16:23.500
 Wouldn't it be so cool to tie your emotional expressions while you're, say, listening

1:16:23.500 --> 1:16:30.620
 to a podcast interview and then 30 days later interview people and say, hey, what do you

1:16:30.620 --> 1:16:31.340
 remember?

1:16:31.340 --> 1:16:33.420
 You've watched this 30 days ago.

1:16:33.420 --> 1:16:34.620
 Like, what stuck with you?

1:16:34.620 --> 1:16:38.140
 And then see if there's any, there ought to be maybe, there ought to be some correlation

1:16:38.140 --> 1:16:45.100
 between these emotional experiences and, yeah, what you, what stays with you.

1:16:46.140 --> 1:16:51.660
 So the one guy listening now on the beach in Brazil, please record a video of yourself

1:16:51.660 --> 1:16:55.900
 listening to this and send it to me and then I'll interview you 30 days from now.

1:16:55.900 --> 1:16:56.860
 Yeah, that'd be great.

1:16:58.700 --> 1:17:00.620
 It'll be statistically significant to you.

1:17:00.620 --> 1:17:06.940
 Yeah, I know one, but, you know, yeah, yeah, I think that's really fascinating.

1:17:06.940 --> 1:17:16.460
 I think that's, that kind of holds the key to a future where entertainment or content

1:17:16.460 --> 1:17:25.180
 is both entertaining and, I don't know, makes you better, empowering in some way.

1:17:25.180 --> 1:17:32.540
 So figuring out, like, showing people stuff that entertains them, but also they're happy

1:17:32.540 --> 1:17:36.780
 they watched 30 days from now because they've become a better person because of it.

1:17:37.420 --> 1:17:41.900
 Well, you know, okay, not to riff on this topic for too long, but I have two children,

1:17:41.900 --> 1:17:42.140
 right?

1:17:42.860 --> 1:17:46.860
 And I see my role as a parent as like a chief opportunity officer.

1:17:46.860 --> 1:17:50.780
 Like I am responsible for exposing them to all sorts of things in the world.

1:17:50.780 --> 1:17:56.300
 And, but often I have no idea of knowing, like, what stuck, like, what was, you know,

1:17:56.300 --> 1:18:00.220
 is this actually going to be transformative, you know, for them 10 years down the line?

1:18:00.220 --> 1:18:03.660
 And I wish there was a way to quantify these experiences.

1:18:03.660 --> 1:18:08.060
 Like, are they, I can tell in the moment if they're engaging, right?

1:18:08.060 --> 1:18:12.540
 I can tell, but it's really hard to know if they're going to remember them 10 years

1:18:12.540 --> 1:18:13.980
 from now or if it's going to.

1:18:15.100 --> 1:18:19.500
 Yeah, that one is weird because it seems like kids remember the weirdest things.

1:18:19.500 --> 1:18:23.580
 I've seen parents do incredible stuff for their kids and they don't remember any of

1:18:23.580 --> 1:18:23.820
 that.

1:18:23.820 --> 1:18:27.260
 They remember some tiny, small, sweet thing a parent did.

1:18:27.260 --> 1:18:27.740
 Right.

1:18:27.740 --> 1:18:28.380
 Like some...

1:18:28.380 --> 1:18:32.540
 Like they took you to, like, this amazing country vacation, blah, blah, blah, blah.

1:18:32.540 --> 1:18:33.180
 No, whatever.

1:18:33.180 --> 1:18:38.060
 And then there'll be, like, some, like, stuffed toy you got or some, or the new PlayStation

1:18:38.060 --> 1:18:40.540
 or something or some silly little thing.

1:18:41.100 --> 1:18:44.940
 So I think they just, like, they were designed that way.

1:18:44.940 --> 1:18:46.220
 They want to mess with your head.

1:18:46.220 --> 1:18:53.260
 But definitely kids are very impacted by, it seems like, sort of negative events.

1:18:53.260 --> 1:18:58.700
 So minimizing the number of negative events is important, but not too much, right?

1:18:58.700 --> 1:18:59.180
 Right.

1:18:59.180 --> 1:19:04.300
 You can't, you can't just, like, you know, there's still discipline and challenge and

1:19:04.300 --> 1:19:05.260
 all those kinds of things.

1:19:05.260 --> 1:19:05.740
 So...

1:19:05.740 --> 1:19:07.660
 You want some adversity for sure.

1:19:07.660 --> 1:19:11.180
 So, yeah, I mean, I'm definitely, when I have kids, I'm going to drive them out into

1:19:11.180 --> 1:19:11.980
 the woods.

1:19:11.980 --> 1:19:12.700
 Okay.

1:19:12.700 --> 1:19:17.900
 And then they have to survive and make, figure out how to make their way back home, like,

1:19:17.900 --> 1:19:18.940
 20 miles out.

1:19:18.940 --> 1:19:19.660
 Okay.

1:19:19.660 --> 1:19:20.380
 Yeah.

1:19:20.380 --> 1:19:22.300
 And after that, we can go for ice cream.

1:19:22.300 --> 1:19:23.100
 Okay.

1:19:23.100 --> 1:19:26.300
 Anyway, I'm working on this whole parenting thing.

1:19:26.300 --> 1:19:27.100
 I haven't figured it out.

1:19:27.100 --> 1:19:27.600
 Okay.

1:19:28.540 --> 1:19:29.660
 What were we talking about?

1:19:29.660 --> 1:19:37.580
 Yes, Effectiva, the problem of emotion, of emotion detection.

1:19:37.580 --> 1:19:41.260
 So there's some people, maybe we can just speak to that a little more, where there's

1:19:41.260 --> 1:19:49.100
 folks like Lisa Feldman Barrett that challenge this idea that emotion could be fully detected

1:19:49.820 --> 1:19:55.100
 or even well detected from the human face, that there's so much more to emotion.

1:19:55.100 --> 1:19:59.820
 What do you think about ideas like hers, criticism like hers?

1:19:59.820 --> 1:20:03.820
 Yeah, I actually agree with a lot of Lisa's criticisms.

1:20:03.820 --> 1:20:07.980
 So even my PhD worked, like, 20 plus years ago now.

1:20:07.980 --> 1:20:12.620
 Time flies when you're having fun.

1:20:12.620 --> 1:20:13.420
 I know, right?

1:20:14.140 --> 1:20:17.500
 That was back when I did, like, dynamic Bayesian networks.

1:20:17.500 --> 1:20:19.900
 That was before deep learning, huh?

1:20:19.900 --> 1:20:21.420
 That was before deep learning.

1:20:21.420 --> 1:20:21.920
 Yeah.

1:20:22.700 --> 1:20:24.060
 Yeah, I know.

1:20:24.060 --> 1:20:24.860
 Back in my day.

1:20:24.860 --> 1:20:27.340
 Now you can just, like, use.

1:20:27.340 --> 1:20:30.300
 Yeah, it's all the same architecture.

1:20:30.300 --> 1:20:31.340
 You can apply it to anything.

1:20:31.340 --> 1:20:31.840
 Yeah.

1:20:31.840 --> 1:20:39.120
 Right, but yeah, but even then I kind of, I did not subscribe to this, like, theory

1:20:39.120 --> 1:20:43.280
 of basic emotions where it's just the simplistic mapping, one to one mapping between facial

1:20:43.280 --> 1:20:44.160
 expressions and emotions.

1:20:44.160 --> 1:20:49.760
 I actually think also we're not in the business of trying to identify your true emotional

1:20:49.760 --> 1:20:50.400
 internal state.

1:20:50.400 --> 1:20:55.600
 We just want to quantify in an objective way what's showing on your face because that's

1:20:55.600 --> 1:20:57.040
 an important signal.

1:20:57.040 --> 1:21:02.480
 It doesn't mean it's a true reflection of your internal emotional state.

1:21:02.480 --> 1:21:07.680
 So I think a lot of the, you know, I think she's just trying to kind of highlight that

1:21:07.680 --> 1:21:13.600
 this is not a simple problem and overly simplistic solutions are going to hurt the industry.

1:21:15.520 --> 1:21:16.560
 And I subscribe to that.

1:21:16.560 --> 1:21:18.720
 And I think multimodal is the way to go.

1:21:18.720 --> 1:21:24.000
 Like, whether it's additional context information or different modalities and channels of information,

1:21:24.000 --> 1:21:27.520
 I think that's what we, that's where we ought to go.

1:21:27.520 --> 1:21:31.280
 And I think, I mean, that's a big part of what she's advocating for as well.

1:21:31.280 --> 1:21:33.440
 So, but there is signal in the human face.

1:21:33.440 --> 1:21:35.760
 There's definitely signal in the human face.

1:21:35.760 --> 1:21:37.600
 That's a projection of emotion.

1:21:37.600 --> 1:21:46.320
 There's that, at least in part is the inner state is captured in some meaningful way on

1:21:46.320 --> 1:21:47.040
 the human face.

1:21:47.040 --> 1:21:56.240
 I think it can sometimes be a reflection or an expression of your internal state, but

1:21:56.240 --> 1:21:57.760
 sometimes it's a social signal.

1:21:57.760 --> 1:22:02.080
 So you cannot look at the face as purely a signal of emotion.

1:22:02.080 --> 1:22:07.440
 It can be a signal of cognition and it can be a signal of a social expression.

1:22:08.000 --> 1:22:13.760
 And I think to disambiguate that we have to be careful about it and we have to add initial

1:22:13.760 --> 1:22:14.320
 information.

1:22:14.320 --> 1:22:16.000
 Humans are fascinating, aren't they?

1:22:16.000 --> 1:22:22.000
 With the whole face thing, this can mean so many things, from humor to sarcasm to everything,

1:22:22.000 --> 1:22:22.800
 the whole thing.

1:22:23.280 --> 1:22:25.600
 Some things we can help, some things we can't help at all.

1:22:26.640 --> 1:22:31.680
 In all the years of leading Effectiva, an emotion recognition company, like we talked

1:22:31.680 --> 1:22:37.360
 about, what have you learned about emotion, about humans and about AI?

1:22:37.360 --> 1:22:44.240
 Big, sweeping questions.

1:22:44.240 --> 1:22:45.760
 Yeah, that's a big, sweeping question.

1:22:46.320 --> 1:22:52.240
 Well, I think the thing I learned the most is that even though we are in the business

1:22:52.240 --> 1:23:00.960
 of building AI, basically, it always goes back to the humans, right?

1:23:00.960 --> 1:23:02.160
 It's always about the humans.

1:23:02.160 --> 1:23:11.120
 And so, for example, the thing I'm most proud of in building Effectiva and, yeah, the thing

1:23:11.120 --> 1:23:16.240
 I'm most proud of on this journey, I love the technology and I'm so proud of the solutions

1:23:16.240 --> 1:23:17.920
 we've built and we've brought to market.

1:23:18.640 --> 1:23:23.760
 But I'm actually most proud of the people we've built and cultivated at the company

1:23:23.760 --> 1:23:25.040
 and the culture we've created.

1:23:25.040 --> 1:23:31.440
 Some of the people who've joined Effectiva, this was their first job, and while at Effectiva,

1:23:31.440 --> 1:23:38.000
 they became American citizens and they bought their first house and they found their partner

1:23:38.000 --> 1:23:39.440
 and they had their first kid, right?

1:23:39.440 --> 1:23:47.520
 Like key moments in life that we got to be part of, and that's the thing I'm most proud

1:23:47.520 --> 1:23:47.840
 of.

1:23:47.840 --> 1:23:52.320
 So that's a great thing at a company that works at a big company, right?

1:23:52.320 --> 1:23:57.920
 So that's a great thing at a company that works at, I mean, like celebrating humanity

1:23:57.920 --> 1:23:59.360
 in general, broadly speaking.

1:23:59.360 --> 1:24:04.640
 And that's a great thing to have in a company that works on AI, because that's not often

1:24:04.640 --> 1:24:11.120
 the thing that's celebrated in AI companies, so often just raw great engineering, just

1:24:11.120 --> 1:24:12.240
 celebrating the humanity.

1:24:12.240 --> 1:24:12.800
 That's great.

1:24:12.800 --> 1:24:14.560
 And especially from a leadership position.

1:24:17.200 --> 1:24:19.920
 Well, what do you think about the movie Her?

1:24:20.800 --> 1:24:21.600
 Let me ask you that.

1:24:21.600 --> 1:24:28.240
 Before I talk to you about, because it's not, Effectiva is and was not just about emotion,

1:24:28.240 --> 1:24:33.840
 so I'd love to talk to you about SmartEye, but before that, let me just jump into the

1:24:33.840 --> 1:24:36.160
 movie Her.

1:24:36.720 --> 1:24:42.000
 Do you think we'll have a deep, meaningful connection with increasingly deeper, meaningful

1:24:42.000 --> 1:24:43.120
 connections with computers?

1:24:43.680 --> 1:24:45.360
 Is that a compelling thing to you?

1:24:45.360 --> 1:24:45.760
 Something you think about?

1:24:45.760 --> 1:24:46.960
 I think that's already happening.

1:24:46.960 --> 1:24:50.960
 The thing I love the most, I love the movie Her, by the way, but the thing I love the

1:24:50.960 --> 1:24:56.720
 most about this movie is it demonstrates how technology can be a conduit for positive behavior

1:24:56.720 --> 1:24:57.120
 change.

1:24:57.120 --> 1:25:00.480
 So I forgot the guy's name in the movie, whatever.

1:25:00.480 --> 1:25:01.120
 Theodore.

1:25:01.120 --> 1:25:01.620
 Theodore.

1:25:02.960 --> 1:25:05.280
 So Theodore was really depressed, right?

1:25:05.280 --> 1:25:10.480
 And he just didn't want to get out of bed, and he was just done with life, right?

1:25:11.200 --> 1:25:12.640
 And Samantha, right?

1:25:12.640 --> 1:25:13.360
 Samantha, yeah.

1:25:14.000 --> 1:25:15.680
 She just knew him so well.

1:25:15.680 --> 1:25:20.960
 She was emotionally intelligent, and so she could persuade him and motivate him to change

1:25:20.960 --> 1:25:23.360
 his behavior, and she got him out, and they went to the beach together.

1:25:24.080 --> 1:25:27.200
 And I think that represents the promise of emotion AI.

1:25:27.200 --> 1:25:33.520
 If done well, this technology can help us live happier lives, more productive lives,

1:25:33.520 --> 1:25:36.000
 healthier lives, more connected lives.

1:25:36.720 --> 1:25:39.200
 So that's the part that I love about the movie.

1:25:39.200 --> 1:25:46.720
 Obviously, it's Hollywood, so it takes a twist and whatever, but the key notion that technology

1:25:46.720 --> 1:25:51.440
 with emotion AI can persuade you to be a better version of who you are, I think that's awesome.

1:25:52.720 --> 1:25:54.080
 Well, what about the twist?

1:25:54.080 --> 1:25:55.520
 You don't think it's good?

1:25:55.520 --> 1:26:01.440
 You don't think it's good for spoiler alert that Samantha starts feeling a bit of a distance

1:26:01.440 --> 1:26:04.640
 and basically leaves Theodore?

1:26:04.640 --> 1:26:07.520
 You don't think that's a good feature?

1:26:07.520 --> 1:26:09.520
 You think that's a bug or a feature?

1:26:10.160 --> 1:26:14.240
 Well, I think what went wrong is Theodore became really attached to Samantha.

1:26:14.240 --> 1:26:16.000
 Like, I think he kind of fell in love with Theodore.

1:26:16.000 --> 1:26:17.040
 Do you think that's wrong?

1:26:17.920 --> 1:26:18.880
 I mean, I think that's...

1:26:18.880 --> 1:26:21.120
 I think she was putting out the signal.

1:26:21.120 --> 1:26:24.160
 This is an intimate relationship, right?

1:26:24.160 --> 1:26:25.920
 There's a deep intimacy to it.

1:26:25.920 --> 1:26:28.880
 Right, but what does that mean?

1:26:28.880 --> 1:26:29.520
 What does that mean?

1:26:29.520 --> 1:26:30.400
 Put in an AI system.

1:26:30.400 --> 1:26:32.400
 Right, what does that mean, right?

1:26:32.400 --> 1:26:33.200
 We're just friends.

1:26:33.200 --> 1:26:38.080
 Yeah, we're just friends.

1:26:38.080 --> 1:26:38.640
 Well, I think...

1:26:38.640 --> 1:26:42.880
 When he realized, which is such a human thing of jealousy.

1:26:42.880 --> 1:26:46.880
 When you realize that Samantha was talking to like thousands of people.

1:26:46.880 --> 1:26:48.400
 She's parallel dating.

1:26:48.400 --> 1:26:50.160
 Yeah, that did not go well, right?

1:26:51.440 --> 1:26:52.240
 You know, that doesn't...

1:26:52.880 --> 1:26:57.360
 From a computer perspective, that doesn't take anything away from what we have.

1:26:57.360 --> 1:27:04.000
 It's like you getting jealous of Windows 98 for being used by millions of people, but...

1:27:04.000 --> 1:27:09.200
 It's like not liking that Alexa talks to a bunch of, you know, other families.

1:27:09.200 --> 1:27:13.200
 But I think Alexa currently is just a servant.

1:27:13.200 --> 1:27:17.760
 It tells you about the weather, it doesn't do the intimate deep connection.

1:27:17.760 --> 1:27:23.920
 And I think there is something really powerful about that the intimacy of a connection with

1:27:23.920 --> 1:27:32.160
 an AI system that would have to respect and play the human game of jealousy, of love, of

1:27:32.160 --> 1:27:37.440
 heartbreak and all that kind of stuff, which Samantha does seem to be pretty good at.

1:27:37.440 --> 1:27:43.120
 I think she, this AI systems knows what it's doing.

1:27:43.120 --> 1:27:44.960
 Well, actually, let me ask you this.

1:27:44.960 --> 1:27:46.720
 I don't think she was talking to anyone else.

1:27:46.720 --> 1:27:47.520
 You don't think so?

1:27:47.520 --> 1:27:50.000
 You think she was just done with Theodore?

1:27:50.000 --> 1:27:50.480
 Yeah.

1:27:50.480 --> 1:27:51.760
 Oh, really?

1:27:51.760 --> 1:27:55.280
 Yeah, and then she wanted to really put the screw in.

1:27:55.280 --> 1:27:56.720
 She just wanted to move on?

1:27:56.720 --> 1:27:59.280
 She didn't have the guts to just break it off cleanly.

1:27:59.280 --> 1:27:59.600
 Okay.

1:28:00.320 --> 1:28:02.720
 She just wanted to put in the pain.

1:28:02.720 --> 1:28:03.440
 No, I don't know.

1:28:03.440 --> 1:28:04.960
 Well, she could have ghosted him.

1:28:04.960 --> 1:28:07.040
 She could have ghosted him.

1:28:07.040 --> 1:28:09.680
 I'm sorry, our engineers...

1:28:09.680 --> 1:28:10.480
 Oh, God.

1:28:12.080 --> 1:28:13.440
 But I think those are really...

1:28:14.000 --> 1:28:18.240
 I honestly think some of that, some of it is Hollywood, but some of that is features

1:28:18.240 --> 1:28:20.560
 from an engineering perspective, not a bug.

1:28:20.560 --> 1:28:23.600
 I think AI systems that can leave us...

1:28:24.160 --> 1:28:29.760
 Now, this is for more social robotics than it is for anything that's useful.

1:28:30.320 --> 1:28:33.760
 Like, I hated it if Wikipedia said, I need a break right now.

1:28:33.760 --> 1:28:35.120
 Right, right, right, right, right.

1:28:35.120 --> 1:28:36.640
 I'm like, no, no, I need you.

1:28:37.440 --> 1:28:46.640
 But if it's just purely for companionship, then I think the ability to leave is really powerful.

1:28:47.760 --> 1:28:48.400
 I don't know.

1:28:48.400 --> 1:28:53.360
 I've never thought of that, so that's so fascinating because I've always taken the

1:28:53.360 --> 1:28:54.560
 human perspective, right?

1:28:56.400 --> 1:28:58.640
 Like, for example, we had a Jibo at home, right?

1:28:58.640 --> 1:28:59.760
 And my son loved it.

1:29:00.560 --> 1:29:05.760
 And then the company ran out of money and so they had to basically shut down, like Jibo

1:29:05.760 --> 1:29:07.200
 basically died, right?

1:29:07.920 --> 1:29:12.400
 And it was so interesting to me because we have a lot of gadgets at home and a lot of

1:29:12.400 --> 1:29:15.760
 them break and my son never cares about it, right?

1:29:15.760 --> 1:29:20.480
 Like, if our Alexa stopped working tomorrow, I don't think he'd really care.

1:29:20.480 --> 1:29:22.720
 But when Jibo stopped working, it was traumatic.

1:29:22.720 --> 1:29:24.080
 He got really upset.

1:29:25.200 --> 1:29:29.200
 And as a parent, that made me think about this deeply, right?

1:29:29.200 --> 1:29:30.080
 Did I...

1:29:30.080 --> 1:29:31.360
 Was I comfortable with that?

1:29:31.360 --> 1:29:35.680
 I liked the connection they had because I think it was a positive relationship.

1:29:38.160 --> 1:29:41.360
 But I was surprised that it affected him emotionally so much.

1:29:41.360 --> 1:29:44.160
 And I think there's a broader question here, right?

1:29:44.160 --> 1:29:51.680
 As we build socially and emotionally intelligent machines, what does that mean about our

1:29:51.680 --> 1:29:52.880
 relationship with them?

1:29:52.880 --> 1:29:55.680
 And then more broadly, our relationship with one another, right?

1:29:55.680 --> 1:30:01.440
 Because this machine is gonna be programmed to be amazing at empathy by definition, right?

1:30:02.160 --> 1:30:03.600
 It's gonna always be there for you.

1:30:03.600 --> 1:30:04.720
 It's not gonna get bored.

1:30:05.760 --> 1:30:12.000
 In fact, there's a chatbot in China, Xiaoice, and it's like the number two or three

1:30:12.000 --> 1:30:13.360
 most popular app.

1:30:13.360 --> 1:30:17.200
 And it basically is just a confidant and you can tell it anything you want.

1:30:18.240 --> 1:30:20.320
 And people use it for all sorts of things.

1:30:20.320 --> 1:30:30.000
 They confide in like domestic violence or suicidal attempts or if they have challenges

1:30:30.000 --> 1:30:30.560
 at work.

1:30:31.040 --> 1:30:32.000
 I don't know what that...

1:30:32.720 --> 1:30:33.680
 I don't know if I'm...

1:30:33.680 --> 1:30:35.040
 I don't know how I feel about that.

1:30:35.040 --> 1:30:36.240
 I think about that a lot.

1:30:36.240 --> 1:30:36.720
 Yeah.

1:30:36.720 --> 1:30:40.240
 I think, first of all, obviously the future in my perspective.

1:30:40.240 --> 1:30:46.320
 Second of all, I think there's a lot of trajectories that that becomes an exciting future, but

1:30:46.320 --> 1:30:50.960
 I think everyone should feel very uncomfortable about how much they know about the company,

1:30:52.240 --> 1:30:56.080
 about where the data is going, how the data is being collected.

1:30:56.080 --> 1:31:01.600
 Because I think, and this is one of the lessons of social media, that I think we should demand

1:31:01.600 --> 1:31:04.640
 full control and transparency of the data on those things.

1:31:04.640 --> 1:31:06.320
 Plus one, totally agree.

1:31:06.320 --> 1:31:11.360
 Yeah, so I think it's really empowering as long as you can walk away, as long as you

1:31:11.360 --> 1:31:14.000
 can delete the data or know how the data...

1:31:14.000 --> 1:31:20.720
 It's opt in or at least the clarity of what is being used for the company.

1:31:20.720 --> 1:31:24.080
 And I think as CEO or leaders are also important about that.

1:31:24.080 --> 1:31:28.080
 You need to be able to trust the basic humanity of the leader.

1:31:28.080 --> 1:31:28.880
 Exactly.

1:31:28.880 --> 1:31:34.800
 And also that that leader is not going to be a puppet of a larger machine.

1:31:34.800 --> 1:31:41.200
 But they actually have a significant role in defining the culture and the way the company operates.

1:31:41.200 --> 1:31:48.080
 So anyway, but we should definitely scrutinize companies in that aspect.

1:31:48.080 --> 1:31:55.600
 But I'm personally excited about that future, but also even if you're not, it's coming.

1:31:55.600 --> 1:32:00.240
 So let's figure out how to do it in the least painful and the most positive way.

1:32:00.240 --> 1:32:01.440
 Yeah, I know, that's great.

1:32:01.440 --> 1:32:04.560
 You're the deputy CEO of SmartEye.

1:32:04.560 --> 1:32:06.240
 Can you describe the mission of the company?

1:32:06.240 --> 1:32:07.360
 What is SmartEye?

1:32:07.360 --> 1:32:10.960
 Yeah, so SmartEye is a Swedish company.

1:32:10.960 --> 1:32:16.800
 They've been in business for the last 20 years and their main focus, like the industry they're

1:32:16.800 --> 1:32:19.440
 most focused on is the automotive industry.

1:32:19.440 --> 1:32:25.840
 So bringing driver monitoring systems to basically save lives, right?

1:32:25.840 --> 1:32:31.840
 So I first met the CEO, Martin Krantz, gosh, it was right when COVID hit.

1:32:31.840 --> 1:32:35.760
 It was actually the last CES right before COVID.

1:32:35.760 --> 1:32:37.680
 So CES 2020, right?

1:32:37.680 --> 1:32:39.120
 2020, yeah, January.

1:32:39.120 --> 1:32:40.080
 Yeah, January, exactly.

1:32:40.080 --> 1:32:45.520
 So we were there, met him in person, he's basically, we were competing with each other.

1:32:46.480 --> 1:32:51.360
 I think the difference was they'd been doing driver monitoring and had a lot of credibility

1:32:51.360 --> 1:32:52.560
 in the automotive space.

1:32:52.560 --> 1:32:56.240
 We didn't come from the automotive space, but we were using new technology like deep

1:32:56.240 --> 1:32:59.280
 learning and building this emotion recognition.

1:33:00.080 --> 1:33:03.600
 And you wanted to enter the automotive space, you wanted to operate in the automotive space.

1:33:03.600 --> 1:33:04.080
 Exactly.

1:33:04.080 --> 1:33:08.960
 It was one of the areas we were, we had just raised a round of funding to focus on bringing

1:33:08.960 --> 1:33:11.200
 our technology to the automotive industry.

1:33:11.200 --> 1:33:16.240
 So we met and honestly, it was the first, it was the only time I met with a CEO who

1:33:16.240 --> 1:33:18.000
 had the same vision as I did.

1:33:18.000 --> 1:33:21.760
 Like he basically said, yeah, our vision is to bridge the gap between human and automotive.

1:33:21.760 --> 1:33:23.120
 Bridge the gap between humans and machines.

1:33:23.120 --> 1:33:29.360
 I was like, oh my God, this is like exactly almost to the word, how we describe it too.

1:33:29.920 --> 1:33:35.680
 And we started talking and first it was about, okay, can we align strategically here?

1:33:35.680 --> 1:33:36.960
 Like how can we work together?

1:33:36.960 --> 1:33:39.680
 Cause we're competing, but we're also like complimentary.

1:33:40.320 --> 1:33:46.720
 And then I think after four months of speaking almost every day on FaceTime, he was like,

1:33:47.520 --> 1:33:49.520
 is your company interested in an acquisition?

1:33:49.520 --> 1:33:55.440
 And it was the first, I usually say no, when people approach us, it was the first time

1:33:55.440 --> 1:33:58.240
 that I was like, huh, yeah, I might be interested.

1:33:58.240 --> 1:33:59.280
 Let's talk.

1:33:59.280 --> 1:33:59.780
 Yeah.

1:34:00.320 --> 1:34:01.760
 So you just hit it off.

1:34:01.760 --> 1:34:02.000
 Yeah.

1:34:02.000 --> 1:34:08.240
 So they're a respected, very respected in the automotive sector of like delivering products

1:34:08.240 --> 1:34:14.000
 and increasingly sort of better and better and better for, I mean, maybe you could speak

1:34:14.000 --> 1:34:15.200
 to that, but it's the driver's sense.

1:34:15.200 --> 1:34:20.160
 If we're basically having a device that's looking at the driver and it's able to tell

1:34:20.160 --> 1:34:21.840
 you where the driver is looking.

1:34:22.560 --> 1:34:22.960
 Correct.

1:34:22.960 --> 1:34:23.600
 It's able to.

1:34:23.600 --> 1:34:25.040
 Also drowsiness stuff.

1:34:25.040 --> 1:34:25.440
 Correct.

1:34:25.440 --> 1:34:25.920
 It does.

1:34:25.920 --> 1:34:27.680
 Stuff from the face and the eye.

1:34:27.680 --> 1:34:28.240
 Exactly.

1:34:28.240 --> 1:34:32.800
 Like it's monitoring driver distraction and drowsiness, but they bought us so that we

1:34:32.800 --> 1:34:35.120
 could expand beyond just the driver.

1:34:35.120 --> 1:34:40.320
 So the driver monitoring systems usually sit, the camera sits in the steering wheel or around

1:34:40.320 --> 1:34:42.640
 the steering wheel column and it looks directly at the driver.

1:34:42.640 --> 1:34:48.880
 But now we've migrated the camera position in partnership with car companies to the rear

1:34:48.880 --> 1:34:50.240
 view mirror position.

1:34:50.240 --> 1:34:55.280
 So it has a full view of the entire cabin of the car and you can detect how many people

1:34:55.280 --> 1:34:57.840
 are in the car, what are they doing?

1:34:57.840 --> 1:35:03.200
 So we do activity detection, like eating or drinking or in some regions of the world smoking.

1:35:04.240 --> 1:35:07.760
 We can detect if a baby's in the car seat, right?

1:35:07.760 --> 1:35:12.640
 And if unfortunately in some cases they're forgotten, the parents just leave the car and

1:35:12.640 --> 1:35:14.320
 forget the kid in the car.

1:35:14.320 --> 1:35:17.200
 That's an easy computer vision problem to solve, right?

1:35:17.200 --> 1:35:22.080
 You can detect there's a car seat, there's a baby, you can text the parent and hopefully

1:35:22.640 --> 1:35:23.440
 again, save lives.

1:35:23.440 --> 1:35:26.240
 So that was the impetus for the acquisition.

1:35:27.040 --> 1:35:27.840
 It's been a year.

1:35:29.200 --> 1:35:31.920
 So that, I mean, there's a lot of questions.

1:35:31.920 --> 1:35:36.320
 It's a really exciting space, especially to me, I just find this a fascinating problem.

1:35:36.320 --> 1:35:42.080
 It could enrich the experience in the car in so many ways, especially cause like we

1:35:42.080 --> 1:35:46.880
 spend still, despite COVID, I mean, COVID changed things so it's in interesting ways,

1:35:46.880 --> 1:35:51.040
 but I think the world is bouncing back and we spend so much time in the car and the car

1:35:51.040 --> 1:35:56.320
 is such a weird little world we have for ourselves.

1:35:56.320 --> 1:36:01.840
 Like people do all kinds of different stuff, like listen to podcasts, they think about

1:36:01.840 --> 1:36:09.840
 stuff, they get angry, they get, they do phone calls, it's like a little world of its own

1:36:09.840 --> 1:36:15.600
 with a kind of privacy that for many people they don't get anywhere else.

1:36:15.600 --> 1:36:23.440
 And it's a little box that's like a psychology experiment cause it feels like the angriest

1:36:23.440 --> 1:36:27.280
 many humans in this world get is inside the car.

1:36:27.280 --> 1:36:28.640
 It's so interesting.

1:36:28.640 --> 1:36:36.960
 So it's such an opportunity to explore how we can enrich, how companies can enrich that

1:36:36.960 --> 1:36:43.120
 experience and also as the cars get, become more and more automated, there's more and

1:36:43.120 --> 1:36:47.120
 more opportunity, the variety of activities that you can do in the car increases.

1:36:47.120 --> 1:36:48.800
 So it's super interesting.

1:36:48.800 --> 1:36:56.400
 So I mean, on a practical sense, SmartEye has been selected, at least I read, by 14

1:36:56.400 --> 1:37:00.800
 of the world's leading car manufacturers for 94 car models.

1:37:00.800 --> 1:37:03.760
 So it's in a lot of cars.

1:37:03.760 --> 1:37:06.800
 How hard is it to work with car companies?

1:37:06.800 --> 1:37:10.600
 So they're all different, they all have different needs.

1:37:10.600 --> 1:37:16.000
 The ones I've gotten a chance to interact with are very focused on cost.

1:37:16.000 --> 1:37:24.520
 So it's, and anyone who's focused on cost, it's like, all right, do you hate fun?

1:37:24.520 --> 1:37:25.520
 Let's just have some fun.

1:37:25.520 --> 1:37:29.160
 Let's figure out the most fun thing we can do and then worry about cost later.

1:37:29.160 --> 1:37:35.640
 But I think because the way the car industry works, I mean, it's a very thin margin that

1:37:35.640 --> 1:37:36.640
 you get to operate under.

1:37:36.640 --> 1:37:40.640
 So you have to really, really make sure that everything you add to the car makes sense

1:37:40.640 --> 1:37:41.640
 financially.

1:37:41.640 --> 1:37:49.880
 So anyway, is this new industry, especially at this scale of SmartEye, does it hold any

1:37:49.880 --> 1:37:50.880
 lessons for you?

1:37:50.880 --> 1:37:56.880
 Yeah, I think it is a very tough market to penetrate, but once you're in, it's awesome

1:37:56.880 --> 1:38:00.960
 because once you're in, you're designed into these car models for like somewhere between

1:38:00.960 --> 1:38:02.920
 five to seven years, which is awesome.

1:38:02.920 --> 1:38:07.400
 And you just, once they're on the road, you just get paid a royalty fee per vehicle.

1:38:07.400 --> 1:38:11.480
 So it's a high barrier to entry, but once you're in, it's amazing.

1:38:11.480 --> 1:38:16.620
 I think the thing that I struggle the most with in this industry is the time to market.

1:38:16.620 --> 1:38:22.440
 So often we're asked to lock or do a code freeze two years before the car is going to

1:38:22.440 --> 1:38:23.440
 be on the road.

1:38:23.440 --> 1:38:28.160
 I'm like, guys, like, do you understand the pace with which technology moves?

1:38:28.160 --> 1:38:35.280
 So I think car companies are really trying to make the Tesla, the Tesla transition to

1:38:35.280 --> 1:38:39.480
 become more of a software driven architecture.

1:38:39.480 --> 1:38:41.100
 And that's hard for many.

1:38:41.100 --> 1:38:42.320
 It's just the cultural change.

1:38:42.320 --> 1:38:43.920
 I mean, I'm sure you've experienced that, right?

1:38:43.920 --> 1:38:51.040
 Oh, definitely, I think one of the biggest inventions or imperatives created by Tesla

1:38:51.040 --> 1:38:56.680
 is like to me personally, okay, people are going to complain about this, but I know electric

1:38:56.680 --> 1:38:59.920
 vehicle, I know autopilot AI stuff.

1:38:59.920 --> 1:39:06.920
 To me, the software over there, software updates is like the biggest revolution in cars.

1:39:06.920 --> 1:39:12.920
 And it is extremely difficult to switch to that because it is a culture shift.

1:39:12.920 --> 1:39:17.320
 At first, especially if you're not comfortable with it, it seems dangerous.

1:39:17.320 --> 1:39:23.840
 Like there's a, there's an approach to cars is so safety focused for so many decades that

1:39:23.840 --> 1:39:27.880
 like, what do you mean we dynamically change code?

1:39:27.880 --> 1:39:36.600
 The whole point is you have a thing that you test, like, and like, it's not reliable because

1:39:36.600 --> 1:39:41.320
 do you know how much it costs if we have to recall this cars, right?

1:39:41.320 --> 1:39:47.760
 There's a, there's a, and there's an understandable obsession with safety, but the downside of

1:39:47.760 --> 1:39:54.840
 an obsession with safety is the same as with being obsessed with safety as a parent is

1:39:54.840 --> 1:40:00.520
 like, if you do that too much, you limit the potential development and the flourishing

1:40:00.520 --> 1:40:04.960
 of in that particular aspect human being, when this particular aspect, the software,

1:40:04.960 --> 1:40:07.760
 the artificial neural network of it.

1:40:07.760 --> 1:40:09.880
 And but it's tough to do.

1:40:09.880 --> 1:40:14.080
 It's really tough to do culturally and technically like the deployment, the mass deployment of

1:40:14.080 --> 1:40:18.400
 software is really, really difficult, but I hope that's where the industry is doing.

1:40:18.400 --> 1:40:21.700
 One of the reasons I really want Tesla to succeed is exactly about that point.

1:40:21.700 --> 1:40:28.440
 Not autopilot, not the electrical vehicle, but the softwareization of basically everything

1:40:28.440 --> 1:40:33.640
 but cars, especially because to me, that's actually going to increase two things, increase

1:40:33.640 --> 1:40:40.200
 safety because you can update much faster, but also increase the effectiveness of folks

1:40:40.200 --> 1:40:47.320
 like you who dream about enriching the human experience with AI because you can just like,

1:40:47.320 --> 1:40:51.840
 there's a feature, like you want like a new emoji or whatever, like the way TikTok releases

1:40:51.840 --> 1:40:55.680
 filters, you can just release that for in car, in car stuff.

1:40:55.680 --> 1:40:59.680
 So, but yeah, that, that, that's definitely.

1:40:59.680 --> 1:41:05.240
 One of the use cases we're looking into is once you know the sentiment of the passengers

1:41:05.240 --> 1:41:08.800
 in the vehicle, you can optimize the temperature in the car.

1:41:08.800 --> 1:41:10.440
 You can change the lighting, right?

1:41:10.440 --> 1:41:14.440
 So if the backseat passengers are falling asleep, you can dim the lights, you can lower

1:41:14.440 --> 1:41:15.440
 the music, right?

1:41:15.440 --> 1:41:17.000
 You can do all sorts of things.

1:41:17.000 --> 1:41:18.000
 Yeah.

1:41:18.000 --> 1:41:23.760
 I mean, of course you could do that kind of stuff with a two year delay, but it's tougher.

1:41:23.760 --> 1:41:24.760
 Right.

1:41:24.760 --> 1:41:25.760
 Yeah.

1:41:25.760 --> 1:41:30.760
 Do you think, do you think a Tesla or Waymo or some of these companies that are doing

1:41:30.760 --> 1:41:35.800
 semi or fully autonomous driving should be doing driver sensing?

1:41:35.800 --> 1:41:36.800
 Yes.

1:41:36.800 --> 1:41:39.000
 Are you thinking about that kind of stuff?

1:41:39.000 --> 1:41:43.960
 So not just how we can enhance the in cab experience for cars that are manly driven,

1:41:43.960 --> 1:41:47.520
 but the ones that are increasingly more autonomously driven.

1:41:47.520 --> 1:41:48.520
 Yes.

1:41:48.520 --> 1:41:53.080
 So if we fast forward to the universe where it's fully autonomous, I think interior sensing

1:41:53.080 --> 1:41:57.160
 becomes extremely important because the role of the driver isn't just to drive.

1:41:57.160 --> 1:42:02.000
 If you think about it, the driver almost manages, manages the dynamics within a vehicle.

1:42:02.000 --> 1:42:06.120
 And so who's going to play that role when it's an autonomous car?

1:42:06.120 --> 1:42:11.800
 We want a solution that is able to say, Oh my God, like, you know, Lex is bored to death

1:42:11.800 --> 1:42:13.700
 cause the car's moving way too slow.

1:42:13.700 --> 1:42:18.040
 Let's engage Lex or Rana's freaking out because she doesn't trust this vehicle yet.

1:42:18.040 --> 1:42:22.420
 So let's tell Rana like a little bit more information about the route or, right?

1:42:22.420 --> 1:42:27.220
 So I think, or somebody's having a heart attack in the car, like you need interior sensing

1:42:27.220 --> 1:42:29.420
 and fully autonomous vehicles.

1:42:29.420 --> 1:42:34.100
 But with semi autonomous vehicles, I think it's, I think it's really key to have driver

1:42:34.100 --> 1:42:39.120
 monitoring because semi autonomous means that sometimes the car is in charge.

1:42:39.120 --> 1:42:41.360
 Sometimes the driver is in charge or the copilot, right?

1:42:41.360 --> 1:42:44.800
 And you need this, you need both systems to be on the same page.

1:42:44.800 --> 1:42:49.560
 You need to know the car needs to know if the driver's asleep before it transitions

1:42:49.560 --> 1:42:51.880
 control over to the driver.

1:42:51.880 --> 1:42:56.600
 And sometimes if the driver's too tired, the car can say, I'm going to be a better driver

1:42:56.600 --> 1:42:57.600
 than you are right now.

1:42:57.600 --> 1:42:58.640
 I'm taking control over.

1:42:58.640 --> 1:43:03.200
 So this dynamic, this dance is so key and you can't do that without driver sensing.

1:43:03.200 --> 1:43:04.200
 Yeah.

1:43:04.200 --> 1:43:07.720
 There's a disagreement for the longest time I've had with Elon that this is obvious that

1:43:07.720 --> 1:43:10.240
 this should be in the Tesla from day one.

1:43:10.240 --> 1:43:13.920
 And it's obvious that driver sensing is not a hindrance.

1:43:13.920 --> 1:43:15.920
 It's not obvious.

1:43:15.920 --> 1:43:22.300
 I should be careful because having studied this problem, nothing is really obvious, but

1:43:22.300 --> 1:43:26.620
 it seems very likely a driver sensing is not a hindrance to an experience.

1:43:26.620 --> 1:43:34.760
 It's only enriching to the experience and likely increases the safety.

1:43:34.760 --> 1:43:42.360
 That said, it is very surprising to me just having studied semi autonomous driving, how

1:43:42.360 --> 1:43:47.800
 well humans are able to manage that dance because it was the intuition before you were

1:43:47.800 --> 1:43:54.080
 doing that kind of thing that humans will become just incredibly distracted.

1:43:54.080 --> 1:43:57.920
 They would just like let the thing do its thing, but they're able to, you know, cause

1:43:57.920 --> 1:44:01.000
 it is life and death and they're able to manage that somehow.

1:44:01.000 --> 1:44:04.640
 But that said, there's no reason not to have driver sensing on top of that.

1:44:04.640 --> 1:44:11.240
 I feel like that's going to allow you to do that dance that you're currently doing without

1:44:11.240 --> 1:44:15.920
 driver sensing, except touching the steering wheel to do that even better.

1:44:15.920 --> 1:44:20.000
 I mean, the possibilities are endless and the machine learning possibilities are endless.

1:44:20.000 --> 1:44:26.160
 It's such a beautiful, it's also a constrained environment so you could do a much more effectively

1:44:26.160 --> 1:44:31.440
 than you can with the external environment, external environment is full of weird edge

1:44:31.440 --> 1:44:33.600
 cases and complexities just inside.

1:44:33.600 --> 1:44:36.600
 There's so much, it's so fascinating, such a fascinating world.

1:44:36.600 --> 1:44:44.680
 I do hope that companies like Tesla and others, even Waymo, which I don't even know if Waymo

1:44:44.680 --> 1:44:46.920
 is doing anything sophisticated inside the cab.

1:44:46.920 --> 1:44:47.920
 I don't think so.

1:44:47.920 --> 1:44:51.400
 It's like, like what, what, what is it?

1:44:51.400 --> 1:44:55.560
 I honestly think, I honestly think it goes back to the robotics thing we were talking

1:44:55.560 --> 1:45:02.400
 about, which is like great engineers that are building these AI systems just are afraid

1:45:02.400 --> 1:45:03.760
 of the human being.

1:45:03.760 --> 1:45:08.000
 They're not thinking about the human experience, they're thinking about the features and yeah,

1:45:08.000 --> 1:45:10.840
 the perceptual abilities of that thing.

1:45:10.840 --> 1:45:16.760
 They think the best way I can serve the human is by doing the best perception and control

1:45:16.760 --> 1:45:20.640
 I can by looking at the external environment, keeping the human safe.

1:45:20.640 --> 1:45:31.040
 But like, there's a huge, I'm here, like, you know, I need to be noticed and interacted

1:45:31.040 --> 1:45:34.760
 with and understood and all those kinds of things, even just on a personal level for

1:45:34.760 --> 1:45:38.640
 entertainment, honestly, for entertainment.

1:45:38.640 --> 1:45:42.440
 You know, one of the coolest work we did in collaboration with MIT around this was we

1:45:42.440 --> 1:45:52.880
 looked at longitudinal data, right, because, you know, MIT had access to like tons of data.

1:45:52.880 --> 1:45:57.300
 And like just seeing the patterns of people like driving in the morning off to work versus

1:45:57.300 --> 1:46:02.460
 like commuting back from work or weekend driving versus weekday driving.

1:46:02.460 --> 1:46:08.300
 And wouldn't it be so cool if your car knew that and then was able to optimize either

1:46:08.300 --> 1:46:12.360
 the route or the experience or even make recommendations?

1:46:12.360 --> 1:46:13.360
 I think it's very powerful.

1:46:13.360 --> 1:46:15.960
 Yeah, like, why are you taking this route?

1:46:15.960 --> 1:46:18.360
 You're always unhappy when you take this route.

1:46:18.360 --> 1:46:20.520
 And you're always happy when you take this alternative route.

1:46:20.520 --> 1:46:21.520
 Take that route.

1:46:21.520 --> 1:46:22.520
 Exactly.

1:46:22.520 --> 1:46:27.920
 But I mean, to have that even that little step of relationship with a car, I think,

1:46:27.920 --> 1:46:28.920
 is incredible.

1:46:28.920 --> 1:46:32.720
 Of course, you have to get the privacy right, you have to get all that kind of stuff right.

1:46:32.720 --> 1:46:37.440
 But I wish I honestly, you know, people are like paranoid about this, but I would like

1:46:37.440 --> 1:46:39.640
 a smart refrigerator.

1:46:39.640 --> 1:46:44.840
 We have such a deep connection with food as a human civilization.

1:46:44.840 --> 1:46:51.480
 I would like to have a refrigerator that would understand me that, you know, I also have

1:46:51.480 --> 1:46:56.280
 a complex relationship with food because I, you know, pig out too easily and all that

1:46:56.280 --> 1:46:57.280
 kind of stuff.

1:46:57.280 --> 1:47:02.720
 So, you know, like, maybe I want the refrigerator to be like, are you sure about this?

1:47:02.720 --> 1:47:05.200
 Because maybe you're just feeling down or tired.

1:47:05.200 --> 1:47:06.200
 Like maybe let's sleep on it.

1:47:06.200 --> 1:47:10.220
 Your vision of the smart refrigerator is way kinder than mine.

1:47:10.220 --> 1:47:11.920
 Is it just me yelling at you?

1:47:11.920 --> 1:47:18.600
 No, it was just because I don't, you know, I don't drink alcohol, I don't smoke, but

1:47:18.600 --> 1:47:22.200
 I eat a ton of chocolate, like it sticks to my vice.

1:47:22.200 --> 1:47:26.640
 And so I, and sometimes I scream too, and I'm like, okay, my smart refrigerator will

1:47:26.640 --> 1:47:27.640
 just lock down.

1:47:27.640 --> 1:47:32.400
 It'll just say, dude, you've had way too many today, like down.

1:47:32.400 --> 1:47:33.400
 Yeah.

1:47:33.400 --> 1:47:41.120
 No, but here's the thing, are you, do you regret having, like, let's say not the next

1:47:41.120 --> 1:47:48.560
 day, but 30 days later, what would you like the refrigerator to have done then?

1:47:48.560 --> 1:47:54.400
 Well, I think actually like the more positive relationship would be one where there's a

1:47:54.400 --> 1:47:55.900
 conversation, right?

1:47:55.900 --> 1:48:00.800
 As opposed to like, that's probably like the more sustainable relationship.

1:48:00.800 --> 1:48:06.200
 It's like late at night, just, no, listen, listen, I know I told you an hour ago, that

1:48:06.200 --> 1:48:09.720
 it's not a good idea, but just listen, things have changed.

1:48:09.720 --> 1:48:17.000
 I can just imagine a bunch of stuff being made up just to convince, but I mean, I just

1:48:17.000 --> 1:48:22.400
 think that there's opportunities that, I mean, maybe not locking down, but for our systems

1:48:22.400 --> 1:48:32.880
 that are such a deep part of our lives, like we use a lot of us, a lot of people that commute

1:48:32.880 --> 1:48:34.360
 use their car every single day.

1:48:34.360 --> 1:48:38.240
 A lot of us use a refrigerator every single day, the microwave every single day.

1:48:38.240 --> 1:48:47.600
 Like we just, like, I feel like certain things could be made more efficient, more enriching,

1:48:47.600 --> 1:48:54.200
 and AI is there to help, like some just basic recognition of you as a human being, but your

1:48:54.200 --> 1:48:57.520
 patterns of what makes you happy and not happy and all that kind of stuff.

1:48:57.520 --> 1:48:58.520
 And the car, obviously.

1:48:58.520 --> 1:49:05.320
 Maybe, maybe, maybe we'll say, wait, wait, wait, wait, instead of this, like, Ben and

1:49:05.320 --> 1:49:09.440
 Jerry's ice cream, how about this hummus and carrots or something?

1:49:09.440 --> 1:49:10.440
 I don't know.

1:49:10.440 --> 1:49:14.960
 It would make it like a just in time recommendation, right?

1:49:14.960 --> 1:49:21.240
 But not like a generic one, but a reminder that last time you chose the carrots, you

1:49:21.240 --> 1:49:24.800
 smiled 17 times more the next day.

1:49:24.800 --> 1:49:26.400
 You're happier the next day, right?

1:49:26.400 --> 1:49:28.160
 You're happier the next day.

1:49:28.160 --> 1:49:34.480
 And but yeah, I don't, but then again, if you're the kind of person that gets better

1:49:34.480 --> 1:49:40.040
 from negative, negative comments, you could say like, hey, remember like that wedding

1:49:40.040 --> 1:49:43.880
 you're going to, you want to fit into that dress?

1:49:43.880 --> 1:49:44.880
 Remember about that?

1:49:44.880 --> 1:49:48.760
 Let's think about that before you're eating this.

1:49:48.760 --> 1:49:53.400
 It's for some, probably that would work for me, like a refrigerator that is just ruthless

1:49:53.400 --> 1:49:54.920
 at shaming me.

1:49:54.920 --> 1:49:59.600
 But like, I would, of course, welcome it, like that would work for me.

1:49:59.600 --> 1:50:00.600
 Just that.

1:50:00.600 --> 1:50:05.320
 So it would know, I think it would, if it's really like smart, it would optimize its nudging

1:50:05.320 --> 1:50:07.280
 based on what works for you, right?

1:50:07.280 --> 1:50:08.280
 Exactly.

1:50:08.280 --> 1:50:09.280
 That's the whole point.

1:50:09.280 --> 1:50:10.280
 Personalization.

1:50:10.280 --> 1:50:11.920
 In every way, depersonalization.

1:50:11.920 --> 1:50:18.120
 You were a part of a webinar titled Advancing Road Safety, the State of Alcohol Intoxication

1:50:18.120 --> 1:50:19.600
 Research.

1:50:19.600 --> 1:50:24.520
 So for people who don't know, every year 1.3 million people around the world die in road

1:50:24.520 --> 1:50:31.320
 crashes and more than 20% of these fatalities are estimated to be alcohol related.

1:50:31.320 --> 1:50:33.320
 A lot of them are also distraction related.

1:50:33.320 --> 1:50:36.800
 So can AI help with the alcohol thing?

1:50:36.800 --> 1:50:40.240
 I think the answer is yes.

1:50:40.240 --> 1:50:46.560
 There are signals and we know that as humans, like we can tell when a person, you know,

1:50:46.560 --> 1:50:51.200
 is at different phases of being drunk, right?

1:50:51.200 --> 1:50:53.680
 And I think you can use technology to do the same.

1:50:53.680 --> 1:50:58.640
 And again, I think the ultimate solution is going to be a combination of different sensors.

1:50:58.640 --> 1:51:01.440
 How hard is the problem from the vision perspective?

1:51:01.440 --> 1:51:02.880
 I think it's non trivial.

1:51:02.880 --> 1:51:06.720
 I think it's non trivial and I think the biggest part is getting the data, right?

1:51:06.720 --> 1:51:09.200
 It's like getting enough data examples.

1:51:09.200 --> 1:51:15.240
 So we, for this research project, we partnered with the transportation authorities of Sweden

1:51:15.240 --> 1:51:20.680
 and we literally had a racetrack with a safety driver and we basically progressively got

1:51:20.680 --> 1:51:21.680
 people drunk.

1:51:21.680 --> 1:51:22.680
 Nice.

1:51:22.680 --> 1:51:29.280
 So, but, you know, that's a very expensive data set to collect and you want to collect

1:51:29.280 --> 1:51:32.080
 it globally and in multiple conditions.

1:51:32.080 --> 1:51:33.480
 Yeah.

1:51:33.480 --> 1:51:38.800
 The ethics of collecting a data set where people are drunk is tricky, which is funny

1:51:38.800 --> 1:51:43.400
 because I mean, let's put drunk driving aside.

1:51:43.400 --> 1:51:47.120
 The number of drunk people in the world every day is very large.

1:51:47.120 --> 1:51:50.320
 It'd be nice to have a large data set of drunk people getting progressively drunk.

1:51:50.320 --> 1:51:54.600
 In fact, you could build an app where people can donate their data cause it's hilarious.

1:51:54.600 --> 1:51:55.600
 Right.

1:51:55.600 --> 1:51:56.600
 Actually, yeah.

1:51:56.600 --> 1:51:57.600
 But the liability.

1:51:57.600 --> 1:52:00.800
 Liability, the ethics, how do you get it right?

1:52:00.800 --> 1:52:01.800
 It's tricky.

1:52:01.800 --> 1:52:02.800
 It's really, really tricky.

1:52:02.800 --> 1:52:07.440
 Cause like drinking is one of those things that's funny and hilarious and we're loves

1:52:07.440 --> 1:52:10.240
 it's social, the so on and so forth.

1:52:10.240 --> 1:52:13.520
 But it's also the thing that hurts a lot of people.

1:52:13.520 --> 1:52:19.040
 Like a lot of people, like alcohol is one of those things it's legal, but it's really

1:52:19.040 --> 1:52:21.200
 damaging to a lot of lives.

1:52:21.200 --> 1:52:26.320
 It destroys lives and not just in the driving context.

1:52:26.320 --> 1:52:32.160
 I should mention people should listen to Andrew Huberman who recently talked about alcohol.

1:52:32.160 --> 1:52:33.160
 He has an amazing pocket.

1:52:33.160 --> 1:52:37.920
 Andrew Huberman is a neuroscientist from Stanford and a good friend of mine.

1:52:37.920 --> 1:52:43.560
 And he, he's like a human encyclopedia about all health related wisdom.

1:52:43.560 --> 1:52:45.880
 So if there's a podcast, you would love it.

1:52:45.880 --> 1:52:46.880
 I would love that.

1:52:46.880 --> 1:52:47.880
 No, no, no, no, no.

1:52:47.880 --> 1:52:49.600
 You don't know Andrew Huberman.

1:52:49.600 --> 1:52:50.600
 Okay.

1:52:50.600 --> 1:52:54.160
 Listen, you listen to Andrew, it's called Huberman Lab Podcast.

1:52:54.160 --> 1:52:55.160
 This is your assignment.

1:52:55.160 --> 1:52:56.160
 Just listen to one.

1:52:56.160 --> 1:52:57.160
 Okay.

1:52:57.160 --> 1:53:01.360
 I guarantee you this will be a thing where you say, Lex, this is the greatest human I

1:53:01.360 --> 1:53:02.360
 have ever discovered.

1:53:02.360 --> 1:53:03.360
 So.

1:53:03.360 --> 1:53:04.360
 Oh my God.

1:53:04.360 --> 1:53:08.120
 Cause I've really, I've, I'm really on a journey of kind of health and wellness and

1:53:08.120 --> 1:53:13.240
 I'm learning lots and I'm trying to like build these, I guess, atomic habits around just

1:53:13.240 --> 1:53:14.240
 being healthy.

1:53:14.240 --> 1:53:17.200
 So I, yeah, I'm definitely going to do this.

1:53:17.200 --> 1:53:21.960
 His whole thing, this is, this is, this is, this is great.

1:53:21.960 --> 1:53:30.160
 He's a legit scientist, like really well published, but in his podcast, what he does, he's not,

1:53:30.160 --> 1:53:31.920
 he's not talking about his own work.

1:53:31.920 --> 1:53:34.640
 He's like a human encyclopedia of papers.

1:53:34.640 --> 1:53:39.720
 And so he, his whole thing is he takes the topic and in a very fast, you mentioned atomic

1:53:39.720 --> 1:53:46.220
 habits, like very clear way summarizes the research in a way that leads to protocols

1:53:46.220 --> 1:53:47.400
 of what you should do.

1:53:47.400 --> 1:53:52.600
 He's really big on like, not like this is what the science says, but like this is literally

1:53:52.600 --> 1:53:54.280
 what you should be doing according to science.

1:53:54.280 --> 1:54:01.360
 So like he's really big and there's a lot of recommendations he does which several of

1:54:01.360 --> 1:54:08.880
 them I definitely don't do, like get some light as soon as possible from waking up and

1:54:08.880 --> 1:54:11.040
 like for prolonged periods of time.

1:54:11.040 --> 1:54:14.880
 That's a really big one and he's, there's a lot of science behind that one.

1:54:14.880 --> 1:54:19.880
 There's a bunch of stuff that you're going to be like, Lex, this is a, this is my new

1:54:19.880 --> 1:54:20.880
 favorite person.

1:54:20.880 --> 1:54:21.880
 I guarantee it.

1:54:21.880 --> 1:54:27.840
 And if you guys somehow don't know Andrew Huberman and you care about your wellbeing,

1:54:27.840 --> 1:54:29.560
 you know, you should definitely listen to him.

1:54:29.560 --> 1:54:31.920
 I love you, Andrew.

1:54:31.920 --> 1:54:36.040
 Anyway, so what were we talking about?

1:54:36.040 --> 1:54:39.480
 Oh, alcohol and detecting alcohol.

1:54:39.480 --> 1:54:42.240
 So this is a problem you care about and you're trying to solve.

1:54:42.240 --> 1:54:48.960
 And actually like broadening it, I do believe that the car is going to be a wellness center,

1:54:48.960 --> 1:54:55.240
 like because again, imagine if you have a variety of sensors inside the vehicle, tracking

1:54:55.240 --> 1:55:03.840
 not just your emotional state or level of distraction and drowsiness and intoxication,

1:55:03.840 --> 1:55:09.440
 but also maybe even things like your, you know, your heart rate and your heart rate

1:55:09.440 --> 1:55:13.960
 variability and your breathing rate.

1:55:13.960 --> 1:55:19.520
 And it can start like optimizing, yeah, it can optimize the ride based on what your goals

1:55:19.520 --> 1:55:20.520
 are.

1:55:20.520 --> 1:55:24.040
 So I think we're going to start to see more of that and I'm excited about that.

1:55:24.040 --> 1:55:25.040
 Yeah.

1:55:25.040 --> 1:55:28.960
 What are the, what are the challenges you're tackling while with SmartEye currently?

1:55:28.960 --> 1:55:34.640
 What's like the, the trickiest things to get, is it, is it basically convincing more and

1:55:34.640 --> 1:55:41.160
 more car companies that having AI inside the car is a good idea or is there some, is there

1:55:41.160 --> 1:55:45.360
 more technical algorithmic challenges?

1:55:45.360 --> 1:55:47.700
 What's been keeping you mentally busy?

1:55:47.700 --> 1:55:52.360
 I think a lot of the car companies we are in conversations with are already interested

1:55:52.360 --> 1:55:54.160
 in definitely driver monitoring.

1:55:54.160 --> 1:55:59.340
 Like I think it's becoming a must have, but even interior sensing, I can see like we're

1:55:59.340 --> 1:56:04.040
 engaged in a lot of like advanced engineering projects and proof of concepts.

1:56:04.040 --> 1:56:09.620
 I think technologically though, and that even the technology, I can see a path to making

1:56:09.620 --> 1:56:10.620
 it happen.

1:56:10.620 --> 1:56:11.620
 I think it's the use case.

1:56:11.620 --> 1:56:16.360
 Like how does the car respond once it knows something about you?

1:56:16.360 --> 1:56:20.880
 Because you want it to respond in a thoughtful way that doesn't, that isn't off putting to

1:56:20.880 --> 1:56:23.240
 the consumer in the car.

1:56:23.240 --> 1:56:25.640
 So I think that's like the user experience.

1:56:25.640 --> 1:56:27.600
 I don't think we've really nailed that.

1:56:27.600 --> 1:56:33.040
 And we usually, that's not part, we're the sensing platform, but we usually collaborate

1:56:33.040 --> 1:56:35.960
 with the car manufacturer to decide what the use case is.

1:56:35.960 --> 1:56:40.680
 So say you do, you figure out that somebody's angry while driving, okay, what should the

1:56:40.680 --> 1:56:43.680
 car do?

1:56:43.680 --> 1:56:50.100
 Do you see yourself as a role of nudging, of like basically coming up with solutions

1:56:50.100 --> 1:56:56.360
 essentially that, and then the car manufacturers kind of put their own little spin on it?

1:56:56.360 --> 1:56:57.360
 Right.

1:56:57.360 --> 1:57:03.620
 So we, we are like the ideation, creative thought partner, but at the end of the day,

1:57:03.620 --> 1:57:06.640
 the car company needs to decide what's on brand for them, right?

1:57:06.640 --> 1:57:11.720
 Like maybe when it figures out that you're distracted or drowsy, it shows you a coffee

1:57:11.720 --> 1:57:12.720
 cup, right?

1:57:12.720 --> 1:57:16.640
 Or maybe it takes more aggressive behaviors and basically said, okay, if you don't like

1:57:16.640 --> 1:57:19.640
 take a rest in the next five minutes, the car's going to shut down, right?

1:57:19.640 --> 1:57:25.400
 Like there's a whole range of actions the car can take and doing the thing that is most,

1:57:25.400 --> 1:57:29.320
 yeah, that builds trust with the driver and the passengers.

1:57:29.320 --> 1:57:32.840
 I think that's what we need to be very careful about.

1:57:32.840 --> 1:57:33.840
 Yeah.

1:57:33.840 --> 1:57:38.600
 Car companies are funny cause they have their own, like, I mean, that's why people get cars

1:57:38.600 --> 1:57:39.600
 still.

1:57:39.600 --> 1:57:44.240
 I hope that changes, but they get it cause it's a certain feel and look and it's a certain,

1:57:44.240 --> 1:57:51.840
 they become proud, like Mercedes Benz or BMW or whatever, and that's their thing.

1:57:51.840 --> 1:57:56.400
 That's the family brand or something like that, or Ford or GM, whatever, they stick

1:57:56.400 --> 1:57:57.400
 to that thing.

1:57:57.400 --> 1:57:58.400
 Yeah.

1:57:58.400 --> 1:57:59.400
 It's interesting.

1:57:59.400 --> 1:58:04.160
 It's like, it should be, I don't know, it should be a little more about the technology

1:58:04.160 --> 1:58:06.800
 inside.

1:58:06.800 --> 1:58:12.440
 And I suppose there too, there could be a branding, like a very specific style of luxury

1:58:12.440 --> 1:58:13.440
 or fun.

1:58:13.440 --> 1:58:14.440
 Right.

1:58:14.440 --> 1:58:15.440
 Right.

1:58:15.440 --> 1:58:16.440
 All that kind of stuff.

1:58:16.440 --> 1:58:17.440
 Yeah.

1:58:17.440 --> 1:58:22.720
 And I have an AI focused fund to invest in early stage kind of AI driven companies.

1:58:22.720 --> 1:58:27.560
 And one of the companies we're looking at is trying to do what Tesla did, but for boats,

1:58:27.560 --> 1:58:28.760
 for recreational boats.

1:58:28.760 --> 1:58:29.760
 Yeah.

1:58:29.760 --> 1:58:34.840
 So they're building an electric and kind of slash autonomous boat and it's kind of the

1:58:34.840 --> 1:58:35.840
 same issues.

1:58:35.840 --> 1:58:38.600
 Like what kind of sensors can you put in?

1:58:38.600 --> 1:58:43.320
 What kind of states can you detect both exterior and interior within the boat?

1:58:43.320 --> 1:58:45.480
 Anyways, it's like really interesting.

1:58:45.480 --> 1:58:46.760
 Do you boat at all?

1:58:46.760 --> 1:58:49.960
 No, not well, not in that way.

1:58:49.960 --> 1:58:57.400
 I do like to get on the lake or a river and fish from a boat, but that's not boating.

1:58:57.400 --> 1:58:58.400
 That's the difference.

1:58:58.400 --> 1:58:59.400
 That's the difference.

1:58:59.400 --> 1:59:00.400
 Still boating.

1:59:00.400 --> 1:59:01.400
 Low tech.

1:59:01.400 --> 1:59:02.400
 A low tech boat.

1:59:02.400 --> 1:59:04.400
 Get away from, get closer to nature boat.

1:59:04.400 --> 1:59:12.200
 I guess going out into the ocean is also getting closer to nature in some deep sense.

1:59:12.200 --> 1:59:15.800
 I mean, I guess that's why people love it.

1:59:15.800 --> 1:59:18.920
 The enormity of the water just underneath you.

1:59:18.920 --> 1:59:19.920
 Yeah.

1:59:19.920 --> 1:59:20.920
 I love the water.

1:59:20.920 --> 1:59:22.800
 I love the, I love both.

1:59:22.800 --> 1:59:23.800
 I love salt water.

1:59:23.800 --> 1:59:28.420
 It was like the big and just, it's humbling to be in front of this giant thing that's

1:59:28.420 --> 1:59:31.680
 so powerful that was here before us and be here after.

1:59:31.680 --> 1:59:37.480
 But I also love the piece of a small like wooded lake and it's just, it's everything's

1:59:37.480 --> 1:59:38.480
 calm.

1:59:38.480 --> 1:59:39.480
 Therapeutic.

1:59:39.480 --> 1:59:49.600
 You tweeted that I'm excited about Amazon's acquisition of iRobot.

1:59:49.600 --> 1:59:54.480
 I think it's a super interesting, just given the trajectory of what you're part of, of

1:59:54.480 --> 2:00:00.040
 these honestly small number of companies that are playing in this space that are like trying

2:00:00.040 --> 2:00:02.180
 to have an impact on human beings.

2:00:02.180 --> 2:00:09.200
 So the, it is an interesting moment in time that Amazon would acquire iRobot.

2:00:09.200 --> 2:00:16.320
 You tweet, I imagine a future where home robots are as ubiquitous as microwaves or toasters.

2:00:16.320 --> 2:00:18.920
 Here are three reasons why I think this is exciting.

2:00:18.920 --> 2:00:23.240
 If you remember, I can look it up, but what, why is this exciting to you?

2:00:23.240 --> 2:00:27.320
 I mean, I think the first reason why this is exciting, I kind of remember the exact

2:00:27.320 --> 2:00:33.540
 like order in which I put them, but one is just, it's, it's going to be an incredible

2:00:33.540 --> 2:00:37.640
 platform for understanding our behaviors within the home, right?

2:00:37.640 --> 2:00:42.880
 Like you know, if you think about Roomba, which is, you know, the robot vacuum cleaner,

2:00:42.880 --> 2:00:48.640
 the flagship product of iRobot at the moment, it's like running around your home, understanding

2:00:48.640 --> 2:00:51.200
 the layout, it's understanding what's clean and what's not.

2:00:51.200 --> 2:00:52.640
 How often do you clean your house?

2:00:52.640 --> 2:00:57.500
 And all of these like behaviors are a piece of the puzzle in terms of understanding who

2:00:57.500 --> 2:00:58.760
 you are as a consumer.

2:00:58.760 --> 2:01:05.580
 And I think that could be, again, used in really meaningful ways, not just to recommend

2:01:05.580 --> 2:01:09.640
 better products or whatever, but actually to improve your experience as a human being.

2:01:09.640 --> 2:01:12.900
 So I think, I think that's very interesting.

2:01:12.900 --> 2:01:18.480
 I think the natural evolution of these robots in the, in the home.

2:01:18.480 --> 2:01:24.280
 So it's, it's interesting, Roomba isn't really a social robot, right, at the moment.

2:01:24.280 --> 2:01:29.160
 But I once interviewed one of the chief engineers on the Roomba team, and he talked about how

2:01:29.160 --> 2:01:31.400
 people named their Roombas.

2:01:31.400 --> 2:01:36.520
 And if the Roomba broke down, they would call in and say, you know, my Roomba broke down

2:01:36.520 --> 2:01:38.920
 and the company would say, well, we'll just send you a new one.

2:01:38.920 --> 2:01:45.680
 And no, no, no, Rosie, like you have to like, yeah, I want you to fix this particular robot.

2:01:45.680 --> 2:01:51.680
 So people have already built like interesting emotional connections with these home robots.

2:01:51.680 --> 2:01:57.320
 And I think that, again, that provides a platform for really interesting things to, to just

2:01:57.320 --> 2:01:58.320
 motivate change.

2:01:58.320 --> 2:01:59.320
 Like it could help you.

2:01:59.320 --> 2:02:05.740
 I mean, one of the companies that spun out of MIT, Catalia Health, the guy who started

2:02:05.740 --> 2:02:09.640
 it spent a lot of time building robots that help with weight management.

2:02:09.640 --> 2:02:14.320
 So weight management, sleep, eating better, yeah, all of these things.

2:02:14.320 --> 2:02:20.280
 Well, if I'm being honest, Amazon does not exactly have a track record of winning over

2:02:20.280 --> 2:02:22.240
 people in terms of trust.

2:02:22.240 --> 2:02:27.840
 Now that said, it's a really difficult problem for a human being to let a robot in their

2:02:27.840 --> 2:02:30.680
 home that has a camera on it.

2:02:30.680 --> 2:02:31.680
 Right.

2:02:31.680 --> 2:02:33.400
 That's really, really, really tough.

2:02:33.400 --> 2:02:40.480
 And I think Roomba actually, I have to think about this, but I'm pretty sure now or for

2:02:40.480 --> 2:02:46.040
 some time already has had cameras because they're doing the, the, the most recent Roomba.

2:02:46.040 --> 2:02:47.040
 I have so many Roombas.

2:02:47.040 --> 2:02:48.040
 Oh, you actually do?

2:02:48.040 --> 2:02:49.560
 Well, I programmed it.

2:02:49.560 --> 2:02:51.440
 I don't use a Roomba for VECO.

2:02:51.440 --> 2:02:54.280
 People that have been to my place, they're like, yeah, you definitely don't use these

2:02:54.280 --> 2:02:55.280
 Roombas.

2:02:55.280 --> 2:03:00.920
 That could be a good, I can't tell like the valence of this comment.

2:03:00.920 --> 2:03:02.400
 Was it a compliment or like?

2:03:02.400 --> 2:03:05.320
 No, it's a giant, it's just a bunch of electronics everywhere.

2:03:05.320 --> 2:03:11.160
 There's, I have six or seven computers, I have robots everywhere, Lego robots, I have

2:03:11.160 --> 2:03:20.240
 small robots and big robots and it's just giant, just piles of robot stuff and yeah.

2:03:20.240 --> 2:03:25.560
 But including the Roombas, they're, they're, they're being used for their body and intelligence,

2:03:25.560 --> 2:03:26.720
 but not for their purpose.

2:03:26.720 --> 2:03:33.300
 I have, I've changed them, repurposed them for other purposes, for deeper, more meaningful

2:03:33.300 --> 2:03:39.240
 purposes than just like the Bota Roba, which is, you know, brings a lot of people happiness,

2:03:39.240 --> 2:03:41.060
 I'm sure.

2:03:41.060 --> 2:03:46.560
 They have a camera because the thing they advertised, I had my own camera still, but

2:03:46.560 --> 2:03:52.320
 the, the, the camera on the new Roomba, they have like state of the art poop detection

2:03:52.320 --> 2:03:56.760
 as they advertised, which is a very difficult, apparently it's a big problem for, for vacuum

2:03:56.760 --> 2:04:01.000
 cleaners is, you know, if they go over like dog poop, it just runs it, it runs it over

2:04:01.000 --> 2:04:02.140
 and creates a giant mess.

2:04:02.140 --> 2:04:08.360
 So they have like, and apparently they collected like a huge amount of data and different shapes

2:04:08.360 --> 2:04:12.400
 and looks and whatever of poop and then now they're able to avoid it and so on.

2:04:12.400 --> 2:04:14.440
 They're very proud of this.

2:04:14.440 --> 2:04:19.200
 So there is a camera, but you don't think of it as having a camera.

2:04:19.200 --> 2:04:20.380
 Yeah.

2:04:20.380 --> 2:04:24.600
 You don't think of it as having a camera because you've grown to trust that, I guess, because

2:04:24.600 --> 2:04:31.600
 our phones, at least most of us seem to trust this phone, even though there's a camera looking

2:04:31.600 --> 2:04:33.960
 directly at you.

2:04:33.960 --> 2:04:41.680
 I think that if you trust that the company is taking security very seriously, I actually

2:04:41.680 --> 2:04:46.760
 don't know how that trust was earned with smartphones, I think it just started to provide

2:04:46.760 --> 2:04:51.520
 a lot of positive value to your life where you just took it in and then the company over

2:04:51.520 --> 2:04:55.200
 time has shown that it takes privacy very seriously, that kind of stuff.

2:04:55.200 --> 2:05:01.520
 But I just, Amazon is not always in the, in its social robots communicated.

2:05:01.520 --> 2:05:07.080
 This is a trustworthy thing, both in terms of culture and competence, because I think

2:05:07.080 --> 2:05:12.620
 privacy is not just about what do you intend to do, but also how well, how good are you

2:05:12.620 --> 2:05:14.600
 at doing that kind of thing.

2:05:14.600 --> 2:05:16.800
 So that's a really hard problem to solve.

2:05:16.800 --> 2:05:22.640
 But I mean, but a lot of us have Alexas at home and I mean, Alexa could be listening

2:05:22.640 --> 2:05:24.520
 in the whole time, right?

2:05:24.520 --> 2:05:27.440
 And doing all sorts of nefarious things with the data.

2:05:27.440 --> 2:05:28.440
 Yeah.

2:05:28.440 --> 2:05:32.320
 Hopefully it's not, but I don't think it is.

2:05:32.320 --> 2:05:36.640
 But you know, Amazon is not, it's such a tricky thing for a company to get right, which

2:05:36.640 --> 2:05:38.200
 is like to earn the trust.

2:05:38.200 --> 2:05:41.520
 I don't think Alexa's earned people's trust quite yet.

2:05:41.520 --> 2:05:42.520
 Yeah.

2:05:42.520 --> 2:05:44.640
 I think it's, it's not there quite yet.

2:05:44.640 --> 2:05:45.640
 I agree.

2:05:45.640 --> 2:05:46.640
 They struggle with this kind of stuff.

2:05:46.640 --> 2:05:50.240
 In fact, when these topics are brought up, people are always get like nervous.

2:05:50.240 --> 2:05:57.560
 And I think if you get nervous about it, that mean that like the way to earn people's trust

2:05:57.560 --> 2:06:00.680
 is not by like, Ooh, don't talk about this.

2:06:00.680 --> 2:06:05.920
 It's just be open, be frank, be transparent, and also create a culture of like where it

2:06:05.920 --> 2:06:17.120
 radiates at every level from engineer to CEO that like you're good people that have a common

2:06:17.120 --> 2:06:23.040
 sense idea of what it means to respect basic human rights and the privacy of people and

2:06:23.040 --> 2:06:24.040
 all that kind of stuff.

2:06:24.040 --> 2:06:30.640
 And I think that propagates throughout the, that's the best PR, which is like over time

2:06:30.640 --> 2:06:34.920
 you understand that these are good folks doing good things.

2:06:34.920 --> 2:06:42.240
 Anyway, speaking of social robots, have you heard about Tesla, Tesla bot, the humanoid

2:06:42.240 --> 2:06:43.240
 robot?

2:06:43.240 --> 2:06:44.240
 Yes, I have.

2:06:44.240 --> 2:06:45.240
 Yes, yes, yes.

2:06:45.240 --> 2:06:48.680
 But I don't exactly know what it's designed to do to you.

2:06:48.680 --> 2:06:49.680
 You probably do.

2:06:49.680 --> 2:06:54.960
 No, I know it's designed to do, but I have a different perspective on it, but it's designed

2:06:54.960 --> 2:07:02.040
 to, it's a humanoid form and it's designed to, for automation tasks in the same way that

2:07:02.040 --> 2:07:06.260
 industrial robot arms automate tasks in the factory.

2:07:06.260 --> 2:07:08.280
 So it's designed to automate tasks in the factory.

2:07:08.280 --> 2:07:18.040
 But I think that humanoid form, as we were talking about before, is one that we connect

2:07:18.040 --> 2:07:19.800
 with as human beings.

2:07:19.800 --> 2:07:25.200
 Anything legged, obviously, but the humanoid form especially, we anthropomorphize it most

2:07:25.200 --> 2:07:26.200
 intensely.

2:07:26.200 --> 2:07:34.440
 And so the possibility to me, it's exciting to see both Atlas developed by Boston Dynamics

2:07:34.440 --> 2:07:43.720
 and anyone, including Tesla, trying to make humanoid robots cheaper and more effective.

2:07:43.720 --> 2:07:51.140
 The obvious way it transforms the world is social robotics to me versus automation of

2:07:51.140 --> 2:07:53.120
 tasks in the factory.

2:07:53.120 --> 2:07:58.840
 So yeah, I just wanted, in case that was something you were interested in, because I find its

2:07:58.840 --> 2:08:01.600
 application of social robotics super interesting.

2:08:01.600 --> 2:08:06.320
 We did a lot of work with Pepper, Pepper the robot, a while back.

2:08:06.320 --> 2:08:11.480
 We were like the emotion engine for Pepper, which is Softbank's humanoid robot.

2:08:11.480 --> 2:08:12.480
 How tall is Pepper?

2:08:12.480 --> 2:08:13.480
 It's like...

2:08:13.480 --> 2:08:18.360
 Yeah, like, I don't know, like five foot maybe, right?

2:08:18.360 --> 2:08:19.360
 Yeah.

2:08:19.360 --> 2:08:20.360
 Yeah.

2:08:20.360 --> 2:08:21.360
 Pretty, pretty big.

2:08:21.360 --> 2:08:22.360
 Pretty big.

2:08:22.360 --> 2:08:28.920
 It's designed to be at like airport lounges and, you know, retail stores, mostly customer

2:08:28.920 --> 2:08:30.680
 service, right?

2:08:30.680 --> 2:08:37.200
 Hotel lobbies, and I mean, I don't know where the state of the robot is, but I think it's

2:08:37.200 --> 2:08:38.200
 very promising.

2:08:38.200 --> 2:08:40.400
 I think there are a lot of applications where this can be helpful.

2:08:40.400 --> 2:08:45.200
 I'm also really interested in, yeah, social robotics for the home, right?

2:08:45.200 --> 2:08:50.880
 Like that can help elderly people, for example, transport things from one location of the

2:08:50.880 --> 2:08:55.520
 mind to the other, or even like just have your back in case something happens.

2:08:55.520 --> 2:08:58.000
 Yeah, I don't know.

2:08:58.000 --> 2:08:59.840
 I do think it's a very interesting space.

2:08:59.840 --> 2:09:00.840
 It seems early though.

2:09:00.840 --> 2:09:04.960
 Do you feel like the timing is now?

2:09:04.960 --> 2:09:09.840
 Yes, 100%.

2:09:09.840 --> 2:09:12.160
 So it always seems early until it's not, right?

2:09:12.160 --> 2:09:13.160
 Right, right, right.

2:09:13.160 --> 2:09:24.240
 I think the time, I definitely think that the time is now, like this decade for social

2:09:24.240 --> 2:09:25.920
 robots.

2:09:25.920 --> 2:09:29.640
 Whether the humanoid form is right, I don't think so, no.

2:09:29.640 --> 2:09:40.000
 I don't, I think the, like if we just look at Jibo as an example, I feel like most of

2:09:40.000 --> 2:09:46.680
 the problem, the challenge, the opportunity of social connection between an AI system

2:09:46.680 --> 2:09:52.720
 and a human being does not require you to also solve the problem of robot manipulation

2:09:52.720 --> 2:09:55.320
 and bipedal mobility.

2:09:55.320 --> 2:09:59.980
 So I think you could do that with just a screen, honestly, but there's something about the

2:09:59.980 --> 2:10:03.880
 interface of Jibo where it can rotate and so on that's also compelling.

2:10:03.880 --> 2:10:09.400
 But you get to see all these robot companies that fail, incredible companies like Jibo

2:10:09.400 --> 2:10:17.600
 and even, I mean, the iRobot in some sense is a big success story that it was able to

2:10:17.600 --> 2:10:24.000
 find a niche thing and focus on it, but in some sense it's not a success story because

2:10:24.000 --> 2:10:30.960
 they didn't build any other robot, like any other, it didn't expand into all kinds of

2:10:30.960 --> 2:10:31.960
 robotics.

2:10:31.960 --> 2:10:34.880
 Like once you're in the home, maybe that's what happens with Amazon is they'll flourish

2:10:34.880 --> 2:10:37.200
 into all kinds of other robots.

2:10:37.200 --> 2:10:43.760
 But do you have a sense, by the way, why it's so difficult to build a robotics company?

2:10:43.760 --> 2:10:47.080
 Like why so many companies have failed?

2:10:47.080 --> 2:10:50.780
 I think it's like you're building a vertical stack, right?

2:10:50.780 --> 2:10:54.480
 Like you are building the hardware plus the software and you find you have to do this

2:10:54.480 --> 2:10:56.040
 at a cost that makes sense.

2:10:56.040 --> 2:11:05.080
 So I think Jibo was retailing at like, I don't know, like $800, like $700, $800, which for

2:11:05.080 --> 2:11:10.020
 the use case, right, there's a dissonance there.

2:11:10.020 --> 2:11:11.020
 It's too high.

2:11:11.020 --> 2:11:20.380
 So I think cost of building the whole platform in a way that is affordable for what value

2:11:20.380 --> 2:11:23.720
 it's bringing, I think that's a challenge.

2:11:23.720 --> 2:11:30.920
 I think for these home robots that are going to help you do stuff around the home, that's

2:11:30.920 --> 2:11:33.400
 a challenge too, like the mobility piece of it.

2:11:33.400 --> 2:11:34.400
 That's hard.

2:11:34.400 --> 2:11:40.480
 Well, one of the things I'm really excited with Tesla Bot is the people working on it.

2:11:40.480 --> 2:11:44.560
 And that's probably the criticism I would apply to some of the other folks who worked

2:11:44.560 --> 2:11:50.200
 on social robots is the people working on Tesla Bot know how to, they're focused on

2:11:50.200 --> 2:11:54.360
 and know how to do mass manufacture and create a product that's super cheap.

2:11:54.360 --> 2:11:55.360
 Very cool.

2:11:55.360 --> 2:11:56.360
 That's the focus.

2:11:56.360 --> 2:12:00.480
 The engineering focus isn't, I would say that you can also criticize them for that, is they're

2:12:00.480 --> 2:12:03.920
 not focused on the experience of the robot.

2:12:03.920 --> 2:12:09.920
 They're focused on how to get this thing to do the basic stuff that the humanoid form

2:12:09.920 --> 2:12:13.560
 requires to do it as cheap as possible.

2:12:13.560 --> 2:12:18.360
 Then the fewest number of actuators, the fewest numbers of motors, the increasing efficiency,

2:12:18.360 --> 2:12:20.400
 they decrease the weight, all that kind of stuff.

2:12:20.400 --> 2:12:21.600
 So that's really interesting.

2:12:21.600 --> 2:12:26.520
 I would say that Jibo and all those folks, they focus on the design, the experience,

2:12:26.520 --> 2:12:29.840
 all of that, and it's secondary how to manufacture.

2:12:29.840 --> 2:12:30.840
 Right.

2:12:30.840 --> 2:12:36.880
 So you have to think like the Tesla Bot folks from first principles, what is the fewest

2:12:36.880 --> 2:12:41.720
 number of components, the cheapest components, how can I build it as much in house as possible

2:12:41.720 --> 2:12:47.680
 without having to consider all the complexities of a supply chain, all that kind of stuff.

2:12:47.680 --> 2:12:48.680
 It's interesting.

2:12:48.680 --> 2:12:54.200
 Because if you have to build a robotics company, you're not building one robot, you're building

2:12:54.200 --> 2:12:58.600
 hopefully millions of robots, you have to figure out how to do that where the final

2:12:58.600 --> 2:13:04.240
 thing, I mean, if it's Jibo type of robot, is there a reason why Jibo, like we can have

2:13:04.240 --> 2:13:08.880
 this lengthy discussion, is there a reason why Jibo has to be over $100?

2:13:08.880 --> 2:13:09.880
 It shouldn't be.

2:13:09.880 --> 2:13:10.880
 Right.

2:13:10.880 --> 2:13:11.880
 Like the basic components.

2:13:11.880 --> 2:13:12.880
 Right.

2:13:12.880 --> 2:13:13.880
 Components of it.

2:13:13.880 --> 2:13:14.880
 Right.

2:13:14.880 --> 2:13:19.080
 Like you could start to actually discuss like, okay, what is the essential thing about Jibo?

2:13:19.080 --> 2:13:21.440
 How much, what is the cheapest way I can have a screen?

2:13:21.440 --> 2:13:23.760
 What's the cheapest way I can have a rotating base?

2:13:23.760 --> 2:13:24.760
 Right.

2:13:24.760 --> 2:13:25.760
 All that kind of stuff.

2:13:25.760 --> 2:13:29.960
 Right, get down, continuously drive down costs.

2:13:29.960 --> 2:13:35.520
 Speaking of which, you have launched an extremely successful companies, you have helped others,

2:13:35.520 --> 2:13:37.920
 you've invested in companies.

2:13:37.920 --> 2:13:44.160
 Can you give advice on how to start a successful company?

2:13:44.160 --> 2:13:48.780
 I would say have a problem that you really, really, really want to solve, right?

2:13:48.780 --> 2:13:53.800
 Something that you're deeply passionate about.

2:13:53.800 --> 2:13:55.880
 And honestly, take the first step.

2:13:55.880 --> 2:13:58.520
 Like that's often the hardest.

2:13:58.520 --> 2:13:59.520
 And don't overthink it.

2:13:59.520 --> 2:14:04.000
 Like, you know, like this idea of a minimum viable product or a minimum viable version

2:14:04.000 --> 2:14:05.000
 of an idea, right?

2:14:05.000 --> 2:14:09.160
 Like, yes, you're thinking about this, like a humongous, like super elegant, super beautiful

2:14:09.160 --> 2:14:10.160
 thing.

2:14:10.160 --> 2:14:14.640
 What, like reduce it to the littlest thing you can bring to market that can solve a problem

2:14:14.640 --> 2:14:20.880
 or that can, you know, that can help address a pain point that somebody has.

2:14:20.880 --> 2:14:24.320
 They often tell you, like, start with a customer of one, right?

2:14:24.320 --> 2:14:28.400
 If you can solve a problem for one person, then there's probably going to be yourself

2:14:28.400 --> 2:14:29.400
 or some other person.

2:14:29.400 --> 2:14:30.400
 Right.

2:14:30.400 --> 2:14:31.400
 Pick a person.

2:14:31.400 --> 2:14:32.400
 Exactly.

2:14:32.400 --> 2:14:33.400
 It could be you.

2:14:33.400 --> 2:14:37.240
 Yeah, that's actually often a good sign that if you enjoy a thing, enjoy a thing where

2:14:37.240 --> 2:14:41.000
 you have a specific problem that you'd like to solve, that's a good, that's a good end

2:14:41.000 --> 2:14:43.600
 of one to focus on.

2:14:43.600 --> 2:14:49.360
 What else, what else is there to actually step one is the hardest, but there's other

2:14:49.360 --> 2:14:51.200
 steps as well, right?

2:14:51.200 --> 2:14:58.080
 I also think like who you bring around the table early on is so key, right?

2:14:58.080 --> 2:15:02.440
 Like being clear on, on what I call like your core values or your North Star.

2:15:02.440 --> 2:15:04.840
 It might sound fluffy, but actually it's not.

2:15:04.840 --> 2:15:08.840
 So and Roz and I feel like we did that very early on.

2:15:08.840 --> 2:15:13.040
 We sat around her kitchen table and we said, okay, there's so many applications of this

2:15:13.040 --> 2:15:14.040
 technology.

2:15:14.040 --> 2:15:15.040
 How are we going to draw the line?

2:15:15.040 --> 2:15:16.940
 How are we going to set boundaries?

2:15:16.940 --> 2:15:22.680
 We came up with a set of core values that in the hardest of times we fell back on to

2:15:22.680 --> 2:15:25.320
 determine how we make decisions.

2:15:25.320 --> 2:15:28.760
 And so I feel like just getting clarity on these core, like for us, it was respecting

2:15:28.760 --> 2:15:33.400
 people's privacy, only engaging with industries where it's clear opt in.

2:15:33.400 --> 2:15:38.680
 So for instance, we don't do any work in security and surveillance.

2:15:38.680 --> 2:15:42.480
 So things like that, just getting, we very big on, you know, one of our core values is

2:15:42.480 --> 2:15:44.720
 human connection and empathy, right?

2:15:44.720 --> 2:15:47.840
 And that is, yes, it's an AI company, but it's about people.

2:15:47.840 --> 2:15:54.520
 Well, these are all, they become encoded in how we act, even if you're a small, tiny team

2:15:54.520 --> 2:15:57.460
 of two or three or whatever.

2:15:57.460 --> 2:15:59.520
 So I think that's another piece of advice.

2:15:59.520 --> 2:16:02.680
 So what about finding people, hiring people?

2:16:02.680 --> 2:16:07.800
 If you care about people as much as you do, like this, it seems like such a difficult

2:16:07.800 --> 2:16:10.680
 thing to hire the right people.

2:16:10.680 --> 2:16:16.120
 I think early on as a startup, you want people who have, who share the passion and the conviction

2:16:16.120 --> 2:16:17.880
 because it's going to be tough.

2:16:17.880 --> 2:16:25.000
 Like I've yet to meet a startup where it was just a straight line to success, right?

2:16:25.000 --> 2:16:28.280
 Even not just startup, like even everyday people's lives, right?

2:16:28.280 --> 2:16:36.280
 You always like run into obstacles and you run into naysayers and you need people who

2:16:36.280 --> 2:16:40.600
 are believers, whether they're people on your team or even your investors.

2:16:40.600 --> 2:16:44.960
 You need investors who are really believers in what you're doing, because that means they

2:16:44.960 --> 2:16:47.040
 will stick with you.

2:16:47.040 --> 2:16:49.280
 They won't give up at the first obstacle.

2:16:49.280 --> 2:16:50.920
 I think that's important.

2:16:50.920 --> 2:16:51.920
 What about raising money?

2:16:51.920 --> 2:16:59.720
 What about finding investors, first of all, raising money, but also raising money from

2:16:59.720 --> 2:17:05.960
 the right sources from that ultimately don't hinder you, but help you, empower you, all

2:17:05.960 --> 2:17:06.960
 that kind of stuff.

2:17:06.960 --> 2:17:08.600
 What advice would you give there?

2:17:08.600 --> 2:17:12.120
 You successfully raised money many times in your life.

2:17:12.120 --> 2:17:13.120
 Yeah.

2:17:13.120 --> 2:17:15.080
 Again, it's not just about the money.

2:17:15.080 --> 2:17:20.360
 It's about finding the right investors who are going to be aligned in terms of what you

2:17:20.360 --> 2:17:23.160
 want to build and believe in your core values.

2:17:23.160 --> 2:17:31.280
 For example, especially later on, in my latest round of funding, I try to bring in investors

2:17:31.280 --> 2:17:40.120
 that really care about the ethics of AI and the alignment of vision and mission and core

2:17:40.120 --> 2:17:41.120
 values is really important.

2:17:41.120 --> 2:17:43.920
 It's like you're picking a life partner.

2:17:43.920 --> 2:17:45.160
 It's the same kind of...

2:17:45.160 --> 2:17:47.560
 So you take it that seriously for investors?

2:17:47.560 --> 2:17:50.040
 Yeah, because they're going to have to stick with you.

2:17:50.040 --> 2:17:51.480
 You're stuck together.

2:17:51.480 --> 2:17:52.480
 For a while anyway.

2:17:52.480 --> 2:17:53.480
 Yeah.

2:17:53.480 --> 2:17:56.880
 Maybe not for life, but for a while, for sure.

2:17:56.880 --> 2:17:57.880
 For better or worse.

2:17:57.880 --> 2:17:59.920
 I forget what the vowels usually sound like.

2:17:59.920 --> 2:18:00.920
 For better or worse?

2:18:00.920 --> 2:18:01.920
 Through something.

2:18:01.920 --> 2:18:02.920
 Yeah.

2:18:02.920 --> 2:18:03.920
 Oh boy.

2:18:03.920 --> 2:18:04.920
 Yeah.

2:18:04.920 --> 2:18:15.320
 Anyway, it's romantic and deep and you're in it for a while.

2:18:15.320 --> 2:18:18.040
 So it's not just about the money.

2:18:18.040 --> 2:18:23.560
 You tweeted about going to your first capital camp investing get together and that you learned

2:18:23.560 --> 2:18:24.560
 a lot.

2:18:24.560 --> 2:18:27.840
 So this is about investing.

2:18:27.840 --> 2:18:30.240
 So what have you learned from that?

2:18:30.240 --> 2:18:34.160
 What have you learned about investing in general from both because you've been on both ends

2:18:34.160 --> 2:18:35.160
 of it?

2:18:35.160 --> 2:18:41.720
 I mean, I try to use my experience as an operator now with my investor hat on when I'm identifying

2:18:41.720 --> 2:18:45.280
 companies to invest in.

2:18:45.280 --> 2:18:49.460
 First of all, I think the good news is because I have a technology background and I really

2:18:49.460 --> 2:18:54.600
 understand machine learning and computer vision and AI, et cetera, I can apply that level

2:18:54.600 --> 2:18:59.720
 of understanding because everybody says they're an AI company or they're an AI tech.

2:18:59.720 --> 2:19:02.880
 And I'm like, no, no, no, no, no, show me the technology.

2:19:02.880 --> 2:19:07.640
 So I can do that level of diligence, which I actually love.

2:19:07.640 --> 2:19:12.760
 And then I have to do the litmus test of, if I'm in a conversation with you, am I excited

2:19:12.760 --> 2:19:16.520
 to tell you about this new company that I just met?

2:19:16.520 --> 2:19:22.400
 And if I'm an ambassador for that company and I'm passionate about what they're doing,

2:19:22.400 --> 2:19:24.720
 I usually use that.

2:19:24.720 --> 2:19:25.720
 Yeah.

2:19:25.720 --> 2:19:27.720
 That's important to me when I'm investing.

2:19:27.720 --> 2:19:34.720
 So that means you actually can explain what they're doing and you're excited about it.

2:19:34.720 --> 2:19:35.720
 Exactly.

2:19:35.720 --> 2:19:36.720
 Exactly.

2:19:36.720 --> 2:19:41.720
 Thank you for putting it so succinctly, like rambling, but exactly that's it.

2:19:41.720 --> 2:19:48.280
 No, but sometimes it's funny, but sometimes it's unclear exactly.

2:19:48.280 --> 2:19:53.120
 I'll hear people tell me, you know, in the talk for a while and it sounds cool, like

2:19:53.120 --> 2:19:56.600
 they paint a picture of a world, but then when you try to summarize it, you're not

2:19:56.600 --> 2:19:57.600
 exactly clear.

2:19:57.600 --> 2:20:05.200
 Like maybe what the core powerful idea is, like you can't just build another Facebook

2:20:05.200 --> 2:20:15.360
 or there has to be a core, simple to explain idea that then you can or can't get excited

2:20:15.360 --> 2:20:19.000
 about, but it's there, it's right there.

2:20:19.000 --> 2:20:20.000
 Yeah.

2:20:20.000 --> 2:20:25.520
 But how do you ultimately pick who you think will be successful?

2:20:25.520 --> 2:20:29.320
 It's not just about the thing you're excited about, like there's other stuff.

2:20:29.320 --> 2:20:30.320
 Right.

2:20:30.320 --> 2:20:34.400
 And then there's all the, you know, with early stage companies, like pre seed companies,

2:20:34.400 --> 2:20:40.640
 which is where I'm investing, sometimes the business model isn't clear yet, or the go

2:20:40.640 --> 2:20:42.240
 to market strategy isn't clear.

2:20:42.240 --> 2:20:45.560
 There's usually like, it's very early on that some of these things haven't been hashed

2:20:45.560 --> 2:20:47.840
 out, which is okay.

2:20:47.840 --> 2:20:51.720
 So the way I like to think about it is like, if this company is successful, will this be

2:20:51.720 --> 2:20:56.660
 a multi billion slash trillion dollar market, you know, or company?

2:20:56.660 --> 2:21:01.280
 And so that's definitely a lens that I use.

2:21:01.280 --> 2:21:02.280
 What's pre seed?

2:21:02.280 --> 2:21:07.880
 What are the different stages and what's the most exciting stage and what's, or no, what's

2:21:07.880 --> 2:21:09.680
 interesting about every stage, I guess.

2:21:09.680 --> 2:21:10.680
 Yeah.

2:21:10.680 --> 2:21:16.000
 So pre seed is usually when you're just starting out, you've maybe raised the friends and family

2:21:16.000 --> 2:21:17.000
 rounds.

2:21:17.000 --> 2:21:20.720
 So you've raised some money from people, you know, and you're getting ready to take your

2:21:20.720 --> 2:21:25.680
 first institutional check in, like first check from an investor.

2:21:25.680 --> 2:21:28.920
 And I love the stage.

2:21:28.920 --> 2:21:30.780
 There's a lot of uncertainty.

2:21:30.780 --> 2:21:36.760
 Some investors really don't like the stage because the financial models aren't there.

2:21:36.760 --> 2:21:40.920
 Often the teams aren't even like formed really, really early.

2:21:40.920 --> 2:21:48.480
 But to me, it's like a magical stage because it's the time when there's so much conviction,

2:21:48.480 --> 2:21:51.800
 so much belief, almost delusional, right?

2:21:51.800 --> 2:21:57.120
 And there's a little bit of naivete around with founders at the stage.

2:21:57.120 --> 2:21:58.120
 I just love it.

2:21:58.120 --> 2:21:59.120
 It's contagious.

2:21:59.120 --> 2:22:06.560
 And I love that I can, often they're first time founders, not always, but often they're

2:22:06.560 --> 2:22:12.620
 first time founders and I can share my experience as a founder myself and I can empathize, right?

2:22:12.620 --> 2:22:18.520
 And I can almost, I create a safe ground where, because, you know, you have to be careful

2:22:18.520 --> 2:22:21.200
 what you tell your investors, right?

2:22:21.200 --> 2:22:24.800
 And I will often like say, I've been in your shoes as a founder.

2:22:24.800 --> 2:22:28.360
 You can tell me if it's challenging, you can tell me what you're struggling with.

2:22:28.360 --> 2:22:30.160
 It's okay to vent.

2:22:30.160 --> 2:22:34.760
 So I create that safe ground and I think that's a superpower.

2:22:34.760 --> 2:22:35.760
 Yeah.

2:22:35.760 --> 2:22:40.400
 You have to, I guess you have to figure out if this kind of person is going to be able

2:22:40.400 --> 2:22:48.280
 to ride the roller coaster, like of many pivots and challenges and all that kind of stuff.

2:22:48.280 --> 2:22:53.040
 And if the space of ideas they're working in is interesting, like the way they think

2:22:53.040 --> 2:22:54.040
 about the world.

2:22:54.040 --> 2:22:55.040
 Yeah.

2:22:55.040 --> 2:23:00.000
 Because if it's successful, the thing they end up with might be very different, the reason

2:23:00.000 --> 2:23:01.560
 it's successful for them.

2:23:01.560 --> 2:23:07.480
 Actually, you know, I was going to say the third, so the technology is one aspect, the

2:23:07.480 --> 2:23:11.240
 market or the idea, right, is the second and the third is the founder, right?

2:23:11.240 --> 2:23:18.160
 Is this somebody who I believe has conviction, is a hustler, you know, is going to overcome

2:23:18.160 --> 2:23:19.160
 obstacles?

2:23:19.160 --> 2:23:23.200
 Yeah, I think that is going to be a great leader, right?

2:23:23.200 --> 2:23:28.440
 Like as a startup, as a founder, you're often, you are the first person and your role is

2:23:28.440 --> 2:23:32.880
 to bring amazing people around you to build this thing.

2:23:32.880 --> 2:23:36.160
 And so you're an evangelist, right?

2:23:36.160 --> 2:23:38.000
 So how good are you going to be at that?

2:23:38.000 --> 2:23:41.360
 So I try to evaluate that too.

2:23:41.360 --> 2:23:46.960
 You also in the tweet thread about it, mention, is this a known concept, random rich dudes

2:23:46.960 --> 2:23:53.280
 are RDS and saying that there should be like random rich women, I guess.

2:23:53.280 --> 2:23:58.280
 What's the dudes, what's the dudes version of women, the women version of dudes, ladies?

2:23:58.280 --> 2:23:59.280
 I don't know.

2:23:59.280 --> 2:24:00.280
 I don't know.

2:24:00.280 --> 2:24:01.500
 What's, what's, is this a technical term?

2:24:01.500 --> 2:24:02.500
 Is this known?

2:24:02.500 --> 2:24:03.500
 Random rich dudes?

2:24:03.500 --> 2:24:09.680
 I didn't make that up, but I was at this capital camp, which is a get together for investors

2:24:09.680 --> 2:24:11.600
 of all types.

2:24:11.600 --> 2:24:19.160
 And there must have been maybe 400 or so attendees, maybe 20 were women.

2:24:19.160 --> 2:24:25.680
 It was just very disproportionately, you know, male dominated, which I'm used to.

2:24:25.680 --> 2:24:26.960
 I think you're used to this kind of thing.

2:24:26.960 --> 2:24:29.920
 I'm used to it, but it's still surprising.

2:24:29.920 --> 2:24:36.320
 And as I'm raising money for this fund, so my fund partner is a guy called Rob May, who's

2:24:36.320 --> 2:24:37.680
 done this before.

2:24:37.680 --> 2:24:42.000
 So I'm new to the investing world, but he's done this before.

2:24:42.000 --> 2:24:45.880
 Most of our investors in the fund are these, I mean, awesome.

2:24:45.880 --> 2:24:47.800
 I'm super grateful to them.

2:24:47.800 --> 2:24:48.800
 Random just rich guys.

2:24:48.800 --> 2:24:50.400
 I'm like, where are the rich women?

2:24:50.400 --> 2:24:57.160
 So I'm really adamant in both investing in women led AI companies, but I also would love

2:24:57.160 --> 2:25:03.240
 to have women investors be part of my fund because I think that's how we drive change.

2:25:03.240 --> 2:25:04.240
 Yeah.

2:25:04.240 --> 2:25:09.640
 So that takes time, of course, but there's been quite a lot of progress, but yeah, for

2:25:09.640 --> 2:25:13.840
 the next Mark Zuckerberg to be a woman and all that kind of stuff, because that's just

2:25:13.840 --> 2:25:19.760
 like a huge number of wealth generated by women and then controlled by women and allocated

2:25:19.760 --> 2:25:22.200
 by women and all that kind of stuff.

2:25:22.200 --> 2:25:28.880
 And then beyond just women, just broadly across all different measures of diversity and so

2:25:28.880 --> 2:25:29.880
 on.

2:25:29.880 --> 2:25:35.880
 Let me ask you to put on your wise sage hat.

2:25:35.880 --> 2:25:45.120
 So you already gave advice on startups and just advice for women, but in general advice

2:25:45.120 --> 2:25:51.080
 for folks in high school or college today, how to have a career they can be proud of,

2:25:51.080 --> 2:25:55.560
 how to have a life they can be proud of.

2:25:55.560 --> 2:25:58.560
 I suppose you have to give this kind of advice to your kids.

2:25:58.560 --> 2:25:59.560
 Yeah.

2:25:59.560 --> 2:26:03.400
 Well, here's the number one advice that I give to my kids.

2:26:03.400 --> 2:26:08.200
 My daughter's now 19 by the way, and my son's 13 and a half, so they're not little kids

2:26:08.200 --> 2:26:09.200
 anymore.

2:26:09.200 --> 2:26:11.560
 Does it break your heart?

2:26:11.560 --> 2:26:12.560
 It does.

2:26:12.560 --> 2:26:13.560
 They're awesome.

2:26:13.560 --> 2:26:19.880
 They're my best friends, but yeah, I think the number one advice I would share is embark

2:26:19.880 --> 2:26:25.200
 on a journey without attaching to outcomes and enjoy the journey, right?

2:26:25.200 --> 2:26:34.360
 So we often were so obsessed with the end goal that doesn't allow us to be open to different

2:26:34.360 --> 2:26:41.840
 endings of a journey or a story, so you become like so fixated on a particular path.

2:26:41.840 --> 2:26:48.520
 You don't see the beauty in the other alternative path, and then you forget to enjoy the journey

2:26:48.520 --> 2:26:53.320
 because you're just so fixated on the goal, and I've been guilty of that for many, many

2:26:53.320 --> 2:27:00.180
 years of my life, and I'm now trying to make the shift of, no, no, no, I'm going to again

2:27:00.180 --> 2:27:04.480
 trust that things are going to work out and it'll be amazing and maybe even exceed your

2:27:04.480 --> 2:27:05.480
 dreams.

2:27:05.480 --> 2:27:07.280
 We have to be open to that.

2:27:07.280 --> 2:27:08.280
 Yeah.

2:27:08.280 --> 2:27:09.840
 Taking a leap into all kinds of things.

2:27:09.840 --> 2:27:13.800
 I think you tweeted like you went on vacation by yourself or something like this.

2:27:13.800 --> 2:27:14.800
 I know.

2:27:14.800 --> 2:27:19.120
 Yes, and just going, just taking the leap.

2:27:19.120 --> 2:27:20.120
 Doing it.

2:27:20.120 --> 2:27:21.120
 Totally doing it.

2:27:21.120 --> 2:27:26.720
 And enjoying it, enjoying the moment, enjoying the weeks, enjoying not looking at some kind

2:27:26.720 --> 2:27:29.640
 of career ladder, next step and so on.

2:27:29.640 --> 2:27:34.320
 Yeah, there's something to that, like over planning too.

2:27:34.320 --> 2:27:37.800
 I'm surrounded by a lot of people that kind of, so I don't plan.

2:27:37.800 --> 2:27:38.800
 You don't?

2:27:38.800 --> 2:27:39.800
 No.

2:27:39.800 --> 2:27:43.240
 Do you not do goal setting?

2:27:43.240 --> 2:27:52.760
 My goal setting is very like, I like the affirmations, it's very, it's almost, I don't know how to

2:27:52.760 --> 2:28:02.040
 put it into words, but it's a little bit like what my heart yearns for kind of, and I guess

2:28:02.040 --> 2:28:08.640
 in the space of emotions more than in the space of like, this will be like in the rational

2:28:08.640 --> 2:28:16.280
 space because I just try to picture a world that I would like to be in and that world

2:28:16.280 --> 2:28:19.400
 is not clearly pictured, it's mostly in the emotional world.

2:28:19.400 --> 2:28:26.640
 I mean, I think about that from robots because I have this desire, I've had it my whole life

2:28:26.640 --> 2:28:33.180
 to, well, it took different shapes, but I think once I discovered AI, the desire was

2:28:33.180 --> 2:28:41.120
 to, I think in the context of this conversation could be easily easier described as basically

2:28:41.120 --> 2:28:50.880
 a social robotics company and that's something I dreamed of doing and well, there's a lot

2:28:50.880 --> 2:28:55.680
 of complexity to that story, but that's the only thing, honestly, I dream of doing.

2:28:55.680 --> 2:29:05.560
 So I imagine a world that I could help create, but it's not, there's no steps along the way

2:29:05.560 --> 2:29:12.720
 and I think I'm just kind of stumbling around and following happiness and working my ass

2:29:12.720 --> 2:29:18.240
 off in almost random, like an ant does in random directions, but a lot of people, a

2:29:18.240 --> 2:29:20.920
 lot of successful people around me say this, you should have a plan, you should have a

2:29:20.920 --> 2:29:23.960
 clear goal, you have a goal at the end of the month, you have a goal at the end of the

2:29:23.960 --> 2:29:33.320
 month, I don't, I don't, I don't and there's a balance to be struck, of course, but there's

2:29:33.320 --> 2:29:40.360
 something to be said about really making sure that you're living life to the fullest, that

2:29:40.360 --> 2:29:43.760
 goals can actually get in the way of.

2:29:43.760 --> 2:29:52.560
 So one of the best, like kind of most, what do you call it when it challenges your brain,

2:29:52.560 --> 2:29:56.760
 what do you call it?

2:29:56.760 --> 2:30:00.320
 The only thing that comes to mind, and this is me saying is the mindfuck, but yes.

2:30:00.320 --> 2:30:01.320
 Okay.

2:30:01.320 --> 2:30:02.320
 Okay.

2:30:02.320 --> 2:30:03.320
 Okay.

2:30:03.320 --> 2:30:04.320
 Something like that.

2:30:04.320 --> 2:30:05.320
 Yes.

2:30:05.320 --> 2:30:06.320
 Super inspiring talk.

2:30:06.320 --> 2:30:11.320
 Kenneth Stanley, he was at OpenAI, he just laughed and he has a book called Why Greatness

2:30:11.320 --> 2:30:14.160
 Can't Be Planned and it's actually an AI book.

2:30:14.160 --> 2:30:20.400
 So and he's done all these experiments that basically show that when you over optimize,

2:30:20.400 --> 2:30:23.760
 you, like the trade off is you're less creative, right?

2:30:23.760 --> 2:30:30.760
 And to create true greatness and truly creative solutions to problems, you can't over plan

2:30:30.760 --> 2:30:31.760
 it.

2:30:31.760 --> 2:30:32.760
 You can't.

2:30:32.760 --> 2:30:36.800
 And I thought that was, and so he generalizes it beyond AI and he talks about how we apply

2:30:36.800 --> 2:30:42.320
 that in our personal life and in our organizations and our companies, which are over KPIs, right?

2:30:42.320 --> 2:30:45.960
 Like look at any company in the world and it's all like, these aren't the goals, these

2:30:45.960 --> 2:30:51.080
 aren't weekly goals and the sprints and then the quarterly goals, blah, blah, blah.

2:30:51.080 --> 2:30:58.560
 And he just shows with a lot of his AI experiments that that's not how you create truly game

2:30:58.560 --> 2:30:59.640
 changing ideas.

2:30:59.640 --> 2:31:00.640
 So there you go.

2:31:00.640 --> 2:31:01.640
 Yeah, yeah.

2:31:01.640 --> 2:31:02.640
 You can.

2:31:02.640 --> 2:31:03.640
 He's awesome.

2:31:03.640 --> 2:31:04.640
 Yeah.

2:31:04.640 --> 2:31:05.640
 There's a balance of course.

2:31:05.640 --> 2:31:11.660
 That's yeah, many moments of genius will not come from planning and goals, but you still

2:31:11.660 --> 2:31:15.200
 have to build factories and you still have to manufacture and you still have to deliver

2:31:15.200 --> 2:31:17.280
 and there's still deadlines and all that kind of stuff.

2:31:17.280 --> 2:31:19.280
 And that for that, it's good to have goals.

2:31:19.280 --> 2:31:25.200
 I do goal setting with my kids, we all have our goals, but, but, but I think we're starting

2:31:25.200 --> 2:31:30.800
 to morph into more of these like bigger picture goals and not obsess about like, I don't know,

2:31:30.800 --> 2:31:31.800
 it's hard.

2:31:31.800 --> 2:31:34.840
 Well, I honestly think with, especially with kids, it's better, much, much better to have

2:31:34.840 --> 2:31:38.400
 a plan and have goals and so on because you have to, you have to learn the muscle of like

2:31:38.400 --> 2:31:40.480
 what it feels like to get stuff done.

2:31:40.480 --> 2:31:41.480
 Yeah.

2:31:41.480 --> 2:31:46.720
 And once you learn that, there's flexibility for me because I spend most of my life with

2:31:46.720 --> 2:31:48.040
 goal setting and so on.

2:31:48.040 --> 2:31:50.560
 So like I've gotten good with grades and school.

2:31:50.560 --> 2:31:55.040
 I mean, school, if you want to be successful at school, yeah, I mean the kind of stuff

2:31:55.040 --> 2:31:59.280
 in high school and college, the kids have to do in terms of managing their time and

2:31:59.280 --> 2:32:01.160
 getting so much stuff done.

2:32:01.160 --> 2:32:06.500
 It's like, you know, taking five, six, seven classes in college, they're like that would

2:32:06.500 --> 2:32:13.200
 break the spirit of most humans if they took one of them later in life, it's like really

2:32:13.200 --> 2:32:16.560
 difficult stuff, especially engineering curricula.

2:32:16.560 --> 2:32:22.680
 So I think you have to learn that skill, but once you learn it, you can maybe, cause you're,

2:32:22.680 --> 2:32:27.280
 you can be a little bit on autopilot and use that momentum and then allow yourself to be

2:32:27.280 --> 2:32:28.920
 lost in the flow of life.

2:32:28.920 --> 2:32:38.760
 You know, just kind of, or also give like, I worked pretty hard to allow myself to have

2:32:38.760 --> 2:32:39.760
 the freedom to do that.

2:32:39.760 --> 2:32:44.600
 That's really, that's a tricky freedom to have because like a lot of people get lost

2:32:44.600 --> 2:32:52.920
 in the rat race and they, and they also like financially, they, whenever you get a raise,

2:32:52.920 --> 2:32:55.680
 they'll get like a bigger house or something like this.

2:32:55.680 --> 2:32:59.720
 I put very, so like, there's, you're always trapped in this race, I put a lot of emphasis

2:32:59.720 --> 2:33:05.240
 on living like below my means always.

2:33:05.240 --> 2:33:12.240
 And so there's a lot of freedom to do whatever, whatever the heart desires that that's a relief,

2:33:12.240 --> 2:33:15.580
 but everyone has to decide what's the right thing, what's the right thing for them.

2:33:15.580 --> 2:33:21.560
 For some people having a lot of responsibilities, like a house they can barely afford or having

2:33:21.560 --> 2:33:27.600
 a lot of kids, the responsibility side of that is really, helps them get their shit

2:33:27.600 --> 2:33:28.600
 together.

2:33:28.600 --> 2:33:32.080
 Like, all right, I need to be really focused and get, some of the most successful people

2:33:32.080 --> 2:33:34.760
 I know have kids and the kids bring out the best in them.

2:33:34.760 --> 2:33:36.720
 They make them more productive and less productive.

2:33:36.720 --> 2:33:37.720
 Right, it's accountability.

2:33:37.720 --> 2:33:38.720
 Yeah.

2:33:38.720 --> 2:33:39.720
 It's an accountability thing, absolutely.

2:33:39.720 --> 2:33:45.400
 And almost something to actually live and fight and work for, like having a family,

2:33:45.400 --> 2:33:49.520
 it's fascinating to see because you would think kids would be a hit on productivity,

2:33:49.520 --> 2:33:53.680
 but they're not, for a lot of really successful people, they really like, they're like an

2:33:53.680 --> 2:33:54.680
 engine of.

2:33:54.680 --> 2:33:55.680
 Right, efficiency.

2:33:55.680 --> 2:33:56.680
 Oh my God.

2:33:56.680 --> 2:33:57.680
 Yeah.

2:33:57.680 --> 2:33:58.680
 Yeah.

2:33:58.680 --> 2:33:59.680
 It's weird.

2:33:59.680 --> 2:34:00.680
 Yeah.

2:34:00.680 --> 2:34:01.680
 I mean, it's beautiful.

2:34:01.680 --> 2:34:02.680
 It's beautiful to see.

2:34:02.680 --> 2:34:03.680
 And also a source of happiness.

2:34:03.680 --> 2:34:12.080
 Speaking of which, what role do you think love plays in the human condition, love?

2:34:12.080 --> 2:34:19.880
 I think love is, yeah, I think it's why we're all here.

2:34:19.880 --> 2:34:26.640
 I think it would be very hard to live life without love in any of its forms, right?

2:34:26.640 --> 2:34:35.080
 Yeah, that's the most beautiful of forms that human connection takes, right?

2:34:35.080 --> 2:34:36.080
 Yeah.

2:34:36.080 --> 2:34:42.200
 And everybody wants to feel loved, right, in one way or another, right?

2:34:42.200 --> 2:34:43.200
 And to love.

2:34:43.200 --> 2:34:44.200
 Yeah.

2:34:44.200 --> 2:34:45.200
 It feels good.

2:34:45.200 --> 2:34:46.200
 And to love too, totally.

2:34:46.200 --> 2:34:47.200
 Yeah, I agree with that.

2:34:47.200 --> 2:34:48.200
 Both of it.

2:34:48.200 --> 2:34:49.200
 Yeah.

2:34:49.200 --> 2:34:50.200
 I'm not even sure what feels better.

2:34:50.200 --> 2:34:51.200
 Both, both like that.

2:34:51.200 --> 2:34:54.760
 Yeah, to give and to give love too, yeah.

2:34:54.760 --> 2:34:59.680
 And it is like we've been talking about an interesting question, whether some of that,

2:34:59.680 --> 2:35:02.640
 whether one day we'll be able to love a toaster.

2:35:02.640 --> 2:35:03.640
 Okay.

2:35:03.640 --> 2:35:05.240
 It's some small.

2:35:05.240 --> 2:35:10.320
 I wasn't quite thinking about that when I said like, yeah, like we all need love and

2:35:10.320 --> 2:35:11.320
 give love.

2:35:11.320 --> 2:35:12.320
 That's all I was thinking about.

2:35:12.320 --> 2:35:13.320
 Okay.

2:35:13.320 --> 2:35:14.320
 I was thinking about Brad Pitt and toasters.

2:35:14.320 --> 2:35:15.320
 Okay, toasters, great.

2:35:15.320 --> 2:35:16.320
 All right.

2:35:16.320 --> 2:35:20.200
 Well, I think we started on love and ended on love.

2:35:20.200 --> 2:35:22.320
 This was an incredible conversation, Rhonda.

2:35:22.320 --> 2:35:23.320
 Thank you so much.

2:35:23.320 --> 2:35:24.320
 Thank you.

2:35:24.320 --> 2:35:25.320
 You're an incredible person.

2:35:25.320 --> 2:35:32.960
 Thank you for everything you're doing in AI, in the space of just caring about humanity,

2:35:32.960 --> 2:35:38.160
 caring about emotion, about love, and being an inspiration to a huge number of people

2:35:38.160 --> 2:35:42.320
 in robotics, in AI, in science, in the world in general.

2:35:42.320 --> 2:35:43.320
 So thank you for talking to me.

2:35:43.320 --> 2:35:44.320
 It's an honor.

2:35:44.320 --> 2:35:45.320
 Thank you for having me.

2:35:45.320 --> 2:35:47.320
 And you know, I'm a big fan of yours as well.

2:35:47.320 --> 2:35:49.740
 So it's been a pleasure.

2:35:49.740 --> 2:35:52.840
 Thanks for listening to this conversation with Rhonda Alkalioubi.

2:35:52.840 --> 2:35:56.940
 To support this podcast, please check out our sponsors in the description.

2:35:56.940 --> 2:36:00.680
 And now let me leave you with some words from Helen Keller.

2:36:00.680 --> 2:36:05.480
 The best and most beautiful things in the world cannot be seen or even touched.

2:36:05.480 --> 2:36:09.440
 They must be felt with the heart.

2:36:09.440 --> 2:36:31.560
 Thank you for listening and hope to see you next time.

