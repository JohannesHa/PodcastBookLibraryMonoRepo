WEBVTT

00:00.000 --> 00:02.440
 The following is a conversation with George Hotz,

00:02.440 --> 00:06.520
 AKA Geohot, his second time on the podcast.

00:06.520 --> 00:09.320
 He's the founder of Comma AI,

00:09.320 --> 00:12.880
 an autonomous and semi autonomous vehicle technology company

00:12.880 --> 00:15.840
 that seeks to be to Tesla Autopilot

00:15.840 --> 00:18.920
 what Android is to the iOS.

00:18.920 --> 00:22.760
 They sell the Comma 2 device for $1,000

00:22.760 --> 00:25.600
 that when installed in many of their supported cars

00:25.600 --> 00:27.960
 can keep the vehicle centered in the lane

00:27.960 --> 00:30.760
 even when there are no lane markings.

00:30.760 --> 00:32.400
 It includes driver sensing

00:32.400 --> 00:35.640
 that ensures that the driver's eyes are on the road.

00:35.640 --> 00:38.440
 As you may know, I'm a big fan of driver sensing.

00:38.440 --> 00:40.560
 I do believe Tesla Autopilot and others

00:40.560 --> 00:43.520
 should definitely include it in their sensor suite.

00:43.520 --> 00:47.120
 Also, I'm a fan of Android and a big fan of George

00:47.120 --> 00:48.240
 for many reasons,

00:48.240 --> 00:51.640
 including his nonlinear out of the box brilliance

00:51.640 --> 00:55.120
 and the fact that he's a superstar programmer

00:55.120 --> 00:57.360
 of a very different style than myself.

00:57.360 --> 01:01.160
 Styles make fights and styles make conversations.

01:01.160 --> 01:02.920
 So I really enjoyed this chat

01:02.920 --> 01:06.280
 and I'm sure we'll talk many more times on this podcast.

01:06.280 --> 01:07.680
 Quick mention of a sponsor

01:07.680 --> 01:10.160
 followed by some thoughts related to the episode.

01:10.160 --> 01:12.240
 First is Four Sigmatic,

01:12.240 --> 01:15.360
 the maker of delicious mushroom coffee.

01:15.360 --> 01:17.520
 Second is The Coding Digital,

01:17.520 --> 01:19.760
 a podcast on tech and entrepreneurship

01:19.760 --> 01:22.080
 that I listen to and enjoy.

01:22.080 --> 01:24.520
 And finally, ExpressVPN,

01:24.520 --> 01:27.800
 the VPN I've used for many years to protect my privacy

01:27.800 --> 01:29.320
 on the internet.

01:29.320 --> 01:31.280
 Please check out the sponsors in the description

01:31.280 --> 01:34.840
 to get a discount and to support this podcast.

01:34.840 --> 01:38.080
 As a side note, let me say that my work at MIT

01:38.080 --> 01:40.480
 on autonomous and semi autonomous vehicles

01:40.480 --> 01:43.080
 led me to study the human side of autonomy

01:43.080 --> 01:46.600
 enough to understand that it's a beautifully complicated

01:46.600 --> 01:48.600
 and interesting problem space,

01:48.600 --> 01:51.800
 much richer than what can be studied in the lab.

01:51.800 --> 01:55.360
 In that sense, the data that Comma AI, Tesla Autopilot

01:55.360 --> 01:58.480
 and perhaps others like Cadillac Super Cruiser collecting

01:58.480 --> 02:00.560
 gives us a chance to understand

02:00.560 --> 02:03.800
 how we can design safe semi autonomous vehicles

02:03.800 --> 02:07.560
 for real human beings in real world conditions.

02:07.560 --> 02:09.920
 I think this requires bold innovation

02:09.920 --> 02:13.000
 and a serious exploration of the first principles

02:13.000 --> 02:15.640
 of the driving task itself.

02:15.640 --> 02:17.880
 If you enjoyed this thing, subscribe on YouTube,

02:17.880 --> 02:20.160
 review it with five stars and up a podcast,

02:20.160 --> 02:22.760
 follow on Spotify, support on Patreon

02:22.760 --> 02:26.280
 or connect with me on Twitter at Lex Friedman.

02:26.280 --> 02:30.400
 And now here's my conversation with George Hotz.

02:31.360 --> 02:34.040
 So last time we started talking about the simulation,

02:34.040 --> 02:35.640
 this time let me ask you,

02:35.640 --> 02:37.480
 do you think there's intelligent life out there

02:37.480 --> 02:38.600
 in the universe?

02:38.600 --> 02:41.640
 I've always maintained my answer to the Fermi paradox.

02:41.640 --> 02:44.440
 I think there has been intelligent life

02:44.440 --> 02:45.880
 elsewhere in the universe.

02:45.880 --> 02:47.920
 So intelligent civilizations existed

02:47.920 --> 02:49.240
 but they've blown themselves up.

02:49.240 --> 02:50.760
 So your general intuition is that

02:50.760 --> 02:54.560
 intelligent civilizations quickly,

02:54.560 --> 02:57.760
 like there's that parameter in the Drake equation.

02:57.760 --> 02:59.640
 Your sense is they don't last very long.

02:59.640 --> 03:00.480
 Yeah.

03:00.480 --> 03:01.560
 How are we doing on that?

03:01.560 --> 03:03.680
 Like, have we lasted pretty good?

03:03.680 --> 03:04.520
 Oh no.

03:04.520 --> 03:05.360
 Are we do?

03:05.360 --> 03:06.200
 Oh yeah.

03:06.200 --> 03:08.120
 I mean, not quite yet.

03:09.280 --> 03:10.440
 Well, it was good to tell you,

03:10.440 --> 03:13.520
 as you'd ask the IQ required to destroy the world

03:13.520 --> 03:15.440
 falls by one point every year.

03:15.440 --> 03:16.280
 Okay.

03:16.280 --> 03:21.120
 Technology democratizes the destruction of the world.

03:21.120 --> 03:23.120
 When can a meme destroy the world?

03:23.120 --> 03:27.280
 It kind of is already, right?

03:27.280 --> 03:28.480
 Somewhat.

03:28.480 --> 03:32.240
 I don't think we've seen anywhere near the worst of it yet.

03:32.240 --> 03:34.000
 Well, it's going to get weird.

03:34.000 --> 03:36.480
 Well, maybe a meme can save the world.

03:36.480 --> 03:37.480
 You thought about that?

03:37.480 --> 03:40.800
 The meme Lord Elon Musk fighting on the side of good

03:40.800 --> 03:44.560
 versus the meme Lord of the darkness,

03:44.560 --> 03:48.280
 which is not saying anything bad about Donald Trump,

03:48.280 --> 03:51.720
 but he is the Lord of the meme on the dark side.

03:51.720 --> 03:53.760
 He's a Darth Vader of memes.

03:53.760 --> 03:58.360
 I think in every fairy tale they always end it with,

03:58.360 --> 03:59.920
 and they lived happily ever after.

03:59.920 --> 04:00.960
 And I'm like, please tell me more

04:00.960 --> 04:02.440
 about this happily ever after.

04:02.440 --> 04:05.880
 I've heard 50% of marriages end in divorce.

04:05.880 --> 04:07.840
 Why doesn't your marriage end up there?

04:07.840 --> 04:09.280
 You can't just say happily ever after.

04:09.280 --> 04:12.280
 So it's the thing about destruction

04:12.280 --> 04:14.840
 is it's over after the destruction.

04:14.840 --> 04:18.160
 We have to do everything right in order to avoid it.

04:18.160 --> 04:20.040
 And one thing wrong,

04:20.040 --> 04:21.920
 I mean, actually this is what I really like

04:21.920 --> 04:22.960
 about cryptography.

04:22.960 --> 04:24.600
 Cryptography, it seems like we live in a world

04:24.600 --> 04:29.640
 where the defense wins versus like nuclear weapons.

04:29.640 --> 04:30.920
 The opposite is true.

04:30.920 --> 04:32.960
 It is much easier to build a warhead

04:32.960 --> 04:34.520
 that splits into a hundred little warheads

04:34.520 --> 04:36.440
 than to build something that can, you know,

04:36.440 --> 04:38.880
 take out a hundred little warheads.

04:38.880 --> 04:41.400
 The offense has the advantage there.

04:41.400 --> 04:43.640
 So maybe our future is in crypto, but.

04:44.520 --> 04:45.720
 So cryptography, right.

04:45.720 --> 04:49.760
 The Goliath is the defense.

04:49.760 --> 04:54.280
 And then all the different hackers are the Davids.

04:54.280 --> 04:56.920
 And that equation is flipped for nuclear war.

04:57.840 --> 04:58.800
 Cause there's so many,

04:58.800 --> 05:01.960
 like one nuclear weapon destroys everything essentially.

05:01.960 --> 05:06.960
 Yeah, and it is much easier to attack with a nuclear weapon

05:06.960 --> 05:09.640
 than it is to like the technology required to intercept

05:09.640 --> 05:12.080
 and destroy a rocket is much more complicated

05:12.080 --> 05:13.800
 than the technology required to just, you know,

05:13.800 --> 05:16.400
 orbital trajectory, send a rocket to somebody.

05:17.480 --> 05:18.520
 So, okay.

05:18.520 --> 05:21.880
 Your intuition that there were intelligent civilizations

05:21.880 --> 05:24.360
 out there, but it's very possible

05:24.360 --> 05:26.240
 that they're no longer there.

05:26.240 --> 05:27.640
 That's kind of a sad picture.

05:27.640 --> 05:29.520
 They enter some steady state.

05:29.520 --> 05:31.520
 They all wirehead themselves.

05:31.520 --> 05:32.360
 What's wirehead?

05:33.360 --> 05:35.320
 Stimulate, stimulate their pleasure centers

05:35.320 --> 05:39.680
 and just, you know, live forever in this kind of stasis.

05:39.680 --> 05:42.560
 They become, well, I mean,

05:42.560 --> 05:46.320
 I think the reason I believe this is because where are they?

05:46.320 --> 05:49.800
 If there's some reason they stopped expanding,

05:50.680 --> 05:52.160
 cause otherwise they would have taken over the universe.

05:52.160 --> 05:53.440
 The universe isn't that big.

05:53.440 --> 05:54.280
 Or at least, you know,

05:54.280 --> 05:56.120
 let's just talk about the galaxy, right?

05:56.120 --> 05:57.800
 That's 70,000 light years across.

05:58.720 --> 05:59.920
 I took that number from Star Trek Voyager.

05:59.920 --> 06:04.680
 I don't know how true it is, but yeah, that's not big.

06:04.680 --> 06:07.320
 Right? 70,000 light years is nothing.

06:07.320 --> 06:10.040
 For some possible technology that you can imagine

06:10.040 --> 06:12.320
 that can leverage like wormholes or something like that.

06:12.320 --> 06:13.320
 Or you don't even need wormholes.

06:13.320 --> 06:15.120
 Just a von Neumann probe is enough.

06:15.120 --> 06:18.440
 A von Neumann probe and a million years of sublight travel

06:18.440 --> 06:20.440
 and you'd have taken over the whole universe.

06:20.440 --> 06:22.480
 That clearly didn't happen.

06:22.480 --> 06:24.000
 So something stopped it.

06:24.000 --> 06:25.400
 So you mean if you, right,

06:25.400 --> 06:27.040
 for like a few million years,

06:27.040 --> 06:29.880
 if you sent out probes that travel close,

06:29.880 --> 06:30.720
 what's sublight?

06:30.720 --> 06:32.280
 You mean close to the speed of light?

06:32.280 --> 06:33.720
 Let's say 0.1 C.

06:33.720 --> 06:34.800
 And it just spreads.

06:34.800 --> 06:35.640
 Interesting.

06:35.640 --> 06:38.800
 Actually, that's an interesting calculation, huh?

06:38.800 --> 06:40.640
 So what makes you think that we'd be able

06:40.640 --> 06:42.280
 to communicate with them?

06:42.280 --> 06:45.160
 Like, yeah, what's,

06:45.160 --> 06:47.920
 why do you think we would be able to be able

06:47.920 --> 06:50.640
 to comprehend intelligent lives that are out there?

06:51.920 --> 06:54.960
 Like even if they were among us kind of thing,

06:54.960 --> 06:57.600
 like, or even just flying around?

06:57.600 --> 07:01.200
 Well, I mean, that's possible.

07:01.200 --> 07:04.640
 It's possible that there is some sort of prime directive.

07:04.640 --> 07:07.040
 That'd be a really cool universe to live in.

07:07.040 --> 07:08.000
 And there's some reason

07:08.000 --> 07:10.920
 they're not making themselves visible to us.

07:10.920 --> 07:15.200
 But it makes sense that they would use the same,

07:15.200 --> 07:16.960
 well, at least the same entropy.

07:16.960 --> 07:18.800
 Well, you're implying the same laws of physics.

07:18.800 --> 07:20.800
 I don't know what you mean by entropy in this case.

07:20.800 --> 07:21.920
 Oh, yeah.

07:21.920 --> 07:25.040
 I mean, if entropy is the scarce resource in the universe.

07:25.040 --> 07:26.960
 So what do you think about like Stephen Wolfram

07:26.960 --> 07:28.840
 and everything is a computation?

07:28.840 --> 07:31.440
 And then what if they are traveling through

07:31.440 --> 07:32.640
 this world of computation?

07:32.640 --> 07:34.240
 So if you think of the universe

07:34.240 --> 07:36.600
 as just information processing,

07:36.600 --> 07:40.840
 then what you're referring to with entropy

07:40.840 --> 07:44.240
 and then these pockets of interesting complex computation

07:44.240 --> 07:47.160
 swimming around, how do we know they're not already here?

07:47.160 --> 07:51.040
 How do we know that this,

07:51.040 --> 07:53.040
 like all the different amazing things

07:53.040 --> 07:55.080
 that are full of mystery on earth

07:55.080 --> 07:58.640
 are just like little footprints of intelligence

07:58.640 --> 08:01.160
 from light years away?

08:01.160 --> 08:02.840
 Maybe.

08:02.840 --> 08:05.760
 I mean, I tend to think that as civilizations expand,

08:05.760 --> 08:07.800
 they use more and more energy

08:07.800 --> 08:10.240
 and you can never overcome the problem of waste heat.

08:10.240 --> 08:11.880
 So where is there waste heat?

08:11.880 --> 08:13.560
 So we'd be able to, with our crude methods,

08:13.560 --> 08:18.560
 be able to see like, there's a whole lot of energy here.

08:18.840 --> 08:20.560
 But it could be something we're not,

08:20.560 --> 08:22.480
 I mean, we don't understand dark energy, right?

08:22.480 --> 08:23.560
 Dark matter.

08:23.560 --> 08:26.160
 It could be just stuff we don't understand at all.

08:26.160 --> 08:29.080
 Or they can have a fundamentally different physics,

08:29.080 --> 08:32.200
 you know, like that we just don't even comprehend.

08:32.200 --> 08:33.440
 Well, I think, okay,

08:33.440 --> 08:35.120
 I mean, it depends how far out you wanna go.

08:35.120 --> 08:36.840
 I don't think physics is very different

08:36.840 --> 08:38.440
 on the other side of the galaxy.

08:39.760 --> 08:41.920
 I would suspect that they have,

08:41.920 --> 08:43.680
 I mean, if they're in our universe,

08:43.680 --> 08:45.760
 they have the same physics.

08:45.760 --> 08:47.600
 Well, yeah, that's the assumption we have,

08:47.600 --> 08:50.000
 but there could be like super trippy things

08:50.000 --> 08:57.000
 like our cognition only gets to a slice,

08:57.040 --> 08:59.440
 and all the possible instruments that we can design

08:59.440 --> 09:01.520
 only get to a particular slice of the universe.

09:01.520 --> 09:04.080
 And there's something much like weirder.

09:04.080 --> 09:06.880
 Maybe we can try a thought experiment.

09:06.880 --> 09:10.040
 Would people from the past

09:10.040 --> 09:14.000
 be able to detect the remnants of our,

09:14.000 --> 09:16.600
 or would we be able to detect our modern civilization?

09:16.600 --> 09:18.840
 I think the answer is obviously yes.

09:18.840 --> 09:20.720
 You mean past from a hundred years ago?

09:20.720 --> 09:22.080
 Well, let's even go back further.

09:22.080 --> 09:24.400
 Let's go to a million years ago, right?

09:24.400 --> 09:26.520
 The humans who were lying around in the desert

09:26.520 --> 09:27.720
 probably didn't even have,

09:27.720 --> 09:29.320
 maybe they just barely had fire.

09:31.360 --> 09:34.080
 They would understand if a 747 flew overhead.

09:35.200 --> 09:43.200
 Oh, in this vicinity, but not if a 747 flew on Mars.

09:43.200 --> 09:45.080
 Like, cause they wouldn't be able to see far,

09:45.080 --> 09:47.240
 cause we're not actually communicating that well

09:47.240 --> 09:48.920
 with the rest of the universe.

09:48.920 --> 09:50.160
 We're doing okay.

09:50.160 --> 09:54.200
 Just sending out random like fifties tracks of music.

09:54.200 --> 09:55.160
 True.

09:55.160 --> 09:57.800
 And yeah, I mean, they'd have to, you know,

09:57.800 --> 10:02.480
 we've only been broadcasting radio waves for 150 years.

10:02.480 --> 10:04.560
 And well, there's your light cone.

10:04.560 --> 10:05.800
 So.

10:05.800 --> 10:06.920
 Yeah. Okay.

10:06.920 --> 10:08.800
 What do you make about all the,

10:08.800 --> 10:13.800
 I recently came across this having talked to David Fravor.

10:14.720 --> 10:16.960
 I don't know if you caught what the videos

10:16.960 --> 10:18.840
 of the Pentagon released

10:18.840 --> 10:23.480
 and the New York Times reporting of the UFO sightings.

10:23.480 --> 10:26.040
 So I kind of looked into it, quote unquote.

10:26.040 --> 10:30.800
 And there's actually been like hundreds

10:30.800 --> 10:33.800
 of thousands of UFO sightings, right?

10:33.800 --> 10:35.960
 And a lot of it you can explain away

10:35.960 --> 10:37.080
 in different kinds of ways.

10:37.080 --> 10:40.160
 So one is it could be interesting physical phenomena.

10:40.160 --> 10:44.640
 Two, it could be people wanting to believe

10:44.640 --> 10:46.760
 and therefore they conjure up a lot of different things

10:46.760 --> 10:48.920
 that just, you know, when you see different kinds of lights,

10:48.920 --> 10:50.760
 some basic physics phenomena,

10:50.760 --> 10:53.640
 and then you just conjure up ideas

10:53.640 --> 10:56.680
 of possible out there mysterious worlds.

10:56.680 --> 10:58.960
 But, you know, it's also possible,

10:58.960 --> 11:02.480
 like you have a case of David Fravor,

11:02.480 --> 11:05.160
 who is a Navy pilot, who's, you know,

11:06.160 --> 11:08.840
 as legit as it gets in terms of humans

11:08.840 --> 11:13.440
 who are able to perceive things in the environment

11:13.440 --> 11:15.320
 and make conclusions,

11:15.320 --> 11:17.600
 whether those things are a threat or not.

11:17.600 --> 11:22.000
 And he and several other pilots saw a thing,

11:22.000 --> 11:23.440
 I don't know if you followed this,

11:23.440 --> 11:26.840
 but they saw a thing that they've since then called TikTok

11:26.840 --> 11:29.520
 that moved in all kinds of weird ways.

11:29.520 --> 11:30.640
 They don't know what it is.

11:30.640 --> 11:35.640
 It could be technology developed by the United States

11:36.640 --> 11:38.040
 and they're just not aware of it

11:38.040 --> 11:40.000
 and the surface level from the Navy, right?

11:40.000 --> 11:42.280
 It could be different kind of lighting technology

11:42.280 --> 11:45.000
 or drone technology, all that kind of stuff.

11:45.000 --> 11:46.560
 It could be the Russians and the Chinese,

11:46.560 --> 11:48.000
 all that kind of stuff.

11:48.000 --> 11:51.400
 And of course their mind, our mind,

11:51.400 --> 11:54.160
 can also venture into the possibility

11:54.160 --> 11:56.320
 that it's from another world.

11:56.320 --> 11:58.160
 Have you looked into this at all?

11:58.160 --> 11:59.640
 What do you think about it?

11:59.640 --> 12:01.680
 I think all the news is a psyop.

12:01.680 --> 12:05.240
 I think that the most plausible.

12:05.240 --> 12:06.440
 Nothing is real.

12:06.440 --> 12:10.040
 Yeah, I listened to the, I think it was Bob Lazar

12:10.920 --> 12:12.360
 on Joe Rogan.

12:12.360 --> 12:15.840
 And like, I believe everything this guy is saying.

12:15.840 --> 12:18.440
 And then I think that it's probably just some like MKUltra

12:18.440 --> 12:19.680
 kind of thing, you know?

12:20.680 --> 12:21.520
 What do you mean?

12:21.520 --> 12:24.720
 Like they, you know, they made some weird thing

12:24.720 --> 12:26.160
 and they called it an alien spaceship.

12:26.160 --> 12:27.480
 You know, maybe it was just to like

12:27.480 --> 12:29.560
 stimulate young physicists minds.

12:29.560 --> 12:31.040
 We'll tell them it's alien technology

12:31.040 --> 12:33.600
 and we'll see what they come up with, right?

12:33.600 --> 12:36.080
 Do you find any conspiracy theories compelling?

12:36.080 --> 12:38.320
 Like have you pulled at the string

12:38.320 --> 12:42.440
 of the rich complex world of conspiracy theories

12:42.440 --> 12:43.920
 that's out there?

12:43.920 --> 12:46.520
 I think that I've heard a conspiracy theory

12:46.520 --> 12:48.960
 that conspiracy theories were invented by the CIA

12:48.960 --> 12:52.040
 in the 60s to discredit true things.

12:52.040 --> 12:52.880
 Yeah.

12:53.880 --> 12:58.520
 So, you know, you can go to ridiculous conspiracy theories

12:58.520 --> 13:01.000
 like Flat Earth and Pizza Gate.

13:01.000 --> 13:05.440
 And, you know, these things are almost to hide

13:05.440 --> 13:08.000
 like conspiracy theories that like,

13:08.000 --> 13:09.640
 you know, remember when the Chinese like locked up

13:09.640 --> 13:11.360
 the doctors who discovered coronavirus?

13:11.360 --> 13:12.840
 Like I tell people this and I'm like,

13:12.840 --> 13:14.400
 no, no, no, that's not a conspiracy theory.

13:14.400 --> 13:15.920
 That actually happened.

13:15.920 --> 13:18.000
 Do you remember the time that the money used to be backed

13:18.000 --> 13:20.080
 by gold and now it's backed by nothing?

13:20.080 --> 13:21.680
 This is not a conspiracy theory.

13:21.680 --> 13:23.800
 This actually happened.

13:23.800 --> 13:26.360
 Well, that's one of my worries today

13:26.360 --> 13:31.360
 with the idea of fake news is that when nothing is real,

13:32.880 --> 13:37.600
 then like you dilute the possibility of anything being true

13:37.600 --> 13:41.000
 by conjuring up all kinds of conspiracy theories.

13:41.000 --> 13:42.400
 And then you don't know what to believe.

13:42.400 --> 13:46.280
 And then like the idea of truth of objectivity

13:46.280 --> 13:47.840
 is lost completely.

13:47.840 --> 13:50.120
 Everybody has their own truth.

13:50.120 --> 13:53.600
 So you used to control information by censoring it.

13:53.600 --> 13:55.880
 And then the internet happened and governments were like,

13:55.880 --> 13:58.360
 oh shit, we can't censor things anymore.

13:58.360 --> 13:59.580
 I know what we'll do.

14:00.560 --> 14:04.160
 You know, it's the old story of the story of like

14:04.160 --> 14:07.080
 tying a flag with a leprechaun tells you his gold is buried

14:07.080 --> 14:09.080
 and you tie one flag and you make the leprechaun swear

14:09.080 --> 14:10.040
 to not remove the flag.

14:10.040 --> 14:11.720
 And you come back to the field later with a shovel

14:11.720 --> 14:13.120
 and there's flags everywhere.

14:14.640 --> 14:16.320
 That's one way to maintain privacy, right?

14:16.320 --> 14:20.280
 It's like in order to protect the contents

14:20.280 --> 14:21.800
 of this conversation, for example,

14:21.800 --> 14:25.120
 we could just generate like millions of deep,

14:25.120 --> 14:27.560
 fake conversations where you and I talk

14:27.560 --> 14:29.120
 and say random things.

14:29.120 --> 14:30.440
 So this is just one of them

14:30.440 --> 14:32.720
 and nobody knows which one was the real one.

14:32.720 --> 14:34.440
 This could be fake right now.

14:34.440 --> 14:37.240
 Classic steganography technique.

14:37.240 --> 14:39.960
 Okay, another absurd question about intelligent life.

14:39.960 --> 14:43.960
 Cause you know, you're an incredible programmer

14:43.960 --> 14:45.520
 outside of everything else we'll talk about

14:45.520 --> 14:46.780
 just as a programmer.

14:49.320 --> 14:52.960
 Do you think intelligent beings out there,

14:52.960 --> 14:54.520
 the civilizations that were out there,

14:54.520 --> 14:58.240
 had computers and programming?

14:58.240 --> 15:01.440
 Did they, do we naturally have to develop something

15:01.440 --> 15:05.480
 where we engineer machines and are able to encode

15:05.480 --> 15:08.680
 both knowledge into those machines

15:08.680 --> 15:11.720
 and instructions that process that knowledge,

15:11.720 --> 15:14.260
 process that information to make decisions

15:14.260 --> 15:15.680
 and actions and so on?

15:15.680 --> 15:18.320
 And would those programming languages,

15:18.320 --> 15:21.320
 if you think they exist, be at all similar

15:21.320 --> 15:22.760
 to anything we've developed?

15:24.120 --> 15:26.600
 So I don't see that much of a difference

15:26.600 --> 15:29.400
 between quote unquote natural languages

15:29.400 --> 15:31.740
 and programming languages.

15:34.160 --> 15:35.000
 Yeah.

15:35.000 --> 15:36.680
 I think there's so many similarities.

15:36.680 --> 15:39.920
 So when asked the question,

15:39.920 --> 15:42.380
 what do alien languages look like?

15:42.380 --> 15:46.480
 I imagine they're not all that dissimilar from ours.

15:46.480 --> 15:49.280
 And I think translating in and out of them

15:51.560 --> 15:52.920
 wouldn't be that crazy.

15:52.920 --> 15:57.520
 Well, it's difficult to compile like DNA to Python

15:57.520 --> 15:59.160
 and then to C.

15:59.160 --> 16:02.000
 There's a little bit of a gap in the kind of languages

16:02.000 --> 16:06.840
 we use for touring machines

16:06.840 --> 16:10.160
 and the kind of languages nature seems to use a little bit.

16:10.160 --> 16:13.800
 Maybe that's just, we just haven't understood

16:13.800 --> 16:16.440
 the kind of language that nature uses well yet.

16:16.440 --> 16:17.900
 DNA is a CAD model.

16:19.120 --> 16:21.200
 It's not quite a programming language.

16:21.200 --> 16:25.280
 It has no sort of a serial execution.

16:25.280 --> 16:29.480
 It's not quite a, yeah, it's a CAD model.

16:29.480 --> 16:30.920
 So I think in that sense,

16:30.920 --> 16:32.520
 we actually completely understand it.

16:32.520 --> 16:37.300
 The problem is, well, simulating on these CAD models,

16:37.300 --> 16:38.520
 I played with it a bit this year,

16:38.520 --> 16:41.120
 is super computationally intensive.

16:41.120 --> 16:43.720
 If you wanna go down to like the molecular level

16:43.720 --> 16:45.880
 where you need to go to see a lot of these phenomenon

16:45.880 --> 16:46.920
 like protein folding.

16:48.080 --> 16:52.200
 So yeah, it's not that we don't understand it.

16:52.200 --> 16:55.160
 It just requires a whole lot of compute to kind of compile it.

16:55.160 --> 16:56.680
 For our human minds, it's inefficient,

16:56.680 --> 17:00.520
 both for the data representation and for the programming.

17:00.520 --> 17:02.800
 Yeah, it runs well on raw nature.

17:02.800 --> 17:03.880
 It runs well on raw nature.

17:03.880 --> 17:06.840
 And when we try to build emulators or simulators for that,

17:07.840 --> 17:10.640
 well, they're mad slow, but I've tried it.

17:10.640 --> 17:14.280
 It runs in that, yeah, you've commented elsewhere,

17:14.280 --> 17:15.780
 I don't remember where,

17:15.780 --> 17:20.780
 that one of the problems is simulating nature is tough.

17:20.780 --> 17:25.300
 And if you want to sort of deploy a prototype,

17:25.300 --> 17:28.020
 I forgot how you put it, but it made me laugh,

17:28.020 --> 17:30.420
 but animals or humans would need to be involved

17:31.700 --> 17:36.700
 in order to try to run some prototype code on,

17:38.120 --> 17:41.180
 like if we're talking about COVID and viruses and so on,

17:41.180 --> 17:42.900
 if you were trying to engineer

17:42.900 --> 17:45.020
 some kind of defense mechanisms,

17:45.020 --> 17:49.580
 like a vaccine against COVID and all that kind of stuff

17:49.580 --> 17:52.060
 that doing any kind of experimentation,

17:52.060 --> 17:53.980
 like you can with like autonomous vehicles

17:53.980 --> 17:58.980
 would be very technically and ethically costly.

17:59.660 --> 18:00.940
 I'm not sure about that.

18:00.940 --> 18:05.060
 I think you can do tons of crazy biology and test tubes.

18:05.060 --> 18:08.480
 I think my bigger complaint is more,

18:08.480 --> 18:10.120
 oh, the tools are so bad.

18:11.420 --> 18:14.540
 Like literally, you mean like libraries and?

18:14.540 --> 18:16.100
 I don't know, I'm not pipetting shit.

18:16.100 --> 18:20.140
 Like you're handing me a, I got a, no, no, no,

18:20.140 --> 18:21.300
 there has to be some.

18:22.580 --> 18:24.140
 Like automating stuff.

18:24.140 --> 18:28.300
 And like the, yeah, but human biology is messy.

18:28.300 --> 18:29.140
 Like it seems.

18:29.140 --> 18:31.240
 But like, look at those Toronto's videos.

18:31.240 --> 18:32.080
 They were a joke.

18:32.080 --> 18:33.460
 It's like a little gantry.

18:33.460 --> 18:34.660
 It's like little X, Y gantry,

18:34.660 --> 18:36.660
 high school science project with the pipette.

18:36.660 --> 18:38.260
 I'm like, really?

18:38.260 --> 18:39.220
 Gotta be something better.

18:39.220 --> 18:41.420
 You can't build like nice microfluidics

18:41.420 --> 18:45.300
 and I can program the computation to bio interface.

18:45.300 --> 18:47.180
 I mean, this is gonna happen.

18:47.180 --> 18:50.020
 But like right now, if you are asking me

18:50.020 --> 18:53.580
 to pipette 50 milliliters of solution, I'm out.

18:54.460 --> 18:55.460
 This is so crude.

18:55.460 --> 18:56.700
 Yeah.

18:56.700 --> 18:59.980
 Okay, let's get all the crazy out of the way.

18:59.980 --> 19:02.260
 So a bunch of people asked me,

19:02.260 --> 19:05.060
 since we talked about the simulation last time,

19:05.060 --> 19:06.860
 we talked about hacking the simulation.

19:06.860 --> 19:09.900
 Do you have any updates, any insights

19:09.900 --> 19:13.740
 about how we might be able to go about hacking simulation

19:13.740 --> 19:16.260
 if we indeed do live in a simulation?

19:17.180 --> 19:19.900
 I think a lot of people misinterpreted

19:19.900 --> 19:22.420
 the point of that South by talk.

19:22.420 --> 19:23.500
 The point of the South by talk

19:23.500 --> 19:25.560
 was not literally to hack the simulation.

19:26.720 --> 19:31.720
 I think that this is an idea is literally just,

19:33.520 --> 19:34.580
 I think theoretical physics.

19:34.580 --> 19:39.580
 I think that's the whole goal, right?

19:39.900 --> 19:42.300
 You want your grand unified theory, but then, okay,

19:42.300 --> 19:45.140
 build a grand unified theory search for exploits, right?

19:45.140 --> 19:47.640
 I think we're nowhere near actually there yet.

19:47.640 --> 19:50.180
 My hope with that was just more to like,

19:51.500 --> 19:52.740
 are you people kidding me

19:52.740 --> 19:54.980
 with the things you spend time thinking about?

19:54.980 --> 19:58.020
 Do you understand like kind of how small you are?

19:58.020 --> 20:02.540
 You are bytes and God's computer, really?

20:02.540 --> 20:06.700
 And the things that people get worked up about, you know?

20:06.700 --> 20:10.060
 So basically, it was more a message

20:10.060 --> 20:12.540
 of we should humble ourselves.

20:12.540 --> 20:17.540
 That we get to, like what are we humans in this byte code?

20:19.460 --> 20:22.380
 Yeah, and not just humble ourselves,

20:22.380 --> 20:24.900
 but like I'm not trying to like make people guilty

20:24.900 --> 20:25.740
 or anything like that.

20:25.740 --> 20:27.260
 I'm trying to say like, literally,

20:27.260 --> 20:30.200
 look at what you are spending time on, right?

20:30.200 --> 20:31.040
 What are you referring to?

20:31.040 --> 20:32.460
 You're referring to the Kardashians?

20:32.460 --> 20:34.140
 What are we talking about?

20:34.140 --> 20:34.980
 Twitter?

20:34.980 --> 20:38.060
 No, the Kardashians, everyone knows that's kind of fun.

20:38.060 --> 20:42.980
 I'm referring more to like the economy, you know?

20:42.980 --> 20:47.980
 This idea that we gotta up our stock price.

20:50.720 --> 20:55.380
 Or what is the goal function of humanity?

20:55.380 --> 20:57.600
 You don't like the game of capitalism?

20:57.600 --> 20:59.340
 Like you don't like the games we've constructed

20:59.340 --> 21:00.660
 for ourselves as humans?

21:00.660 --> 21:02.860
 I'm a big fan of capitalism.

21:02.860 --> 21:05.120
 I don't think that's really the game we're playing right now.

21:05.120 --> 21:07.260
 I think we're playing a different game

21:07.260 --> 21:08.660
 where the rules are rigged.

21:10.300 --> 21:12.580
 Okay, which games are interesting to you

21:12.580 --> 21:14.780
 that we humans have constructed and which aren't?

21:14.780 --> 21:18.380
 Which are productive and which are not?

21:18.380 --> 21:21.880
 Actually, maybe that's the real point of the talk.

21:21.880 --> 21:25.100
 It's like, stop playing these fake human games.

21:25.100 --> 21:26.780
 There's a real game here.

21:26.780 --> 21:28.660
 We can play the real game.

21:28.660 --> 21:31.260
 The real game is, you know, nature wrote the rules.

21:31.260 --> 21:32.540
 This is a real game.

21:32.540 --> 21:35.180
 There still is a game to play.

21:35.180 --> 21:36.940
 But if you look at, sorry to interrupt,

21:36.940 --> 21:38.420
 I don't know if you've seen the Instagram account,

21:38.420 --> 21:40.220
 nature is metal.

21:40.220 --> 21:42.820
 The game that nature seems to be playing

21:42.820 --> 21:47.340
 is a lot more cruel than we humans want to put up with.

21:47.340 --> 21:49.520
 Or at least we see it as cruel.

21:49.520 --> 21:52.780
 It's like the bigger thing eats the smaller thing

21:53.660 --> 21:58.180
 and does it to impress another big thing

21:58.180 --> 22:00.460
 so it can mate with that thing.

22:00.460 --> 22:01.300
 And that's it.

22:01.300 --> 22:04.040
 That seems to be the entirety of it.

22:04.040 --> 22:07.260
 Well, there's no art, there's no music,

22:07.260 --> 22:10.860
 there's no comma AI, there's no comma one,

22:10.860 --> 22:14.940
 no comma two, no George Hots with his brilliant talks

22:14.940 --> 22:17.020
 at South by Southwest.

22:17.020 --> 22:17.860
 I disagree, though.

22:17.860 --> 22:19.620
 I disagree that this is what nature is.

22:19.620 --> 22:24.620
 I think nature just provided basically a open world MMORPG.

22:26.780 --> 22:29.860
 And, you know, here it's open world.

22:29.860 --> 22:31.100
 I mean, if that's the game you want to play,

22:31.100 --> 22:32.260
 you can play that game.

22:32.260 --> 22:33.780
 But isn't that beautiful?

22:33.780 --> 22:35.580
 I don't know if you played Diablo.

22:35.580 --> 22:39.420
 They used to have, I think, cow level where it's...

22:39.420 --> 22:44.420
 So everybody will go just, they figured out this,

22:44.420 --> 22:48.420
 like the best way to gain like experience points

22:48.420 --> 22:51.020
 is to just slaughter cows over and over and over.

22:52.180 --> 22:55.900
 And so they figured out this little sub game

22:55.900 --> 22:58.800
 within the bigger game that this is the most efficient way

22:58.800 --> 22:59.900
 to get experience points.

22:59.900 --> 23:01.860
 And everybody somehow agreed

23:01.860 --> 23:04.460
 that getting experience points in RPG context

23:04.460 --> 23:06.500
 where you always want to be getting more stuff,

23:06.500 --> 23:09.140
 more skills, more levels, keep advancing.

23:09.140 --> 23:10.440
 That seems to be good.

23:10.440 --> 23:14.740
 So might as well sacrifice actual enjoyment

23:14.740 --> 23:17.640
 of playing a game, exploring a world,

23:17.640 --> 23:21.580
 and spending like hundreds of hours of your time

23:21.580 --> 23:22.420
 at cow level.

23:22.420 --> 23:25.540
 I mean, the number of hours I spent in cow level,

23:26.400 --> 23:28.140
 I'm not like the most impressive person

23:28.140 --> 23:30.460
 because people have spent probably thousands of hours there,

23:30.460 --> 23:31.580
 but it's ridiculous.

23:31.580 --> 23:35.220
 So that's a little absurd game that brought me joy

23:35.220 --> 23:37.500
 in some weird dopamine drug kind of way.

23:37.500 --> 23:40.060
 So you don't like those games.

23:40.060 --> 23:45.060
 You don't think that's us humans feeling the nature.

23:46.500 --> 23:47.340
 I think so.

23:47.340 --> 23:49.640
 And that was the point of the talk.

23:49.640 --> 23:50.480
 Yeah.

23:50.480 --> 23:51.460
 So how do we hack it then?

23:51.460 --> 23:52.740
 Well, I want to live forever.

23:52.740 --> 23:55.820
 And I want to live forever.

23:55.820 --> 23:56.780
 And this is the goal.

23:56.780 --> 23:59.220
 Well, that's a game against nature.

23:59.220 --> 24:02.600
 Yeah, immortality is the good objective function to you?

24:03.540 --> 24:05.100
 I mean, start there and then you can do whatever else

24:05.100 --> 24:07.380
 you want because you got a long time.

24:07.380 --> 24:10.740
 What if immortality makes the game just totally not fun?

24:10.740 --> 24:13.860
 I mean, like, why do you assume immortality

24:13.860 --> 24:18.160
 is somehow a good objective function?

24:18.160 --> 24:19.940
 It's not immortality that I want.

24:19.940 --> 24:22.580
 A true immortality where I could not die,

24:22.580 --> 24:25.020
 I would prefer what we have right now.

24:25.020 --> 24:27.180
 But I want to choose my own death, of course.

24:27.180 --> 24:29.840
 I don't want nature to decide when I die,

24:29.840 --> 24:30.780
 I'm going to win.

24:30.780 --> 24:31.780
 I'm going to be you.

24:33.100 --> 24:36.920
 And then at some point, if you choose commit suicide,

24:36.920 --> 24:40.420
 like how long do you think you'd live?

24:41.700 --> 24:43.140
 Until I get bored.

24:43.140 --> 24:48.060
 See, I don't think people like brilliant people like you

24:48.060 --> 24:52.380
 that really ponder living a long time

24:52.380 --> 24:57.380
 are really considering how meaningless life becomes.

24:58.620 --> 25:01.060
 Well, I want to know everything and then I'm ready to die.

25:03.620 --> 25:04.460
 As long as there's...

25:04.460 --> 25:05.280
 Yeah, but why do you want,

25:05.280 --> 25:06.940
 isn't it possible that you want to know everything

25:06.940 --> 25:09.700
 because it's finite?

25:09.700 --> 25:12.220
 Like the reason you want to know quote unquote everything

25:12.220 --> 25:16.380
 is because you don't have enough time to know everything.

25:16.380 --> 25:18.780
 And once you have unlimited time,

25:18.780 --> 25:22.220
 then you realize like, why do anything?

25:22.220 --> 25:24.060
 Like why learn anything?

25:25.140 --> 25:27.100
 I want to know everything and then I'm ready to die.

25:27.100 --> 25:28.460
 So you have, yeah.

25:28.460 --> 25:30.940
 It's not a, like, it's a terminal value.

25:30.940 --> 25:34.740
 It's not in service of anything else.

25:34.740 --> 25:37.900
 I'm conscious of the possibility, this is not a certainty,

25:37.900 --> 25:41.800
 but the possibility of that engine of curiosity

25:41.800 --> 25:44.340
 that you're speaking to is actually

25:47.100 --> 25:49.780
 a symptom of the finiteness of life.

25:49.780 --> 25:54.780
 Like without that finiteness, your curiosity would vanish.

25:55.060 --> 25:57.000
 Like a morning fog.

25:57.000 --> 25:57.840
 All right, cool.

25:57.840 --> 25:59.300
 Bukowski talked about love like that.

25:59.300 --> 26:01.340
 Then let me solve immortality

26:01.340 --> 26:02.900
 and let me change the thing in my brain

26:02.900 --> 26:04.700
 that reminds me of the fact that I'm immortal,

26:04.700 --> 26:06.260
 tells me that life is finite shit.

26:06.260 --> 26:09.060
 Maybe I'll have it tell me that life ends next week.

26:09.060 --> 26:10.660
 Right?

26:10.660 --> 26:12.660
 I'm okay with some self manipulation like that.

26:12.660 --> 26:14.420
 I'm okay with deceiving myself.

26:14.420 --> 26:17.020
 Oh, Rika, changing the code.

26:17.020 --> 26:18.300
 Yeah, well, if that's the problem, right?

26:18.300 --> 26:20.580
 If the problem is that I will no longer have that,

26:20.580 --> 26:24.460
 that curiosity, I'd like to have backup copies of myself,

26:24.460 --> 26:27.460
 which I check in with occasionally

26:27.460 --> 26:29.240
 to make sure they're okay with the trajectory

26:29.240 --> 26:31.000
 and they can kind of override it.

26:31.000 --> 26:33.180
 Maybe a nice, like, I think of like those wave nets,

26:33.180 --> 26:35.180
 those like logarithmic go back to the copies.

26:35.180 --> 26:36.700
 Yeah, but sometimes it's not reversible.

26:36.700 --> 26:39.980
 Like I've done this with video games.

26:39.980 --> 26:41.620
 Once you figure out the cheat code

26:41.620 --> 26:43.940
 or like you look up how to cheat old school,

26:43.940 --> 26:46.860
 like single player, it ruins the game for you.

26:46.860 --> 26:47.700
 Absolutely.

26:47.700 --> 26:48.540
 It ruins that feeling.

26:48.540 --> 26:51.900
 But again, that just means our brain manipulation

26:51.900 --> 26:53.260
 technology is not good enough yet.

26:53.260 --> 26:54.700
 Remove that cheat code from your brain.

26:54.700 --> 26:55.820
 Here you go.

26:55.820 --> 26:59.380
 So it's also possible that if we figure out immortality,

27:00.500 --> 27:03.460
 that all of us will kill ourselves

27:03.460 --> 27:06.100
 before we advance far enough

27:06.100 --> 27:08.820
 to be able to revert the change.

27:08.820 --> 27:11.900
 I'm not killing myself till I know everything, so.

27:11.900 --> 27:15.020
 That's what you say now, because your life is finite.

27:15.020 --> 27:19.060
 You know, I think yes, self modifying systems gets,

27:19.060 --> 27:21.020
 comes up with all these hairy complexities

27:21.020 --> 27:23.020
 and can I promise that I'll do it perfectly?

27:23.020 --> 27:25.920
 No, but I think I can put good safety structures in place.

27:27.180 --> 27:29.740
 So that talk and your thinking here

27:29.740 --> 27:34.740
 is not literally referring to a simulation

27:36.180 --> 27:40.520
 and that our universe is a kind of computer program

27:40.520 --> 27:42.240
 running on a computer.

27:42.240 --> 27:45.180
 That's more of a thought experiment.

27:45.180 --> 27:50.180
 Do you also think of the potential of the sort of Bostrom,

27:51.780 --> 27:56.780
 Elon Musk and others that talk about an actual program

27:57.820 --> 27:59.700
 that simulates our universe?

27:59.700 --> 28:01.980
 Oh, I don't doubt that we're in a simulation.

28:01.980 --> 28:05.300
 I just think that it's not quite that important.

28:05.300 --> 28:06.940
 I mean, I'm interested only in simulation theory

28:06.940 --> 28:09.740
 as far as like it gives me power over nature.

28:09.740 --> 28:13.140
 If it's totally unfalsifiable, then who cares?

28:13.140 --> 28:15.300
 I mean, what do you think that experiment would look like?

28:15.300 --> 28:17.740
 Like somebody on Twitter asks,

28:17.740 --> 28:20.780
 asks George what signs we would look for

28:20.780 --> 28:22.980
 to know whether or not we're in the simulation,

28:22.980 --> 28:25.880
 which is exactly what you're asking is like,

28:25.880 --> 28:29.500
 the step that precedes the step of knowing

28:29.500 --> 28:32.200
 how to get more power from this knowledge

28:32.200 --> 28:35.220
 is to get an indication that there's some power to be gained.

28:35.220 --> 28:37.900
 So get an indication that there,

28:37.900 --> 28:42.100
 you can discover and exploit cracks in the simulation

28:42.100 --> 28:45.340
 or it doesn't have to be in the physics of the universe.

28:45.340 --> 28:46.720
 Yeah.

28:46.720 --> 28:50.640
 Show me, I mean, like a memory leak could be cool.

28:51.620 --> 28:54.000
 Like some scrying technology, you know?

28:54.000 --> 28:55.260
 What kind of technology?

28:55.260 --> 28:56.220
 Scrying?

28:56.220 --> 28:57.060
 What's that?

28:57.060 --> 28:58.460
 Oh, that's a weird,

28:58.460 --> 29:03.460
 scrying is the paranormal ability to like remote viewing,

29:03.460 --> 29:06.980
 like being able to see somewhere where you're not.

29:08.220 --> 29:10.100
 So, you know, I don't think you can do it

29:10.100 --> 29:11.220
 by chanting in a room,

29:11.220 --> 29:14.680
 but if we could find, it's a memory leak, basically.

29:16.300 --> 29:17.300
 It's a memory leak.

29:17.300 --> 29:19.980
 Yeah, you're able to access parts you're not supposed to.

29:19.980 --> 29:20.820
 Yeah, yeah, yeah.

29:20.820 --> 29:22.100
 And thereby discover a shortcut.

29:22.100 --> 29:24.720
 Yeah, maybe memory leak means the other thing as well,

29:24.720 --> 29:25.560
 but I mean like, yeah,

29:25.560 --> 29:28.100
 like an ability to read arbitrary memory, right?

29:28.100 --> 29:29.820
 And that one's not that horrifying, right?

29:29.820 --> 29:31.420
 The right ones start to be horrifying.

29:31.420 --> 29:32.260
 Read, right.

29:32.260 --> 29:34.900
 It's the reading is not the problem.

29:34.900 --> 29:37.220
 Yeah, it's like Heartfleet for the universe.

29:37.220 --> 29:40.740
 Oh boy, the writing is a big, big problem.

29:40.740 --> 29:42.040
 It's a big problem.

29:43.060 --> 29:44.620
 It's the moment you can write anything,

29:44.620 --> 29:46.480
 even if it's just random noise.

29:47.740 --> 29:49.180
 That's terrifying.

29:49.180 --> 29:51.560
 I mean, even without that,

29:51.560 --> 29:52.580
 like even some of the, you know,

29:52.580 --> 29:54.980
 the nanotech stuff that's coming, I think is.

29:57.140 --> 29:58.340
 I don't know if you're paying attention,

29:58.340 --> 30:00.500
 but actually Eric Weistand came out

30:00.500 --> 30:02.260
 with the theory of everything.

30:02.260 --> 30:03.580
 I mean, that came out.

30:03.580 --> 30:05.460
 He's been working on a theory of everything

30:05.460 --> 30:08.060
 in the physics world called geometric unity.

30:08.060 --> 30:11.700
 And then for me, from computer science person like you,

30:11.700 --> 30:14.220
 Steven Wolfram's theory of everything,

30:14.220 --> 30:17.660
 of like hypergraphs is super interesting and beautiful,

30:17.660 --> 30:19.460
 but not from a physics perspective,

30:19.460 --> 30:20.940
 but from a computational perspective.

30:20.940 --> 30:23.020
 I don't know, have you paid attention to any of that?

30:23.020 --> 30:26.440
 So again, like what would make me pay attention

30:26.440 --> 30:29.540
 and like why like I hate string theory is,

30:29.540 --> 30:31.780
 okay, make a testable prediction, right?

30:31.780 --> 30:33.700
 I'm only interested in,

30:33.700 --> 30:36.060
 I'm not interested in theories for their intrinsic beauty.

30:36.060 --> 30:37.060
 I'm interested in theories

30:37.060 --> 30:38.900
 that give me power over the universe.

30:39.900 --> 30:42.340
 So if these theories do, I'm very interested.

30:43.220 --> 30:45.100
 Can I just say how beautiful that is?

30:45.100 --> 30:47.140
 Because a lot of physicists say,

30:47.140 --> 30:49.980
 I'm interested in experimental validation

30:49.980 --> 30:52.940
 and they skip out the part where they say

30:52.940 --> 30:55.500
 to give me more power in the universe.

30:55.500 --> 30:57.500
 I just love the.

30:57.500 --> 30:59.780
 No, I want. The clarity of that.

30:59.780 --> 31:02.020
 I want 100 gigahertz processors.

31:02.020 --> 31:04.120
 I want transistors that are smaller than atoms.

31:04.120 --> 31:05.600
 I want like power.

31:08.100 --> 31:10.580
 That's true.

31:10.580 --> 31:12.460
 And that's where people from aliens

31:12.460 --> 31:14.420
 to this kind of technology where people are worried

31:14.420 --> 31:19.320
 that governments, like who owns that power?

31:19.320 --> 31:20.780
 Is it George Harts?

31:20.780 --> 31:25.020
 Is it thousands of distributed hackers across the world?

31:25.020 --> 31:26.580
 Is it governments?

31:26.580 --> 31:28.660
 Is it Mark Zuckerberg?

31:28.660 --> 31:32.500
 There's a lot of people that,

31:32.500 --> 31:35.540
 I don't know if anyone trusts any one individual with power.

31:35.540 --> 31:37.380
 So they're always worried.

31:37.380 --> 31:39.340
 It's the beauty of blockchains.

31:39.340 --> 31:43.140
 That's the beauty of blockchains, which we'll talk about.

31:43.140 --> 31:46.220
 On Twitter, somebody pointed me to a story,

31:46.220 --> 31:49.300
 a bunch of people pointed me to a story a few months ago

31:49.300 --> 31:51.660
 where you went into a restaurant in New York.

31:51.660 --> 31:53.660
 And you can correct me if any of this is wrong.

31:53.660 --> 31:56.580
 And ran into a bunch of folks from a company

31:56.580 --> 32:00.580
 in a crypto company who are trying to scale up Ethereum.

32:01.780 --> 32:03.300
 And they had a technical deadline

32:03.300 --> 32:07.380
 related to solidity to OVM compiler.

32:07.380 --> 32:09.660
 So these are all Ethereum technologies.

32:09.660 --> 32:13.440
 So you stepped in, they recognized you,

32:14.700 --> 32:16.220
 pulled you aside, explained their problem.

32:16.220 --> 32:19.540
 And you stepped in and helped them solve the problem,

32:19.540 --> 32:23.100
 thereby creating legend status story.

32:23.100 --> 32:28.100
 So can you tell me the story in a little more detail?

32:28.980 --> 32:30.620
 It seems kind of incredible.

32:31.540 --> 32:32.380
 Did this happen?

32:32.380 --> 32:34.060
 Yeah, yeah, it's a true story, it's a true story.

32:34.060 --> 32:36.620
 I mean, they wrote a very flattering account of it.

32:39.340 --> 32:43.820
 So Optimism is the company called Optimism,

32:43.820 --> 32:45.420
 spin off of Plasma.

32:45.420 --> 32:47.820
 They're trying to build L2 solutions on Ethereum.

32:47.820 --> 32:52.620
 So right now, every Ethereum node

32:52.620 --> 32:56.420
 has to run every transaction on the Ethereum network.

32:56.420 --> 32:58.540
 And this kind of doesn't scale, right?

32:58.540 --> 33:00.020
 Because if you have N computers,

33:00.020 --> 33:02.340
 well, if that becomes two N computers,

33:02.340 --> 33:05.420
 you actually still get the same amount of compute, right?

33:05.420 --> 33:07.740
 This is like all of one scaling

33:09.120 --> 33:10.140
 because they all have to run it.

33:10.140 --> 33:12.840
 Okay, fine, you get more blockchain security,

33:12.840 --> 33:15.380
 but like, blockchain is already so secure.

33:15.380 --> 33:17.780
 Can we trade some of that off for speed?

33:17.780 --> 33:20.500
 So that's kind of what these L2 solutions are.

33:20.500 --> 33:23.020
 They built this thing, which kind of,

33:23.020 --> 33:26.300
 kind of sandbox for Ethereum contracts.

33:26.300 --> 33:28.200
 So they can run it in this L2 world

33:28.200 --> 33:30.900
 and it can't do certain things in L world, in L1.

33:30.900 --> 33:32.420
 Can I ask you for some definitions?

33:32.420 --> 33:33.420
 What's L2?

33:33.420 --> 33:34.840
 Oh, L2 is layer two.

33:34.840 --> 33:37.180
 So L1 is like the base Ethereum chain.

33:37.180 --> 33:40.920
 And then layer two is like a computational layer

33:40.920 --> 33:44.420
 that runs elsewhere,

33:44.420 --> 33:47.680
 but still is kind of secured by layer one.

33:47.680 --> 33:49.720
 And I'm sure a lot of people know,

33:49.720 --> 33:51.940
 but Ethereum is a cryptocurrency,

33:51.940 --> 33:53.720
 probably one of the most popular cryptocurrency

33:53.720 --> 33:55.320
 second to Bitcoin.

33:55.320 --> 33:58.800
 And a lot of interesting technological innovation there.

33:58.800 --> 34:03.240
 Maybe you could also slip in whenever you talk about this

34:03.240 --> 34:06.380
 and things that are exciting to you in the Ethereum space.

34:06.380 --> 34:07.800
 And why Ethereum?

34:07.800 --> 34:11.120
 Well, I mean, Bitcoin is not Turing complete.

34:12.260 --> 34:13.820
 Ethereum is not technically Turing complete

34:13.820 --> 34:16.040
 with the gas limit, but close enough.

34:16.040 --> 34:16.880
 With the gas limit?

34:16.880 --> 34:19.080
 What's the gas limit, resources?

34:19.080 --> 34:21.180
 Yeah, I mean, no computer is actually Turing complete.

34:21.180 --> 34:22.020
 Right.

34:23.160 --> 34:24.720
 You're gonna find out RAM, you know?

34:24.720 --> 34:25.560
 I can actually solve the whole thing.

34:25.560 --> 34:26.720
 What's the word gas limit?

34:26.720 --> 34:28.660
 You just have so many brilliant words.

34:28.660 --> 34:29.500
 I'm not even gonna ask.

34:29.500 --> 34:32.220
 That's not my word, that's Ethereum's word.

34:32.220 --> 34:33.060
 Gas limit.

34:33.060 --> 34:35.360
 Ethereum, you have to spend gas per instruction.

34:35.360 --> 34:37.820
 So like different op codes use different amounts of gas

34:37.820 --> 34:40.680
 and you buy gas with ether to prevent people

34:40.680 --> 34:42.740
 from basically DDoSing the network.

34:42.740 --> 34:45.440
 So Bitcoin is proof of work.

34:45.440 --> 34:47.120
 And then what's Ethereum?

34:47.120 --> 34:48.360
 It's also proof of work.

34:48.360 --> 34:49.960
 They're working on some proof of stake,

34:49.960 --> 34:51.040
 Ethereum 2.0 stuff.

34:51.040 --> 34:52.720
 But right now it's proof of work.

34:52.720 --> 34:54.700
 It uses a different hash function from Bitcoin.

34:54.700 --> 34:57.340
 That's more ASIC resistance, because you need RAM.

34:57.340 --> 34:59.960
 So we're all talking about Ethereum 1.0.

34:59.960 --> 35:03.840
 So what were they trying to do to scale this whole process?

35:03.840 --> 35:07.800
 So they were like, well, if we could run contracts elsewhere

35:07.800 --> 35:10.480
 and then only save the results of that computation,

35:13.120 --> 35:14.680
 well, we don't actually have to do the compute on the chain.

35:14.680 --> 35:15.680
 We can do the compute off chain

35:15.680 --> 35:17.440
 and just post what the results are.

35:17.440 --> 35:18.800
 Now, the problem with that is,

35:18.800 --> 35:21.120
 well, somebody could lie about what the results are.

35:21.120 --> 35:23.240
 So you need a resolution mechanism.

35:23.240 --> 35:26.500
 And the resolution mechanism can be really expensive

35:26.500 --> 35:29.000
 because you just have to make sure

35:29.000 --> 35:31.140
 that the person who is saying,

35:31.140 --> 35:33.800
 look, I swear that this is the real computation.

35:33.800 --> 35:36.640
 I'm staking $10,000 on that fact.

35:36.640 --> 35:39.000
 And if you prove it wrong,

35:39.000 --> 35:42.820
 yeah, it might cost you $3,000 in gas fees to prove wrong,

35:42.820 --> 35:44.800
 but you'll get the $10,000 bounty.

35:44.800 --> 35:47.740
 So you can secure using those kinds of systems.

35:47.740 --> 35:52.500
 So it's effectively a sandbox, which runs contracts.

35:52.500 --> 35:55.600
 And like, it's like any kind of normal sandbox,

35:55.600 --> 35:57.960
 you have to like replace syscalls

35:57.960 --> 36:00.580
 with calls into the hypervisor.

36:02.920 --> 36:05.080
 Sandbox, syscalls, hypervisor.

36:05.080 --> 36:06.680
 What do these things mean?

36:06.680 --> 36:09.240
 As long as it's interesting to talk about.

36:09.240 --> 36:11.280
 Yeah, I mean, you can take like the Chrome sandbox

36:11.280 --> 36:12.720
 is maybe the one to think about, right?

36:12.720 --> 36:16.000
 So the Chrome process that's doing a rendering,

36:16.000 --> 36:18.800
 can't, for example, read a file from the file system.

36:18.800 --> 36:21.800
 It has, if it tries to make an open syscall in Linux,

36:21.800 --> 36:23.720
 the open syscall, you can't make it open syscall,

36:23.720 --> 36:24.780
 no, no, no.

36:24.780 --> 36:29.160
 You have to request from the kind of hypervisor process

36:29.160 --> 36:31.680
 or like, I don't know what it's called in Chrome,

36:31.680 --> 36:36.400
 but the, hey, could you open this file for me?

36:36.400 --> 36:37.520
 And then it does all these checks

36:37.520 --> 36:39.200
 and then it passes the file handle back in

36:39.200 --> 36:40.160
 if it's approved.

36:41.160 --> 36:42.640
 So that's, yeah.

36:42.640 --> 36:45.320
 So what's the, in the context of Ethereum,

36:45.320 --> 36:47.200
 what are the boundaries of the sandbox

36:47.200 --> 36:48.400
 that we're talking about?

36:48.400 --> 36:50.800
 Well, like one of the calls that you,

36:50.800 --> 36:53.960
 actually reading and writing any state

36:53.960 --> 36:55.480
 to the Ethereum contract,

36:55.480 --> 36:57.040
 or to the Ethereum blockchain.

36:58.720 --> 37:01.060
 Writing state is one of those calls

37:01.060 --> 37:04.500
 that you're going to have to sandbox in layer two,

37:04.500 --> 37:08.120
 because if you let layer two just arbitrarily write

37:08.120 --> 37:09.480
 to the Ethereum blockchain.

37:09.480 --> 37:14.480
 So layer two is really sitting on top of layer one.

37:15.120 --> 37:17.160
 So you're going to have a lot of different kinds of ideas

37:17.160 --> 37:18.680
 that you can play with.

37:18.680 --> 37:21.360
 And they're all, they're not fundamentally changing

37:21.360 --> 37:25.140
 the source code level of Ethereum.

37:25.140 --> 37:28.900
 Well, you have to replace a bunch of calls

37:28.900 --> 37:31.120
 with calls into the hypervisor.

37:31.120 --> 37:33.840
 So instead of doing the syscall directly,

37:33.840 --> 37:37.360
 you replace it with a call to the hypervisor.

37:37.360 --> 37:39.320
 So originally they were doing this

37:39.320 --> 37:43.520
 by first running the, so Solidity is the language

37:43.520 --> 37:45.440
 that most Ethereum contracts are written in.

37:45.440 --> 37:47.320
 It compiles to a bytecode.

37:47.320 --> 37:50.040
 And then they wrote this thing they called the transpiler.

37:50.040 --> 37:52.440
 And the transpiler took the bytecode

37:52.440 --> 37:56.000
 and it transpiled it into OVM safe bytecode.

37:56.000 --> 37:57.520
 Basically bytecode that didn't make any

37:57.520 --> 37:58.800
 of those restricted syscalls

37:58.800 --> 38:00.700
 and added the calls to the hypervisor.

38:01.680 --> 38:04.640
 This transpiler was a 3000 line mess.

38:05.640 --> 38:07.100
 And it's hard to do.

38:07.100 --> 38:09.060
 It's hard to do if you're trying to do it like that,

38:09.060 --> 38:12.200
 because you have to kind of like deconstruct the bytecode,

38:12.200 --> 38:15.360
 change things about it, and then reconstruct it.

38:15.360 --> 38:17.760
 And I mean, as soon as I hear this, I'm like,

38:17.760 --> 38:20.440
 well, why don't you just change the compiler, right?

38:20.440 --> 38:22.340
 Why not the first place you build the bytecode,

38:22.340 --> 38:23.680
 just do it in the compiler.

38:25.560 --> 38:28.040
 So yeah, I asked them how much they wanted it.

38:29.160 --> 38:32.220
 Of course, measured in dollars and I'm like, well, okay.

38:33.040 --> 38:34.600
 And yeah.

38:34.600 --> 38:35.920
 And you wrote the compiler.

38:35.920 --> 38:39.040
 Yeah, I modified, I wrote a 300 line diff to the compiler.

38:39.040 --> 38:40.840
 It's open source, you can look at it.

38:40.840 --> 38:43.000
 Yeah, it's, yeah, I looked at the code last night.

38:43.000 --> 38:46.680
 It's, yeah, exactly.

38:46.680 --> 38:49.440
 Cute is a good word for it.

38:49.440 --> 38:52.000
 And it's C++.

38:52.000 --> 38:52.840
 C++, yeah.

38:54.320 --> 38:57.040
 So when asked how you were able to do it,

38:57.040 --> 39:01.820
 you said, you just gotta think and then do it right.

39:03.080 --> 39:04.680
 So can you break that apart a little bit?

39:04.680 --> 39:09.680
 What's your process of one, thinking and two, doing it right?

39:09.920 --> 39:12.440
 You know, the people that I was working for

39:12.440 --> 39:13.480
 were amused that I said that.

39:13.480 --> 39:14.800
 It doesn't really mean anything.

39:14.800 --> 39:15.640
 Okay.

39:16.480 --> 39:19.640
 I mean, is there some deep, profound insights

39:19.640 --> 39:23.600
 to draw from like how you problem solve from that?

39:23.600 --> 39:24.640
 This is always what I say.

39:24.640 --> 39:26.060
 I'm like, do you wanna be a good programmer?

39:26.060 --> 39:27.840
 Do it for 20 years.

39:27.840 --> 39:29.600
 Yeah, there's no shortcuts.

39:29.600 --> 39:30.440
 No.

39:31.360 --> 39:33.440
 What are your thoughts on crypto in general?

39:33.440 --> 39:38.080
 So what parts technically or philosophically

39:38.080 --> 39:40.040
 do you find especially beautiful maybe?

39:40.040 --> 39:42.800
 Oh, I'm extremely bullish on crypto longterm.

39:42.800 --> 39:47.340
 Not any specific crypto project, but this idea of,

39:48.680 --> 39:50.320
 well, two ideas.

39:50.320 --> 39:54.240
 One, the Nakamoto Consensus Algorithm

39:54.240 --> 39:57.320
 is I think one of the greatest innovations

39:57.320 --> 39:58.600
 of the 21st century.

39:58.600 --> 40:01.260
 This idea that people can reach consensus.

40:01.260 --> 40:03.400
 You can reach a group consensus.

40:03.400 --> 40:08.220
 Using a relatively straightforward algorithm is wild.

40:08.220 --> 40:13.220
 And like, you know, Satoshi Nakamoto,

40:14.120 --> 40:15.580
 people always ask me who I look up to.

40:15.580 --> 40:17.800
 It's like, whoever that is.

40:17.800 --> 40:19.120
 Who do you think it is?

40:19.120 --> 40:21.020
 I mean, Elon Musk?

40:21.020 --> 40:22.320
 Is it you?

40:22.320 --> 40:24.040
 It is definitely not me.

40:24.040 --> 40:26.060
 And I do not think it's Elon Musk.

40:26.060 --> 40:31.060
 But yeah, this idea of groups reaching consensus

40:31.060 --> 40:34.940
 in a decentralized yet formulaic way

40:34.940 --> 40:37.680
 is one extremely powerful idea from crypto.

40:40.540 --> 40:45.540
 Maybe the second idea is this idea of smart contracts.

40:45.860 --> 40:49.100
 When you write a contract between two parties,

40:49.100 --> 40:53.720
 any contract, this contract, if there are disputes,

40:53.720 --> 40:56.140
 it's interpreted by lawyers.

40:56.140 --> 41:00.020
 Lawyers are just really shitty overpaid interpreters.

41:00.020 --> 41:02.060
 Imagine you had, let's talk about them in terms of a,

41:02.060 --> 41:05.260
 in terms of like, let's compare a lawyer to Python, right?

41:05.260 --> 41:07.140
 So lawyer, well, okay.

41:07.140 --> 41:10.020
 That's really, I never thought of it that way.

41:10.020 --> 41:11.620
 It's hilarious.

41:11.620 --> 41:15.900
 So Python, I'm paying even 10 cents an hour.

41:15.900 --> 41:17.180
 I'll use the nice Azure machine.

41:17.180 --> 41:19.660
 I can run Python for 10 cents an hour.

41:19.660 --> 41:21.480
 Lawyers cost $1,000 an hour.

41:21.480 --> 41:25.900
 So Python is 10,000 X better on that axis.

41:25.900 --> 41:29.540
 Lawyers don't always return the same answer.

41:31.100 --> 41:32.460
 Python almost always does.

41:36.700 --> 41:37.740
 Cost.

41:37.740 --> 41:40.940
 Yeah, I mean, just cost, reliability,

41:40.940 --> 41:43.860
 everything about Python is so much better than lawyers.

41:43.860 --> 41:46.100
 So if you can make smart contracts,

41:46.100 --> 41:49.080
 this whole concept of code is law.

41:50.400 --> 41:53.180
 I love, and I would love to live in a world

41:53.180 --> 41:55.620
 where everybody accepted that fact.

41:55.620 --> 42:00.620
 So maybe you can talk about what smart contracts are.

42:01.180 --> 42:05.020
 So let's say, let's say, you know,

42:05.020 --> 42:08.700
 we have a, even something as simple

42:08.700 --> 42:11.700
 as a safety deposit box, right?

42:11.700 --> 42:14.780
 Safety deposit box that holds a million dollars.

42:14.780 --> 42:17.040
 I have a contract with the bank that says

42:17.040 --> 42:22.040
 two out of these three parties must be present

42:22.620 --> 42:25.280
 to open the safety deposit box and get the money out.

42:25.280 --> 42:26.300
 So that's a contract for the bank,

42:26.300 --> 42:29.020
 and it's only as good as the bank and the lawyers, right?

42:29.020 --> 42:32.820
 Let's say, you know, somebody dies and now,

42:32.820 --> 42:34.580
 oh, we're gonna go through a big legal dispute

42:34.580 --> 42:36.140
 about whether, oh, well, was it in the will,

42:36.140 --> 42:37.540
 was it not in the will?

42:37.540 --> 42:39.780
 What, like, it's just so messy,

42:39.780 --> 42:44.620
 and the cost to determine truth is so expensive.

42:44.620 --> 42:47.220
 Versus a smart contract, which just uses cryptography

42:47.220 --> 42:50.100
 to check if two out of three keys are present.

42:50.100 --> 42:53.860
 Well, I can look at that, and I can have certainty

42:53.860 --> 42:55.980
 in the answer that it's going to return.

42:55.980 --> 42:58.220
 And that's what, all businesses want is certainty.

42:58.220 --> 42:59.620
 You know, they say businesses don't care.

42:59.620 --> 43:02.020
 Viacom, YouTube, YouTube's like,

43:02.020 --> 43:04.100
 look, we don't care which way this lawsuit goes.

43:04.100 --> 43:07.220
 Just please tell us so we can have certainty.

43:07.220 --> 43:09.100
 Yeah, I wonder how many agreements in this,

43:09.100 --> 43:12.720
 because we're talking about financial transactions only

43:12.720 --> 43:15.620
 in this case, correct, the smart contracts?

43:15.620 --> 43:17.380
 Oh, you can go to anything.

43:17.380 --> 43:19.820
 You can put a prenup in the Ethereum blockchain.

43:21.820 --> 43:23.020
 A married smart contract?

43:23.020 --> 43:24.900
 Sorry, divorce lawyer, sorry.

43:24.900 --> 43:26.820
 You're going to be replaced by Python.

43:29.580 --> 43:34.580
 Okay, so that's another beautiful idea.

43:34.980 --> 43:37.700
 Do you think there's something that's appealing to you

43:37.700 --> 43:40.260
 about any one specific implementation?

43:40.260 --> 43:45.100
 So if you look 10, 20, 50 years down the line,

43:45.100 --> 43:48.140
 do you see any, like, Bitcoin, Ethereum,

43:48.140 --> 43:51.140
 any of the other hundreds of cryptocurrencies winning out?

43:51.140 --> 43:53.340
 Is there, like, what's your intuition about the space?

43:53.340 --> 43:55.380
 Or are you just sitting back and watching the chaos

43:55.380 --> 43:57.380
 and look who cares what emerges?

43:57.380 --> 43:58.220
 Oh, I don't.

43:58.220 --> 43:59.060
 I don't speculate.

43:59.060 --> 43:59.900
 I don't really care.

43:59.900 --> 44:02.620
 I don't really care which one of these projects wins.

44:02.620 --> 44:05.500
 I'm kind of in the Bitcoin as a meme coin camp.

44:05.500 --> 44:07.000
 I mean, why does Bitcoin have value?

44:07.000 --> 44:10.040
 It's technically kind of, you know,

44:11.740 --> 44:14.380
 not great, like the block size debate.

44:14.380 --> 44:16.140
 When I found out what the block size debate was,

44:16.140 --> 44:18.020
 I'm like, are you guys kidding?

44:18.020 --> 44:21.180
 What's the block size debate?

44:21.180 --> 44:22.020
 You know what?

44:22.020 --> 44:23.820
 It's really, it's too stupid to even talk.

44:23.820 --> 44:27.140
 People can look it up, but I'm like, wow.

44:27.140 --> 44:28.420
 You know, Ethereum seems,

44:28.420 --> 44:31.300
 the governance of Ethereum seems much better.

44:31.300 --> 44:34.460
 I've come around a bit on proof of stake ideas.

44:35.360 --> 44:37.740
 You know, very smart people thinking about some things.

44:37.740 --> 44:40.340
 Yeah, you know, governance is interesting.

44:40.340 --> 44:44.100
 It does feel like Vitalik,

44:44.100 --> 44:46.100
 like it does feel like an open,

44:46.100 --> 44:48.020
 even in these distributed systems,

44:48.020 --> 44:50.100
 leaders are helpful

44:51.220 --> 44:54.580
 because they kind of help you drive the mission

44:54.580 --> 44:58.420
 and the vision and they put a face to a project.

44:58.420 --> 45:00.140
 It's a weird thing about us humans.

45:00.140 --> 45:02.820
 Geniuses are helpful, like Vitalik.

45:02.820 --> 45:03.900
 Yeah, brilliant.

45:06.540 --> 45:10.320
 Leaders are not necessarily, yeah.

45:10.320 --> 45:15.340
 So you think the reason he's the face of Ethereum

45:15.340 --> 45:17.100
 is because he's a genius.

45:17.100 --> 45:18.060
 That's interesting.

45:18.060 --> 45:19.640
 I mean, that was,

45:21.460 --> 45:22.940
 it's interesting to think about

45:22.940 --> 45:24.860
 that we need to create systems

45:25.860 --> 45:30.340
 in which the quote unquote leaders that emerge

45:30.340 --> 45:32.040
 are the geniuses in the system.

45:33.060 --> 45:35.000
 I mean, that's arguably why

45:35.000 --> 45:36.980
 the current state of democracy is broken

45:36.980 --> 45:39.420
 is the people who are emerging as the leaders

45:39.420 --> 45:40.980
 are not the most competent,

45:40.980 --> 45:43.340
 are not the superstars of the system.

45:43.340 --> 45:45.020
 And it seems like at least for now

45:45.020 --> 45:47.220
 in the crypto world oftentimes

45:47.220 --> 45:49.380
 the leaders are the superstars.

45:49.380 --> 45:51.720
 Imagine at the debate they asked,

45:51.720 --> 45:53.640
 what's the sixth amendment?

45:53.640 --> 45:56.300
 What are the four fundamental forces in the universe?

45:56.300 --> 45:58.100
 What's the integral of two to the X?

45:59.940 --> 46:01.740
 I'd love to see those questions asked

46:01.740 --> 46:03.380
 and that's what I want as our leader.

46:03.380 --> 46:04.220
 It's a little bit.

46:04.220 --> 46:05.140
 What's Bayes rule?

46:07.160 --> 46:10.900
 Yeah, I mean, even, oh wow, you're hurting my brain.

46:10.900 --> 46:15.020
 It's that my standard was even lower

46:15.020 --> 46:17.660
 but I would have loved to see

46:17.660 --> 46:20.600
 just this basic brilliance.

46:20.600 --> 46:22.180
 Like I've talked to historians.

46:22.180 --> 46:23.900
 There's just these, they're not even like

46:23.900 --> 46:26.720
 they don't have a PhD or even education history.

46:26.720 --> 46:30.040
 They just like a Dan Carlin type character

46:30.040 --> 46:32.780
 who just like, holy shit.

46:32.780 --> 46:35.420
 How did all this information get into your head?

46:35.420 --> 46:38.460
 They're able to just connect Genghis Khan

46:38.460 --> 46:41.820
 to the entirety of the history of the 20th century.

46:41.820 --> 46:46.180
 They know everything about every single battle that happened

46:46.180 --> 46:51.020
 and they know the game of Thrones

46:51.020 --> 46:55.100
 of the different power plays and all that happened there.

46:55.100 --> 46:56.700
 And they know like the individuals

46:56.700 --> 46:58.580
 and all the documents involved

46:58.580 --> 47:02.060
 and they integrate that into their regular life.

47:02.060 --> 47:03.980
 It's not like they're ultra history nerds.

47:03.980 --> 47:06.380
 They're just, they know this information.

47:06.380 --> 47:08.020
 That's what competence looks like.

47:08.020 --> 47:09.060
 Yeah.

47:09.060 --> 47:10.660
 Cause I've seen that with programmers too, right?

47:10.660 --> 47:12.580
 That's what great programmers do.

47:12.580 --> 47:15.780
 But yeah, it would be, it's really unfortunate

47:15.780 --> 47:19.340
 that those kinds of people aren't emerging as our leaders.

47:19.340 --> 47:21.820
 But for now, at least in the crypto world

47:21.820 --> 47:23.260
 that seems to be the case.

47:23.260 --> 47:26.700
 I don't know if that always, you could imagine

47:26.700 --> 47:28.820
 that in a hundred years, it's not the case, right?

47:28.820 --> 47:31.940
 Crypto world has one very powerful idea going for it

47:31.940 --> 47:35.020
 and that's the idea of forks, right?

47:35.020 --> 47:40.020
 I mean, imagine, we'll use a less controversial example.

47:42.940 --> 47:47.140
 This was actually in my joke app in 2012.

47:47.140 --> 47:49.460
 I was like, Barack Obama, Mitt Romney,

47:49.460 --> 47:51.060
 let's let them both be president, right?

47:51.060 --> 47:52.940
 Like imagine we could fork America

47:52.940 --> 47:54.500
 and just let them both be president.

47:54.500 --> 47:56.060
 And then the Americas could compete

47:56.060 --> 47:58.180
 and people could invest in one,

47:58.180 --> 48:00.500
 pull their liquidity out of one, put it in the other.

48:00.500 --> 48:02.420
 You have this in the crypto world.

48:02.420 --> 48:05.540
 Ethereum forks into Ethereum and Ethereum classic.

48:05.540 --> 48:07.340
 And you can pull your liquidity out of one

48:07.340 --> 48:08.980
 and put it in another.

48:08.980 --> 48:10.740
 And people vote with their dollars,

48:11.980 --> 48:16.420
 which forks, companies should be able to fork.

48:16.420 --> 48:18.180
 I'd love to fork Nvidia, you know?

48:20.260 --> 48:22.780
 Yeah, like different business strategies

48:22.780 --> 48:26.180
 and then try them out and see what works.

48:26.180 --> 48:34.780
 Like even take, yeah, take comma AI that closes its source

48:34.780 --> 48:38.300
 and then take one that's open source and see what works.

48:38.300 --> 48:41.060
 Take one that's purchased by GM

48:41.060 --> 48:43.540
 and one that remains Android Renegade

48:43.540 --> 48:45.300
 and all these different versions and see.

48:45.300 --> 48:47.900
 The beauty of comma AI is someone can actually do that.

48:47.900 --> 48:49.660
 Please take comma AI and fork it.

48:50.620 --> 48:53.020
 That's right, that's the beauty of open source.

48:53.020 --> 48:56.420
 So you're, I mean, we'll talk about autonomous vehicle space,

48:56.420 --> 49:02.100
 but it does seem that you're really knowledgeable

49:02.100 --> 49:03.940
 about a lot of different topics.

49:03.940 --> 49:06.100
 So the natural question a bunch of people ask this,

49:06.100 --> 49:09.980
 which is how do you keep learning new things?

49:09.980 --> 49:12.420
 Do you have like practical advice

49:12.420 --> 49:15.820
 if you were to introspect, like taking notes,

49:15.820 --> 49:19.220
 allocate time, or do you just mess around

49:19.220 --> 49:21.340
 and just allow your curiosity to drive?

49:21.340 --> 49:23.060
 I'll write these people a self help book

49:23.060 --> 49:25.060
 and I'll charge $67 for it.

49:25.060 --> 49:28.500
 And I will write, I will write,

49:28.500 --> 49:30.300
 I will write on the cover of the self help book.

49:30.300 --> 49:32.500
 All of this advice is completely meaningless.

49:32.500 --> 49:34.740
 You're gonna be a sucker and buy this book anyway.

49:34.740 --> 49:38.860
 And the one lesson that I hope they take away from the book

49:38.860 --> 49:42.580
 is that I can't give you a meaningful answer to that.

49:42.580 --> 49:44.020
 That's interesting.

49:44.020 --> 49:45.420
 Let me translate that.

49:45.420 --> 49:50.340
 Is you haven't really thought about what it is you do

49:51.580 --> 49:53.900
 systematically because you could reduce it.

49:53.900 --> 49:56.980
 And there's some people, I mean, I've met brilliant people

49:56.980 --> 50:00.180
 that this is really clear with athletes.

50:00.180 --> 50:03.580
 Some are just, you know, the best in the world

50:03.580 --> 50:06.820
 at something and they have zero interest

50:06.820 --> 50:09.500
 in writing like a self help book,

50:09.500 --> 50:11.620
 or how to master this game.

50:11.620 --> 50:15.620
 And then there's some athletes who become great coaches

50:15.620 --> 50:18.820
 and they love the analysis, perhaps the over analysis.

50:18.820 --> 50:20.740
 And you right now, at least at your age,

50:20.740 --> 50:23.180
 which is an interesting, you're in the middle of the battle.

50:23.180 --> 50:25.620
 You're like the warriors that have zero interest

50:25.620 --> 50:26.660
 in writing books.

50:27.820 --> 50:29.140
 So you're in the middle of the battle.

50:29.140 --> 50:30.580
 So you have, yeah.

50:30.580 --> 50:31.740
 This is a fair point.

50:31.740 --> 50:34.060
 I do think I have a certain aversion

50:34.060 --> 50:39.060
 to this kind of deliberate intentional way of living life.

50:39.060 --> 50:41.820
 You're eventually, the hilarity of this,

50:41.820 --> 50:43.620
 especially since this is recorded,

50:43.620 --> 50:47.620
 it will reveal beautifully the absurdity

50:47.620 --> 50:49.620
 when you finally do publish this book.

50:49.620 --> 50:51.540
 I guarantee you, you will.

50:51.540 --> 50:56.060
 The story of comma AI, maybe it'll be a biography

50:56.060 --> 50:57.060
 written about you.

50:57.060 --> 50:58.740
 That'll be better, I guess.

50:58.740 --> 51:00.740
 And you might be able to learn some cute lessons

51:00.740 --> 51:03.700
 if you're starting a company like comma AI from that book.

51:03.700 --> 51:05.660
 But if you're asking generic questions,

51:05.660 --> 51:07.740
 like how do I be good at things?

51:07.740 --> 51:10.260
 How do I be good at things?

51:10.260 --> 51:11.900
 Dude, I don't know.

51:11.900 --> 51:14.380
 Do them a lot.

51:14.380 --> 51:15.220
 Do them a lot.

51:15.220 --> 51:18.580
 But the interesting thing here is learning things

51:18.580 --> 51:22.020
 outside of your current trajectory,

51:22.020 --> 51:24.660
 which is what it feels like from an outsider's perspective.

51:28.140 --> 51:30.900
 I don't know if there's advice on that,

51:30.900 --> 51:33.340
 but it is an interesting curiosity.

51:33.340 --> 51:38.020
 When you become really busy, you're running a company.

51:38.020 --> 51:38.860
 Hard time.

51:40.340 --> 51:41.580
 Yeah.

51:41.580 --> 51:46.100
 But there's a natural inclination and trend.

51:46.100 --> 51:48.980
 Just the momentum of life carries you

51:48.980 --> 51:51.060
 into a particular direction of wanting to focus.

51:51.060 --> 51:55.140
 And this kind of dispersion that curiosity can lead to

51:55.140 --> 51:58.220
 gets harder and harder with time.

51:58.220 --> 52:00.940
 Because you get really good at certain things

52:00.940 --> 52:03.820
 and it sucks trying things that you're not good at,

52:03.820 --> 52:05.340
 like trying to figure them out.

52:05.340 --> 52:07.380
 When you do this with your live streams,

52:07.380 --> 52:10.060
 you're on the fly figuring stuff out.

52:10.060 --> 52:11.780
 You don't mind looking dumb.

52:11.780 --> 52:12.620
 No.

52:14.140 --> 52:16.660
 You just figure it out pretty quickly.

52:16.660 --> 52:19.180
 Sometimes I try things and I don't figure them out quickly.

52:19.180 --> 52:20.980
 My chest rating is like a 1400,

52:20.980 --> 52:23.300
 despite putting like a couple of hundred hours in.

52:23.300 --> 52:24.340
 It's pathetic.

52:24.340 --> 52:26.860
 I mean, to be fair, I know that I could do it better

52:26.860 --> 52:27.700
 if I did it better.

52:27.700 --> 52:29.860
 Like don't play five minute games,

52:29.860 --> 52:31.380
 play 15 minute games at least.

52:31.380 --> 52:34.260
 Like I know these things, but it just doesn't,

52:34.260 --> 52:37.260
 it doesn't stick nicely in my knowledge stream.

52:37.260 --> 52:39.300
 All right, let's talk about Comma AI.

52:39.300 --> 52:42.140
 What's the mission of the company?

52:42.140 --> 52:44.860
 Let's like look at the biggest picture.

52:44.860 --> 52:46.820
 Oh, I have an exact statement.

52:46.820 --> 52:48.660
 Solve self driving cars

52:48.660 --> 52:51.540
 while delivering shippable intermediaries.

52:51.540 --> 52:56.300
 So longterm vision is have fully autonomous vehicles

52:56.300 --> 52:59.060
 and make sure you're making money along the way.

52:59.060 --> 53:00.220
 I think it doesn't really speak to money,

53:00.220 --> 53:03.260
 but I can talk about what solve self driving cars means.

53:03.260 --> 53:05.260
 Solve self driving cars of course means

53:06.700 --> 53:08.020
 you're not building a new car,

53:08.020 --> 53:10.460
 you're building a person replacement.

53:10.460 --> 53:12.340
 That person can sit in the driver's seat

53:12.340 --> 53:14.580
 and drive you anywhere a person can drive

53:14.580 --> 53:17.980
 with a human or better level of safety,

53:17.980 --> 53:19.500
 speed, quality, comfort.

53:21.420 --> 53:23.260
 And what's the second part of that?

53:23.260 --> 53:26.620
 Delivering shippable intermediaries is well,

53:26.620 --> 53:28.180
 it's a way to fund the company, that's true.

53:28.180 --> 53:30.140
 But it's also a way to keep us honest.

53:31.980 --> 53:34.900
 If you don't have that, it is very easy

53:34.900 --> 53:39.020
 with this technology to think you're making progress

53:39.020 --> 53:40.180
 when you're not.

53:40.180 --> 53:42.380
 I've heard it best described on Hacker News as

53:43.420 --> 53:46.660
 you can set any arbitrary milestone,

53:46.660 --> 53:49.500
 meet that milestone and still be infinitely far away

53:49.500 --> 53:51.740
 from solving self driving cars.

53:51.740 --> 53:53.820
 So it's hard to have like real deadlines

53:53.820 --> 53:58.820
 when you're like Cruz or Waymo when you don't have revenue.

54:02.100 --> 54:04.500
 Is that, I mean, is revenue essentially

54:06.060 --> 54:07.820
 the thing we're talking about here?

54:07.820 --> 54:11.460
 Revenue is, capitalism is based around consent.

54:11.460 --> 54:13.100
 Capitalism, the way that you get revenue

54:13.100 --> 54:16.340
 is real capitalism comes in the real capitalism camp.

54:16.340 --> 54:17.340
 There's definitely scams out there,

54:17.340 --> 54:19.580
 but real capitalism is based around consent.

54:19.580 --> 54:20.740
 It's based around this idea that like,

54:20.740 --> 54:22.980
 if we're getting revenue, it's because we're providing

54:22.980 --> 54:24.780
 at least that much value to another person.

54:24.780 --> 54:27.460
 When someone buys $1,000 comma two from us,

54:27.460 --> 54:29.100
 we're providing them at least $1,000 of value

54:29.100 --> 54:30.180
 or they wouldn't buy it.

54:30.180 --> 54:32.940
 Brilliant, so can you give a whirlwind overview

54:32.940 --> 54:34.940
 of the products that Comma AI provides,

54:34.940 --> 54:38.180
 like throughout its history and today?

54:38.180 --> 54:40.780
 I mean, yeah, the past ones aren't really that interesting.

54:40.780 --> 54:43.860
 It's kind of just been refinement of the same idea.

54:45.180 --> 54:48.300
 The real only product we sell today is the Comma 2.

54:48.300 --> 54:50.500
 Which is a piece of hardware with cameras.

54:50.500 --> 54:54.700
 Mm, so the Comma 2, I mean, you can think about it

54:54.700 --> 54:55.780
 kind of like a person.

54:57.100 --> 54:58.340
 Future hardware will probably be

54:58.340 --> 55:00.300
 even more and more personlike.

55:00.300 --> 55:05.300
 So it has eyes, ears, a mouth, a brain,

55:07.460 --> 55:09.540
 and a way to interface with the car.

55:09.540 --> 55:10.980
 Does it have consciousness?

55:10.980 --> 55:13.500
 Just kidding, that was a trick question.

55:13.500 --> 55:15.140
 I don't have consciousness either.

55:15.140 --> 55:16.420
 Me and the Comma 2 are the same.

55:16.420 --> 55:17.260
 You're the same?

55:17.260 --> 55:18.660
 I have a little more compute than it.

55:18.660 --> 55:23.260
 It only has like the same compute as a B, you know.

55:23.260 --> 55:25.300
 You're more efficient energy wise

55:25.300 --> 55:26.460
 for the compute you're doing.

55:26.460 --> 55:28.100
 Far more efficient energy wise.

55:29.020 --> 55:30.580
 20 petaflops, 20 watts, crazy.

55:30.580 --> 55:32.220
 Do you lack consciousness?

55:32.220 --> 55:33.060
 Sure.

55:33.060 --> 55:33.900
 Do you fear death?

55:33.900 --> 55:35.500
 You do, you want immortality.

55:35.500 --> 55:36.340
 Of course I fear death.

55:36.340 --> 55:38.100
 Does Comma AI fear death?

55:38.100 --> 55:39.580
 I don't think so.

55:39.580 --> 55:40.500
 Of course it does.

55:40.500 --> 55:42.980
 It very much fears, well, it fears negative loss.

55:42.980 --> 55:43.820
 Oh yeah.

55:43.820 --> 55:48.820
 Okay, so Comma 2, when did that come out?

55:49.220 --> 55:50.500
 That was a year ago?

55:50.500 --> 55:52.100
 No, two.

55:52.100 --> 55:53.540
 Early this year.

55:53.540 --> 55:56.900
 Wow, time, it feels like, yeah.

55:56.900 --> 56:00.820
 2020 feels like it's taken 10 years to get to the end.

56:00.820 --> 56:01.820
 It's a long year.

56:01.820 --> 56:03.140
 It's a long year.

56:03.140 --> 56:08.140
 So what's the sexiest thing about Comma 2 feature wise?

56:08.140 --> 56:13.140
 So, I mean, maybe you can also link on like, what is it?

56:14.340 --> 56:15.740
 Like what's its purpose?

56:15.740 --> 56:18.540
 Cause there's a hardware, there's a software component.

56:18.540 --> 56:20.020
 You've mentioned the sensors,

56:20.020 --> 56:23.060
 but also like what is it, its features and capabilities?

56:23.060 --> 56:25.340
 I think our slogan summarizes it well.

56:25.340 --> 56:27.220
 Comma slogan is make driving chill.

56:28.860 --> 56:30.660
 I love it, okay.

56:30.660 --> 56:33.020
 Yeah, I mean, it is, you know,

56:33.020 --> 56:35.340
 if you like cruise control, imagine cruise control,

56:35.340 --> 56:36.660
 but much, much more.

56:36.660 --> 56:41.020
 So it can do adaptive cruise control things,

56:41.020 --> 56:42.980
 which is like slow down for cars in front of it,

56:42.980 --> 56:44.220
 maintain a certain speed.

56:44.220 --> 56:46.260
 And it can also do lane keeping.

56:46.260 --> 56:48.700
 So staying in the lane and doing it better

56:48.700 --> 56:50.020
 and better and better over time.

56:50.020 --> 56:52.060
 It's very much machine learning based.

56:53.060 --> 56:56.660
 So this camera is, there's a driver facing camera too.

57:01.100 --> 57:02.100
 What else is there?

57:02.100 --> 57:02.940
 What am I thinking?

57:02.940 --> 57:04.380
 So the hardware versus software.

57:04.380 --> 57:09.020
 So open pilot versus the actual hardware of the device.

57:09.020 --> 57:10.580
 What's, can you draw that distinction?

57:10.580 --> 57:11.580
 What's one, what's the other?

57:11.580 --> 57:13.820
 I mean, the hardware is pretty much a cell phone

57:13.820 --> 57:14.700
 with a few additions.

57:14.700 --> 57:16.940
 A cell phone with a cooling system

57:16.940 --> 57:20.980
 and with a car interface connected to it.

57:20.980 --> 57:24.460
 And by cell phone, you mean like Qualcomm Snapdragon.

57:25.620 --> 57:27.980
 Yeah, the current hardware is a Snapdragon 821.

57:29.380 --> 57:32.180
 It has wifi radio, it has an LTE radio, it has a screen.

57:32.180 --> 57:35.980
 We use every part of the cell phone.

57:35.980 --> 57:37.540
 And then the interface with the car

57:37.540 --> 57:38.460
 is specific to the car.

57:38.460 --> 57:40.580
 So you keep supporting more and more cars.

57:41.460 --> 57:42.660
 Yeah, so the interface to the car,

57:42.660 --> 57:45.340
 I mean, the device itself just has four CAN buses.

57:45.340 --> 57:46.860
 It has four CAN interfaces on it

57:46.860 --> 57:49.420
 that are connected through the USB port to the phone.

57:49.420 --> 57:53.300
 And then, yeah, on those four CAN buses,

57:53.300 --> 57:54.460
 you connect it to the car.

57:54.460 --> 57:56.460
 And there's a little harness to do this.

57:56.460 --> 57:58.460
 Cars are actually surprisingly similar.

57:58.460 --> 58:01.500
 So CAN is the protocol by which cars communicate.

58:01.500 --> 58:04.260
 And then you're able to read stuff and write stuff

58:04.260 --> 58:06.900
 to be able to control the car depending on the car.

58:06.900 --> 58:08.260
 So what's the software side?

58:08.260 --> 58:09.300
 What's OpenPilot?

58:10.380 --> 58:11.740
 So I mean, OpenPilot is,

58:11.740 --> 58:13.740
 the hardware is pretty simple compared to OpenPilot.

58:13.740 --> 58:18.740
 OpenPilot is, well, so you have a machine learning model,

58:21.020 --> 58:24.540
 which it's in OpenPilot, it's a blob.

58:24.540 --> 58:25.620
 It's just a blob of weights.

58:25.620 --> 58:27.420
 It's not like people are like, oh, it's closed source.

58:27.420 --> 58:28.860
 I'm like, it's a blob of weights.

58:28.860 --> 58:29.780
 What do you expect?

58:29.780 --> 58:33.380
 So it's primarily neural network based.

58:33.380 --> 58:36.020
 You, well, OpenPilot is all the software

58:36.020 --> 58:37.660
 kind of around that neural network.

58:37.660 --> 58:39.060
 That if you have a neural network that says,

58:39.060 --> 58:40.860
 here's where you wanna send the car,

58:40.860 --> 58:43.600
 OpenPilot actually goes and executes all of that.

58:44.780 --> 58:46.900
 It cleans up the input to the neural network.

58:46.900 --> 58:49.060
 It cleans up the output and executes on it.

58:49.060 --> 58:50.860
 So it connects, it's the glue

58:50.860 --> 58:51.860
 that connects everything together.

58:51.860 --> 58:54.420
 Runs the sensors, does a bunch of calibration

58:54.420 --> 58:58.100
 for the neural network, deals with like,

58:58.100 --> 59:00.140
 if the car is on a banked road,

59:00.140 --> 59:02.060
 you have to counter steer against that.

59:02.060 --> 59:03.820
 And the neural network can't necessarily know that

59:03.820 --> 59:05.120
 by looking at the picture.

59:06.420 --> 59:08.100
 So you do that with other sensors

59:08.100 --> 59:09.900
 and Fusion and Localizer.

59:09.900 --> 59:11.780
 OpenPilot also is responsible

59:11.780 --> 59:14.780
 for sending the data up to our servers.

59:14.780 --> 59:17.940
 So we can learn from it, logging it, recording it,

59:17.940 --> 59:21.500
 running the cameras, thermally managing the device,

59:21.500 --> 59:23.180
 managing the disk space on the device,

59:23.180 --> 59:24.780
 managing all the resources on the device.

59:24.780 --> 59:26.860
 So what, since we last spoke,

59:26.860 --> 59:28.460
 I don't remember when, maybe a year ago,

59:28.460 --> 59:30.180
 maybe a little bit longer,

59:30.180 --> 59:33.100
 how has OpenPilot improved?

59:33.100 --> 59:34.860
 We did exactly what I promised you.

59:34.860 --> 59:36.760
 I promised you that by the end of the year,

59:36.760 --> 59:38.860
 where you'd be able to remove the lanes.

59:40.500 --> 59:45.500
 The lateral policy is now almost completely end to end.

59:46.060 --> 59:48.600
 You can turn the lanes off and it will drive,

59:48.600 --> 59:49.980
 drive slightly worse on the highway

59:49.980 --> 59:51.060
 if you turn the lanes off,

59:51.060 --> 59:54.260
 but you can turn the lanes off and it will drive well,

59:54.260 --> 59:57.220
 trained completely end to end on user data.

59:57.220 --> 59:58.700
 And this year we hope to do the same

59:58.700 --> 1:00:00.140
 for the longitudinal policy.

1:00:00.140 --> 1:00:03.380
 So that's the interesting thing is you're not doing,

1:00:03.380 --> 1:00:05.380
 you don't appear to be, maybe you can correct me,

1:00:05.380 --> 1:00:08.700
 you don't appear to be doing lane detection

1:00:08.700 --> 1:00:12.420
 or lane marking detection or kind of the segmentation task

1:00:12.420 --> 1:00:15.100
 or any kind of object detection task.

1:00:15.100 --> 1:00:17.580
 You're doing what's traditionally more called

1:00:17.580 --> 1:00:19.420
 like end to end learning.

1:00:19.420 --> 1:00:24.180
 So, and trained on actual behavior of drivers

1:00:24.180 --> 1:00:26.100
 when they're driving the car manually.

1:00:27.780 --> 1:00:29.820
 And this is hard to do.

1:00:29.820 --> 1:00:31.340
 It's not supervised learning.

1:00:32.220 --> 1:00:34.740
 Yeah, but so the nice thing is there's a lot of data.

1:00:34.740 --> 1:00:37.060
 So it's hard and easy, right?

1:00:37.060 --> 1:00:37.900
 It's a...

1:00:37.900 --> 1:00:40.020
 We have a lot of high quality data, yeah.

1:00:40.020 --> 1:00:41.700
 Like more than you need in the second.

1:00:41.700 --> 1:00:42.700
 Well...

1:00:42.700 --> 1:00:43.520
 We have way more than we do.

1:00:43.520 --> 1:00:44.980
 We have way more data than we need.

1:00:44.980 --> 1:00:47.040
 I mean, it's an interesting question actually,

1:00:47.040 --> 1:00:50.440
 because in terms of amount, you have more than you need,

1:00:50.440 --> 1:00:54.260
 but the driving is full of edge cases.

1:00:54.260 --> 1:00:57.180
 So how do you select the data you train on?

1:00:58.260 --> 1:01:00.540
 I think this is an interesting open question.

1:01:00.540 --> 1:01:04.220
 Like what's the cleverest way to select data?

1:01:04.220 --> 1:01:06.660
 That's the question Tesla is probably working on.

1:01:07.660 --> 1:01:09.900
 That's, I mean, the entirety of machine learning can be,

1:01:09.900 --> 1:01:11.060
 they don't seem to really care.

1:01:11.060 --> 1:01:12.260
 They just kind of select data.

1:01:12.260 --> 1:01:14.860
 But I feel like that if you want to solve,

1:01:14.860 --> 1:01:16.220
 if you want to create intelligent systems,

1:01:16.220 --> 1:01:18.900
 you have to pick data well, right?

1:01:18.900 --> 1:01:22.900
 And so do you have any hints, ideas of how to do it well?

1:01:22.900 --> 1:01:25.140
 So in some ways that is...

1:01:25.140 --> 1:01:27.400
 The definition I like of reinforcement learning

1:01:27.400 --> 1:01:29.300
 versus supervised learning.

1:01:29.300 --> 1:01:32.720
 In supervised learning, the weights depend on the data.

1:01:32.720 --> 1:01:33.560
 Right?

1:01:34.560 --> 1:01:35.700
 And this is obviously true,

1:01:35.700 --> 1:01:38.320
 but in reinforcement learning,

1:01:38.320 --> 1:01:40.420
 the data depends on the weights.

1:01:40.420 --> 1:01:41.380
 Yeah.

1:01:41.380 --> 1:01:42.540
 And actually both ways.

1:01:42.540 --> 1:01:43.980
 That's poetry.

1:01:43.980 --> 1:01:46.260
 So how does it know what data to train on?

1:01:46.260 --> 1:01:47.540
 Well, let it pick.

1:01:47.540 --> 1:01:49.460
 We're not there yet, but that's the eventual.

1:01:49.460 --> 1:01:51.100
 So you're thinking this almost like

1:01:51.100 --> 1:01:53.320
 a reinforcement learning framework.

1:01:53.320 --> 1:01:55.380
 We're going to do RL on the world.

1:01:55.380 --> 1:01:58.140
 Every time a car makes a mistake, user disengages,

1:01:58.140 --> 1:02:00.140
 we train on that and do RL on the world.

1:02:00.140 --> 1:02:03.260
 Ship out a new model, that's an epoch, right?

1:02:03.260 --> 1:02:08.260
 And for now you're not doing the Elon style promising

1:02:08.540 --> 1:02:09.700
 that it's going to be fully autonomous.

1:02:09.700 --> 1:02:12.420
 You really are sticking to level two

1:02:12.420 --> 1:02:15.300
 and like it's supposed to be supervised.

1:02:15.300 --> 1:02:16.900
 It is definitely supposed to be supervised

1:02:16.900 --> 1:02:19.740
 and we enforce the fact that it's supervised.

1:02:19.740 --> 1:02:23.660
 We look at our rate of improvement in disengagements.

1:02:23.660 --> 1:02:25.700
 OpenPilot now has an unplanned disengagement

1:02:25.700 --> 1:02:27.440
 about every a hundred miles.

1:02:27.440 --> 1:02:31.620
 This is up from 10 miles, like maybe,

1:02:32.580 --> 1:02:36.560
 maybe maybe a year ago.

1:02:36.560 --> 1:02:37.400
 Yeah.

1:02:37.400 --> 1:02:38.500
 So maybe we've seen 10 X improvement in a year,

1:02:38.500 --> 1:02:41.740
 but a hundred miles is still a far cry

1:02:41.740 --> 1:02:43.880
 from the a hundred thousand you're going to need.

1:02:43.880 --> 1:02:48.060
 So you're going to somehow need to get three more 10 Xs

1:02:48.060 --> 1:02:48.900
 in there.

1:02:49.880 --> 1:02:52.300
 And you're, what's your intuition?

1:02:52.300 --> 1:02:54.540
 You're basically hoping that there's exponential

1:02:54.540 --> 1:02:56.940
 improvement built into the baked into the cake somewhere.

1:02:56.940 --> 1:02:58.420
 Well, that's even, I mean, 10 X improvement,

1:02:58.420 --> 1:03:00.540
 that's already assuming exponential, right?

1:03:00.540 --> 1:03:02.620
 There's definitely exponential improvement.

1:03:02.620 --> 1:03:04.340
 And I think when Elon talks about exponential,

1:03:04.340 --> 1:03:06.340
 like these things, these systems are going to

1:03:06.340 --> 1:03:09.820
 exponentially improve, just exponential doesn't mean

1:03:09.820 --> 1:03:12.660
 you're getting a hundred gigahertz processors tomorrow.

1:03:12.660 --> 1:03:15.060
 Right? Like it's going to still take a while

1:03:15.060 --> 1:03:18.340
 because the gap between even our best system

1:03:18.340 --> 1:03:20.300
 and humans is still large.

1:03:20.300 --> 1:03:22.340
 So that's an interesting distinction to draw.

1:03:22.340 --> 1:03:25.100
 So if you look at the way Tesla is approaching the problem

1:03:26.100 --> 1:03:28.340
 and the way you're approaching the problem,

1:03:28.340 --> 1:03:31.780
 which is very different than the rest of the self driving

1:03:31.780 --> 1:03:32.720
 car world.

1:03:32.720 --> 1:03:35.380
 So let's put them aside is you're treating most

1:03:35.380 --> 1:03:37.500
 the driving task as a machine learning problem.

1:03:37.500 --> 1:03:40.260
 And the way Tesla is approaching it is with the multitask

1:03:40.260 --> 1:03:44.100
 learning where you break the task of driving into hundreds

1:03:44.100 --> 1:03:47.180
 of different tasks and you have this multiheaded

1:03:47.180 --> 1:03:51.660
 neural network that's very good at performing each task.

1:03:51.660 --> 1:03:54.620
 And there there's presumably something on top that's

1:03:54.620 --> 1:03:59.240
 stitching stuff together in order to make control

1:03:59.240 --> 1:04:02.180
 decisions, policy decisions about how you move the car.

1:04:02.180 --> 1:04:04.420
 But what that allows you, there's a brilliance to this

1:04:04.420 --> 1:04:08.360
 because it allows you to master each task,

1:04:08.360 --> 1:04:13.360
 like lane detection, stop sign detection,

1:04:13.380 --> 1:04:17.620
 the traffic light detection, drivable area segmentation,

1:04:19.180 --> 1:04:21.840
 you know, vehicle, bicycle, pedestrian detection.

1:04:23.000 --> 1:04:25.420
 There's some localization tasks in there.

1:04:25.420 --> 1:04:30.420
 Also predicting of like, yeah,

1:04:30.440 --> 1:04:34.060
 predicting how the entities in the scene are going to move.

1:04:34.060 --> 1:04:36.360
 Like everything is basically a machine learning task.

1:04:36.360 --> 1:04:40.380
 So there's a classification, segmentation, prediction.

1:04:40.380 --> 1:04:44.460
 And it's nice because you can have this entire engine,

1:04:44.460 --> 1:04:48.740
 data engine that's mining for edge cases for each one of

1:04:48.740 --> 1:04:49.580
 these tasks.

1:04:49.580 --> 1:04:52.200
 And you can have people like engineers that are basically

1:04:52.200 --> 1:04:53.820
 masters of that task,

1:04:53.820 --> 1:04:56.600
 like become the best person in the world at,

1:04:56.600 --> 1:04:59.860
 as you talk about the cone guy for Waymo,

1:04:59.860 --> 1:05:06.700
 the becoming the best person in the world at cone detection.

1:05:06.700 --> 1:05:10.140
 So that's a compelling notion from a supervised learning

1:05:10.140 --> 1:05:15.380
 perspective, automating much of the process of edge case

1:05:15.380 --> 1:05:17.820
 discovery and retraining neural network for each of the

1:05:17.820 --> 1:05:19.780
 individual perception tasks.

1:05:19.780 --> 1:05:22.460
 And then you're looking at the machine learning in a more

1:05:22.460 --> 1:05:27.240
 holistic way, basically doing end to end learning on the

1:05:27.240 --> 1:05:31.500
 driving tasks, supervised, trained on the data of the

1:05:31.500 --> 1:05:34.420
 actual driving of people.

1:05:34.420 --> 1:05:37.580
 They use comma AI, like actual human drivers,

1:05:37.580 --> 1:05:38.700
 their manual control,

1:05:38.700 --> 1:05:43.700
 plus the moments of disengagement that maybe with some

1:05:44.400 --> 1:05:47.340
 labeling could indicate the failure of the system.

1:05:47.340 --> 1:05:48.900
 So you have the,

1:05:48.900 --> 1:05:52.420
 you have a huge amount of data for positive control of the

1:05:52.420 --> 1:05:55.560
 vehicle, like successful control of the vehicle,

1:05:55.560 --> 1:05:58.340
 both maintaining the lane as,

1:05:58.340 --> 1:06:01.060
 as I think you're also working on longitudinal control of

1:06:01.060 --> 1:06:04.700
 the vehicle and then failure cases where the vehicle does

1:06:04.700 --> 1:06:08.380
 something wrong that needs disengagement.

1:06:08.380 --> 1:06:09.820
 So like what,

1:06:09.820 --> 1:06:14.220
 why do you think you're right and Tesla is wrong on this?

1:06:14.220 --> 1:06:15.660
 And do you think,

1:06:15.660 --> 1:06:17.540
 do you think you'll come around the Tesla way?

1:06:17.540 --> 1:06:20.100
 Do you think Tesla will come around to your way?

1:06:21.400 --> 1:06:23.920
 If you were to start a chess engine company,

1:06:23.920 --> 1:06:26.080
 would you hire a Bishop guy?

1:06:26.080 --> 1:06:27.660
 See, we have a,

1:06:27.660 --> 1:06:29.060
 this is Monday morning.

1:06:29.060 --> 1:06:33.740
 Quarterbacking is a yes, probably.

1:06:36.340 --> 1:06:37.400
 Oh, our Rook guy.

1:06:37.400 --> 1:06:39.420
 Oh, we stole the Rook guy from that company.

1:06:39.420 --> 1:06:40.780
 Oh, we're going to have real good Rooks.

1:06:40.780 --> 1:06:42.940
 Well, there's not many pieces, right?

1:06:43.900 --> 1:06:44.740
 You can,

1:06:46.220 --> 1:06:48.820
 there's not many guys and gals to hire.

1:06:48.820 --> 1:06:51.060
 You just have a few that work in the Bishop,

1:06:51.060 --> 1:06:52.860
 a few that work in the Rook.

1:06:52.860 --> 1:06:55.340
 Is that not ludicrous today to think about

1:06:55.340 --> 1:06:57.520
 in a world of AlphaZero?

1:06:57.520 --> 1:06:58.860
 But AlphaZero is a chess game.

1:06:58.860 --> 1:07:01.780
 So the fundamental question is,

1:07:01.780 --> 1:07:04.340
 how hard is driving compared to chess?

1:07:04.340 --> 1:07:07.200
 Because, so long term,

1:07:07.200 --> 1:07:08.980
 end to end,

1:07:08.980 --> 1:07:10.620
 will be the right solution.

1:07:10.620 --> 1:07:13.460
 The question is how many years away is that?

1:07:13.460 --> 1:07:15.740
 End to end is going to be the only solution for level five.

1:07:15.740 --> 1:07:17.220
 For the only way we'll get there.

1:07:17.220 --> 1:07:18.220
 Of course, and of course,

1:07:18.220 --> 1:07:19.740
 Tesla is going to come around to my way.

1:07:19.740 --> 1:07:22.980
 And if you're a Rook guy out there, I'm sorry.

1:07:22.980 --> 1:07:23.940
 The cone guy.

1:07:24.980 --> 1:07:25.820
 I don't know.

1:07:25.820 --> 1:07:26.940
 We're going to specialize each task.

1:07:26.940 --> 1:07:29.100
 We're going to really understand Rook placement.

1:07:29.100 --> 1:07:30.540
 Yeah.

1:07:30.540 --> 1:07:32.060
 I understand the intuition you have.

1:07:32.060 --> 1:07:32.900
 I mean, that,

1:07:35.100 --> 1:07:36.820
 that is a very compelling notion

1:07:36.820 --> 1:07:39.140
 that we can learn the task end to end,

1:07:39.140 --> 1:07:40.820
 like the same compelling notion you might have

1:07:40.820 --> 1:07:42.620
 for natural language conversation.

1:07:42.620 --> 1:07:43.460
 But I'm not

1:07:44.800 --> 1:07:45.740
 sure,

1:07:47.060 --> 1:07:48.940
 because one thing you sneaked in there

1:07:48.940 --> 1:07:53.180
 is the assertion that it's impossible to get to level five

1:07:53.180 --> 1:07:55.420
 without this kind of approach.

1:07:55.420 --> 1:07:57.140
 I don't know if that's obvious.

1:07:57.140 --> 1:07:58.300
 I don't know if that's obvious either.

1:07:58.300 --> 1:08:01.340
 I don't actually mean that.

1:08:01.340 --> 1:08:03.500
 I think that it is much easier

1:08:03.500 --> 1:08:05.700
 to get to level five with an end to end approach.

1:08:05.700 --> 1:08:08.900
 I think that the other approach is doable,

1:08:08.900 --> 1:08:11.380
 but the magnitude of the engineering challenge

1:08:11.380 --> 1:08:13.780
 may exceed what humanity is capable of.

1:08:13.780 --> 1:08:18.780
 But what do you think of the Tesla data engine approach,

1:08:19.140 --> 1:08:21.100
 which to me is an active learning task,

1:08:21.100 --> 1:08:22.500
 is kind of fascinating,

1:08:22.500 --> 1:08:25.700
 is breaking it down into these multiple tasks

1:08:25.700 --> 1:08:29.500
 and mining their data constantly for like edge cases

1:08:29.500 --> 1:08:30.500
 for these different tasks.

1:08:30.500 --> 1:08:32.460
 Yeah, but the tasks themselves are not being learned.

1:08:32.460 --> 1:08:33.860
 This is feature engineering.

1:08:35.700 --> 1:08:40.700
 Yeah, I mean, it's a higher abstraction level

1:08:40.740 --> 1:08:43.340
 of feature engineering for the different tasks.

1:08:43.340 --> 1:08:44.740
 Task engineering in a sense.

1:08:44.740 --> 1:08:46.780
 It's slightly better feature engineering,

1:08:46.780 --> 1:08:49.260
 but it's still fundamentally is feature engineering.

1:08:49.260 --> 1:08:51.300
 And if anything about the history of AI

1:08:51.300 --> 1:08:52.620
 has taught us anything,

1:08:52.620 --> 1:08:54.540
 it's that feature engineering approaches

1:08:54.540 --> 1:08:57.660
 will always be replaced and lose to end to end.

1:08:57.660 --> 1:09:02.060
 Now, to be fair, I cannot really make promises on timelines,

1:09:02.060 --> 1:09:05.700
 but I can say that when you look at the code for Stockfish

1:09:05.700 --> 1:09:06.940
 and the code for AlphaZero,

1:09:06.940 --> 1:09:09.060
 one is a lot shorter than the other,

1:09:09.060 --> 1:09:09.900
 a lot more elegant,

1:09:09.900 --> 1:09:12.580
 required a lot less programmer hours to write.

1:09:12.580 --> 1:09:17.580
 Yeah, but there was a lot more murder of bad agents

1:09:21.620 --> 1:09:24.740
 on the AlphaZero side.

1:09:24.740 --> 1:09:29.220
 By murder, I mean agents that played a game

1:09:29.220 --> 1:09:30.420
 and failed miserably.

1:09:30.420 --> 1:09:31.460
 Yeah.

1:09:31.460 --> 1:09:32.300
 Oh, oh.

1:09:32.300 --> 1:09:34.740
 In simulation, that failure is less costly.

1:09:34.740 --> 1:09:35.580
 Yeah.

1:09:35.580 --> 1:09:37.300
 In real world, it's...

1:09:37.300 --> 1:09:38.260
 Do you mean in practice,

1:09:38.260 --> 1:09:40.660
 like AlphaZero has lost games miserably?

1:09:40.660 --> 1:09:41.500
 No.

1:09:41.500 --> 1:09:42.540
 Wow.

1:09:42.540 --> 1:09:43.380
 I haven't seen that.

1:09:43.380 --> 1:09:47.380
 No, but I know, but the requirement for AlphaZero is...

1:09:47.380 --> 1:09:48.220
 A simulator.

1:09:48.220 --> 1:09:51.500
 To be able to like evolution, human evolution,

1:09:51.500 --> 1:09:54.420
 not human evolution, biological evolution of life on earth

1:09:54.420 --> 1:09:58.700
 from the origin of life has murdered trillions

1:09:58.700 --> 1:10:02.260
 upon trillions of organisms on the path thus humans.

1:10:02.260 --> 1:10:03.100
 Yeah.

1:10:03.100 --> 1:10:05.860
 So the question is, can we stitch together

1:10:05.860 --> 1:10:07.940
 a human like object without having to go

1:10:07.940 --> 1:10:09.900
 through the entirety process of evolution?

1:10:09.900 --> 1:10:11.940
 Well, no, but do the evolution in simulation.

1:10:11.940 --> 1:10:12.860
 Yeah, that's the question.

1:10:12.860 --> 1:10:13.700
 Can we simulate?

1:10:13.700 --> 1:10:15.060
 So do you have a sense that it's possible

1:10:15.060 --> 1:10:16.220
 to simulate some aspect?

1:10:16.220 --> 1:10:18.220
 MuZero is exactly this.

1:10:18.220 --> 1:10:21.300
 MuZero is the solution to this.

1:10:21.300 --> 1:10:23.980
 MuZero I think is going to be looked back

1:10:23.980 --> 1:10:25.220
 as the canonical paper.

1:10:25.220 --> 1:10:26.860
 And I don't think deep learning is everything.

1:10:26.860 --> 1:10:28.700
 I think that there's still a bunch of things missing

1:10:28.700 --> 1:10:31.420
 to get there, but MuZero I think is going to be looked back

1:10:31.420 --> 1:10:34.260
 as the kind of cornerstone paper

1:10:34.260 --> 1:10:37.060
 of this whole deep learning era.

1:10:37.060 --> 1:10:39.580
 And MuZero is the solution to self driving cars.

1:10:39.580 --> 1:10:41.220
 You have to make a few tweaks to it,

1:10:41.220 --> 1:10:42.820
 but MuZero does effectively that.

1:10:42.820 --> 1:10:45.500
 It does those rollouts and those murdering

1:10:45.500 --> 1:10:48.740
 in a learned simulator and a learned dynamics model.

1:10:50.180 --> 1:10:51.020
 That's interesting.

1:10:51.020 --> 1:10:51.860
 It doesn't get enough love.

1:10:51.860 --> 1:10:54.220
 I was blown away when I read that paper.

1:10:54.220 --> 1:10:57.060
 I'm like, okay, I've always said a comma.

1:10:57.060 --> 1:10:58.460
 I'm going to sit and I'm going to wait for the solution

1:10:58.460 --> 1:11:00.340
 to self driving cars to come along.

1:11:00.340 --> 1:11:01.180
 This year I saw it.

1:11:01.180 --> 1:11:02.020
 It's MuZero.

1:11:05.060 --> 1:11:05.900
 So.

1:11:06.860 --> 1:11:09.180
 Sit back and let the winning roll in.

1:11:09.180 --> 1:11:12.300
 So your sense, just to elaborate a little bit,

1:11:12.300 --> 1:11:13.380
 it's a link on the topic.

1:11:13.380 --> 1:11:16.300
 Your sense is neural networks will solve driving.

1:11:16.300 --> 1:11:17.140
 Yes.

1:11:17.140 --> 1:11:18.820
 Like we don't need anything else.

1:11:18.820 --> 1:11:21.260
 I think the same way chess was maybe the chess

1:11:21.260 --> 1:11:25.060
 and maybe Google are the pinnacle of like search algorithms

1:11:25.060 --> 1:11:27.100
 and things that look kind of like a star.

1:11:28.420 --> 1:11:32.780
 The pinnacle of this era is going to be self driving cars.

1:11:34.700 --> 1:11:38.180
 But on the path of that, you have to deliver products

1:11:38.180 --> 1:11:42.340
 and it's possible that the path to full self driving cars

1:11:42.340 --> 1:11:44.420
 will take decades.

1:11:44.420 --> 1:11:45.340
 I doubt it.

1:11:45.340 --> 1:11:46.980
 How long would you put on it?

1:11:47.820 --> 1:11:52.420
 Like what are we, you're chasing it, Tesla's chasing it.

1:11:53.460 --> 1:11:54.340
 What are we talking about?

1:11:54.340 --> 1:11:56.180
 Five years, 10 years, 50 years.

1:11:56.180 --> 1:11:58.060
 Let's say in the 2020s.

1:11:58.060 --> 1:11:59.540
 In the 2020s.

1:11:59.540 --> 1:12:01.180
 The later part of the 2020s.

1:12:03.580 --> 1:12:05.500
 With the neural network.

1:12:05.500 --> 1:12:06.580
 Well, that would be nice to see.

1:12:06.580 --> 1:12:09.140
 And then the path to that, you're delivering products,

1:12:09.140 --> 1:12:10.580
 which is a nice L2 system.

1:12:10.580 --> 1:12:13.060
 That's what Tesla's doing, a nice L2 system.

1:12:13.060 --> 1:12:14.380
 Just gets better every time.

1:12:14.380 --> 1:12:16.660
 L2, the only difference between L2 and the other levels

1:12:16.660 --> 1:12:17.620
 is who takes liability.

1:12:17.620 --> 1:12:20.100
 And I'm not a liability guy, I don't wanna take liability.

1:12:20.100 --> 1:12:21.540
 I'm gonna level two forever.

1:12:22.700 --> 1:12:25.780
 Now on that little transition,

1:12:25.780 --> 1:12:29.060
 I mean, how do you make the transition work?

1:12:29.060 --> 1:12:32.780
 Is this where driver sensing comes in?

1:12:32.780 --> 1:12:35.940
 Like how do you make the, cause you said a hundred miles,

1:12:35.940 --> 1:12:40.940
 like, is there some sort of human factor psychology thing

1:12:41.340 --> 1:12:43.140
 where people start to overtrust the system,

1:12:43.140 --> 1:12:45.020
 all those kinds of effects,

1:12:45.020 --> 1:12:46.780
 once it gets better and better and better and better,

1:12:46.780 --> 1:12:49.340
 they get lazier and lazier and lazier.

1:12:49.340 --> 1:12:52.460
 Is that, like, how do you get that transition right?

1:12:52.460 --> 1:12:54.500
 First off, our monitoring is already adaptive.

1:12:54.500 --> 1:12:56.620
 Our monitoring is already seen adaptive.

1:12:56.620 --> 1:12:58.940
 Driver monitoring is just the camera

1:12:58.940 --> 1:13:00.060
 that's looking at the driver.

1:13:00.060 --> 1:13:03.060
 You have an infrared camera in the...

1:13:03.060 --> 1:13:06.340
 Our policy for how we enforce the driver monitoring

1:13:06.340 --> 1:13:07.940
 is seen adaptive.

1:13:07.940 --> 1:13:08.780
 What's that mean?

1:13:08.780 --> 1:13:11.420
 Well, for example, in one of the extreme cases,

1:13:12.580 --> 1:13:14.860
 if the car is not moving,

1:13:14.860 --> 1:13:19.460
 we do not actively enforce driver monitoring, right?

1:13:19.460 --> 1:13:22.380
 If you are going through a,

1:13:22.380 --> 1:13:24.860
 like a 45 mile an hour road with lights

1:13:25.780 --> 1:13:27.980
 and stop signs and potentially pedestrians,

1:13:27.980 --> 1:13:30.780
 we enforce a very tight driver monitoring policy.

1:13:30.780 --> 1:13:33.860
 If you are alone on a perfectly straight highway,

1:13:33.860 --> 1:13:35.620
 and this is, it's all machine learning.

1:13:35.620 --> 1:13:36.940
 None of that is hand coded.

1:13:36.940 --> 1:13:39.060
 Actually, the stop is hand coded, but...

1:13:39.060 --> 1:13:41.100
 So there's some kind of machine learning

1:13:41.100 --> 1:13:42.300
 estimation of risk.

1:13:42.300 --> 1:13:43.620
 Yes.

1:13:43.620 --> 1:13:44.460
 Yeah.

1:13:44.460 --> 1:13:45.860
 I mean, I've always been a huge fan of that.

1:13:45.860 --> 1:13:47.500
 That's a...

1:13:47.500 --> 1:13:48.340
 Because...

1:13:48.340 --> 1:13:53.340
 It's difficult to do every step into that direction

1:13:53.780 --> 1:13:55.100
 is a worthwhile step to take.

1:13:55.100 --> 1:13:56.540
 It might be difficult to do really well.

1:13:56.540 --> 1:13:59.980
 Like us humans are able to estimate risk pretty damn well,

1:13:59.980 --> 1:14:01.500
 whatever the hell that is.

1:14:01.500 --> 1:14:05.540
 That feels like one of the nice features of us humans.

1:14:06.500 --> 1:14:08.980
 Cause like we humans are really good drivers

1:14:08.980 --> 1:14:11.180
 when we're really like tuned in

1:14:11.180 --> 1:14:12.940
 and we're good at estimating risk.

1:14:12.940 --> 1:14:14.900
 Like when are we supposed to be tuned in?

1:14:14.900 --> 1:14:15.980
 Yeah.

1:14:15.980 --> 1:14:17.580
 And, you know, people are like,

1:14:17.580 --> 1:14:18.420
 oh, well, you know,

1:14:18.420 --> 1:14:20.420
 why would you ever make the driver monitoring policy

1:14:20.420 --> 1:14:21.260
 less aggressive?

1:14:21.260 --> 1:14:23.980
 Why would you always not keep it at its most aggressive?

1:14:23.980 --> 1:14:25.780
 Because then people are just going to get fatigued from it.

1:14:25.780 --> 1:14:26.620
 Yes.

1:14:26.620 --> 1:14:27.460
 When they get annoyed.

1:14:27.460 --> 1:14:28.300
 You want them...

1:14:28.300 --> 1:14:29.140
 Yeah.

1:14:29.140 --> 1:14:30.900
 You want the experience to be pleasant.

1:14:30.900 --> 1:14:32.500
 Obviously I want the experience to be pleasant,

1:14:32.500 --> 1:14:35.980
 but even just from a straight up safety perspective,

1:14:35.980 --> 1:14:39.740
 if you alert people when they look around and they're like,

1:14:39.740 --> 1:14:41.020
 why is this thing alerting me?

1:14:41.020 --> 1:14:42.980
 There's nothing I could possibly hit right now.

1:14:42.980 --> 1:14:45.220
 People will just learn to tune it out.

1:14:45.220 --> 1:14:46.900
 People will just learn to tune it out,

1:14:46.900 --> 1:14:48.060
 to put weights on the steering wheel,

1:14:48.060 --> 1:14:49.820
 to do whatever to overcome it.

1:14:49.820 --> 1:14:52.580
 And remember that you're always part

1:14:52.580 --> 1:14:53.580
 of this adaptive system.

1:14:53.580 --> 1:14:55.660
 So all I can really say about, you know,

1:14:55.660 --> 1:14:57.180
 how this scales going forward is yeah,

1:14:57.180 --> 1:14:59.420
 it's something we have to monitor for.

1:14:59.420 --> 1:15:00.260
 Ooh, we don't know.

1:15:00.260 --> 1:15:02.060
 This is a great psychology experiment at scale.

1:15:02.060 --> 1:15:03.220
 Like we'll see.

1:15:03.220 --> 1:15:04.060
 Yeah, it's fascinating.

1:15:04.060 --> 1:15:04.900
 Track it.

1:15:04.900 --> 1:15:09.140
 And making sure you have a good understanding of attention

1:15:09.140 --> 1:15:11.420
 is a very key part of that psychology problem.

1:15:11.420 --> 1:15:12.260
 Yeah.

1:15:12.260 --> 1:15:14.100
 I think you and I probably have a different,

1:15:14.100 --> 1:15:16.660
 come to it differently, but to me,

1:15:16.660 --> 1:15:19.700
 it's a fascinating psychology problem

1:15:19.700 --> 1:15:22.100
 to explore something much deeper than just driving.

1:15:22.100 --> 1:15:26.700
 It's such a nice way to explore human attention

1:15:26.700 --> 1:15:30.180
 and human behavior, which is why, again,

1:15:30.180 --> 1:15:34.100
 we've probably both criticized Mr. Elon Musk

1:15:34.100 --> 1:15:38.220
 on this one topic from different avenues.

1:15:38.220 --> 1:15:39.740
 So both offline and online,

1:15:39.740 --> 1:15:44.380
 I had little chats with Elon and like,

1:15:44.380 --> 1:15:48.620
 I love human beings as a computer vision problem,

1:15:48.620 --> 1:15:51.020
 as an AI problem, it's fascinating.

1:15:51.020 --> 1:15:53.260
 He wasn't so much interested in that problem.

1:15:53.260 --> 1:15:56.820
 It's like in order to solve driving,

1:15:56.820 --> 1:15:58.780
 the whole point is you want to remove the human

1:15:58.780 --> 1:15:59.780
 from the picture.

1:16:01.300 --> 1:16:04.140
 And it seems like you can't do that quite yet.

1:16:04.140 --> 1:16:07.900
 Eventually, yes, but you can't quite do that yet.

1:16:07.900 --> 1:16:12.300
 So this is the moment where you can't yet say,

1:16:12.300 --> 1:16:17.300
 I told you so to Tesla, but it's getting there

1:16:17.700 --> 1:16:19.220
 because I don't know if you've seen this,

1:16:19.220 --> 1:16:21.100
 there's some reporting that they're in fact

1:16:21.100 --> 1:16:23.260
 starting to do driver monitoring.

1:16:23.260 --> 1:16:25.020
 Yeah, they shift the model in shadow mode.

1:16:26.260 --> 1:16:29.620
 With, I believe, only a visible light camera,

1:16:29.620 --> 1:16:31.780
 it might even be fisheye.

1:16:31.780 --> 1:16:33.260
 It's like a low resolution.

1:16:33.260 --> 1:16:34.820
 Low resolution, visible light.

1:16:34.820 --> 1:16:37.100
 I mean, to be fair, that's what we have in the Eon as well,

1:16:37.100 --> 1:16:38.820
 our last generation product.

1:16:38.820 --> 1:16:41.060
 This is the one area where I can say

1:16:41.060 --> 1:16:42.180
 our hardware is ahead of Tesla.

1:16:42.180 --> 1:16:43.820
 The rest of our hardware, way, way behind,

1:16:43.820 --> 1:16:46.020
 but our driver monitoring camera.

1:16:46.020 --> 1:16:50.940
 So you think, I think on the third row Tesla podcast,

1:16:50.940 --> 1:16:54.580
 or somewhere else, I've heard you say that obviously,

1:16:54.580 --> 1:16:57.220
 eventually they're gonna have driver monitoring.

1:16:57.220 --> 1:16:59.660
 I think what I've said is Elon will definitely ship

1:16:59.660 --> 1:17:01.700
 driver monitoring before he ships level five.

1:17:01.700 --> 1:17:02.540
 Before level five.

1:17:02.540 --> 1:17:04.660
 And I'm willing to bet 10 grand on that.

1:17:04.660 --> 1:17:06.180
 And you bet 10 grand on that.

1:17:07.060 --> 1:17:08.260
 I mean, now I don't wanna take the bet,

1:17:08.260 --> 1:17:09.500
 but before, maybe someone would have,

1:17:09.500 --> 1:17:10.500
 oh, I should have got my money in.

1:17:10.500 --> 1:17:11.900
 Yeah.

1:17:11.900 --> 1:17:12.980
 It's an interesting bet.

1:17:12.980 --> 1:17:16.460
 I think you're right.

1:17:16.460 --> 1:17:19.180
 I'm actually on a human level

1:17:19.180 --> 1:17:24.180
 because he's been, he's made the decision.

1:17:24.380 --> 1:17:27.660
 Like he said that driver monitoring is the wrong way to go.

1:17:27.660 --> 1:17:31.260
 But like, you have to think of as a human, as a CEO,

1:17:31.260 --> 1:17:34.980
 I think that's the right thing to say when,

1:17:36.460 --> 1:17:40.140
 like sometimes you have to say things publicly

1:17:40.140 --> 1:17:41.780
 that are different than when you actually believe,

1:17:41.780 --> 1:17:45.420
 because when you're producing a large number of vehicles

1:17:45.420 --> 1:17:47.860
 and the decision was made not to include the camera,

1:17:47.860 --> 1:17:49.460
 like what are you supposed to say?

1:17:49.460 --> 1:17:51.780
 Like our cars don't have the thing

1:17:51.780 --> 1:17:54.020
 that I think is right to have.

1:17:54.020 --> 1:17:55.780
 It's an interesting thing.

1:17:55.780 --> 1:17:58.340
 But like on the other side, as a CEO,

1:17:58.340 --> 1:18:01.220
 I mean, something you could probably speak to as a leader,

1:18:01.220 --> 1:18:03.820
 I think about me as a human

1:18:04.940 --> 1:18:07.020
 to publicly change your mind on something.

1:18:07.020 --> 1:18:08.420
 How hard is that?

1:18:08.420 --> 1:18:10.620
 Especially when assholes like George Haas say,

1:18:10.620 --> 1:18:12.380
 I told you so.

1:18:12.380 --> 1:18:14.740
 All I will say is I am not a leader

1:18:14.740 --> 1:18:17.060
 and I am happy to change my mind.

1:18:17.060 --> 1:18:17.900
 And I will.

1:18:17.900 --> 1:18:18.740
 You think Elon will?

1:18:20.580 --> 1:18:21.420
 Yeah, I do.

1:18:22.300 --> 1:18:24.260
 I think he'll come up with a good way

1:18:24.260 --> 1:18:27.420
 to make it psychologically okay for him.

1:18:27.420 --> 1:18:29.700
 Well, it's such an important thing, man.

1:18:29.700 --> 1:18:31.420
 Especially for a first principles thinker,

1:18:31.420 --> 1:18:34.780
 because he made a decision that driver monitoring

1:18:34.780 --> 1:18:35.740
 is not the right way to go.

1:18:35.740 --> 1:18:37.660
 And I could see that decision.

1:18:37.660 --> 1:18:39.260
 And I could even make that decision.

1:18:39.260 --> 1:18:41.660
 Like I was on the fence too.

1:18:41.660 --> 1:18:42.500
 Like I'm not a,

1:18:42.500 --> 1:18:47.140
 driver monitoring is such an obvious,

1:18:47.140 --> 1:18:49.980
 simple solution to the problem of attention.

1:18:49.980 --> 1:18:52.820
 It's not obvious to me that just by putting a camera there,

1:18:52.820 --> 1:18:54.260
 you solve things.

1:18:54.260 --> 1:18:59.060
 You have to create an incredible, compelling experience.

1:18:59.060 --> 1:19:01.060
 Just like you're talking about.

1:19:01.060 --> 1:19:03.300
 I don't know if it's easy to do that.

1:19:03.300 --> 1:19:05.980
 It's not at all easy to do that, in fact, I think.

1:19:05.980 --> 1:19:10.980
 So as a creator of a car that's trying to create a product

1:19:10.980 --> 1:19:14.180
 that people love, which is what Tesla tries to do, right?

1:19:14.180 --> 1:19:18.340
 It's not obvious to me that as a design decision,

1:19:18.340 --> 1:19:20.940
 whether adding a camera is a good idea.

1:19:20.940 --> 1:19:22.460
 From a safety perspective either,

1:19:22.460 --> 1:19:25.100
 like in the human factors community,

1:19:25.100 --> 1:19:27.380
 everybody says that you should obviously

1:19:27.380 --> 1:19:30.500
 have driver sensing, driver monitoring.

1:19:30.500 --> 1:19:35.500
 But that's like saying it's obvious as parents,

1:19:36.900 --> 1:19:39.780
 you shouldn't let your kids go out at night.

1:19:39.780 --> 1:19:43.460
 But okay, but like,

1:19:43.460 --> 1:19:45.620
 they're still gonna find ways to do drugs.

1:19:45.620 --> 1:19:49.860
 Like, you have to also be good parents.

1:19:49.860 --> 1:19:52.580
 So like, it's much more complicated than just the,

1:19:52.580 --> 1:19:54.140
 you need to have driver monitoring.

1:19:54.140 --> 1:19:58.740
 I totally disagree on, okay, if you have a camera there

1:19:58.740 --> 1:20:00.300
 and the camera's watching the person,

1:20:00.300 --> 1:20:03.540
 but never throws an alert, they'll never think about it.

1:20:03.540 --> 1:20:04.380
 Right?

1:20:04.380 --> 1:20:08.380
 The driver monitoring policy that you choose to,

1:20:08.380 --> 1:20:10.180
 how you choose to communicate with the user

1:20:10.180 --> 1:20:14.220
 is entirely separate from the data collection perspective.

1:20:14.220 --> 1:20:15.060
 Right?

1:20:15.060 --> 1:20:15.900
 Right?

1:20:15.900 --> 1:20:20.900
 So, you know, like, there's one thing to say,

1:20:20.980 --> 1:20:24.500
 like, you know, tell your teenager they can't do something.

1:20:24.500 --> 1:20:27.020
 There's another thing to like, you know, gather the data.

1:20:27.020 --> 1:20:28.500
 So you can make informed decisions.

1:20:28.500 --> 1:20:29.340
 That's really interesting.

1:20:29.340 --> 1:20:30.980
 But you have to make that,

1:20:30.980 --> 1:20:33.380
 that's the interesting thing about cars.

1:20:33.380 --> 1:20:35.020
 But even true with common AI,

1:20:35.020 --> 1:20:37.900
 like you don't have to manufacture the thing

1:20:37.900 --> 1:20:40.060
 into the car, is you have to make a decision

1:20:40.060 --> 1:20:44.180
 that anticipates the right strategy longterm.

1:20:44.180 --> 1:20:46.620
 So like, you have to start collecting the data

1:20:46.620 --> 1:20:47.780
 and start making decisions.

1:20:47.780 --> 1:20:49.900
 Started it three years ago.

1:20:49.900 --> 1:20:52.660
 I believe that we have the best driver monitoring solution

1:20:52.660 --> 1:20:53.500
 in the world.

1:20:54.620 --> 1:20:57.220
 I think that when you compare it to Super Cruise

1:20:57.220 --> 1:20:59.460
 is the only other one that I really know that shipped.

1:20:59.460 --> 1:21:01.420
 And ours is better.

1:21:01.420 --> 1:21:06.420
 What do you like and not like about Super Cruise?

1:21:06.420 --> 1:21:08.740
 I mean, I had a few Super Cruise,

1:21:08.740 --> 1:21:12.020
 the sun would be shining through the window,

1:21:12.020 --> 1:21:13.180
 would blind the camera,

1:21:13.180 --> 1:21:14.580
 and it would say I wasn't paying attention.

1:21:14.580 --> 1:21:16.100
 When I was looking completely straight,

1:21:16.100 --> 1:21:19.140
 I couldn't reset the attention with a steering wheel touch

1:21:19.140 --> 1:21:21.060
 and Super Cruise would disengage.

1:21:21.060 --> 1:21:22.980
 Like I was communicating to the car, I'm like, look,

1:21:22.980 --> 1:21:24.420
 I am here, I am paying attention.

1:21:24.420 --> 1:21:26.340
 Why are you really gonna force me to disengage?

1:21:26.340 --> 1:21:27.380
 And it did.

1:21:28.620 --> 1:21:32.100
 So it's a constant conversation with the user.

1:21:32.100 --> 1:21:33.660
 And yeah, there's no way to ship a system

1:21:33.660 --> 1:21:35.500
 like this if you can OTA.

1:21:35.500 --> 1:21:37.180
 We're shipping a new one every month.

1:21:37.180 --> 1:21:40.140
 Sometimes we balance it with our users on Discord.

1:21:40.140 --> 1:21:41.980
 Like sometimes we make the driver monitoring

1:21:41.980 --> 1:21:43.820
 a little more aggressive and people complain.

1:21:43.820 --> 1:21:45.500
 Sometimes they don't.

1:21:45.500 --> 1:21:47.060
 We want it to be as aggressive as possible

1:21:47.060 --> 1:21:49.180
 where people don't complain and it doesn't feel intrusive.

1:21:49.180 --> 1:21:51.100
 So being able to update the system over the air

1:21:51.100 --> 1:21:52.540
 is an essential component.

1:21:52.540 --> 1:21:55.300
 I mean, that's probably to me, you mentioned,

1:21:56.620 --> 1:22:01.060
 I mean, to me that is the biggest innovation of Tesla,

1:22:01.060 --> 1:22:04.860
 that it made people realize that over the air updates

1:22:04.860 --> 1:22:06.460
 is essential.

1:22:06.460 --> 1:22:07.460
 Yeah.

1:22:07.460 --> 1:22:10.140
 I mean, was that not obvious from the iPhone?

1:22:10.140 --> 1:22:13.060
 The iPhone was the first real product that OTA'd, I think.

1:22:13.060 --> 1:22:15.220
 Was it actually, that's brilliant, you're right.

1:22:15.220 --> 1:22:17.060
 I mean, the game consoles used to not, right?

1:22:17.060 --> 1:22:18.980
 The game consoles were maybe the second thing that did.

1:22:18.980 --> 1:22:22.260
 Wow, I didn't really think about one of the amazing features

1:22:22.260 --> 1:22:26.180
 of a smartphone isn't just like the touchscreen

1:22:26.180 --> 1:22:30.740
 isn't the thing, it's the ability to constantly update.

1:22:30.740 --> 1:22:31.980
 Yeah, it gets better.

1:22:31.980 --> 1:22:35.100
 It gets better.

1:22:35.100 --> 1:22:36.820
 Love my iOS 14.

1:22:36.820 --> 1:22:38.300
 Yeah.

1:22:38.300 --> 1:22:41.540
 Well, one thing that I probably disagree with you

1:22:41.540 --> 1:22:45.580
 on driver monitoring is you said that it's easy.

1:22:46.700 --> 1:22:48.940
 I mean, you tend to say stuff is easy.

1:22:48.940 --> 1:22:52.700
 I'm sure the, I guess you said it's easy

1:22:52.700 --> 1:22:55.500
 relative to the external perception problem.

1:22:58.180 --> 1:23:00.940
 Can you elaborate why you think it's easy?

1:23:00.940 --> 1:23:03.500
 Feature engineering works for driver monitoring.

1:23:03.500 --> 1:23:05.860
 Feature engineering does not work for the external.

1:23:05.860 --> 1:23:10.700
 So human faces are not, human faces and the movement

1:23:10.700 --> 1:23:14.740
 of human faces and head and body is not as variable

1:23:14.740 --> 1:23:17.140
 as the external environment, is your intuition?

1:23:17.140 --> 1:23:20.140
 Yes, and there's another big difference as well.

1:23:20.140 --> 1:23:22.500
 Your reliability of a driver monitoring system

1:23:22.500 --> 1:23:24.380
 doesn't actually need to be that high.

1:23:24.380 --> 1:23:27.220
 The uncertainty, if you have something that's detecting

1:23:27.220 --> 1:23:29.140
 whether the human's paying attention and it only works

1:23:29.140 --> 1:23:31.700
 92% of the time, you're still getting almost all

1:23:31.700 --> 1:23:33.620
 the benefit of that because the human,

1:23:33.620 --> 1:23:35.500
 like you're training the human, right?

1:23:35.500 --> 1:23:39.100
 You're dealing with a system that's really helping you out.

1:23:39.100 --> 1:23:40.180
 It's a conversation.

1:23:40.180 --> 1:23:43.460
 It's not like the external thing where guess what?

1:23:43.460 --> 1:23:46.140
 If you swerve into a tree, you swerve into a tree, right?

1:23:46.140 --> 1:23:48.340
 Like you get no margin for error there.

1:23:48.340 --> 1:23:49.620
 Yeah, I think that's really well put.

1:23:49.620 --> 1:23:54.020
 I think that's the right, exactly the place

1:23:54.020 --> 1:23:58.660
 where comparing to the external perception,

1:23:58.660 --> 1:24:01.500
 the control problem, the driver monitoring is easier

1:24:01.500 --> 1:24:04.400
 because you don't, the bar for success is much lower.

1:24:05.260 --> 1:24:09.100
 Yeah, but I still think like the human face

1:24:09.100 --> 1:24:12.140
 is more complicated actually than the external environment,

1:24:12.140 --> 1:24:14.380
 but for driving, you don't give a damn.

1:24:14.380 --> 1:24:15.620
 I don't need, yeah, I don't need something,

1:24:15.620 --> 1:24:18.300
 I don't need something that complicated

1:24:18.300 --> 1:24:22.220
 to have to communicate the idea to the human

1:24:22.220 --> 1:24:23.980
 that I want to communicate, which is,

1:24:23.980 --> 1:24:25.740
 yo, system might mess up here.

1:24:25.740 --> 1:24:26.940
 You gotta pay attention.

1:24:26.940 --> 1:24:31.940
 Yeah, see, that's my love and fascination is the human face.

1:24:32.500 --> 1:24:37.500
 And it feels like this is a nice place to create products

1:24:38.420 --> 1:24:40.100
 that create an experience in the car.

1:24:40.100 --> 1:24:42.640
 So like, it feels like there should be

1:24:42.640 --> 1:24:47.540
 more richer experiences in the car, you know?

1:24:47.540 --> 1:24:51.500
 Like that's an opportunity for like something like On My Eye

1:24:51.500 --> 1:24:53.900
 or just any kind of system like a Tesla

1:24:53.900 --> 1:24:56.220
 or any of the autonomous vehicle companies

1:24:56.220 --> 1:24:59.180
 is because software is, there's much more sensors

1:24:59.180 --> 1:25:00.660
 and so much is on our software

1:25:00.660 --> 1:25:02.940
 and you're doing machine learning anyway,

1:25:02.940 --> 1:25:06.340
 there's an opportunity to create totally new experiences

1:25:06.340 --> 1:25:08.260
 that we're not even anticipating.

1:25:08.260 --> 1:25:09.180
 You don't think so?

1:25:10.140 --> 1:25:10.980
 Nah.

1:25:10.980 --> 1:25:12.900
 You think it's a box that gets you from A to B

1:25:12.900 --> 1:25:14.920
 and you want to do it chill?

1:25:14.920 --> 1:25:16.940
 Yeah, I mean, I think as soon as we get to level three

1:25:16.940 --> 1:25:19.300
 on highways, okay, enjoy your candy crush,

1:25:19.300 --> 1:25:23.660
 enjoy your Hulu, enjoy your, you know, whatever, whatever.

1:25:23.660 --> 1:25:26.260
 Sure, you get this, you can look at screens basically

1:25:26.260 --> 1:25:28.700
 versus right now where you have music and audio books.

1:25:28.700 --> 1:25:31.020
 So level three is where you can kind of disengage

1:25:31.020 --> 1:25:32.280
 in stretches of time.

1:25:34.860 --> 1:25:37.460
 Well, you think level three is possible?

1:25:37.460 --> 1:25:39.180
 Like on the highway going for 100 miles

1:25:39.180 --> 1:25:40.500
 and you can just go to sleep?

1:25:40.500 --> 1:25:43.620
 Oh yeah, sleep.

1:25:43.620 --> 1:25:47.340
 So again, I think it's really all on a spectrum.

1:25:47.340 --> 1:25:50.060
 I think that being able to use your phone

1:25:50.060 --> 1:25:53.500
 while you're on the highway and like this all being okay

1:25:53.500 --> 1:25:55.360
 and being aware that the car might alert you

1:25:55.360 --> 1:25:57.180
 and you have five seconds to basically.

1:25:57.180 --> 1:25:59.060
 So the five second thing is you think is possible?

1:25:59.060 --> 1:26:00.420
 Yeah, I think it is, oh yeah.

1:26:00.420 --> 1:26:02.180
 Not in all scenarios, right?

1:26:02.180 --> 1:26:03.940
 Some scenarios it's not.

1:26:03.940 --> 1:26:06.300
 It's the whole risk thing that you mentioned is nice

1:26:06.300 --> 1:26:10.660
 is to be able to estimate like how risky is this situation?

1:26:10.660 --> 1:26:12.620
 That's really important to understand.

1:26:12.620 --> 1:26:15.380
 One other thing you mentioned comparing KAMA

1:26:15.380 --> 1:26:20.380
 and Autopilot is that something about the haptic feel

1:26:20.380 --> 1:26:25.380
 of the way KAMA controls the car when things are uncertain.

1:26:25.920 --> 1:26:27.760
 Like it behaves a little bit more uncertain

1:26:27.760 --> 1:26:29.240
 when things are uncertain.

1:26:29.240 --> 1:26:31.080
 That's kind of an interesting point.

1:26:31.080 --> 1:26:34.080
 And then Autopilot is much more confident always

1:26:34.080 --> 1:26:37.460
 even when it's uncertain until it runs into trouble.

1:26:39.200 --> 1:26:40.920
 That's a funny thing.

1:26:40.920 --> 1:26:42.700
 I actually mentioned that to Elon, I think.

1:26:42.700 --> 1:26:46.280
 And then the first time we talked, he wasn't biting.

1:26:46.280 --> 1:26:48.880
 It's like communicating uncertainty.

1:26:48.880 --> 1:26:51.880
 I guess KAMA doesn't really communicate uncertainty

1:26:51.880 --> 1:26:55.040
 explicitly, it communicates it through haptic feel.

1:26:55.040 --> 1:26:57.420
 Like what's the role of communicating uncertainty

1:26:57.420 --> 1:26:58.260
 do you think?

1:26:58.260 --> 1:26:59.840
 Oh, we do some stuff explicitly.

1:26:59.840 --> 1:27:01.800
 Like we do detect the lanes when you're on the highway

1:27:01.800 --> 1:27:04.380
 and we'll show you how many lanes we're using to drive with.

1:27:04.380 --> 1:27:06.200
 You can look at where it thinks the lanes are.

1:27:06.200 --> 1:27:08.480
 You can look at the path.

1:27:08.480 --> 1:27:10.440
 And we want to be better about this.

1:27:10.440 --> 1:27:12.900
 We're actually hiring, want to hire some new UI people.

1:27:12.900 --> 1:27:14.320
 UI people, you mentioned this.

1:27:14.320 --> 1:27:17.300
 Cause it's such an, it's a UI problem too, right?

1:27:17.300 --> 1:27:19.720
 We have a great designer now, but you know,

1:27:19.720 --> 1:27:21.160
 we need people who are just going to like build this

1:27:21.160 --> 1:27:23.800
 and debug these UIs, QT people.

1:27:23.800 --> 1:27:24.640
 QT.

1:27:24.640 --> 1:27:26.880
 Is that what the UI is done with, is QT?

1:27:26.880 --> 1:27:27.980
 The new UI is in QT.

1:27:29.320 --> 1:27:30.520
 C++ QT?

1:27:32.000 --> 1:27:33.300
 Tesla uses it too.

1:27:33.300 --> 1:27:34.200
 Yeah.

1:27:34.200 --> 1:27:36.060
 We had some React stuff in there.

1:27:37.760 --> 1:27:39.440
 React JS or just React?

1:27:39.440 --> 1:27:41.160
 React is his own language, right?

1:27:41.160 --> 1:27:44.480
 React Native, React is a JavaScript framework.

1:27:44.480 --> 1:27:45.320
 Yeah.

1:27:45.320 --> 1:27:48.960
 So it's all based on JavaScript, but it's, you know,

1:27:48.960 --> 1:27:49.800
 I like C++.

1:27:51.480 --> 1:27:55.080
 What do you think about Dojo with Tesla

1:27:55.080 --> 1:27:58.760
 and their foray into what appears to be

1:28:00.240 --> 1:28:03.340
 specialized hardware for training your own nets?

1:28:05.120 --> 1:28:07.360
 I guess it's something, maybe you can correct me,

1:28:07.360 --> 1:28:10.000
 from my shallow looking at it,

1:28:10.000 --> 1:28:12.120
 it seems like something like Google did with TPUs,

1:28:12.120 --> 1:28:15.680
 but specialized for driving data.

1:28:15.680 --> 1:28:18.360
 I don't think it's specialized for driving data.

1:28:18.360 --> 1:28:20.120
 It's just legit, just TPU.

1:28:20.120 --> 1:28:22.160
 They want to go the Apple way,

1:28:22.160 --> 1:28:25.640
 basically everything required in the chain is done in house.

1:28:25.640 --> 1:28:27.840
 Well, so you have a problem right now,

1:28:27.840 --> 1:28:31.740
 and this is one of my concerns.

1:28:31.740 --> 1:28:33.800
 I really would like to see somebody deal with this.

1:28:33.800 --> 1:28:35.280
 If anyone out there is doing it,

1:28:35.280 --> 1:28:38.000
 I'd like to help them if I can.

1:28:38.000 --> 1:28:40.620
 You basically have two options right now to train.

1:28:40.620 --> 1:28:43.840
 One, your options are NVIDIA or Google.

1:28:45.980 --> 1:28:48.780
 So Google is not even an option.

1:28:50.120 --> 1:28:53.180
 Their TPUs are only available in Google Cloud.

1:28:53.180 --> 1:28:55.140
 Google has absolutely onerous

1:28:55.140 --> 1:28:56.780
 terms of service restrictions.

1:28:58.060 --> 1:28:59.280
 They may have changed it,

1:28:59.280 --> 1:29:00.620
 but back in Google's terms of service,

1:29:00.620 --> 1:29:03.740
 it said explicitly you are not allowed to use Google Cloud ML

1:29:03.740 --> 1:29:05.340
 for training autonomous vehicles

1:29:05.340 --> 1:29:07.360
 or for doing anything that competes with Google

1:29:07.360 --> 1:29:09.300
 without Google's prior written permission.

1:29:09.300 --> 1:29:10.340
 Wow, okay.

1:29:10.340 --> 1:29:12.440
 I mean, Google is not a platform company.

1:29:14.140 --> 1:29:16.760
 I wouldn't touch TPUs with a 10 foot pole.

1:29:16.760 --> 1:29:19.300
 So that leaves you with the monopoly.

1:29:19.300 --> 1:29:21.020
 NVIDIA? NVIDIA.

1:29:21.020 --> 1:29:22.340
 So, I mean.

1:29:22.340 --> 1:29:23.900
 That you're not a fan of.

1:29:23.900 --> 1:29:28.540
 Well, look, I was a huge fan of in 2016 NVIDIA.

1:29:28.540 --> 1:29:30.000
 Jensen came sat in the car.

1:29:31.880 --> 1:29:32.820
 Cool guy.

1:29:32.820 --> 1:29:34.640
 When the stock was $30 a share.

1:29:35.540 --> 1:29:37.340
 NVIDIA stock has skyrocketed.

1:29:38.220 --> 1:29:39.920
 I witnessed a real change

1:29:39.920 --> 1:29:43.580
 in who was in management over there in like 2018.

1:29:43.580 --> 1:29:46.700
 And now they are, let's exploit.

1:29:46.700 --> 1:29:48.460
 Let's take every dollar we possibly can

1:29:48.460 --> 1:29:49.580
 out of this ecosystem.

1:29:49.580 --> 1:29:51.740
 Let's charge $10,000 for A100s

1:29:51.740 --> 1:29:54.180
 because we know we got the best shit in the game.

1:29:54.180 --> 1:29:57.920
 And let's charge $10,000 for an A100

1:29:57.920 --> 1:30:00.100
 when it's really not that different from a 3080,

1:30:00.100 --> 1:30:01.480
 which is 699.

1:30:03.500 --> 1:30:05.100
 The margins that they are making

1:30:05.100 --> 1:30:08.640
 off of those high end chips are so high

1:30:08.640 --> 1:30:10.260
 that, I mean, I think they're shooting themselves

1:30:10.260 --> 1:30:12.220
 in the foot just from a business perspective.

1:30:12.220 --> 1:30:14.980
 Because there's a lot of people talking like me now

1:30:14.980 --> 1:30:17.380
 who are like, somebody's gotta take NVIDIA down.

1:30:19.060 --> 1:30:19.900
 Yeah.

1:30:19.900 --> 1:30:21.020
 Where they could dominate it.

1:30:21.020 --> 1:30:22.460
 NVIDIA could be the new Intel.

1:30:22.460 --> 1:30:26.940
 Yeah, to be inside everything essentially.

1:30:26.940 --> 1:30:30.660
 And yet the winners in certain spaces

1:30:30.660 --> 1:30:33.780
 like autonomous driving, the winners,

1:30:33.780 --> 1:30:36.620
 only the people who are like desperately falling back

1:30:36.620 --> 1:30:38.540
 and trying to catch up and have a ton of money,

1:30:38.540 --> 1:30:40.660
 like the big automakers are the ones

1:30:40.660 --> 1:30:43.220
 interested in partnering with NVIDIA.

1:30:43.220 --> 1:30:44.820
 Oh, and I think a lot of those things

1:30:44.820 --> 1:30:45.940
 are gonna fall through.

1:30:45.940 --> 1:30:49.340
 If I were NVIDIA, sell chips.

1:30:49.340 --> 1:30:52.260
 Sell chips at a reasonable markup.

1:30:52.260 --> 1:30:53.100
 To everybody.

1:30:53.100 --> 1:30:53.920
 To everybody.

1:30:53.920 --> 1:30:54.940
 Without any restrictions.

1:30:54.940 --> 1:30:56.140
 Without any restrictions.

1:30:56.140 --> 1:30:57.360
 Intel did this.

1:30:57.360 --> 1:30:58.260
 Look at Intel.

1:30:58.260 --> 1:30:59.940
 They had a great long run.

1:30:59.940 --> 1:31:01.580
 NVIDIA is trying to turn their,

1:31:01.580 --> 1:31:04.020
 they're like trying to productize their chips

1:31:04.020 --> 1:31:05.620
 way too much.

1:31:05.620 --> 1:31:07.880
 They're trying to extract way more value

1:31:07.880 --> 1:31:09.380
 than they can sustainably.

1:31:09.380 --> 1:31:10.740
 Sure, you can do it tomorrow.

1:31:10.740 --> 1:31:12.140
 Is it gonna up your share price?

1:31:12.140 --> 1:31:13.540
 Sure, if you're one of those CEOs

1:31:13.540 --> 1:31:15.300
 who's like, how much can I strip mine this company?

1:31:15.300 --> 1:31:17.860
 And I think, you know, and that's what's weird about it too.

1:31:17.860 --> 1:31:19.380
 Like the CEO is the founder.

1:31:19.380 --> 1:31:20.300
 It's the same guy.

1:31:20.300 --> 1:31:21.120
 Yeah.

1:31:21.120 --> 1:31:22.340
 I mean, I still think Jensen's a great guy.

1:31:22.340 --> 1:31:23.300
 He is great.

1:31:23.300 --> 1:31:25.180
 Why do this?

1:31:25.180 --> 1:31:26.660
 You have a choice.

1:31:26.660 --> 1:31:27.940
 You have a choice right now.

1:31:27.940 --> 1:31:28.820
 Are you trying to cash out?

1:31:28.820 --> 1:31:30.660
 Are you trying to buy a yacht?

1:31:30.660 --> 1:31:32.100
 If you are, fine.

1:31:32.100 --> 1:31:34.220
 But if you're trying to be

1:31:34.220 --> 1:31:37.240
 the next huge semiconductor company, sell chips.

1:31:37.240 --> 1:31:40.140
 Well, the interesting thing about Jensen

1:31:40.140 --> 1:31:42.060
 is he is a big vision guy.

1:31:42.060 --> 1:31:47.060
 So he has a plan like for 50 years down the road.

1:31:48.700 --> 1:31:50.500
 So it makes me wonder like.

1:31:50.500 --> 1:31:51.780
 How does price gouging fit into it?

1:31:51.780 --> 1:31:54.000
 Yeah, how does that, like it's,

1:31:54.000 --> 1:31:57.060
 it doesn't seem to make sense as a plan.

1:31:57.060 --> 1:31:59.260
 I worry that he's listening to the wrong people.

1:31:59.260 --> 1:32:02.540
 Yeah, that's the sense I have too sometimes.

1:32:02.540 --> 1:32:07.540
 Because I, despite everything, I think NVIDIA

1:32:07.940 --> 1:32:09.020
 is an incredible company.

1:32:09.020 --> 1:32:12.420
 Well, one, so I'm deeply grateful to NVIDIA

1:32:12.420 --> 1:32:13.740
 for the products they've created in the past.

1:32:13.740 --> 1:32:14.580
 Me too.

1:32:14.580 --> 1:32:15.400
 Right?

1:32:15.400 --> 1:32:16.240
 And so.

1:32:16.240 --> 1:32:18.000
 The 1080 Ti was a great GPU.

1:32:18.000 --> 1:32:18.840
 Still have a lot of them.

1:32:18.840 --> 1:32:20.160
 Still is, yeah.

1:32:21.840 --> 1:32:24.540
 But at the same time, it just feels like,

1:32:26.860 --> 1:32:29.380
 feels like you don't want to put all your stock in NVIDIA.

1:32:29.380 --> 1:32:32.940
 And so like Elon is doing, what Tesla is doing

1:32:32.940 --> 1:32:37.260
 with Autopilot and Dojo is the Apple way is,

1:32:37.260 --> 1:32:40.300
 because they're not going to share Dojo with George Hott's.

1:32:40.300 --> 1:32:42.340
 I know.

1:32:42.340 --> 1:32:43.780
 They should sell that chip.

1:32:43.780 --> 1:32:44.700
 Oh, they should sell that.

1:32:44.700 --> 1:32:46.400
 Even their accelerator.

1:32:46.400 --> 1:32:49.060
 The accelerator that's in all the cars, the 30 watt one.

1:32:49.060 --> 1:32:50.600
 Sell it, why not?

1:32:51.580 --> 1:32:52.700
 So open it up.

1:32:52.700 --> 1:32:55.820
 Like make, why does Tesla have to be a car company?

1:32:55.820 --> 1:32:58.060
 Well, if you sell the chip, here's what you get.

1:32:58.060 --> 1:32:59.020
 Yeah.

1:32:59.020 --> 1:33:00.260
 Make some money off the chips.

1:33:00.260 --> 1:33:02.080
 It doesn't take away from your chip.

1:33:02.080 --> 1:33:03.860
 You're going to make some money, free money.

1:33:03.860 --> 1:33:07.380
 And also the world is going to build an ecosystem

1:33:07.380 --> 1:33:09.020
 of tooling for you.

1:33:09.020 --> 1:33:09.860
 Right?

1:33:09.860 --> 1:33:12.860
 You're not going to have to fix the bug in your 10H layer.

1:33:12.860 --> 1:33:14.100
 Someone else already did.

1:33:15.140 --> 1:33:16.780
 Well, the question, that's an interesting question.

1:33:16.780 --> 1:33:18.740
 I mean, that's the question Steve Jobs asked.

1:33:18.740 --> 1:33:23.620
 That's the question Elon Musk is perhaps asking is,

1:33:24.940 --> 1:33:28.060
 do you want Tesla stuff inside other vehicles?

1:33:28.060 --> 1:33:32.620
 Inside, potentially inside like a iRobot vacuum cleaner.

1:33:32.620 --> 1:33:33.460
 Yeah.

1:33:34.860 --> 1:33:37.160
 I think you should decide where your advantages are.

1:33:37.160 --> 1:33:39.260
 I'm not saying Tesla should start selling battery packs

1:33:39.260 --> 1:33:40.380
 to automakers.

1:33:40.380 --> 1:33:41.720
 Because battery packs to automakers,

1:33:41.720 --> 1:33:43.660
 they are straight up in competition with you.

1:33:43.660 --> 1:33:46.060
 If I were Tesla, I'd keep the battery technology totally.

1:33:46.060 --> 1:33:46.900
 Yeah.

1:33:46.900 --> 1:33:47.940
 As far as we make batteries.

1:33:47.940 --> 1:33:52.940
 But the thing about the Tesla TPU is anybody can build that.

1:33:53.160 --> 1:33:54.600
 It's just a question of, you know,

1:33:54.600 --> 1:33:57.460
 are you willing to spend the money?

1:33:57.460 --> 1:34:00.220
 It could be a huge source of revenue potentially.

1:34:00.220 --> 1:34:02.420
 Are you willing to spend a hundred million dollars?

1:34:02.420 --> 1:34:03.660
 Anyone can build it.

1:34:03.660 --> 1:34:04.680
 And someone will.

1:34:04.680 --> 1:34:06.680
 And a bunch of companies now are starting

1:34:06.680 --> 1:34:08.020
 trying to build AI accelerators.

1:34:08.020 --> 1:34:10.200
 Somebody is going to get the idea right.

1:34:10.200 --> 1:34:13.700
 And yeah, hopefully they don't get greedy

1:34:13.700 --> 1:34:15.780
 because they'll just lose to the next guy who finally,

1:34:15.780 --> 1:34:17.540
 and then eventually the Chinese are going to make knockoff

1:34:17.540 --> 1:34:19.580
 and video chips and that's.

1:34:19.580 --> 1:34:20.400
 From your perspective,

1:34:20.400 --> 1:34:21.820
 I don't know if you're also paying attention

1:34:21.820 --> 1:34:24.180
 to stay on Tesla for a moment.

1:34:24.180 --> 1:34:27.860
 Dave, Elon Musk has talked about a complete rewrite

1:34:28.820 --> 1:34:31.700
 of the neural net that they're using.

1:34:31.700 --> 1:34:34.800
 That seems to, again, I'm half paying attention,

1:34:34.800 --> 1:34:39.000
 but it seems to involve basically a kind of integration

1:34:39.000 --> 1:34:44.000
 of all the sensors to where it's a four dimensional view.

1:34:44.180 --> 1:34:47.540
 You know, you have a 3D model of the world over time.

1:34:47.540 --> 1:34:51.260
 And then you can, I think it's done both for the,

1:34:52.100 --> 1:34:53.280
 for the actually, you know,

1:34:53.280 --> 1:34:55.260
 so the neural network is able to,

1:34:55.260 --> 1:34:56.920
 in a more holistic way,

1:34:56.920 --> 1:34:59.340
 deal with the world and make predictions and so on,

1:34:59.340 --> 1:35:04.340
 but also to make the annotation task more, you know, easier.

1:35:04.780 --> 1:35:08.180
 Like you can annotate the world in one place

1:35:08.180 --> 1:35:10.500
 and then kind of distribute itself across the sensors

1:35:10.500 --> 1:35:12.860
 and across a different,

1:35:12.860 --> 1:35:15.180
 like the hundreds of tasks that are involved

1:35:15.180 --> 1:35:16.540
 in the Hydro Net.

1:35:16.540 --> 1:35:19.140
 What are your thoughts about this rewrite?

1:35:19.140 --> 1:35:22.420
 Is it just like some details that are kind of obvious

1:35:22.420 --> 1:35:24.060
 that are steps that should be taken,

1:35:24.060 --> 1:35:26.100
 or is there something fundamental

1:35:26.100 --> 1:35:27.560
 that could challenge your idea

1:35:27.560 --> 1:35:31.120
 that end to end is the right solution?

1:35:31.120 --> 1:35:33.160
 We're in the middle of a big rewrite now as well.

1:35:33.160 --> 1:35:34.860
 We haven't shipped a new model in a bit.

1:35:34.860 --> 1:35:36.400
 Of what kind?

1:35:36.400 --> 1:35:38.240
 We're going from 2D to 3D.

1:35:38.240 --> 1:35:39.740
 Right now, all our stuff, like for example,

1:35:39.740 --> 1:35:40.980
 when the car pitches back,

1:35:40.980 --> 1:35:43.020
 the lane lines also pitch back

1:35:43.020 --> 1:35:47.160
 because we're assuming the flat world hypothesis.

1:35:47.160 --> 1:35:48.420
 The new models do not do this.

1:35:48.420 --> 1:35:50.420
 The new models output everything in 3D.

1:35:50.420 --> 1:35:53.580
 But there's still no annotation.

1:35:53.580 --> 1:35:56.500
 So the 3D is, it's more about the output.

1:35:56.500 --> 1:35:57.340
 Yeah.

1:35:57.340 --> 1:36:00.100
 We have Zs in everything.

1:36:00.100 --> 1:36:00.940
 We've...

1:36:00.940 --> 1:36:01.760
 Zs.

1:36:01.760 --> 1:36:02.600
 Yeah.

1:36:02.600 --> 1:36:03.420
 We had a Zs.

1:36:03.420 --> 1:36:04.260
 We had a Zs.

1:36:04.260 --> 1:36:06.620
 We unified a lot of stuff as well.

1:36:06.620 --> 1:36:08.900
 We switched from TensorFlow to PyTorch.

1:36:10.720 --> 1:36:13.740
 My understanding of what Tesla's thing is,

1:36:13.740 --> 1:36:15.660
 is that their annotator now annotates

1:36:15.660 --> 1:36:16.960
 across the time dimension.

1:36:16.960 --> 1:36:17.800
 Mm hmm.

1:36:19.980 --> 1:36:22.000
 I mean, cute.

1:36:22.000 --> 1:36:24.400
 Why are you building an annotator?

1:36:24.400 --> 1:36:26.720
 I find their entire pipeline.

1:36:28.280 --> 1:36:30.560
 I find your vision, I mean,

1:36:30.560 --> 1:36:32.860
 the vision of end to end very compelling,

1:36:32.860 --> 1:36:35.880
 but I also like the engineering of the data engine

1:36:35.880 --> 1:36:37.400
 that they've created.

1:36:37.400 --> 1:36:41.560
 In terms of supervised learning pipelines,

1:36:41.560 --> 1:36:43.560
 that thing is damn impressive.

1:36:43.560 --> 1:36:47.480
 You're basically, the idea is that you have

1:36:47.480 --> 1:36:49.280
 hundreds of thousands of people

1:36:49.280 --> 1:36:51.200
 that are doing data collection for you

1:36:51.200 --> 1:36:52.400
 by doing their experience.

1:36:52.400 --> 1:36:55.220
 So that's kind of similar to the Comma AI model.

1:36:55.220 --> 1:36:59.580
 And you're able to mine that data

1:36:59.580 --> 1:37:01.980
 based on the kind of edge cases you need.

1:37:02.940 --> 1:37:07.360
 I think it's harder to do in the end to end learning.

1:37:07.360 --> 1:37:09.520
 The mining of the right edge cases.

1:37:09.520 --> 1:37:11.440
 Like that's where feature engineering

1:37:11.440 --> 1:37:14.120
 is actually really powerful

1:37:14.120 --> 1:37:17.280
 because like us humans are able to do

1:37:17.280 --> 1:37:19.800
 this kind of mining a little better.

1:37:19.800 --> 1:37:21.980
 But yeah, there's obvious, as we know,

1:37:21.980 --> 1:37:24.840
 there's obvious constraints and limitations to that idea.

1:37:25.880 --> 1:37:28.280
 Carpathia just tweeted, he's like,

1:37:28.280 --> 1:37:29.640
 you get really interesting insights

1:37:29.640 --> 1:37:32.800
 if you sort your validation set by loss

1:37:33.720 --> 1:37:36.400
 and look at the highest loss examples.

1:37:36.400 --> 1:37:37.560
 Yeah.

1:37:37.560 --> 1:37:39.180
 So yeah, I mean, you can do,

1:37:39.180 --> 1:37:42.040
 we have a little data engine like thing.

1:37:42.040 --> 1:37:43.560
 We're training a segment.

1:37:43.560 --> 1:37:44.560
 I know it's not fancy.

1:37:44.560 --> 1:37:48.280
 It's just like, okay, train the new segment,

1:37:48.280 --> 1:37:50.080
 run it on 100,000 images

1:37:50.080 --> 1:37:52.160
 and now take the thousand with highest loss.

1:37:52.160 --> 1:37:54.160
 Select a hundred of those by human,

1:37:54.160 --> 1:37:57.840
 put those, get those ones labeled, retrain, do it again.

1:37:57.840 --> 1:38:01.480
 And so it's a much less well written data engine.

1:38:01.480 --> 1:38:03.600
 And yeah, you can take these things really far

1:38:03.600 --> 1:38:06.600
 and it is impressive engineering.

1:38:06.600 --> 1:38:09.920
 And if you truly need supervised data for a problem,

1:38:09.920 --> 1:38:12.560
 yeah, things like data engine are at the high end

1:38:12.560 --> 1:38:14.960
 of what is attention?

1:38:14.960 --> 1:38:15.960
 Is a human paying attention?

1:38:15.960 --> 1:38:17.960
 I mean, we're going to probably build something

1:38:17.960 --> 1:38:18.940
 that looks like data engine

1:38:18.940 --> 1:38:21.120
 to push our driver monitoring further.

1:38:21.120 --> 1:38:22.920
 But for driving itself,

1:38:22.920 --> 1:38:26.400
 you have it all annotated beautifully by what the human does.

1:38:26.400 --> 1:38:27.240
 Yeah, that's interesting.

1:38:27.240 --> 1:38:30.040
 I mean, that applies to driver attention as well.

1:38:30.040 --> 1:38:31.200
 Do you want to detect the eyes?

1:38:31.200 --> 1:38:33.500
 Do you want to detect blinking and pupil movement?

1:38:33.500 --> 1:38:36.680
 Do you want to detect all the like face alignments

1:38:36.680 --> 1:38:38.720
 or landmark detection and so on,

1:38:38.720 --> 1:38:41.560
 and then doing kind of reasoning based on that?

1:38:41.560 --> 1:38:43.880
 Or do you want to take the entirety of the face over time

1:38:43.880 --> 1:38:45.320
 and do end to end?

1:38:45.320 --> 1:38:48.480
 I mean, it's obvious that eventually you have to do end

1:38:48.480 --> 1:38:51.360
 to end with some calibration, some fixes and so on,

1:38:51.360 --> 1:38:55.760
 but it's like, I don't know when that's the right move.

1:38:55.760 --> 1:38:58.420
 Even if it's end to end, there actually is,

1:38:58.420 --> 1:39:03.380
 there is no kind of, you have to supervise that with humans.

1:39:03.380 --> 1:39:05.480
 Whether a human is paying attention or not

1:39:05.480 --> 1:39:07.320
 is a completely subjective judgment.

1:39:08.560 --> 1:39:11.040
 Like you can try to like automatically do it

1:39:11.040 --> 1:39:13.160
 with some stuff, but you don't have,

1:39:13.160 --> 1:39:15.080
 if I record a video of a human,

1:39:15.080 --> 1:39:18.400
 I don't have true annotations anywhere in that video.

1:39:18.400 --> 1:39:21.120
 The only way to get them is with,

1:39:21.120 --> 1:39:22.840
 you know, other humans labeling it really.

1:39:22.840 --> 1:39:23.860
 Well, I don't know.

1:39:26.080 --> 1:39:28.540
 If you think deeply about it,

1:39:28.540 --> 1:39:30.160
 you could, you might be able to just,

1:39:30.160 --> 1:39:31.000
 depending on the task,

1:39:31.000 --> 1:39:34.320
 maybe a discover self annotating things like,

1:39:34.320 --> 1:39:36.920
 you know, you can look at like steering wheel reverse

1:39:36.920 --> 1:39:37.760
 or something like that.

1:39:37.760 --> 1:39:41.200
 You can discover little moments of lapse of attention.

1:39:41.200 --> 1:39:44.540
 I mean, that's where psychology comes in.

1:39:44.540 --> 1:39:45.480
 Is there indicate,

1:39:45.480 --> 1:39:48.000
 cause you have so much data to look at.

1:39:48.000 --> 1:39:51.140
 So you might be able to find moments when there's like,

1:39:51.140 --> 1:39:54.320
 just inattention that even with smartphone,

1:39:54.320 --> 1:39:56.700
 if you want to detect smartphone use,

1:39:56.700 --> 1:39:57.880
 you can start to zoom in.

1:39:57.880 --> 1:40:01.480
 I mean, that's the gold mine, sort of the comma AI.

1:40:01.480 --> 1:40:02.920
 I mean, Tesla is doing this too, right?

1:40:02.920 --> 1:40:06.920
 Is they're doing annotation based on,

1:40:06.920 --> 1:40:10.500
 it's like a self supervised learning too.

1:40:10.500 --> 1:40:13.440
 It's just a small part of the entire picture.

1:40:13.440 --> 1:40:17.760
 That's kind of the challenge of solving a problem

1:40:17.760 --> 1:40:18.660
 in machine learning.

1:40:18.660 --> 1:40:23.660
 If you can discover self annotating parts of the problem,

1:40:24.000 --> 1:40:25.020
 right?

1:40:25.020 --> 1:40:27.760
 Our driver monitoring team is half a person right now.

1:40:27.760 --> 1:40:29.280
 I would, you know, once we have,

1:40:29.280 --> 1:40:33.280
 once we have two, three people on that team,

1:40:33.280 --> 1:40:35.280
 I definitely want to look at self annotating stuff

1:40:35.280 --> 1:40:36.600
 for attention.

1:40:38.200 --> 1:40:43.200
 Let's go back for a sec to a comma and what,

1:40:43.720 --> 1:40:46.240
 you know, for people who are curious to try it out,

1:40:46.240 --> 1:40:51.120
 how do you install a comma in say a 2020 Toyota Corolla

1:40:51.120 --> 1:40:53.400
 or like, what are the cars that are supported?

1:40:53.400 --> 1:40:55.500
 What are the cars that you recommend?

1:40:55.500 --> 1:40:57.880
 And what does it take?

1:40:57.880 --> 1:41:00.000
 You have a few videos out, but maybe through words,

1:41:00.000 --> 1:41:02.880
 can you explain what's it take to actually install a thing?

1:41:02.880 --> 1:41:06.620
 So we support, I think it's 91 cars, 91 makes the models.

1:41:08.080 --> 1:41:10.160
 We've got to 100 this year.

1:41:10.160 --> 1:41:11.000
 Nice.

1:41:11.000 --> 1:41:16.000
 The, yeah, the 2020 Corolla, great choice.

1:41:16.680 --> 1:41:21.200
 The 2020 Sonata, it's using the stock longitudinal.

1:41:21.200 --> 1:41:23.280
 It's using just our lateral control,

1:41:23.280 --> 1:41:25.140
 but it's a very refined car.

1:41:25.140 --> 1:41:28.200
 Their longitudinal control is not bad at all.

1:41:28.200 --> 1:41:31.720
 So yeah, Corolla, Sonata,

1:41:31.720 --> 1:41:34.240
 or if you're willing to get your hands a little dirty

1:41:34.240 --> 1:41:35.940
 and look in the right places on the internet,

1:41:35.940 --> 1:41:37.520
 the Honda Civic is great,

1:41:37.520 --> 1:41:40.600
 but you're going to have to install a modified EPS firmware

1:41:40.600 --> 1:41:42.160
 in order to get a little bit more torque.

1:41:42.160 --> 1:41:43.380
 And I can't help you with that.

1:41:43.380 --> 1:41:45.960
 Comma does not officially endorse that,

1:41:45.960 --> 1:41:47.560
 but we have been doing it.

1:41:47.560 --> 1:41:49.800
 We didn't ever release it.

1:41:49.800 --> 1:41:51.440
 We waited for someone else to discover it.

1:41:51.440 --> 1:41:52.880
 And then, you know.

1:41:52.880 --> 1:41:55.680
 And you have a Discord server where people,

1:41:55.680 --> 1:42:00.360
 there's a very active developer community, I suppose.

1:42:00.360 --> 1:42:04.000
 So depending on the level of experimentation

1:42:04.000 --> 1:42:07.600
 you're willing to do, that's the community.

1:42:07.600 --> 1:42:11.240
 If you just want to buy it and you have a supported car,

1:42:11.240 --> 1:42:13.920
 it's 10 minutes to install.

1:42:13.920 --> 1:42:15.400
 There's YouTube videos.

1:42:15.400 --> 1:42:17.040
 It's Ikea furniture level.

1:42:17.040 --> 1:42:19.040
 If you can set up a table from Ikea,

1:42:19.040 --> 1:42:21.200
 you can install a Comma 2 in your supported car

1:42:21.200 --> 1:42:22.600
 and it will just work.

1:42:22.600 --> 1:42:24.920
 Now you're like, oh, but I want this high end feature

1:42:24.920 --> 1:42:26.160
 or I want to fix this bug.

1:42:26.160 --> 1:42:28.520
 Okay, well, welcome to the developer community.

1:42:29.520 --> 1:42:31.000
 So what, if I wanted to,

1:42:31.000 --> 1:42:34.680
 this is something I asked you offline like a few months ago.

1:42:34.680 --> 1:42:38.800
 If I wanted to run my own code to,

1:42:39.800 --> 1:42:43.420
 so use Comma as a platform

1:42:43.420 --> 1:42:46.040
 and try to run something like OpenPilot,

1:42:46.040 --> 1:42:48.320
 what does it take to do that?

1:42:48.320 --> 1:42:51.840
 So there's a toggle in the settings called enable SSH.

1:42:51.840 --> 1:42:54.600
 And if you toggle that, you can SSH into your device.

1:42:54.600 --> 1:42:55.620
 You can modify the code.

1:42:55.620 --> 1:42:58.260
 You can upload whatever code you want to it.

1:42:58.260 --> 1:42:59.160
 There's a whole lot of people.

1:42:59.160 --> 1:43:03.040
 So about 60% of people are running stock comma.

1:43:03.040 --> 1:43:05.480
 About 40% of people are running forks.

1:43:05.480 --> 1:43:07.280
 And there's a community of,

1:43:07.280 --> 1:43:10.320
 there's a bunch of people who maintain these forks

1:43:10.320 --> 1:43:13.040
 and these forks support different cars

1:43:13.040 --> 1:43:15.700
 or they have different toggles.

1:43:15.700 --> 1:43:17.360
 We try to keep away from the toggles

1:43:17.360 --> 1:43:18.920
 that are like disabled driver monitoring,

1:43:18.920 --> 1:43:21.720
 but there's some people might want that kind of thing

1:43:21.720 --> 1:43:24.560
 and like, yeah, you can, it's your car.

1:43:24.560 --> 1:43:26.620
 I'm not here to tell you.

1:43:29.400 --> 1:43:31.080
 We have some, we ban,

1:43:31.080 --> 1:43:32.920
 if you're trying to subvert safety features,

1:43:32.920 --> 1:43:33.760
 you're banned from our Discord.

1:43:33.760 --> 1:43:35.260
 I don't want anything to do with you,

1:43:35.260 --> 1:43:36.920
 but there's some forks doing that.

1:43:37.920 --> 1:43:38.760
 Got it.

1:43:39.880 --> 1:43:42.880
 So you encourage responsible forking.

1:43:42.880 --> 1:43:43.720
 Yeah, yeah.

1:43:43.720 --> 1:43:46.080
 We encourage, some people, yeah, some people,

1:43:46.080 --> 1:43:48.160
 like there's forks that will do,

1:43:48.160 --> 1:43:52.040
 some people just like having a lot of readouts on the UI,

1:43:52.040 --> 1:43:53.440
 like a lot of like flashing numbers.

1:43:53.440 --> 1:43:55.120
 So there's forks that do that.

1:43:55.120 --> 1:43:57.200
 Some people don't like the fact that it disengages

1:43:57.200 --> 1:43:58.320
 when you press the gas pedal.

1:43:58.320 --> 1:44:00.480
 There's forks that disable that.

1:44:00.480 --> 1:44:01.320
 Got it.

1:44:01.320 --> 1:44:04.920
 Now the stock experience is what like,

1:44:04.920 --> 1:44:06.240
 so it does both lane keeping

1:44:06.240 --> 1:44:08.960
 and longitudinal control all together.

1:44:08.960 --> 1:44:11.020
 So it's not separate like it is in autopilot.

1:44:11.020 --> 1:44:12.520
 No, so, okay.

1:44:12.520 --> 1:44:15.040
 Some cars we use the stock longitudinal control.

1:44:15.040 --> 1:44:17.400
 We don't do the longitudinal control in all the cars.

1:44:17.400 --> 1:44:19.560
 Some cars, the ACCs are pretty good in the cars.

1:44:19.560 --> 1:44:21.360
 It's the lane keep that's atrocious in anything

1:44:21.360 --> 1:44:23.420
 except for autopilot and super cruise.

1:44:23.420 --> 1:44:27.880
 But, you know, you just turn it on and it works.

1:44:27.880 --> 1:44:29.480
 What does this engagement look like?

1:44:29.480 --> 1:44:30.960
 Yeah, so we have, I mean,

1:44:30.960 --> 1:44:32.960
 I'm very concerned about mode confusion.

1:44:32.960 --> 1:44:36.960
 I've experienced it on super cruise and autopilot

1:44:36.960 --> 1:44:39.820
 where like autopilot, like autopilot disengages.

1:44:39.820 --> 1:44:42.400
 I don't realize that the ACC is still on.

1:44:42.400 --> 1:44:44.680
 The lead car moves slightly over

1:44:44.680 --> 1:44:46.160
 and then the Tesla accelerates

1:44:46.160 --> 1:44:48.000
 to like whatever my set speed is super fast.

1:44:48.000 --> 1:44:49.840
 I'm like, what's going on here?

1:44:51.320 --> 1:44:53.720
 We have engaged and disengaged.

1:44:53.720 --> 1:44:56.380
 And this is similar to my understanding, I'm not a pilot,

1:44:56.380 --> 1:45:00.080
 but my understanding is either the pilot is in control

1:45:00.080 --> 1:45:02.080
 or the copilot is in control.

1:45:02.080 --> 1:45:05.120
 And we have the same kind of transition system.

1:45:05.120 --> 1:45:08.620
 Either open pilot is engaged or open pilot is disengaged.

1:45:08.620 --> 1:45:10.160
 Engage with cruise control,

1:45:10.160 --> 1:45:13.320
 disengage with either gas brake or cancel.

1:45:13.320 --> 1:45:14.560
 Let's talk about money.

1:45:14.560 --> 1:45:17.360
 What's the business strategy for Kama?

1:45:17.360 --> 1:45:18.840
 Profitable.

1:45:18.840 --> 1:45:19.680
 Well, so you're.

1:45:19.680 --> 1:45:20.520
 We did it.

1:45:20.520 --> 1:45:23.200
 So congratulations.

1:45:23.200 --> 1:45:25.760
 What, so basically selling,

1:45:25.760 --> 1:45:29.960
 so we should say Kama cost a thousand bucks, Kama two?

1:45:29.960 --> 1:45:31.640
 200 for the interface to the car as well.

1:45:31.640 --> 1:45:33.000
 It's 1200, I'll send that.

1:45:34.360 --> 1:45:36.400
 Nobody's usually upfront like this.

1:45:36.400 --> 1:45:38.200
 Yeah, you gotta add the tack on, right?

1:45:38.200 --> 1:45:39.040
 Yeah.

1:45:39.040 --> 1:45:39.860
 I love it.

1:45:39.860 --> 1:45:41.080
 I'm not gonna lie to you.

1:45:41.080 --> 1:45:43.840
 Trust me, it will add $1,200 of value to your life.

1:45:43.840 --> 1:45:45.560
 Yes, it's still super cheap.

1:45:45.560 --> 1:45:47.880
 30 days, no questions asked, money back guarantee,

1:45:47.880 --> 1:45:50.400
 and prices are only going up.

1:45:50.400 --> 1:45:52.320
 If there ever is future hardware,

1:45:52.320 --> 1:45:53.840
 it could cost a lot more than $1,200.

1:45:53.840 --> 1:45:55.400
 So Kama three is in the works.

1:45:56.960 --> 1:45:57.800
 It could be.

1:45:57.800 --> 1:45:59.640
 All I will say is future hardware

1:45:59.640 --> 1:46:02.900
 is going to cost a lot more than the current hardware.

1:46:02.900 --> 1:46:05.260
 Yeah, the people that use,

1:46:05.260 --> 1:46:07.880
 the people I've spoken with that use Kama,

1:46:07.880 --> 1:46:09.120
 that use open pilot,

1:46:10.320 --> 1:46:12.120
 first of all, they use it a lot.

1:46:12.120 --> 1:46:14.400
 So people that use it, they fall in love with it.

1:46:14.400 --> 1:46:16.840
 Oh, our retention rate is insane.

1:46:16.840 --> 1:46:17.740
 It's a good sign.

1:46:17.740 --> 1:46:18.580
 Yeah.

1:46:18.580 --> 1:46:19.680
 It's a really good sign.

1:46:19.680 --> 1:46:23.720
 70% of Kama two buyers are daily active users.

1:46:23.720 --> 1:46:25.260
 Yeah, it's amazing.

1:46:27.760 --> 1:46:30.520
 Oh, also, we don't plan on stopping selling the Kama two.

1:46:30.520 --> 1:46:31.960
 Like it's, you know.

1:46:31.960 --> 1:46:35.000
 So whatever you create that's beyond Kama two,

1:46:36.600 --> 1:46:40.760
 it would be potentially a phase shift.

1:46:40.760 --> 1:46:42.840
 Like it's so much better that,

1:46:42.840 --> 1:46:44.200
 like you could use Kama two

1:46:44.200 --> 1:46:45.760
 and you can use Kama whatever.

1:46:45.760 --> 1:46:46.600
 Depends what you want.

1:46:46.600 --> 1:46:48.320
 It's 3.41, 42.

1:46:48.320 --> 1:46:49.160
 Yeah.

1:46:49.160 --> 1:46:52.200
 You know, autopilot hardware one versus hardware two.

1:46:52.200 --> 1:46:53.600
 The Kama two is kind of like hardware one.

1:46:53.600 --> 1:46:54.440
 Got it, got it.

1:46:54.440 --> 1:46:55.260
 You can still use both.

1:46:55.260 --> 1:46:56.320
 Got it, got it.

1:46:56.320 --> 1:46:58.000
 I think I heard you talk about retention rate

1:46:58.000 --> 1:47:01.240
 with the VR headsets that the average is just once.

1:47:01.240 --> 1:47:02.080
 Yeah.

1:47:02.080 --> 1:47:02.900
 Just fast.

1:47:02.900 --> 1:47:03.880
 I mean, it's such a fascinating way

1:47:03.880 --> 1:47:05.760
 to think about technology.

1:47:05.760 --> 1:47:07.420
 And this is a really, really good sign.

1:47:07.420 --> 1:47:09.000
 And the other thing that people say about Kama

1:47:09.000 --> 1:47:12.060
 is like they can't believe they're getting this 4,000 bucks.

1:47:12.060 --> 1:47:12.900
 Right?

1:47:12.900 --> 1:47:16.000
 It seems like some kind of steal.

1:47:17.020 --> 1:47:20.040
 So, but in terms of like longterm business strategies

1:47:20.040 --> 1:47:21.640
 that basically to put,

1:47:21.640 --> 1:47:25.120
 so it's currently in like a thousand plus cars.

1:47:27.560 --> 1:47:28.520
 1,200.

1:47:28.520 --> 1:47:29.360
 More, more.

1:47:30.560 --> 1:47:35.560
 So yeah, dailies is about, dailies is about 2,000.

1:47:35.560 --> 1:47:38.960
 Weeklys is about 2,500, monthlys is over 3,000.

1:47:38.960 --> 1:47:39.800
 Wow.

1:47:39.800 --> 1:47:42.120
 We've grown a lot since we last talked.

1:47:42.120 --> 1:47:44.800
 Is the goal, like can we talk crazy for a second?

1:47:44.800 --> 1:47:47.580
 I mean, what's the goal to overtake Tesla?

1:47:48.620 --> 1:47:49.960
 Let's talk, okay, so.

1:47:49.960 --> 1:47:51.480
 I mean, Android did overtake iOS.

1:47:51.480 --> 1:47:52.520
 That's exactly it, right?

1:47:52.520 --> 1:47:55.360
 So they did it.

1:47:55.360 --> 1:47:57.720
 I actually don't know the timeline of that one.

1:47:57.720 --> 1:48:02.160
 But let's talk, because everything is in alpha now.

1:48:02.160 --> 1:48:03.960
 The autopilot you could argue is in alpha

1:48:03.960 --> 1:48:05.840
 in terms of towards the big mission

1:48:05.840 --> 1:48:07.680
 of autonomous driving, right?

1:48:07.680 --> 1:48:11.600
 And so what, yeah, is your goal to overtake

1:48:11.600 --> 1:48:13.920
 millions of cars essentially?

1:48:13.920 --> 1:48:15.520
 Of course.

1:48:15.520 --> 1:48:16.760
 Where would it stop?

1:48:16.760 --> 1:48:18.000
 Like it's open source software.

1:48:18.000 --> 1:48:19.280
 It might not be millions of cars

1:48:19.280 --> 1:48:21.360
 with a piece of comma hardware, but yeah.

1:48:21.360 --> 1:48:24.320
 I think open pilot at some point

1:48:24.320 --> 1:48:26.920
 will cross over autopilot in users,

1:48:26.920 --> 1:48:29.200
 just like Android crossed over iOS.

1:48:29.200 --> 1:48:31.240
 How does Google make money from Android?

1:48:31.240 --> 1:48:34.840
 It's complicated.

1:48:34.840 --> 1:48:36.440
 Their own devices make money.

1:48:37.400 --> 1:48:39.480
 Google, Google makes money

1:48:39.480 --> 1:48:42.240
 by just kind of having you on the internet.

1:48:42.240 --> 1:48:43.080
 Yes.

1:48:43.080 --> 1:48:45.620
 Google search is built in, Gmail is built in.

1:48:45.620 --> 1:48:46.540
 Android is just a shill

1:48:46.540 --> 1:48:48.200
 for the rest of Google's ecosystem.

1:48:48.200 --> 1:48:50.680
 Yeah, but the problem is Android is not,

1:48:50.680 --> 1:48:52.440
 is a brilliant thing.

1:48:52.440 --> 1:48:55.080
 I mean, Android arguably changed the world.

1:48:55.080 --> 1:48:56.440
 So there you go.

1:48:56.440 --> 1:49:00.800
 That's, you can feel good ethically speaking.

1:49:00.800 --> 1:49:04.320
 But as a business strategy, it's questionable.

1:49:04.320 --> 1:49:05.720
 Or sell hardware.

1:49:05.720 --> 1:49:06.560
 Sell hardware.

1:49:06.560 --> 1:49:08.120
 I mean, it took Google a long time to come around to it,

1:49:08.120 --> 1:49:10.000
 but they are now making money on the Pixel.

1:49:10.000 --> 1:49:13.240
 You're not about money, you're more about winning.

1:49:13.240 --> 1:49:14.160
 Yeah, of course.

1:49:14.160 --> 1:49:18.320
 No, but if only 10% of open pilot devices

1:49:18.320 --> 1:49:19.920
 come from comma AI.

1:49:19.920 --> 1:49:20.820
 They still make a lot.

1:49:20.820 --> 1:49:21.660
 That is still, yes.

1:49:21.660 --> 1:49:22.840
 That is a ton of money for our company.

1:49:22.840 --> 1:49:27.120
 But can't somebody create a better comma using open pilot?

1:49:27.120 --> 1:49:28.840
 Or are you basically saying, well, I'll compete them?

1:49:28.840 --> 1:49:29.680
 Well, I'll compete you.

1:49:29.680 --> 1:49:32.280
 Can you create a better Android phone than the Google Pixel?

1:49:32.280 --> 1:49:32.800
 Right.

1:49:32.800 --> 1:49:34.680
 I mean, you can, but like, you know.

1:49:34.680 --> 1:49:35.520
 I love that.

1:49:35.520 --> 1:49:37.360
 So you're confident, like, you know

1:49:37.360 --> 1:49:38.480
 what the hell you're doing.

1:49:38.480 --> 1:49:40.040
 Yeah.

1:49:40.040 --> 1:49:43.520
 It's confidence and merit.

1:49:43.520 --> 1:49:44.960
 I mean, our money comes from, we're

1:49:44.960 --> 1:49:46.160
 a consumer electronics company.

1:49:46.160 --> 1:49:46.660
 Yeah.

1:49:46.660 --> 1:49:48.040
 And put it this way.

1:49:48.040 --> 1:49:51.600
 So we sold like 3,000 comma twos.

1:49:51.600 --> 1:49:54.080
 2,500 right now.

1:49:54.080 --> 1:49:59.720
 And like, OK, we're probably going

1:49:59.720 --> 1:50:01.920
 to sell 10,000 units next year.

1:50:01.920 --> 1:50:04.520
 10,000 units, even just $1,000 a unit, OK,

1:50:04.520 --> 1:50:09.400
 we're at 10 million in revenue.

1:50:09.400 --> 1:50:12.100
 Get that up to 100,000, maybe double the price of the unit.

1:50:12.100 --> 1:50:13.560
 Now we're talking like 200 million revenue.

1:50:13.560 --> 1:50:14.440
 We're talking like series.

1:50:14.440 --> 1:50:15.840
 Yeah, actually making money.

1:50:15.840 --> 1:50:19.360
 One of the rare semi autonomous or autonomous vehicle companies

1:50:19.360 --> 1:50:21.080
 that are actually making money.

1:50:21.080 --> 1:50:22.600
 Yeah.

1:50:22.600 --> 1:50:24.880
 You know, if you look at a model,

1:50:24.880 --> 1:50:26.680
 and we were just talking about this yesterday.

1:50:26.680 --> 1:50:29.760
 If you look at a model, and like you're AB testing your model,

1:50:29.760 --> 1:50:32.360
 and if you're one branch of the AB test,

1:50:32.360 --> 1:50:35.320
 the losses go down very fast in the first five epochs.

1:50:35.320 --> 1:50:37.520
 That model is probably going to converge

1:50:37.520 --> 1:50:39.360
 to something considerably better than the one

1:50:39.360 --> 1:50:41.360
 where the losses are going down slower.

1:50:41.360 --> 1:50:43.000
 Why do people think this is going to stop?

1:50:43.000 --> 1:50:44.500
 Why do people think one day there's

1:50:44.500 --> 1:50:46.840
 going to be a great like, well, Waymo's eventually

1:50:46.840 --> 1:50:49.600
 going to surpass you guys?

1:50:49.600 --> 1:50:52.000
 Well, they're not.

1:50:52.000 --> 1:50:55.800
 Do you see like a world where like a Tesla or a car

1:50:55.800 --> 1:50:59.160
 like a Tesla would be able to basically press a button

1:50:59.160 --> 1:51:01.920
 and you like switch to open pilot?

1:51:01.920 --> 1:51:04.480
 You know, you load in.

1:51:04.480 --> 1:51:06.840
 No, so I think so first off, I think

1:51:06.840 --> 1:51:10.560
 that we may surpass Tesla in terms of users.

1:51:10.560 --> 1:51:12.520
 I do not think we're going to surpass Tesla ever

1:51:12.520 --> 1:51:13.520
 in terms of revenue.

1:51:13.520 --> 1:51:16.380
 I think Tesla can capture a lot more revenue per user

1:51:16.380 --> 1:51:17.320
 than we can.

1:51:17.320 --> 1:51:20.520
 But this mimics the Android iOS model exactly.

1:51:20.520 --> 1:51:22.000
 There may be more Android devices,

1:51:22.000 --> 1:51:24.320
 but there's a lot more iPhones than Google Pixels.

1:51:24.320 --> 1:51:26.360
 So I think there'll be a lot more Tesla cars sold

1:51:26.360 --> 1:51:27.560
 than pieces of common hardware.

1:51:30.280 --> 1:51:34.420
 And then as far as a Tesla owner being

1:51:34.420 --> 1:51:40.920
 able to switch to open pilot, does iPhones run Android?

1:51:40.920 --> 1:51:42.400
 No, but it doesn't make sense.

1:51:42.400 --> 1:51:43.480
 You can if you really want to do it,

1:51:43.480 --> 1:51:44.440
 but it doesn't really make sense.

1:51:44.440 --> 1:51:45.320
 Like it's not.

1:51:45.320 --> 1:51:46.240
 It doesn't make sense.

1:51:46.240 --> 1:51:46.740
 Who cares?

1:51:46.740 --> 1:51:51.640
 What about if a large company like automakers, Ford, GM,

1:51:51.640 --> 1:51:53.720
 Toyota came to George Hots?

1:51:53.720 --> 1:51:58.000
 Or on the tech space, Amazon, Facebook, Google

1:51:58.000 --> 1:52:01.080
 came with a large pile of cash?

1:52:01.080 --> 1:52:07.360
 Would you consider being purchased?

1:52:07.360 --> 1:52:10.500
 Do you see that as a one possible?

1:52:10.500 --> 1:52:12.360
 Not seriously, no.

1:52:12.360 --> 1:52:19.680
 I would probably see how much shit they'll entertain for me.

1:52:19.680 --> 1:52:22.080
 And if they're willing to jump through a bunch of my hoops,

1:52:22.080 --> 1:52:22.960
 then maybe.

1:52:22.960 --> 1:52:25.200
 But no, not the way that M&A works today.

1:52:25.200 --> 1:52:26.520
 I mean, we've been approached.

1:52:26.520 --> 1:52:28.000
 And I laugh in these people's faces.

1:52:28.000 --> 1:52:31.000
 I'm like, are you kidding?

1:52:31.000 --> 1:52:31.600
 Yeah.

1:52:31.600 --> 1:52:33.680
 Because it's so demeaning.

1:52:33.680 --> 1:52:36.960
 The M&A people are so demeaning to companies.

1:52:36.960 --> 1:52:41.340
 They treat the startup world as their innovation ecosystem.

1:52:41.340 --> 1:52:43.640
 And they think that I'm cool with going along with that,

1:52:43.640 --> 1:52:46.440
 so I can have some of their scam fake Fed dollars.

1:52:46.440 --> 1:52:47.680
 Fed coin.

1:52:47.680 --> 1:52:49.680
 What am I going to do with more Fed coin?

1:52:49.680 --> 1:52:50.320
 Fed coin.

1:52:50.320 --> 1:52:51.400
 Fed coin, man.

1:52:51.400 --> 1:52:52.120
 I love that.

1:52:52.120 --> 1:52:54.040
 So that's the cool thing about podcasting,

1:52:54.040 --> 1:52:56.200
 actually, is people criticize.

1:52:56.200 --> 1:53:00.560
 I don't know if you're familiar with Spotify giving Joe Rogan

1:53:00.560 --> 1:53:01.920
 $100 million.

1:53:01.920 --> 1:53:03.720
 I don't know about that.

1:53:03.720 --> 1:53:08.200
 And they respect, despite all the shit

1:53:08.200 --> 1:53:11.440
 that people are talking about Spotify,

1:53:11.440 --> 1:53:15.300
 people understand that podcasters like Joe Rogan

1:53:15.300 --> 1:53:17.200
 know what the hell they're doing.

1:53:17.200 --> 1:53:21.160
 So they give them money and say, just do what you do.

1:53:21.160 --> 1:53:25.440
 And the equivalent for you would be like,

1:53:25.440 --> 1:53:28.440
 George, do what the hell you do, because you're good at it.

1:53:28.440 --> 1:53:31.400
 Try not to murder too many people.

1:53:31.400 --> 1:53:33.480
 There's some kind of common sense things,

1:53:33.480 --> 1:53:37.400
 like just don't go on a weird rampage of it.

1:53:37.400 --> 1:53:38.080
 Yeah.

1:53:38.080 --> 1:53:43.400
 It comes down to what companies I could respect, right?

1:53:43.400 --> 1:53:44.440
 Could I respect GM?

1:53:44.440 --> 1:53:46.520
 Never.

1:53:46.520 --> 1:53:47.480
 No, I couldn't.

1:53:47.480 --> 1:53:50.840
 I mean, could I respect a Hyundai?

1:53:50.840 --> 1:53:52.520
 More so.

1:53:52.520 --> 1:53:53.560
 That's a lot closer.

1:53:53.560 --> 1:53:54.720
 Toyota?

1:53:54.720 --> 1:53:55.840
 What's your?

1:53:55.840 --> 1:53:56.840
 Nah.

1:53:56.840 --> 1:53:57.520
 Nah.

1:53:57.520 --> 1:53:59.400
 Korean is the way.

1:53:59.400 --> 1:54:02.440
 I think that the Japanese, the Germans, the US, they're all

1:54:02.440 --> 1:54:05.720
 too, they're all too, they all think they're too great.

1:54:05.720 --> 1:54:07.520
 What about the tech companies?

1:54:07.520 --> 1:54:08.480
 Apple?

1:54:08.480 --> 1:54:11.000
 Apple is, of the tech companies that I could respect,

1:54:11.000 --> 1:54:12.120
 Apple's the closest.

1:54:12.120 --> 1:54:12.600
 Yeah.

1:54:12.600 --> 1:54:13.560
 I mean, I could never.

1:54:13.560 --> 1:54:14.440
 It would be ironic.

1:54:14.440 --> 1:54:19.840
 It would be ironic if Comma AI is acquired by Apple.

1:54:19.840 --> 1:54:21.840
 I mean, Facebook, look, I quit Facebook 10 years ago

1:54:21.840 --> 1:54:24.320
 because I didn't respect the business model.

1:54:24.320 --> 1:54:28.480
 Google has declined so fast in the last five years.

1:54:28.480 --> 1:54:32.080
 What are your thoughts about Waymo and its present

1:54:32.080 --> 1:54:33.200
 and its future?

1:54:33.200 --> 1:54:39.240
 Let me start by saying something nice, which is I've

1:54:39.240 --> 1:54:45.240
 visited them a few times and have ridden in their cars.

1:54:45.240 --> 1:54:49.760
 And the engineering that they're doing,

1:54:49.760 --> 1:54:51.720
 both the research and the actual development

1:54:51.720 --> 1:54:53.360
 and the engineering they're doing

1:54:53.360 --> 1:54:55.160
 and the scale they're actually achieving

1:54:55.160 --> 1:54:58.400
 by doing it all themselves is really impressive.

1:54:58.400 --> 1:55:01.480
 And the balance of safety and innovation.

1:55:01.480 --> 1:55:07.520
 And the cars work really well for the routes they drive.

1:55:07.520 --> 1:55:10.960
 It drives fast, which was very surprising to me.

1:55:10.960 --> 1:55:14.800
 It drives the speed limit or faster than the speed limit.

1:55:14.800 --> 1:55:16.120
 It goes.

1:55:16.120 --> 1:55:17.800
 And it works really damn well.

1:55:17.800 --> 1:55:19.160
 And the interface is nice.

1:55:19.160 --> 1:55:20.360
 In Chandler, Arizona, yeah.

1:55:20.360 --> 1:55:22.560
 Yeah, in Chandler, Arizona, very specific environment.

1:55:22.560 --> 1:55:27.360
 So it gives me enough material in my mind

1:55:27.360 --> 1:55:30.360
 to push back against the madmen of the world,

1:55:30.360 --> 1:55:36.040
 like George Hotz, to be like, because you kind of imply

1:55:36.040 --> 1:55:38.760
 there's zero probability they're going to win.

1:55:38.760 --> 1:55:43.560
 And after I've used, after I've ridden in it, to me,

1:55:43.560 --> 1:55:44.440
 it's not zero.

1:55:44.440 --> 1:55:46.760
 Oh, it's not for technology reasons.

1:55:46.760 --> 1:55:48.000
 Bureaucracy?

1:55:48.000 --> 1:55:49.520
 No, it's worse than that.

1:55:49.520 --> 1:55:51.840
 It's actually for product reasons, I think.

1:55:51.840 --> 1:55:53.840
 Oh, you think they're just not capable of creating

1:55:53.840 --> 1:55:55.600
 an amazing product?

1:55:55.600 --> 1:55:58.280
 No, I think that the product that they're building

1:55:58.280 --> 1:56:01.040
 doesn't make sense.

1:56:01.040 --> 1:56:03.320
 So a few things.

1:56:03.320 --> 1:56:05.840
 You say the Waymo's are fast.

1:56:05.840 --> 1:56:09.040
 Benchmark a Waymo against a competent Uber driver.

1:56:09.040 --> 1:56:09.600
 Right.

1:56:09.600 --> 1:56:10.080
 Right?

1:56:10.080 --> 1:56:11.080
 The Uber driver's faster.

1:56:11.080 --> 1:56:12.200
 It's not even about speed.

1:56:12.200 --> 1:56:13.240
 It's the thing you said.

1:56:13.240 --> 1:56:16.280
 It's about the experience of being stuck at a stop sign

1:56:16.280 --> 1:56:20.120
 because pedestrians are crossing nonstop.

1:56:20.120 --> 1:56:22.160
 I like when my Uber driver doesn't come to a full stop

1:56:22.160 --> 1:56:22.960
 at the stop sign.

1:56:22.960 --> 1:56:23.520
 Yeah.

1:56:23.520 --> 1:56:24.440
 You know?

1:56:24.440 --> 1:56:31.320
 And so let's say the Waymo's are 20% slower than an Uber.

1:56:31.320 --> 1:56:33.000
 Right?

1:56:33.000 --> 1:56:35.160
 You can argue that they're going to be cheaper.

1:56:35.160 --> 1:56:37.640
 And I argue that users already have the choice

1:56:37.640 --> 1:56:39.360
 to trade off money for speed.

1:56:39.360 --> 1:56:42.280
 It's called UberPool.

1:56:42.280 --> 1:56:45.560
 I think it's like 15% of rides are UberPools.

1:56:45.560 --> 1:56:46.040
 Right?

1:56:46.040 --> 1:56:49.480
 Users are not willing to trade off money for speed.

1:56:49.480 --> 1:56:52.320
 So the whole product that they're building

1:56:52.320 --> 1:56:56.200
 is not going to be competitive with traditional ride sharing

1:56:56.200 --> 1:56:56.720
 networks.

1:56:56.720 --> 1:56:59.560
 Right.

1:56:59.560 --> 1:57:04.360
 And also, whether there's profit to be made

1:57:04.360 --> 1:57:07.360
 depends entirely on one company having a monopoly.

1:57:07.360 --> 1:57:11.200
 I think that the level four autonomous ride sharing

1:57:11.200 --> 1:57:14.800
 vehicles market is going to look a lot like the scooter market

1:57:14.800 --> 1:57:18.680
 if even the technology does come to exist, which I question.

1:57:18.680 --> 1:57:20.400
 Who's doing well in that market?

1:57:20.400 --> 1:57:22.320
 It's a race to the bottom.

1:57:22.320 --> 1:57:25.680
 Well, it could be closer like an Uber and a Lyft,

1:57:25.680 --> 1:57:28.000
 where it's just one or two players.

1:57:28.000 --> 1:57:31.160
 Well, the scooter people have given up

1:57:31.160 --> 1:57:34.760
 trying to market scooters as a practical means

1:57:34.760 --> 1:57:35.480
 of transportation.

1:57:35.480 --> 1:57:37.640
 And they're just like, they're super fun to ride.

1:57:37.640 --> 1:57:38.360
 Look at wheels.

1:57:38.360 --> 1:57:39.160
 I love those things.

1:57:39.160 --> 1:57:40.560
 And they're great on that front.

1:57:40.560 --> 1:57:41.040
 Yeah.

1:57:41.040 --> 1:57:43.840
 But from an actual transportation product

1:57:43.840 --> 1:57:46.240
 perspective, I do not think scooters are viable.

1:57:46.240 --> 1:57:49.200
 And I do not think level four autonomous cars are viable.

1:57:49.200 --> 1:57:51.560
 If you, let's play a fun experiment.

1:57:51.560 --> 1:57:56.960
 If you ran, let's do a Tesla and let's do Waymo.

1:57:56.960 --> 1:58:01.880
 If Elon Musk took a vacation for a year, he just said,

1:58:01.880 --> 1:58:05.400
 screw it, I'm going to go live on an island, no electronics.

1:58:05.400 --> 1:58:07.520
 And the board decides that we need to find somebody

1:58:07.520 --> 1:58:09.160
 to run the company.

1:58:09.160 --> 1:58:11.520
 And they did decide that you should run the company

1:58:11.520 --> 1:58:12.240
 for a year.

1:58:12.240 --> 1:58:14.920
 How do you run Tesla differently?

1:58:14.920 --> 1:58:16.240
 I wouldn't change much.

1:58:16.240 --> 1:58:17.920
 Do you think they're on the right track?

1:58:17.920 --> 1:58:18.680
 I wouldn't change.

1:58:18.680 --> 1:58:21.640
 I mean, I'd have some minor changes.

1:58:21.640 --> 1:58:25.520
 But even my debate with Tesla about end

1:58:25.520 --> 1:58:29.120
 to end versus SegNets, that's just software.

1:58:29.120 --> 1:58:30.720
 Who cares?

1:58:30.720 --> 1:58:33.920
 It's not like you're doing something terrible with SegNets.

1:58:33.920 --> 1:58:35.600
 You're probably building something that's

1:58:35.600 --> 1:58:39.440
 at least going to help you debug the end to end system a lot.

1:58:39.440 --> 1:58:42.200
 It's very easy to transition from what they have

1:58:42.200 --> 1:58:45.840
 to an end to end kind of thing.

1:58:45.840 --> 1:58:50.520
 And then I presume you would, in the Model Y

1:58:50.520 --> 1:58:52.480
 or maybe in the Model 3, start adding driver

1:58:52.480 --> 1:58:53.600
 sensing with infrared.

1:58:53.600 --> 1:58:58.000
 Yes, I would add infrared camera, infrared lights

1:58:58.000 --> 1:58:59.640
 right away to those cars.

1:59:02.120 --> 1:59:04.920
 And start collecting that data and do all that kind of stuff,

1:59:04.920 --> 1:59:05.440
 yeah.

1:59:05.440 --> 1:59:06.080
 Very much.

1:59:06.080 --> 1:59:07.780
 I think they're already kind of doing it.

1:59:07.780 --> 1:59:09.320
 It's an incredibly minor change.

1:59:09.320 --> 1:59:11.160
 If I actually were CEO of Tesla, first off,

1:59:11.160 --> 1:59:13.080
 I'd be horrified that I wouldn't be able to do

1:59:13.080 --> 1:59:14.000
 a better job as Elon.

1:59:14.000 --> 1:59:16.480
 And then I would try to understand

1:59:16.480 --> 1:59:17.760
 the way he's done things before.

1:59:17.760 --> 1:59:20.720
 You would also have to take over his Twitter.

1:59:20.720 --> 1:59:22.200
 I don't tweet.

1:59:22.200 --> 1:59:24.240
 Yeah, what's your Twitter situation?

1:59:24.240 --> 1:59:25.880
 Why are you so quiet on Twitter?

1:59:25.880 --> 1:59:30.280
 Since Dukama is like what's your social network presence like?

1:59:30.280 --> 1:59:34.280
 Because on Instagram, you do live streams.

1:59:34.280 --> 1:59:39.240
 You understand the music of the internet,

1:59:39.240 --> 1:59:41.520
 but you don't always fully engage into it.

1:59:41.520 --> 1:59:42.800
 You're part time.

1:59:42.800 --> 1:59:44.160
 Well, I used to have a Twitter.

1:59:44.160 --> 1:59:47.600
 Yeah, I mean, Instagram is a pretty place.

1:59:47.600 --> 1:59:49.120
 Instagram is a beautiful place.

1:59:49.120 --> 1:59:49.960
 It glorifies beauty.

1:59:49.960 --> 1:59:53.360
 I like Instagram's values as a network.

1:59:53.360 --> 2:00:00.320
 Twitter glorifies conflict, glorifies shots,

2:00:00.320 --> 2:00:01.400
 taking shots of people.

2:00:01.400 --> 2:00:05.280
 And it's like, you know, Twitter and Donald Trump

2:00:05.280 --> 2:00:08.440
 are perfectly, they're perfect for each other.

2:00:08.440 --> 2:00:12.800
 So Tesla's on the right track in your view.

2:00:12.800 --> 2:00:16.600
 OK, so let's try, let's really try this experiment.

2:00:16.600 --> 2:00:19.720
 If you ran Waymo, let's say they're,

2:00:19.720 --> 2:00:21.160
 I don't know if you agree, but they

2:00:21.160 --> 2:00:25.640
 seem to be at the head of the pack of the kind of,

2:00:25.640 --> 2:00:27.040
 what would you call that approach?

2:00:27.040 --> 2:00:29.000
 Like it's not necessarily lighter based

2:00:29.000 --> 2:00:30.320
 because it's not about lighter.

2:00:30.320 --> 2:00:31.520
 Level four robotaxi.

2:00:31.520 --> 2:00:37.040
 Level four robotaxi, all in before making any revenue.

2:00:37.040 --> 2:00:38.720
 So they're probably at the head of the pack.

2:00:38.720 --> 2:00:42.840
 If you were said, hey, George, can you

2:00:42.840 --> 2:00:47.160
 please run this company for a year, how would you change it?

2:00:47.160 --> 2:00:47.760
 I would go.

2:00:47.760 --> 2:00:49.800
 I would get Anthony Levandowski out of jail,

2:00:49.800 --> 2:00:51.720
 and I would put him in charge of the company.

2:00:56.400 --> 2:00:58.200
 Well, let's try to break that apart.

2:00:58.200 --> 2:01:01.600
 Why do you want to destroy the company by doing that?

2:01:01.600 --> 2:01:09.400
 Or do you mean you like renegade style thinking that pushes,

2:01:09.400 --> 2:01:11.560
 that throws away bureaucracy and goes

2:01:11.560 --> 2:01:12.840
 to first principle thinking?

2:01:12.840 --> 2:01:14.080
 What do you mean by that?

2:01:14.080 --> 2:01:16.040
 I think Anthony Levandowski is a genius,

2:01:16.040 --> 2:01:19.640
 and I think he would come up with a much better idea of what

2:01:19.640 --> 2:01:22.360
 to do with Waymo than me.

2:01:22.360 --> 2:01:23.840
 So you mean that unironically.

2:01:23.840 --> 2:01:24.800
 He is a genius.

2:01:24.800 --> 2:01:25.400
 Oh, yes.

2:01:25.400 --> 2:01:26.640
 Oh, absolutely.

2:01:26.640 --> 2:01:27.600
 Without a doubt.

2:01:27.600 --> 2:01:30.960
 I mean, I'm not saying there's no shortcomings,

2:01:30.960 --> 2:01:34.760
 but in the interactions I've had with him, yeah.

2:01:34.760 --> 2:01:35.720
 What?

2:01:35.720 --> 2:01:38.200
 He's also willing to take, like, who knows

2:01:38.200 --> 2:01:39.400
 what he would do with Waymo?

2:01:39.400 --> 2:01:41.400
 I mean, he's also out there, like far more out there

2:01:41.400 --> 2:01:41.880
 than I am.

2:01:41.880 --> 2:01:43.480
 Yeah, there's big risks.

2:01:43.480 --> 2:01:44.480
 What do you make of him?

2:01:44.480 --> 2:01:47.040
 I was going to talk to him on this podcast,

2:01:47.040 --> 2:01:48.880
 and I was going back and forth.

2:01:48.880 --> 2:01:51.720
 I'm such a gullible, naive human.

2:01:51.720 --> 2:01:53.840
 Like, I see the best in people.

2:01:53.840 --> 2:01:56.720
 And I slowly started to realize that there

2:01:56.720 --> 2:02:02.400
 might be some people out there that, like,

2:02:02.400 --> 2:02:05.280
 have multiple faces to the world.

2:02:05.280 --> 2:02:08.160
 They're, like, deceiving and dishonest.

2:02:08.160 --> 2:02:13.040
 I still refuse to, like, I just, I trust people,

2:02:13.040 --> 2:02:14.640
 and I don't care if I get hurt by it.

2:02:14.640 --> 2:02:16.520
 But, like, you know, sometimes you

2:02:16.520 --> 2:02:18.760
 have to be a little bit careful, especially platform

2:02:18.760 --> 2:02:21.040
 wise and podcast wise.

2:02:21.040 --> 2:02:23.160
 What do you, what am I supposed to think?

2:02:23.160 --> 2:02:26.400
 So you think, you think he's a good person?

2:02:26.400 --> 2:02:27.840
 Oh, I don't know.

2:02:27.840 --> 2:02:30.000
 I don't really make moral judgments.

2:02:30.000 --> 2:02:30.840
 It's difficult to.

2:02:30.840 --> 2:02:32.480
 Oh, I mean this about the Waymo.

2:02:32.480 --> 2:02:34.960
 I actually, I mean that whole idea very nonironically

2:02:34.960 --> 2:02:36.200
 about what I would do.

2:02:36.200 --> 2:02:38.160
 The problem with putting me in charge of Waymo

2:02:38.160 --> 2:02:41.720
 is Waymo is already $10 billion in the hole, right?

2:02:41.720 --> 2:02:44.840
 Whatever idea Waymo does, look, commas profitable, commas

2:02:44.840 --> 2:02:46.680
 raised $8.1 million.

2:02:46.680 --> 2:02:48.180
 That's small, you know, that's small money.

2:02:48.180 --> 2:02:50.840
 Like, I can build a reasonable consumer electronics company

2:02:50.840 --> 2:02:54.200
 and succeed wildly at that and still never be able to pay back

2:02:54.200 --> 2:02:55.800
 Waymo's $10 billion.

2:02:55.800 --> 2:02:58.680
 So I think the basic idea with Waymo, well,

2:02:58.680 --> 2:03:00.880
 forget the $10 billion because they have some backing,

2:03:00.880 --> 2:03:04.000
 but your basic thing is, like, what can we do

2:03:04.000 --> 2:03:05.640
 to start making some money?

2:03:05.640 --> 2:03:07.920
 Well, no, I mean, my bigger idea is, like,

2:03:07.920 --> 2:03:10.320
 whatever the idea is that's gonna save Waymo,

2:03:10.320 --> 2:03:11.440
 I don't have it.

2:03:11.440 --> 2:03:13.480
 It's gonna have to be a big risk idea

2:03:13.480 --> 2:03:15.280
 and I cannot think of a better person

2:03:15.280 --> 2:03:17.840
 than Anthony Levandowski to do it.

2:03:17.840 --> 2:03:20.240
 So that is completely what I would do as CEO of Waymo.

2:03:20.240 --> 2:03:22.640
 I would call myself a transitionary CEO,

2:03:22.640 --> 2:03:24.880
 do everything I can to fix that situation up.

2:03:24.880 --> 2:03:25.720
 I'm gonna see.

2:03:25.720 --> 2:03:26.560
 Yeah.

2:03:27.920 --> 2:03:28.760
 Yeah.

2:03:28.760 --> 2:03:29.760
 Because I can't do it, right?

2:03:29.760 --> 2:03:33.440
 Like, I can't, I mean, I can talk about how

2:03:33.440 --> 2:03:35.260
 what I really wanna do is just apologize

2:03:35.260 --> 2:03:38.120
 for all those corny, you know, ad campaigns

2:03:38.120 --> 2:03:40.160
 and be like, here's the real state of the technology.

2:03:40.160 --> 2:03:42.280
 Yeah, that's, like, I have several criticism.

2:03:42.280 --> 2:03:44.320
 I'm a little bit more bullish on Waymo

2:03:44.320 --> 2:03:48.000
 than you seem to be, but one criticism I have

2:03:48.000 --> 2:03:50.880
 is it went into corny mode too early.

2:03:50.880 --> 2:03:52.200
 Like, it's still a startup.

2:03:52.200 --> 2:03:53.660
 It hasn't delivered on anything.

2:03:53.660 --> 2:03:56.320
 So it should be, like, more renegade

2:03:56.320 --> 2:03:59.280
 and show off the engineering that they're doing,

2:03:59.280 --> 2:04:00.560
 which just can be impressive,

2:04:00.560 --> 2:04:02.320
 as opposed to doing these weird commercials

2:04:02.320 --> 2:04:07.040
 of, like, your friendly car company.

2:04:07.040 --> 2:04:10.080
 I mean, that's my biggest snipe at Waymo is always,

2:04:10.080 --> 2:04:11.440
 that guy's a paid actor.

2:04:11.440 --> 2:04:12.800
 That guy's not a Waymo user.

2:04:12.800 --> 2:04:13.640
 He's a paid actor.

2:04:13.640 --> 2:04:15.360
 Look here, I found his call sheet.

2:04:15.360 --> 2:04:17.400
 Do kind of like what SpaceX is doing

2:04:17.400 --> 2:04:18.960
 with the rocket launches.

2:04:18.960 --> 2:04:22.200
 Just put the nerds up front, put the engineers up front,

2:04:22.200 --> 2:04:25.720
 and just, like, show failures too, just.

2:04:25.720 --> 2:04:27.880
 I love SpaceX's, yeah.

2:04:27.880 --> 2:04:29.840
 Yeah, the thing that they're doing is right,

2:04:29.840 --> 2:04:31.720
 and it just feels like the right.

2:04:31.720 --> 2:04:32.560
 But.

2:04:32.560 --> 2:04:34.440
 We're all so excited to see them succeed.

2:04:34.440 --> 2:04:35.260
 Yeah.

2:04:35.260 --> 2:04:37.320
 I can't wait to see when it won't fail, you know?

2:04:37.320 --> 2:04:39.400
 Like, you lie to me, I want you to fail.

2:04:39.400 --> 2:04:41.200
 You tell me the truth, you be honest with me,

2:04:41.200 --> 2:04:42.160
 I want you to succeed.

2:04:42.160 --> 2:04:43.000
 Yeah.

2:04:44.120 --> 2:04:49.020
 Ah, yeah, and that requires the renegade CEO, right?

2:04:50.200 --> 2:04:51.480
 I'm with you, I'm with you.

2:04:51.480 --> 2:04:54.000
 I still have a little bit of faith in Waymo

2:04:54.000 --> 2:04:56.520
 for the renegade CEO to step forward, but.

2:04:57.860 --> 2:05:00.600
 It's not, it's not John Kraftik.

2:05:00.600 --> 2:05:02.980
 Yeah, it's, you can't.

2:05:02.980 --> 2:05:04.980
 It's not Chris Hormiston.

2:05:04.980 --> 2:05:07.680
 And those people may be very good at certain things.

2:05:07.680 --> 2:05:08.520
 Yeah.

2:05:08.520 --> 2:05:10.360
 But they're not renegades.

2:05:10.360 --> 2:05:12.040
 Yeah, because these companies are fundamentally,

2:05:12.040 --> 2:05:14.260
 even though we're talking about billion dollars,

2:05:14.260 --> 2:05:15.640
 all these crazy numbers,

2:05:15.640 --> 2:05:19.300
 they're still, like, early stage startups.

2:05:19.300 --> 2:05:21.840
 I mean, and I just, if you are pre revenue

2:05:21.840 --> 2:05:23.120
 and you've raised 10 billion dollars,

2:05:23.120 --> 2:05:26.520
 I have no idea, like, this just doesn't work.

2:05:26.520 --> 2:05:28.040
 You know, it's against everything Silicon Valley.

2:05:28.040 --> 2:05:29.880
 Where's your minimum viable product?

2:05:29.880 --> 2:05:31.600
 You know, where's your users?

2:05:31.600 --> 2:05:33.320
 Where's your growth numbers?

2:05:33.320 --> 2:05:36.280
 This is traditional Silicon Valley.

2:05:36.280 --> 2:05:38.040
 Why do you not apply it to what you think

2:05:38.040 --> 2:05:39.920
 you're too big to fail already, like?

2:05:41.760 --> 2:05:45.780
 How do you think autonomous driving will change society?

2:05:45.780 --> 2:05:50.780
 So the mission is, for comma, to solve self driving.

2:05:52.520 --> 2:05:54.240
 Do you have, like, a vision of the world

2:05:54.240 --> 2:05:55.840
 of how it'll be different?

2:05:57.940 --> 2:06:00.100
 Is it as simple as A to B transportation?

2:06:00.100 --> 2:06:02.540
 Or is there, like, cause these are robots.

2:06:03.640 --> 2:06:05.880
 It's not about autonomous driving in and of itself.

2:06:05.880 --> 2:06:07.520
 It's what the technology enables.

2:06:09.760 --> 2:06:12.200
 It's, I think it's the coolest applied AI problem.

2:06:12.200 --> 2:06:16.360
 I like it because it has a clear path to monetary value.

2:06:17.720 --> 2:06:21.440
 But as far as that being the thing that changes the world,

2:06:21.440 --> 2:06:25.480
 I mean, no, like, there's cute things we're doing in common.

2:06:25.480 --> 2:06:26.960
 Like, who'd have thought you could stick a phone

2:06:26.960 --> 2:06:29.080
 on the windshield and it'll drive.

2:06:29.080 --> 2:06:31.160
 But like, really, the product that you're building

2:06:31.160 --> 2:06:33.920
 is not something that people were not capable

2:06:33.920 --> 2:06:35.760
 of imagining 50 years ago.

2:06:35.760 --> 2:06:37.800
 So no, it doesn't change the world on that front.

2:06:37.800 --> 2:06:39.480
 Could people have imagined the internet 50 years ago?

2:06:39.480 --> 2:06:42.560
 Only true genius visionaries.

2:06:42.560 --> 2:06:45.120
 Everyone could have imagined autonomous cars 50 years ago.

2:06:45.120 --> 2:06:47.000
 It's like a car, but I don't drive it.

2:06:47.000 --> 2:06:49.920
 See, I have this sense, and I told you, like,

2:06:49.920 --> 2:06:54.920
 my longterm dream is robots with which you have deep,

2:06:55.880 --> 2:06:57.920
 with whom you have deep connections, right?

2:06:59.320 --> 2:07:02.600
 And there's different trajectories towards that.

2:07:03.600 --> 2:07:04.440
 And I've been thinking,

2:07:04.440 --> 2:07:07.060
 so I've been thinking of launching a startup.

2:07:07.060 --> 2:07:09.820
 I see autonomous vehicles

2:07:09.820 --> 2:07:11.420
 as a potential trajectory to that.

2:07:11.420 --> 2:07:16.420
 That's not where the direction I would like to go,

2:07:16.580 --> 2:07:19.100
 but I also see Tesla or even Comma AI,

2:07:19.100 --> 2:07:23.820
 like, pivoting into robotics broadly defined

2:07:24.900 --> 2:07:27.140
 at some stage in the way, like you're mentioning,

2:07:27.140 --> 2:07:28.540
 the internet didn't expect.

2:07:29.580 --> 2:07:32.580
 Let's solve, you know, when I say a comma about this,

2:07:32.580 --> 2:07:33.420
 we could talk about this,

2:07:33.420 --> 2:07:35.740
 but let's solve self driving cars first.

2:07:35.740 --> 2:07:37.140
 You gotta stay focused on the mission.

2:07:37.140 --> 2:07:39.260
 Don't, don't, don't, you're not too big to fail.

2:07:39.260 --> 2:07:41.380
 For however much I think Comma's winning,

2:07:41.380 --> 2:07:43.420
 like, no, no, no, no, no, you're winning

2:07:43.420 --> 2:07:45.020
 when you solve level five self driving cars.

2:07:45.020 --> 2:07:46.780
 And until then, you haven't won.

2:07:46.780 --> 2:07:48.900
 And you know, again, you wanna be arrogant

2:07:48.900 --> 2:07:50.620
 in the face of other people, great.

2:07:50.620 --> 2:07:53.780
 You wanna be arrogant in the face of nature, you're an idiot.

2:07:53.780 --> 2:07:56.420
 Stay mission focused, brilliantly put.

2:07:56.420 --> 2:07:58.660
 Like I mentioned, thinking of launching a startup,

2:07:58.660 --> 2:08:01.220
 I've been considering, actually, before COVID,

2:08:01.220 --> 2:08:03.340
 I've been thinking of moving to San Francisco.

2:08:03.340 --> 2:08:06.020
 Ooh, ooh, I wouldn't go there.

2:08:06.020 --> 2:08:09.020
 So why is, well, and now I'm thinking

2:08:09.020 --> 2:08:13.660
 about potentially Austin and we're in San Diego now.

2:08:13.660 --> 2:08:14.940
 San Diego, come here.

2:08:14.940 --> 2:08:19.940
 So why, what, I mean, you're such an interesting human.

2:08:20.540 --> 2:08:23.060
 You've launched so many successful things.

2:08:23.060 --> 2:08:26.220
 What, why San Diego?

2:08:26.220 --> 2:08:27.060
 What do you recommend?

2:08:27.060 --> 2:08:29.300
 Why not San Francisco?

2:08:29.300 --> 2:08:31.820
 Have you thought, so in your case,

2:08:31.820 --> 2:08:33.780
 San Diego with Qualcomm and Snapdragon,

2:08:33.780 --> 2:08:36.860
 I mean, that's an amazing combination.

2:08:36.860 --> 2:08:37.700
 But.

2:08:37.700 --> 2:08:38.540
 That wasn't really why.

2:08:38.540 --> 2:08:39.500
 That wasn't the why?

2:08:39.500 --> 2:08:41.340
 No, I mean, Qualcomm was an afterthought.

2:08:41.340 --> 2:08:42.900
 Qualcomm was, it was a nice thing to think about.

2:08:42.900 --> 2:08:45.140
 It's like, you can have a tech company here.

2:08:45.140 --> 2:08:45.980
 Yeah.

2:08:45.980 --> 2:08:48.300
 And a good one, I mean, you know, I like Qualcomm, but.

2:08:48.300 --> 2:08:49.140
 No.

2:08:49.140 --> 2:08:50.540
 Well, so why San Diego better than San Francisco?

2:08:50.540 --> 2:08:51.900
 Why does San Francisco suck?

2:08:51.900 --> 2:08:53.020
 Well, so, okay, so first off,

2:08:53.020 --> 2:08:55.300
 we all kind of said like, we wanna stay in California.

2:08:55.300 --> 2:08:56.660
 People like the ocean.

2:08:57.620 --> 2:09:00.020
 You know, California, for its flaws,

2:09:00.020 --> 2:09:02.380
 it's like a lot of the flaws of California

2:09:02.380 --> 2:09:03.900
 are not necessarily California as a whole,

2:09:03.900 --> 2:09:05.740
 and they're much more San Francisco specific.

2:09:05.740 --> 2:09:06.700
 Yeah.

2:09:06.700 --> 2:09:09.860
 San Francisco, so I think first tier cities in general

2:09:09.860 --> 2:09:11.500
 have stopped wanting growth.

2:09:13.140 --> 2:09:15.460
 Well, you have like in San Francisco, you know,

2:09:15.460 --> 2:09:18.740
 the voting class always votes to not build more houses

2:09:18.740 --> 2:09:19.980
 because they own all the houses.

2:09:19.980 --> 2:09:21.820
 And they're like, well, you know,

2:09:21.820 --> 2:09:23.860
 once people have figured out how to vote themselves

2:09:23.860 --> 2:09:25.180
 more money, they're gonna do it.

2:09:25.180 --> 2:09:27.900
 It is so insanely corrupt.

2:09:27.900 --> 2:09:31.620
 It is not balanced at all, like political party wise,

2:09:31.620 --> 2:09:34.700
 you know, it's a one party city and.

2:09:34.700 --> 2:09:36.700
 For all the discussion of diversity,

2:09:38.700 --> 2:09:42.140
 it stops lacking real diversity of thought,

2:09:42.140 --> 2:09:47.140
 of background, of approaches, of strategies, of ideas.

2:09:48.540 --> 2:09:51.180
 It's kind of a strange place

2:09:51.180 --> 2:09:54.380
 that it's the loudest people about diversity

2:09:54.380 --> 2:09:56.420
 and the biggest lack of diversity.

2:09:56.420 --> 2:09:58.620
 I mean, that's what they say, right?

2:09:58.620 --> 2:10:00.340
 It's the projection.

2:10:00.340 --> 2:10:02.140
 Projection, yeah.

2:10:02.140 --> 2:10:02.980
 Yeah, it's interesting.

2:10:02.980 --> 2:10:04.500
 And even people in Silicon Valley tell me

2:10:04.500 --> 2:10:07.940
 that's like high up people,

2:10:07.940 --> 2:10:10.100
 everybody is like, this is a terrible place.

2:10:10.100 --> 2:10:10.940
 It doesn't make sense.

2:10:10.940 --> 2:10:13.220
 I mean, and coronavirus is really what killed it.

2:10:13.220 --> 2:10:17.340
 San Francisco was the number one exodus

2:10:17.340 --> 2:10:18.900
 during coronavirus.

2:10:18.900 --> 2:10:21.660
 We still think San Diego is a good place to be.

2:10:21.660 --> 2:10:22.500
 Yeah.

2:10:23.460 --> 2:10:24.740
 Yeah, I mean, we'll see.

2:10:24.740 --> 2:10:29.740
 We'll see what happens with California a bit longer term.

2:10:29.780 --> 2:10:32.100
 Like Austin's an interesting choice.

2:10:32.100 --> 2:10:33.940
 I wouldn't, I don't have really anything bad to say

2:10:33.940 --> 2:10:35.740
 about Austin either,

2:10:35.740 --> 2:10:37.940
 except for the extreme heat in the summer,

2:10:37.940 --> 2:10:40.180
 which, but that's like very on the surface, right?

2:10:40.180 --> 2:10:43.700
 I think as far as like an ecosystem goes, it's cool.

2:10:43.700 --> 2:10:45.540
 I personally love Colorado.

2:10:45.540 --> 2:10:47.100
 Colorado's great.

2:10:47.100 --> 2:10:49.140
 Yeah, I mean, you have these states that are,

2:10:49.140 --> 2:10:51.380
 like just way better run.

2:10:51.380 --> 2:10:55.380
 California is, you know, it's especially San Francisco.

2:10:55.380 --> 2:10:58.460
 It's not a tie horse and like, yeah.

2:10:58.460 --> 2:11:02.620
 Can I ask you for advice to me and to others

2:11:02.620 --> 2:11:07.540
 about what's it take to build a successful startup?

2:11:07.540 --> 2:11:08.380
 Oh, I don't know.

2:11:08.380 --> 2:11:09.220
 I haven't done that.

2:11:09.220 --> 2:11:10.740
 Talk to someone who did that.

2:11:10.740 --> 2:11:12.140
 Well, you've, you know,

2:11:14.980 --> 2:11:16.460
 this is like another book of years

2:11:16.460 --> 2:11:18.860
 that I'll buy for $67, I suppose.

2:11:18.860 --> 2:11:20.700
 So there's, um.

2:11:20.700 --> 2:11:24.140
 One of these days I'll sell out.

2:11:24.140 --> 2:11:24.980
 Yeah, that's right.

2:11:24.980 --> 2:11:26.020
 Jailbreaks are going to be a dollar

2:11:26.020 --> 2:11:27.860
 and books are going to be 67.

2:11:27.860 --> 2:11:32.060
 How I jailbroke the iPhone by George Hots.

2:11:32.060 --> 2:11:32.900
 That's right.

2:11:32.900 --> 2:11:35.620
 How I jail broke the iPhone and you can too.

2:11:35.620 --> 2:11:36.860
 You can too.

2:11:36.860 --> 2:11:37.700
 67 dollars.

2:11:37.700 --> 2:11:39.020
 In 21 days.

2:11:39.020 --> 2:11:39.860
 That's right.

2:11:39.860 --> 2:11:40.700
 That's right.

2:11:40.700 --> 2:11:41.540
 Oh God.

2:11:41.540 --> 2:11:42.380
 Okay, I can't wait.

2:11:42.380 --> 2:11:44.940
 But quite, so you have an introspective,

2:11:44.940 --> 2:11:49.460
 you have built a very unique company.

2:11:49.460 --> 2:11:52.300
 I mean, not you, but you and others.

2:11:53.180 --> 2:11:54.700
 But I don't know.

2:11:55.620 --> 2:11:56.940
 There's no, there's nothing.

2:11:56.940 --> 2:11:57.780
 You have an introspective,

2:11:57.780 --> 2:12:00.900
 you haven't really sat down and thought about like,

2:12:01.780 --> 2:12:04.180
 well, like if you and I were having a bunch of,

2:12:04.180 --> 2:12:06.180
 we're having some beers

2:12:06.180 --> 2:12:08.100
 and you're seeing that I'm depressed

2:12:08.100 --> 2:12:09.860
 and whatever, I'm struggling.

2:12:09.860 --> 2:12:11.580
 There's no advice you can give?

2:12:11.580 --> 2:12:13.100
 Oh, I mean.

2:12:13.100 --> 2:12:13.940
 More beer?

2:12:13.940 --> 2:12:15.100
 More beer?

2:12:15.100 --> 2:12:20.100
 Um, yeah, I think it's all very like situation dependent.

2:12:23.340 --> 2:12:25.860
 Here's, okay, if I can give a generic piece of advice,

2:12:25.860 --> 2:12:28.140
 it's the technology always wins.

2:12:28.140 --> 2:12:30.740
 The better technology always wins.

2:12:30.740 --> 2:12:33.300
 And lying always loses.

2:12:35.180 --> 2:12:37.220
 Build technology and don't lie.

2:12:38.580 --> 2:12:39.420
 I'm with you.

2:12:39.420 --> 2:12:40.420
 I agree very much.

2:12:40.420 --> 2:12:41.340
 The long run, long run.

2:12:41.340 --> 2:12:42.180
 Sure.

2:12:42.180 --> 2:12:43.020
 That's the long run, yeah.

2:12:43.020 --> 2:12:44.860
 The market can remain irrational longer

2:12:44.860 --> 2:12:46.540
 than you can remain solvent.

2:12:46.540 --> 2:12:47.380
 True fact.

2:12:47.380 --> 2:12:49.260
 Well, this is an interesting point

2:12:49.260 --> 2:12:52.620
 because I ethically and just as a human believe that

2:12:54.380 --> 2:12:58.980
 like hype and smoke and mirrors is not

2:12:58.980 --> 2:13:02.740
 at any stage of the company is a good strategy.

2:13:02.740 --> 2:13:04.900
 I mean, there's some like, you know,

2:13:04.900 --> 2:13:07.020
 PR magic kind of like, you know.

2:13:07.020 --> 2:13:08.540
 Oh, hype around a new product, right?

2:13:08.540 --> 2:13:09.780
 If there's a call to action,

2:13:09.780 --> 2:13:10.980
 if there's like a call to action,

2:13:10.980 --> 2:13:13.060
 like buy my new GPU, look at it.

2:13:13.060 --> 2:13:14.940
 It takes up three slots and it's this big.

2:13:14.940 --> 2:13:15.780
 It's huge.

2:13:15.780 --> 2:13:16.620
 Buy my GPU.

2:13:16.620 --> 2:13:17.460
 Yeah, that's great.

2:13:17.460 --> 2:13:18.300
 If you look at, you know,

2:13:18.300 --> 2:13:20.780
 especially in the AI space broadly,

2:13:20.780 --> 2:13:22.780
 but autonomous vehicles,

2:13:22.780 --> 2:13:26.540
 like you can raise a huge amount of money on nothing.

2:13:26.540 --> 2:13:29.220
 And the question to me is like, I'm against that.

2:13:30.060 --> 2:13:31.500
 I'll never be part of that.

2:13:31.500 --> 2:13:35.820
 I don't think, I hope not, willingly not.

2:13:36.820 --> 2:13:40.020
 But like, is there something to be said

2:13:40.020 --> 2:13:44.980
 to essentially lying to raise money,

2:13:44.980 --> 2:13:47.860
 like fake it till you make it kind of thing?

2:13:47.860 --> 2:13:50.140
 I mean, this is Billy McFarland in the Fyre Festival.

2:13:50.140 --> 2:13:53.460
 Like we all experienced, you know,

2:13:53.460 --> 2:13:54.300
 what happens with that.

2:13:54.300 --> 2:13:57.460
 No, no, don't fake it till you make it.

2:13:57.460 --> 2:14:00.540
 Be honest and hope you make it the whole way.

2:14:00.540 --> 2:14:01.940
 The technology wins.

2:14:01.940 --> 2:14:02.980
 Right, the technology wins.

2:14:02.980 --> 2:14:06.820
 And like, there is, I'm not used to like the anti hype,

2:14:06.820 --> 2:14:08.900
 you know, that's a Slava KPSS reference,

2:14:08.900 --> 2:14:13.140
 but hype isn't necessarily bad.

2:14:13.140 --> 2:14:17.580
 I loved camping out for the iPhones, you know,

2:14:17.580 --> 2:14:21.020
 and as long as the hype is backed by like substance,

2:14:21.020 --> 2:14:23.780
 as long as it's backed by something I can actually buy,

2:14:23.780 --> 2:14:26.740
 and like it's real, then hype is great

2:14:26.740 --> 2:14:28.700
 and it's a great feeling.

2:14:28.700 --> 2:14:30.860
 It's when the hype is backed by lies

2:14:30.860 --> 2:14:32.060
 that it's a bad feeling.

2:14:32.060 --> 2:14:34.540
 I mean, a lot of people call Elon Musk a fraud.

2:14:34.540 --> 2:14:35.620
 How could he be a fraud?

2:14:35.620 --> 2:14:37.740
 I've noticed this, this kind of interesting effect,

2:14:37.740 --> 2:14:42.300
 which is he does tend to over promise

2:14:42.300 --> 2:14:45.660
 and deliver, what's the better way to phrase it?

2:14:45.660 --> 2:14:49.220
 Promise a timeline that he doesn't deliver on,

2:14:49.220 --> 2:14:51.740
 he delivers much later on.

2:14:51.740 --> 2:14:52.820
 What do you think about that?

2:14:52.820 --> 2:14:56.220
 Cause I do that, I think that's a programmer thing too.

2:14:56.220 --> 2:14:57.540
 I do that as well.

2:14:57.540 --> 2:15:01.300
 You think that's a really bad thing to do or is that okay?

2:15:01.300 --> 2:15:03.900
 I think that's, again, as long as like,

2:15:03.900 --> 2:15:06.940
 you're working toward it and you're gonna deliver on it,

2:15:06.940 --> 2:15:10.260
 it's not too far off, right?

2:15:10.260 --> 2:15:11.100
 Right?

2:15:11.100 --> 2:15:14.900
 Like, you know, the whole autonomous vehicle thing,

2:15:14.900 --> 2:15:18.620
 it's like, I mean, I still think Tesla's on track

2:15:18.620 --> 2:15:19.740
 to beat us.

2:15:19.740 --> 2:15:21.900
 I still think even with their missteps,

2:15:21.900 --> 2:15:24.420
 they have advantages we don't have.

2:15:25.660 --> 2:15:28.100
 You know, Elon is better than me

2:15:28.100 --> 2:15:33.060
 at like marshaling massive amounts of resources.

2:15:33.060 --> 2:15:36.260
 So, you know, I still think given the fact

2:15:36.260 --> 2:15:38.220
 they're maybe making some wrong decisions,

2:15:38.220 --> 2:15:39.300
 they'll end up winning.

2:15:39.300 --> 2:15:42.620
 And like, it's fine to hype it

2:15:42.620 --> 2:15:44.900
 if you're actually gonna win, right?

2:15:44.900 --> 2:15:47.220
 Like if Elon says, look, we're gonna be landing rockets

2:15:47.220 --> 2:15:49.340
 back on earth in a year and it takes four,

2:15:49.340 --> 2:15:53.540
 like, you know, he landed a rocket back on earth

2:15:53.540 --> 2:15:55.420
 and he was working toward it the whole time.

2:15:55.420 --> 2:15:57.060
 I think there's some amount of like,

2:15:57.060 --> 2:15:59.260
 I think when it becomes wrong is if you know

2:15:59.260 --> 2:16:00.460
 you're not gonna meet that deadline.

2:16:00.460 --> 2:16:01.540
 If you're lying.

2:16:01.540 --> 2:16:03.260
 Yeah, that's brilliantly put.

2:16:03.260 --> 2:16:06.860
 Like this is what people don't understand, I think.

2:16:06.860 --> 2:16:09.500
 Like Elon believes everything he says.

2:16:09.500 --> 2:16:12.140
 He does, as far as I can tell, he does.

2:16:12.140 --> 2:16:14.780
 And I detected that in myself too.

2:16:14.780 --> 2:16:17.620
 Like if I, it's only bullshit

2:16:17.620 --> 2:16:21.460
 if you're like conscious of yourself lying.

2:16:21.460 --> 2:16:22.500
 Yeah, I think so.

2:16:22.500 --> 2:16:23.420
 Yeah.

2:16:23.420 --> 2:16:25.660
 Now you can't take that to such an extreme, right?

2:16:25.660 --> 2:16:27.620
 Like in a way, I think maybe Billy McFarland

2:16:27.620 --> 2:16:30.020
 believed everything he said too.

2:16:30.020 --> 2:16:31.460
 Right, that's how you start a cult

2:16:31.460 --> 2:16:33.820
 and everybody kills themselves.

2:16:33.820 --> 2:16:34.660
 Yeah.

2:16:34.660 --> 2:16:36.020
 Yeah, like it's, you need, you need,

2:16:36.020 --> 2:16:39.300
 if there's like some factor on it, it's fine.

2:16:39.300 --> 2:16:41.820
 And you need some people to like, you know,

2:16:41.820 --> 2:16:43.700
 keep you in check, but like,

2:16:44.580 --> 2:16:46.580
 if you deliver on most of the things you say

2:16:46.580 --> 2:16:48.780
 and just the timelines are off, yeah.

2:16:48.780 --> 2:16:50.300
 It does piss people off though.

2:16:50.300 --> 2:16:53.300
 I wonder, but who cares?

2:16:53.300 --> 2:16:55.260
 In a long arc of history, the people,

2:16:55.260 --> 2:16:58.100
 everybody gets pissed off at the people who succeed,

2:16:58.100 --> 2:16:59.260
 which is one of the things

2:16:59.260 --> 2:17:01.820
 that frustrates me about this world,

2:17:01.820 --> 2:17:06.820
 is they don't celebrate the success of others.

2:17:07.660 --> 2:17:12.660
 Like there's so many people that want Elon to fail.

2:17:12.900 --> 2:17:14.860
 It's so fascinating to me.

2:17:14.860 --> 2:17:17.220
 Like what is wrong with you?

2:17:18.180 --> 2:17:21.620
 Like, so Elon Musk talks about like people shorting,

2:17:21.620 --> 2:17:23.580
 like they talk about financial,

2:17:23.580 --> 2:17:25.340
 but I think it's much bigger than the financials.

2:17:25.340 --> 2:17:27.860
 I've seen like the human factors community,

2:17:27.860 --> 2:17:31.540
 they want, they want other people to fail.

2:17:31.540 --> 2:17:32.940
 Why, why, why?

2:17:32.940 --> 2:17:35.860
 Like even people, the harshest thing is like,

2:17:36.740 --> 2:17:38.140
 you know, even people that like seem

2:17:38.140 --> 2:17:41.980
 to really hate Donald Trump, they want him to fail

2:17:41.980 --> 2:17:43.020
 or like the other president

2:17:43.020 --> 2:17:45.900
 or they want Barack Obama to fail.

2:17:45.900 --> 2:17:47.140
 It's like.

2:17:47.140 --> 2:17:49.740
 Yeah, we're all on the same boat, man.

2:17:49.740 --> 2:17:51.660
 It's weird, but I want that,

2:17:51.660 --> 2:17:54.100
 I would love to inspire that part of the world to change

2:17:54.100 --> 2:17:58.540
 because damn it, if the human species is gonna survive,

2:17:58.540 --> 2:18:00.460
 we should celebrate success.

2:18:00.460 --> 2:18:02.780
 Like it seems like the efficient thing to do

2:18:02.780 --> 2:18:06.300
 in this objective function that we're all striving for

2:18:06.300 --> 2:18:09.060
 is to celebrate the ones that like figure out

2:18:09.060 --> 2:18:11.820
 how to like do better at that objective function

2:18:11.820 --> 2:18:16.580
 as opposed to like dragging them down back into the mud.

2:18:16.580 --> 2:18:19.100
 I think there is, this is the speech I always give

2:18:19.100 --> 2:18:21.700
 about the commenters on Hacker News.

2:18:21.700 --> 2:18:23.260
 So first off, something to remember

2:18:23.260 --> 2:18:26.140
 about the internet in general is commenters

2:18:26.140 --> 2:18:29.300
 are not representative of the population.

2:18:29.300 --> 2:18:31.460
 I don't comment on anything.

2:18:31.460 --> 2:18:34.180
 You know, commenters are representative

2:18:34.180 --> 2:18:36.660
 of a certain sliver of the population.

2:18:36.660 --> 2:18:39.540
 And on Hacker News, a common thing I'll see

2:18:39.540 --> 2:18:42.100
 is when you'll see something that's like,

2:18:42.100 --> 2:18:47.100
 you know, promises to be wild out there and innovative.

2:18:47.500 --> 2:18:49.700
 There is some amount of, you know,

2:18:49.700 --> 2:18:50.940
 checking them back to earth,

2:18:50.940 --> 2:18:53.860
 but there's also some amount of if this thing succeeds,

2:18:55.260 --> 2:18:57.580
 well, I'm 36 and I've worked

2:18:57.580 --> 2:19:00.340
 at large tech companies my whole life.

2:19:02.300 --> 2:19:05.020
 They can't succeed because if they succeed,

2:19:05.020 --> 2:19:07.180
 that would mean that I could have done something different

2:19:07.180 --> 2:19:09.220
 with my life, but we know that I couldn't have,

2:19:09.220 --> 2:19:10.140
 we know that I couldn't have,

2:19:10.140 --> 2:19:11.940
 and that's why they're gonna fail.

2:19:11.940 --> 2:19:13.100
 And they have to root for them to fail

2:19:13.100 --> 2:19:15.940
 to kind of maintain their world image.

2:19:15.940 --> 2:19:17.860
 So tune it out.

2:19:17.860 --> 2:19:21.860
 And they comment, well, it's hard, I, so one of the things,

2:19:21.860 --> 2:19:25.260
 one of the things I'm considering startup wise

2:19:25.260 --> 2:19:27.620
 is to change that.

2:19:27.620 --> 2:19:31.700
 Cause I think the, I think it's also a technology problem.

2:19:31.700 --> 2:19:33.060
 It's a platform problem.

2:19:33.060 --> 2:19:33.900
 I agree.

2:19:33.900 --> 2:19:35.980
 It's like, because the thing you said,

2:19:35.980 --> 2:19:37.340
 most people don't comment.

2:19:39.660 --> 2:19:41.660
 I think most people want to comment.

2:19:42.940 --> 2:19:45.180
 They just don't because it's all the assholes

2:19:45.180 --> 2:19:46.020
 who are commenting.

2:19:46.020 --> 2:19:47.620
 Exactly, I don't want to be grouped in with them.

2:19:47.620 --> 2:19:49.420
 You don't want to be at a party

2:19:49.420 --> 2:19:50.780
 where everyone is an asshole.

2:19:50.780 --> 2:19:54.500
 And so they, but that's a platform problem.

2:19:54.500 --> 2:19:56.100
 I can't believe what Reddit's become.

2:19:56.100 --> 2:19:59.440
 I can't believe the group thinking, Reddit comments.

2:20:00.580 --> 2:20:02.700
 There's a, Reddit is an interesting one

2:20:02.700 --> 2:20:05.100
 because they're subreddits.

2:20:05.100 --> 2:20:09.260
 And so you can still see, especially small subreddits

2:20:09.260 --> 2:20:11.940
 that like, that are a little like havens

2:20:11.940 --> 2:20:16.140
 of like joy and positivity and like deep,

2:20:16.140 --> 2:20:18.860
 even disagreement, but like nuanced discussion.

2:20:18.860 --> 2:20:21.700
 But it's only like small little pockets,

2:20:21.700 --> 2:20:23.460
 but that's emergent.

2:20:23.460 --> 2:20:26.780
 The platform is not helping that or hurting that.

2:20:26.780 --> 2:20:31.100
 So I guess naturally something about the internet,

2:20:31.100 --> 2:20:34.380
 if you don't put in a lot of effort to encourage

2:20:34.380 --> 2:20:37.180
 nuance and positive, good vibes,

2:20:37.180 --> 2:20:41.140
 it's naturally going to decline into chaos.

2:20:41.140 --> 2:20:42.860
 I would love to see someone do this well.

2:20:42.860 --> 2:20:43.780
 Yeah.

2:20:43.780 --> 2:20:45.540
 I think it's, yeah, very doable.

2:20:45.540 --> 2:20:49.740
 I think actually, so I feel like Twitter

2:20:49.740 --> 2:20:52.060
 could be overthrown.

2:20:52.060 --> 2:20:54.300
 Yashua Bach talked about how like,

2:20:55.220 --> 2:20:58.020
 if you have like and retweet,

2:20:58.020 --> 2:21:02.120
 like that's only positive wiring, right?

2:21:02.120 --> 2:21:05.580
 The only way to do anything like negative there

2:21:05.580 --> 2:21:08.300
 is with a comment.

2:21:08.300 --> 2:21:12.420
 And that's like that asymmetry is what gives,

2:21:12.420 --> 2:21:15.620
 you know, Twitter its particular toxicness.

2:21:15.620 --> 2:21:18.060
 Whereas I find YouTube comments to be much better

2:21:18.060 --> 2:21:21.540
 because YouTube comments have an up and a down

2:21:21.540 --> 2:21:23.500
 and they don't show the downvotes.

2:21:23.500 --> 2:21:26.820
 Without getting into depth of this particular discussion,

2:21:26.820 --> 2:21:29.580
 the point is to explore possibilities

2:21:29.580 --> 2:21:30.860
 and get a lot of data on it.

2:21:30.860 --> 2:21:34.780
 Because I mean, I could disagree with what you just said.

2:21:34.780 --> 2:21:36.100
 The point is it's unclear.

2:21:36.100 --> 2:21:39.860
 It hasn't been explored in a really rich way.

2:21:39.860 --> 2:21:44.700
 Like these questions of how to create platforms

2:21:44.700 --> 2:21:46.100
 that encourage positivity.

2:21:47.100 --> 2:21:49.580
 Yeah, I think it's a technology problem.

2:21:49.580 --> 2:21:51.980
 And I think we'll look back at Twitter as it is now.

2:21:51.980 --> 2:21:53.900
 Maybe it'll happen within Twitter,

2:21:53.900 --> 2:21:56.460
 but most likely somebody overthrows them

2:21:56.460 --> 2:22:00.500
 is we'll look back at Twitter and say,

2:22:00.500 --> 2:22:03.220
 can't believe we put up with this level of toxicity.

2:22:03.220 --> 2:22:05.100
 You need a different business model too.

2:22:05.100 --> 2:22:07.700
 Any social network that fundamentally has advertising

2:22:07.700 --> 2:22:10.460
 as a business model, this was in The Social Dilemma,

2:22:10.460 --> 2:22:11.740
 which I didn't watch, but I liked it.

2:22:11.740 --> 2:22:12.940
 It's like, you know, there's always the, you know,

2:22:12.940 --> 2:22:15.380
 you're the product, you're not the,

2:22:15.380 --> 2:22:17.980
 but they had a nuanced take on it that I really liked.

2:22:17.980 --> 2:22:22.980
 And it said, the product being sold is influence over you.

2:22:24.660 --> 2:22:27.140
 The product being sold is literally your,

2:22:27.140 --> 2:22:28.740
 you know, influence on you.

2:22:29.820 --> 2:22:33.660
 Like that can't be, if that's your idea, okay.

2:22:33.660 --> 2:22:35.420
 Well, you know, guess what?

2:22:35.420 --> 2:22:37.060
 It can't not be toxic.

2:22:37.060 --> 2:22:39.420
 Yeah, maybe there's ways to spin it,

2:22:39.420 --> 2:22:42.540
 like with giving a lot more control to the user

2:22:42.540 --> 2:22:44.660
 and transparency to see what is happening to them

2:22:44.660 --> 2:22:47.300
 as opposed to in the shadows, it's possible,

2:22:47.300 --> 2:22:49.380
 but that can't be the primary source of.

2:22:49.380 --> 2:22:51.980
 But the users aren't, no one's gonna use that.

2:22:51.980 --> 2:22:54.020
 It depends, it depends, it depends.

2:22:54.020 --> 2:22:57.140
 I think that the, you're not going to,

2:22:57.140 --> 2:23:00.220
 you can't depend on self awareness of the users.

2:23:00.220 --> 2:23:04.460
 It's a longer discussion because you can't depend on it,

2:23:04.460 --> 2:23:09.460
 but you can reward self awareness.

2:23:09.740 --> 2:23:12.340
 Like if for the ones who are willing to put in the work

2:23:12.340 --> 2:23:16.060
 of self awareness, you can reward them and incentivize

2:23:16.060 --> 2:23:18.860
 and perhaps be pleasantly surprised how many people

2:23:20.180 --> 2:23:23.260
 are willing to be self aware on the internet.

2:23:23.260 --> 2:23:24.380
 Like we are in real life.

2:23:24.380 --> 2:23:26.940
 Like I'm putting in a lot of effort with you right now,

2:23:26.940 --> 2:23:30.300
 being self aware about if I say something stupid or mean,

2:23:30.300 --> 2:23:32.740
 I'll like look at your like body language.

2:23:32.740 --> 2:23:33.700
 Like I'm putting in that effort.

2:23:33.700 --> 2:23:36.260
 It's costly for an introvert, very costly.

2:23:36.260 --> 2:23:38.740
 But on the internet, fuck it.

2:23:39.620 --> 2:23:42.940
 Like most people are like, I don't care if this hurts

2:23:42.940 --> 2:23:46.220
 somebody, I don't care if this is not interesting

2:23:46.220 --> 2:23:48.660
 or if this is, yeah, it's a mean or whatever.

2:23:48.660 --> 2:23:50.980
 I think so much of the engagement today on the internet

2:23:50.980 --> 2:23:53.060
 is so disingenuine too.

2:23:53.060 --> 2:23:54.620
 You're not doing this out of a genuine,

2:23:54.620 --> 2:23:55.500
 this is what you think.

2:23:55.500 --> 2:23:57.660
 You're doing this just straight up to manipulate others.

2:23:57.660 --> 2:23:59.660
 Whether you're in, you just became an ad.

2:23:59.660 --> 2:24:02.780
 Yeah, okay, let's talk about a fun topic,

2:24:02.780 --> 2:24:04.420
 which is programming.

2:24:04.420 --> 2:24:05.780
 Here's another book idea for you.

2:24:05.780 --> 2:24:07.300
 Let me pitch.

2:24:07.300 --> 2:24:09.700
 What's your perfect programming setup?

2:24:09.700 --> 2:24:12.660
 So like this by George Hots.

2:24:12.660 --> 2:24:17.500
 So like what, listen, you're.

2:24:17.500 --> 2:24:20.340
 Give me a MacBook Air, sit me in a corner of a hotel room

2:24:20.340 --> 2:24:21.180
 and you know I'll still ask you.

2:24:21.180 --> 2:24:22.020
 So you really don't care.

2:24:22.020 --> 2:24:27.020
 You don't fetishize like multiple monitors, keyboard.

2:24:27.020 --> 2:24:30.260
 Those things are nice and I'm not gonna say no to them,

2:24:30.260 --> 2:24:33.300
 but did they automatically unlock tons of productivity?

2:24:33.300 --> 2:24:34.140
 No, not at all.

2:24:34.140 --> 2:24:36.300
 I have definitely been more productive on a MacBook Air

2:24:36.300 --> 2:24:38.300
 in a corner of a hotel room.

2:24:38.300 --> 2:24:41.620
 What about IDE?

2:24:41.620 --> 2:24:45.980
 So which operating system do you love?

2:24:45.980 --> 2:24:49.020
 What text editor do you use IDE?

2:24:49.020 --> 2:24:53.020
 What, is there something that is like the perfect,

2:24:53.020 --> 2:24:57.020
 if you could just say the perfect productivity setup

2:24:57.020 --> 2:24:57.860
 for George Hots.

2:24:57.860 --> 2:24:58.700
 It doesn't matter.

2:24:58.700 --> 2:25:00.420
 It literally doesn't matter.

2:25:00.420 --> 2:25:03.100
 You know, I guess I code most of the time in Vim.

2:25:03.100 --> 2:25:05.020
 Like literally I'm using an editor from the 70s.

2:25:05.020 --> 2:25:07.460
 You know, you didn't make anything better.

2:25:07.460 --> 2:25:09.060
 Okay, VS code is nice for reading code.

2:25:09.060 --> 2:25:10.820
 There's a few things that are nice about it.

2:25:10.820 --> 2:25:13.460
 I think that you can build much better tools.

2:25:13.460 --> 2:25:17.260
 How like IDA's xrefs work way better than VS codes, why?

2:25:18.660 --> 2:25:20.760
 Yeah, actually that's a good question, like why?

2:25:20.760 --> 2:25:24.360
 I still use, sorry, Emacs for most.

2:25:25.320 --> 2:25:28.860
 I've actually never, I have to confess something dark.

2:25:28.860 --> 2:25:31.100
 So I've never used Vim.

2:25:32.620 --> 2:25:34.820
 I think maybe I'm just afraid

2:25:36.460 --> 2:25:39.340
 that my life has been like a waste.

2:25:39.340 --> 2:25:43.500
 I'm so, I'm not evangelical about Emacs.

2:25:43.500 --> 2:25:44.620
 I think this.

2:25:44.620 --> 2:25:47.820
 This is how I feel about TensorFlow versus PyTorch.

2:25:47.820 --> 2:25:50.300
 Having just like, we've switched everything to PyTorch now.

2:25:50.300 --> 2:25:51.900
 Put months into the switch.

2:25:51.900 --> 2:25:54.580
 I have felt like I've wasted years on TensorFlow.

2:25:54.580 --> 2:25:56.300
 I can't believe it.

2:25:56.300 --> 2:25:58.380
 I can't believe how much better PyTorch is.

2:25:58.380 --> 2:25:59.620
 Yeah.

2:25:59.620 --> 2:26:01.500
 I've used Emacs and Vim, doesn't matter.

2:26:01.500 --> 2:26:03.060
 Yeah, it's still just my heart.

2:26:03.060 --> 2:26:04.700
 Somehow I fell in love with Lisp.

2:26:04.700 --> 2:26:05.540
 I don't know why.

2:26:05.540 --> 2:26:08.140
 You can't, the heart wants what the heart wants.

2:26:08.140 --> 2:26:10.480
 I don't understand it, but it just connected with me.

2:26:10.480 --> 2:26:11.700
 Maybe it's the functional language

2:26:11.700 --> 2:26:13.260
 that first I connected with.

2:26:13.260 --> 2:26:15.860
 Maybe it's because so many of the AI courses

2:26:15.860 --> 2:26:17.300
 before the deep learning revolution

2:26:17.300 --> 2:26:19.500
 were taught with Lisp in mind.

2:26:19.500 --> 2:26:20.340
 I don't know.

2:26:20.340 --> 2:26:22.340
 I don't know what it is, but I'm stuck with it.

2:26:22.340 --> 2:26:23.540
 But at the same time, like,

2:26:23.540 --> 2:26:25.100
 why am I not using a modern ID

2:26:25.100 --> 2:26:26.300
 for some of these programming?

2:26:26.300 --> 2:26:27.260
 I don't know.

2:26:27.260 --> 2:26:28.500
 They're not that much better.

2:26:28.500 --> 2:26:30.180
 I've used modern IDs too.

2:26:30.180 --> 2:26:32.040
 But at the same time, so to just,

2:26:32.040 --> 2:26:33.060
 well, not to disagree with you,

2:26:33.060 --> 2:26:35.880
 but like, I like multiple monitors.

2:26:35.880 --> 2:26:38.380
 Like I have to do work on a laptop

2:26:38.380 --> 2:26:41.340
 and it's a pain in the ass.

2:26:41.340 --> 2:26:45.420
 And also I'm addicted to the Kinesis weird keyboard.

2:26:45.420 --> 2:26:46.740
 You could see there.

2:26:46.740 --> 2:26:48.540
 Yeah, yeah, yeah.

2:26:48.540 --> 2:26:50.100
 Yeah, so you don't have any of that.

2:26:50.100 --> 2:26:51.820
 You can just be on a MacBook.

2:26:51.820 --> 2:26:53.020
 I mean, look at work.

2:26:53.020 --> 2:26:55.220
 I have three 24 inch monitors.

2:26:55.220 --> 2:26:56.580
 I have a happy hacking keyboard.

2:26:56.580 --> 2:26:59.820
 I have a Razer Death Hatter mouse, like.

2:26:59.820 --> 2:27:01.300
 But it's not essential for you.

2:27:01.300 --> 2:27:02.120
 No.

2:27:02.120 --> 2:27:04.680
 Let's go to a day in the life of George Hots.

2:27:04.680 --> 2:27:08.780
 What is the perfect day productivity wise?

2:27:08.780 --> 2:27:12.380
 So we're not talking about like Hunter S. Thompson drugs.

2:27:12.380 --> 2:27:13.220
 Yeah, yeah, yeah.

2:27:13.220 --> 2:27:16.420
 And let's look at productivity.

2:27:16.420 --> 2:27:19.820
 Like what's the day look like, like hour by hour?

2:27:19.820 --> 2:27:23.260
 Is there any regularities that create

2:27:23.260 --> 2:27:25.940
 a magical George Hots experience?

2:27:25.940 --> 2:27:28.300
 I can remember three days in my life.

2:27:28.300 --> 2:27:30.300
 And I remember these days vividly

2:27:30.300 --> 2:27:35.300
 when I've gone through kind of radical transformations

2:27:36.620 --> 2:27:37.900
 to the way I think.

2:27:37.900 --> 2:27:40.140
 And what I would give, I would pay $100,000

2:27:40.140 --> 2:27:42.980
 if I could have one of these days tomorrow.

2:27:42.980 --> 2:27:44.900
 The days have been so impactful.

2:27:44.900 --> 2:27:47.780
 And one was first discovering Eliezer Yudkowsky

2:27:47.780 --> 2:27:50.740
 on the singularity and reading that stuff.

2:27:50.740 --> 2:27:52.780
 And like, you know, my mind was blown.

2:27:54.260 --> 2:27:57.820
 The next was discovering the Hutter Prize

2:27:57.820 --> 2:27:59.900
 and that AI is just compression.

2:27:59.900 --> 2:28:03.820
 Like finally understanding AIXI and what all of that was.

2:28:03.820 --> 2:28:05.300
 You know, I like read about it when I was 18, 19,

2:28:05.300 --> 2:28:06.300
 I didn't understand it.

2:28:06.300 --> 2:28:08.380
 And then the fact that like lossless compression

2:28:08.380 --> 2:28:12.240
 implies intelligence, the day that I was shown that.

2:28:12.240 --> 2:28:14.140
 And then the third one is controversial.

2:28:14.140 --> 2:28:17.340
 The day I found a blog called Unqualified Reservations.

2:28:17.340 --> 2:28:20.620
 And read that and I was like.

2:28:20.620 --> 2:28:21.500
 Wait, which one is that?

2:28:21.500 --> 2:28:22.980
 That's, what's the guy's name?

2:28:22.980 --> 2:28:24.020
 Curtis Yarvin.

2:28:24.020 --> 2:28:25.020
 Yeah.

2:28:25.020 --> 2:28:27.640
 So many people tell me I'm supposed to talk to him.

2:28:27.640 --> 2:28:28.480
 Yeah, the day.

2:28:28.480 --> 2:28:30.140
 He looks, he sounds insane.

2:28:30.140 --> 2:28:31.300
 Definitely. Or brilliant,

2:28:31.300 --> 2:28:33.100
 but insane or both, I don't know.

2:28:33.100 --> 2:28:35.140
 The day I found that blog was another like,

2:28:35.140 --> 2:28:37.180
 this was during like Gamergate

2:28:37.180 --> 2:28:39.020
 and kind of the run up to the 2016 election.

2:28:39.020 --> 2:28:42.860
 And I'm like, wow, okay, the world makes sense now.

2:28:42.860 --> 2:28:45.620
 This is like, I had a framework now to interpret this.

2:28:45.620 --> 2:28:47.200
 Just like I got the framework for AI

2:28:47.200 --> 2:28:49.420
 and a framework to interpret technological progress.

2:28:49.420 --> 2:28:52.820
 Like those days when I discovered these new frameworks were.

2:28:52.820 --> 2:28:53.660
 Oh, interesting.

2:28:53.660 --> 2:28:57.160
 So it's not about, but what was special about those days?

2:28:57.160 --> 2:28:58.780
 How did those days come to be?

2:28:58.780 --> 2:28:59.980
 Is it just, you got lucky?

2:28:59.980 --> 2:29:04.900
 Like, you just encountered a hotter prize

2:29:04.900 --> 2:29:07.220
 on Hacker News or something like that?

2:29:09.740 --> 2:29:11.840
 But you see, I don't think it's just,

2:29:11.840 --> 2:29:13.260
 see, I don't think it's just that like,

2:29:13.260 --> 2:29:14.820
 I could have gotten lucky at any point.

2:29:14.820 --> 2:29:16.260
 I think that in a way.

2:29:16.260 --> 2:29:17.820
 You were ready at that moment.

2:29:17.820 --> 2:29:18.660
 Yeah, exactly.

2:29:18.660 --> 2:29:19.960
 To receive the information.

2:29:21.460 --> 2:29:24.300
 But is there some magic to the day today

2:29:24.300 --> 2:29:27.220
 of like eating breakfast?

2:29:27.220 --> 2:29:29.060
 And it's the mundane things.

2:29:29.060 --> 2:29:29.900
 Nah.

2:29:29.900 --> 2:29:30.720
 Nothing.

2:29:30.720 --> 2:29:32.900
 Nah, I drift through life.

2:29:32.900 --> 2:29:34.180
 Without structure.

2:29:34.180 --> 2:29:36.220
 I drift through life hoping and praying

2:29:36.220 --> 2:29:38.540
 that I will get another day like those days.

2:29:38.540 --> 2:29:40.320
 And there's nothing in particular you do

2:29:40.320 --> 2:29:44.740
 to be a receptacle for another, for day number four.

2:29:46.060 --> 2:29:48.060
 No, I didn't do anything to get the other ones.

2:29:48.060 --> 2:29:51.100
 So I don't think I have to really do anything now.

2:29:51.100 --> 2:29:53.420
 I took a month long trip to New York

2:29:53.420 --> 2:29:56.260
 and the Ethereum thing was the highlight of it,

2:29:56.260 --> 2:29:57.980
 but the rest of it was pretty terrible.

2:29:57.980 --> 2:29:59.860
 I did a two week road trip

2:29:59.860 --> 2:30:01.740
 and I got, I had to turn around.

2:30:01.740 --> 2:30:06.620
 I had to turn around driving in Gunnison, Colorado.

2:30:06.620 --> 2:30:08.060
 I passed through Gunnison

2:30:08.060 --> 2:30:10.660
 and the snow starts coming down.

2:30:10.660 --> 2:30:12.160
 There's a pass up there called Monarch Pass

2:30:12.160 --> 2:30:13.000
 in order to get through to Denver,

2:30:13.000 --> 2:30:14.700
 you gotta get over the Rockies.

2:30:14.700 --> 2:30:16.800
 And I had to turn my car around.

2:30:16.800 --> 2:30:20.260
 I couldn't, I watched a F150 go off the road.

2:30:20.260 --> 2:30:21.620
 I'm like, I gotta go back.

2:30:21.620 --> 2:30:24.620
 And like that day was meaningful.

2:30:24.620 --> 2:30:26.040
 Cause like, it was real.

2:30:26.040 --> 2:30:28.900
 Like I actually had to turn my car around.

2:30:28.900 --> 2:30:31.240
 It's rare that anything even real happens in my life.

2:30:31.240 --> 2:30:34.300
 Even as, you know, mundane as the fact that,

2:30:34.300 --> 2:30:36.100
 yeah, there was snow, I had to turn around,

2:30:36.100 --> 2:30:37.700
 stay in Gunnison and leave the next day.

2:30:37.700 --> 2:30:40.260
 Something about that moment felt real.

2:30:40.260 --> 2:30:43.180
 Okay, so actually it's interesting to break apart

2:30:43.180 --> 2:30:45.380
 the three moments you mentioned, if it's okay.

2:30:45.380 --> 2:30:48.540
 So I always have trouble pronouncing his name,

2:30:48.540 --> 2:30:50.620
 but Alousa Yurkowski.

2:30:53.300 --> 2:30:57.820
 So what, how did your worldview change

2:30:57.820 --> 2:31:02.820
 in starting to consider the exponential growth of AI

2:31:02.900 --> 2:31:05.020
 and AGI that he thinks about

2:31:05.020 --> 2:31:07.620
 and the threats of artificial intelligence

2:31:07.620 --> 2:31:09.340
 and all that kind of ideas?

2:31:09.340 --> 2:31:12.460
 Can you, is it just like, can you maybe break apart

2:31:12.460 --> 2:31:15.660
 like what exactly was so magical to you?

2:31:15.660 --> 2:31:17.380
 Is it transformational experience?

2:31:17.380 --> 2:31:20.340
 Today, everyone knows him for threats and AI safety.

2:31:20.340 --> 2:31:22.260
 This was pre that stuff.

2:31:22.260 --> 2:31:25.980
 There was, I don't think a mention of AI safety on the page.

2:31:25.980 --> 2:31:27.860
 This is, this is old Yurkowski stuff.

2:31:27.860 --> 2:31:29.060
 He'd probably denounce it all now.

2:31:29.060 --> 2:31:29.900
 He'd probably be like,

2:31:29.900 --> 2:31:32.220
 that's exactly what I didn't want to happen.

2:31:32.220 --> 2:31:33.060
 Sorry, man.

2:31:33.060 --> 2:31:37.700
 Is there something specific you can take from his work

2:31:37.700 --> 2:31:38.700
 that you can remember?

2:31:38.700 --> 2:31:40.780
 Yeah, it was this realization

2:31:40.780 --> 2:31:45.780
 that computers double in power every 18 months

2:31:45.980 --> 2:31:50.140
 and humans do not, and they haven't crossed yet.

2:31:50.140 --> 2:31:52.700
 But if you have one thing that's doubling every 18 months

2:31:52.700 --> 2:31:55.100
 and one thing that's staying like this, you know,

2:31:55.100 --> 2:31:58.940
 here's your log graph, here's your line, you know,

2:31:58.940 --> 2:31:59.860
 calculate that.

2:31:59.860 --> 2:32:03.540
 And then the data opened the door

2:32:03.540 --> 2:32:06.420
 to the exponential thinking, like thinking that like,

2:32:06.420 --> 2:32:07.740
 you know what, with technology,

2:32:07.740 --> 2:32:11.580
 we can actually transform the world.

2:32:11.580 --> 2:32:13.740
 It opened the door to human obsolescence.

2:32:13.740 --> 2:32:16.980
 It opened the door to realize that in my lifetime,

2:32:16.980 --> 2:32:19.740
 humans are going to be replaced.

2:32:20.620 --> 2:32:23.740
 And then the matching idea to that of artificial intelligence

2:32:23.740 --> 2:32:27.940
 with the Hutter prize, you know, I'm torn.

2:32:27.940 --> 2:32:30.220
 I go back and forth on what I think about it.

2:32:30.220 --> 2:32:31.380
 Yeah.

2:32:31.380 --> 2:32:36.180
 But the basic thesis is it's a nice compelling notion

2:32:36.180 --> 2:32:38.340
 that we can reduce the task of creating

2:32:38.340 --> 2:32:41.380
 an intelligent system, a generally intelligent system

2:32:41.380 --> 2:32:42.940
 into the task of compression.

2:32:43.940 --> 2:32:46.300
 So you can think of all of intelligence in the universe,

2:32:46.300 --> 2:32:48.500
 in fact, as a kind of compression.

2:32:50.220 --> 2:32:52.420
 Do you find that, was that just at the time

2:32:52.420 --> 2:32:53.900
 you found that as a compelling idea

2:32:53.900 --> 2:32:56.980
 or do you still find that a compelling idea?

2:32:56.980 --> 2:32:59.180
 I still find that a compelling idea.

2:32:59.180 --> 2:33:02.180
 I think that it's not that useful day to day,

2:33:02.180 --> 2:33:06.260
 but actually one of maybe my quests before that

2:33:06.260 --> 2:33:09.340
 was a search for the definition of the word intelligence.

2:33:09.340 --> 2:33:10.860
 And I never had one.

2:33:10.860 --> 2:33:14.700
 And I definitely have a definition of the word compression.

2:33:14.700 --> 2:33:18.420
 It's a very simple, straightforward one.

2:33:18.420 --> 2:33:19.740
 And you know what compression is,

2:33:19.740 --> 2:33:21.100
 you know what lossless, it's lossless compression,

2:33:21.100 --> 2:33:22.940
 not lossy, lossless compression.

2:33:22.940 --> 2:33:25.620
 And that that is equivalent to intelligence,

2:33:25.620 --> 2:33:27.540
 which I believe, I'm not sure how useful

2:33:27.540 --> 2:33:28.980
 that definition is day to day,

2:33:28.980 --> 2:33:32.340
 but like I now have a framework to understand what it is.

2:33:32.340 --> 2:33:36.300
 And he just 10X the prize for that competition

2:33:36.300 --> 2:33:37.700
 like recently a few months ago.

2:33:37.700 --> 2:33:39.580
 You ever thought of taking a crack at that?

2:33:39.580 --> 2:33:41.100
 Oh, I did.

2:33:41.100 --> 2:33:41.940
 Oh, I did.

2:33:41.940 --> 2:33:44.660
 I spent the next, after I found the prize,

2:33:44.660 --> 2:33:47.820
 I spent the next six months of my life trying it.

2:33:47.820 --> 2:33:51.300
 And well, that's when I started learning everything about AI.

2:33:51.300 --> 2:33:53.340
 And then I worked at Vicarious for a bit

2:33:53.340 --> 2:33:55.340
 and then I read all the deep learning stuff.

2:33:55.340 --> 2:33:58.460
 And I'm like, okay, now I like I'm caught up to modern AI.

2:33:58.460 --> 2:33:59.300
 Wow.

2:33:59.300 --> 2:34:01.380
 And I had a really good framework to put it all in

2:34:01.380 --> 2:34:04.420
 from the compression stuff, right?

2:34:04.420 --> 2:34:07.340
 Like some of the first deep learning models I played with

2:34:07.340 --> 2:34:12.020
 were GPT basically, but before transformers,

2:34:12.020 --> 2:34:17.020
 before it was still RNNs to do character prediction.

2:34:17.900 --> 2:34:19.940
 But by the way, on the compression side,

2:34:19.940 --> 2:34:22.940
 I mean, especially with neural networks,

2:34:22.940 --> 2:34:25.500
 what do you make of the lossless requirement

2:34:25.500 --> 2:34:26.540
 with the Hutter prize?

2:34:26.540 --> 2:34:31.460
 So, you know, human intelligence and neural networks

2:34:31.460 --> 2:34:33.300
 can probably compress stuff pretty well,

2:34:33.300 --> 2:34:35.220
 but it would be lossy.

2:34:35.220 --> 2:34:36.660
 It's imperfect.

2:34:36.660 --> 2:34:37.860
 You can turn a lossy compression

2:34:37.860 --> 2:34:39.620
 to a lossless compressor pretty easily

2:34:39.620 --> 2:34:41.060
 using an arithmetic encoder, right?

2:34:41.060 --> 2:34:42.540
 You can take an arithmetic encoder

2:34:42.540 --> 2:34:45.980
 and you can just encode the noise with maximum efficiency.

2:34:45.980 --> 2:34:46.820
 Right?

2:34:46.820 --> 2:34:48.900
 So even if you can't predict exactly

2:34:48.900 --> 2:34:50.460
 what the next character is,

2:34:50.460 --> 2:34:52.500
 the better a probability distribution,

2:34:52.500 --> 2:34:54.260
 you can put over the next character.

2:34:54.260 --> 2:34:57.340
 You can then use an arithmetic encoder to, right?

2:34:57.340 --> 2:34:59.300
 You don't have to know whether it's an E or an I,

2:34:59.300 --> 2:35:01.060
 you just have to put good probabilities on them

2:35:01.060 --> 2:35:03.020
 and then, you know, code those.

2:35:03.020 --> 2:35:06.460
 And if you have, it's a bits of entropy thing, right?

2:35:06.460 --> 2:35:07.740
 So let me, on that topic,

2:35:07.740 --> 2:35:10.260
 it'd be interesting as a little side tour.

2:35:10.260 --> 2:35:13.580
 What are your thoughts in this year about GPT3

2:35:13.580 --> 2:35:16.140
 and these language models and these transformers?

2:35:16.140 --> 2:35:20.660
 Is there something interesting to you as an AI researcher,

2:35:20.660 --> 2:35:22.540
 or is there something interesting to you

2:35:22.540 --> 2:35:24.740
 as an autonomous vehicle developer?

2:35:24.740 --> 2:35:27.460
 Nah, I think it's overhyped.

2:35:27.460 --> 2:35:29.020
 I mean, it's not, like, it's cool.

2:35:29.020 --> 2:35:30.820
 It's cool for what it is, but no,

2:35:30.820 --> 2:35:33.500
 we're not just gonna be able to scale up to GPT12

2:35:33.500 --> 2:35:35.340
 and get general purpose intelligence.

2:35:35.340 --> 2:35:38.700
 Like, your loss function is literally just,

2:35:38.700 --> 2:35:41.460
 you know, cross entropy loss on the character, right?

2:35:41.460 --> 2:35:44.860
 Like, that's not the loss function of general intelligence.

2:35:44.860 --> 2:35:45.940
 Is that obvious to you?

2:35:45.940 --> 2:35:47.380
 Yes.

2:35:47.380 --> 2:35:51.460
 Can you imagine that, like,

2:35:51.460 --> 2:35:53.300
 to play devil's advocate on yourself,

2:35:53.300 --> 2:35:55.860
 is it possible that you can,

2:35:55.860 --> 2:35:58.580
 the GPT12 will achieve general intelligence

2:35:58.580 --> 2:36:01.980
 with something as dumb as this kind of loss function?

2:36:01.980 --> 2:36:05.060
 I guess it depends what you mean by general intelligence.

2:36:05.060 --> 2:36:07.860
 So there's another problem with the GPTs,

2:36:07.860 --> 2:36:11.060
 and that's that they don't have a,

2:36:11.060 --> 2:36:13.020
 they don't have longterm memory.

2:36:13.020 --> 2:36:13.860
 Right.

2:36:13.860 --> 2:36:18.580
 So, like, just GPT12,

2:36:18.580 --> 2:36:22.460
 a scaled up version of GPT2 or GPT3,

2:36:22.460 --> 2:36:26.060
 I find it hard to believe.

2:36:26.060 --> 2:36:28.780
 Well, you can scale it in,

2:36:28.780 --> 2:36:32.060
 so it's a hard coded length,

2:36:32.060 --> 2:36:34.300
 but you can make it wider and wider and wider.

2:36:34.300 --> 2:36:36.380
 Yeah.

2:36:36.380 --> 2:36:40.820
 You're gonna get cool things from those systems,

2:36:40.820 --> 2:36:44.820
 but I don't think you're ever gonna get something

2:36:44.820 --> 2:36:47.580
 that can, like, you know, build me a rocket ship.

2:36:47.580 --> 2:36:49.420
 What about solved driving?

2:36:49.420 --> 2:36:53.140
 So, you know, you can use Transformer with video,

2:36:53.140 --> 2:36:54.740
 for example.

2:36:54.740 --> 2:36:57.220
 You think, is there something in there?

2:36:57.220 --> 2:37:01.260
 No, because, I mean, look, we use a GRU.

2:37:01.260 --> 2:37:02.100
 We use a GRU.

2:37:02.100 --> 2:37:04.140
 We could change that GRU out to a Transformer.

2:37:05.900 --> 2:37:09.500
 I think driving is much more Markovian than language.

2:37:09.500 --> 2:37:11.460
 So, Markovian, you mean, like, the memory,

2:37:11.460 --> 2:37:13.660
 which aspect of Markovian?

2:37:13.660 --> 2:37:16.340
 I mean that, like, most of the information

2:37:16.340 --> 2:37:19.540
 in the state at T minus one is also in state T.

2:37:19.540 --> 2:37:20.380
 I see, yeah.

2:37:20.380 --> 2:37:22.820
 Right, and it kind of, like, drops off nicely like this,

2:37:22.820 --> 2:37:23.900
 whereas sometime with language,

2:37:23.900 --> 2:37:25.900
 you have to refer back to the third paragraph

2:37:25.900 --> 2:37:27.260
 on the second page.

2:37:27.260 --> 2:37:28.140
 I feel like.

2:37:28.140 --> 2:37:30.060
 There's not many, like, you can say, like,

2:37:30.060 --> 2:37:32.420
 speed limit signs, but there's really not many things

2:37:32.420 --> 2:37:33.860
 in autonomous driving that look like that.

2:37:33.860 --> 2:37:37.140
 But if you look at, to play devil's advocate,

2:37:37.140 --> 2:37:39.740
 is the risk estimation thing that you've talked about

2:37:39.740 --> 2:37:41.180
 is kind of interesting.

2:37:41.180 --> 2:37:45.700
 Is, it feels like there might be some longer term

2:37:45.700 --> 2:37:49.020
 aggregation of context necessary to be able to figure out,

2:37:49.020 --> 2:37:51.900
 like, the context.

2:37:51.900 --> 2:37:55.820
 Yeah, I'm not even sure I'm believing my devil's advocate.

2:37:55.820 --> 2:37:58.260
 We have a nice, like, vision model,

2:37:58.260 --> 2:38:00.180
 which outputs, like, a one or two,

2:38:00.180 --> 2:38:02.100
 four dimensional perception space.

2:38:03.300 --> 2:38:04.740
 Can I try Transformers on it?

2:38:04.740 --> 2:38:06.580
 Sure, I probably will.

2:38:06.580 --> 2:38:08.260
 At some point, we'll try Transformers,

2:38:08.260 --> 2:38:09.100
 and then we'll just see.

2:38:09.100 --> 2:38:09.940
 Do they do better?

2:38:09.940 --> 2:38:10.780
 Sure, I'm.

2:38:10.780 --> 2:38:12.180
 But it might not be a game changer, you're saying?

2:38:12.180 --> 2:38:13.020
 No, well, I'm not.

2:38:13.020 --> 2:38:15.300
 Like, might Transformers work better than GRUs

2:38:15.300 --> 2:38:16.140
 for autonomous driving?

2:38:16.140 --> 2:38:16.980
 Sure.

2:38:16.980 --> 2:38:17.820
 Might we switch?

2:38:17.820 --> 2:38:18.660
 Sure.

2:38:18.660 --> 2:38:19.500
 Is this some radical change?

2:38:19.500 --> 2:38:20.340
 No.

2:38:20.340 --> 2:38:21.540
 Okay, we use a slightly different,

2:38:21.540 --> 2:38:23.020
 you know, we switch from RNNs to GRUs.

2:38:23.020 --> 2:38:24.980
 Like, okay, maybe it's GRUs to Transformers,

2:38:24.980 --> 2:38:26.540
 but no, it's not.

2:38:26.540 --> 2:38:27.900
 Yeah.

2:38:27.900 --> 2:38:30.500
 Well, on the topic of general intelligence,

2:38:30.500 --> 2:38:32.140
 I don't know how much I've talked to you about it.

2:38:32.140 --> 2:38:36.540
 Like, what, do you think we'll actually build

2:38:36.540 --> 2:38:38.060
 an AGI?

2:38:38.060 --> 2:38:40.740
 Like, if you look at Ray Kurzweil with Singularity,

2:38:40.740 --> 2:38:43.380
 do you have like an intuition about,

2:38:43.380 --> 2:38:45.420
 you're kind of saying driving is easy.

2:38:45.420 --> 2:38:46.260
 Yeah.

2:38:46.260 --> 2:38:51.260
 And I tend to personally believe that solving driving

2:38:52.500 --> 2:38:56.580
 will have really deep, important impacts

2:38:56.580 --> 2:38:59.300
 on our ability to solve general intelligence.

2:38:59.300 --> 2:39:03.380
 Like, I think driving doesn't require general intelligence,

2:39:03.380 --> 2:39:05.220
 but I think they're going to be neighbors

2:39:05.220 --> 2:39:08.380
 in a way that it's like deeply tied.

2:39:08.380 --> 2:39:11.540
 Cause it's so, like driving is so deeply connected

2:39:11.540 --> 2:39:15.020
 to the human experience that I think solving one

2:39:15.020 --> 2:39:17.420
 will help solve the other.

2:39:17.420 --> 2:39:20.980
 But, so I don't see, I don't see driving as like easy

2:39:20.980 --> 2:39:23.540
 and almost like separate than general intelligence,

2:39:23.540 --> 2:39:26.700
 but like, what's your vision of a future with a Singularity?

2:39:26.700 --> 2:39:28.180
 Do you see there'll be a single moment,

2:39:28.180 --> 2:39:30.620
 like a Singularity where it'll be a phase shift?

2:39:30.620 --> 2:39:32.380
 Are we in the Singularity now?

2:39:32.380 --> 2:39:34.540
 Like what, do you have crazy ideas about the future

2:39:34.540 --> 2:39:35.740
 in terms of AGI?

2:39:35.740 --> 2:39:38.020
 We're definitely in the Singularity now.

2:39:38.020 --> 2:39:38.860
 We are?

2:39:38.860 --> 2:39:40.180
 Of course, of course.

2:39:40.180 --> 2:39:41.500
 Look at the bandwidth between people.

2:39:41.500 --> 2:39:43.900
 The bandwidth between people goes up, right?

2:39:44.820 --> 2:39:47.300
 The Singularity is just, you know, when the bandwidth, but.

2:39:47.300 --> 2:39:48.620
 What do you mean by the bandwidth of people?

2:39:48.620 --> 2:39:51.340
 Communications, tools, the whole world is networked.

2:39:51.340 --> 2:39:52.260
 The whole world is networked

2:39:52.260 --> 2:39:54.740
 and we raise the speed of that network, right?

2:39:54.740 --> 2:39:57.380
 Oh, so you think the communication of information

2:39:57.380 --> 2:40:00.980
 in a distributed way is an empowering thing

2:40:00.980 --> 2:40:02.300
 for collective intelligence?

2:40:02.300 --> 2:40:03.660
 Oh, I didn't say it's necessarily a good thing,

2:40:03.660 --> 2:40:04.540
 but I think that's like,

2:40:04.540 --> 2:40:06.500
 when I think of the definition of the Singularity,

2:40:06.500 --> 2:40:08.180
 yeah, it seems kind of right.

2:40:08.180 --> 2:40:12.060
 I see, like it's a change in the world

2:40:12.060 --> 2:40:14.940
 beyond which like the world be transformed

2:40:14.940 --> 2:40:16.780
 in ways that we can't possibly imagine.

2:40:16.780 --> 2:40:18.380
 No, I mean, I think we're in the Singularity now

2:40:18.380 --> 2:40:19.700
 in the sense that there's like, you know,

2:40:19.700 --> 2:40:22.340
 one world and a monoculture and it's also linked.

2:40:22.340 --> 2:40:24.800
 Yeah, I mean, I kind of share the intuition

2:40:24.800 --> 2:40:27.700
 that the Singularity will originate

2:40:27.700 --> 2:40:31.220
 from the collective intelligence of us ands

2:40:31.220 --> 2:40:35.340
 versus the like some single system AGI type thing.

2:40:35.340 --> 2:40:37.020
 Oh, I totally agree with that.

2:40:37.020 --> 2:40:40.460
 Yeah, I don't really believe in like a hard take off AGI

2:40:40.460 --> 2:40:41.300
 kind of thing.

2:40:45.140 --> 2:40:49.500
 Yeah, I don't even think AI is all that different in kind

2:40:49.500 --> 2:40:52.060
 from what we've already been building.

2:40:52.060 --> 2:40:53.380
 With respect to driving,

2:40:53.380 --> 2:40:56.340
 I think driving is a subset of general intelligence

2:40:56.340 --> 2:40:58.060
 and I think it's a pretty complete subset.

2:40:58.060 --> 2:41:00.300
 I think the tools we develop at Kama

2:41:00.300 --> 2:41:02.300
 will also be extremely helpful

2:41:02.300 --> 2:41:04.020
 to solving general intelligence

2:41:04.020 --> 2:41:06.620
 and that's I think the real reason why I'm doing it.

2:41:06.620 --> 2:41:08.420
 I don't care about self driving cars.

2:41:08.420 --> 2:41:10.920
 It's a cool problem to beat people at.

2:41:10.920 --> 2:41:14.540
 But yeah, I mean, yeah, you're kind of, you're of two minds.

2:41:14.540 --> 2:41:16.340
 So one, you do have to have a mission

2:41:16.340 --> 2:41:19.020
 and you wanna focus and make sure you get there.

2:41:19.020 --> 2:41:22.280
 You can't forget that but at the same time,

2:41:22.280 --> 2:41:26.060
 there is a thread that's much bigger

2:41:26.060 --> 2:41:28.460
 than that connects the entirety of your effort.

2:41:28.460 --> 2:41:31.180
 That's much bigger than just driving.

2:41:31.180 --> 2:41:33.380
 With AI and with general intelligence,

2:41:33.380 --> 2:41:35.220
 it is so easy to delude yourself

2:41:35.220 --> 2:41:37.300
 into thinking you've figured something out when you haven't.

2:41:37.300 --> 2:41:39.820
 If we build a level five self driving car,

2:41:39.820 --> 2:41:42.660
 we have indisputably built something.

2:41:42.660 --> 2:41:43.500
 Yeah.

2:41:43.500 --> 2:41:44.800
 Is it general intelligence?

2:41:44.800 --> 2:41:45.940
 I'm not gonna debate that.

2:41:45.940 --> 2:41:47.520
 I will say we've built something

2:41:47.520 --> 2:41:49.740
 that provides huge financial value.

2:41:49.740 --> 2:41:50.580
 Yeah, beautifully put.

2:41:50.580 --> 2:41:51.940
 That's the engineering credo.

2:41:51.940 --> 2:41:53.660
 Like just build the thing.

2:41:53.660 --> 2:41:57.120
 It's like, that's why I'm with Elon

2:41:57.120 --> 2:41:58.780
 on go to Mars.

2:41:58.780 --> 2:41:59.740
 Yeah, that's a great one.

2:41:59.740 --> 2:42:03.700
 You can argue like who the hell cares about going to Mars.

2:42:03.700 --> 2:42:07.220
 But the reality is set that as a mission, get it done.

2:42:07.220 --> 2:42:08.060
 Yeah.

2:42:08.060 --> 2:42:09.540
 And then you're going to crack some problem

2:42:09.540 --> 2:42:11.580
 that you've never even expected

2:42:11.580 --> 2:42:13.900
 in the process of doing that, yeah.

2:42:13.900 --> 2:42:16.220
 Yeah, I mean, no, I think if I had a choice

2:42:16.220 --> 2:42:17.460
 between humanity going to Mars

2:42:17.460 --> 2:42:18.500
 and solving self driving cars,

2:42:18.500 --> 2:42:21.900
 I think going to Mars is better, but I don't know.

2:42:21.900 --> 2:42:23.580
 I'm more suited for self driving cars.

2:42:23.580 --> 2:42:24.420
 I'm an information guy.

2:42:24.420 --> 2:42:26.540
 I'm not a modernist, I'm a postmodernist.

2:42:26.540 --> 2:42:29.620
 Postmodernist, all right, beautifully put.

2:42:29.620 --> 2:42:32.220
 Let me drag you back to programming for a sec.

2:42:32.220 --> 2:42:35.140
 What three, maybe three to five programming languages

2:42:35.140 --> 2:42:36.580
 should people learn, do you think?

2:42:36.580 --> 2:42:38.220
 Like if you look at yourself,

2:42:38.220 --> 2:42:41.860
 what did you get the most out of from learning?

2:42:42.700 --> 2:42:45.980
 Well, so everybody should learn C and assembly.

2:42:45.980 --> 2:42:47.380
 We'll start with those two, right?

2:42:47.380 --> 2:42:48.220
 Assembly?

2:42:48.220 --> 2:42:49.940
 Yeah, if you can't code an assembly,

2:42:49.940 --> 2:42:51.720
 you don't know what the computer's doing.

2:42:51.720 --> 2:42:53.340
 You don't understand like,

2:42:53.340 --> 2:42:54.820
 you don't have to be great in assembly,

2:42:54.820 --> 2:42:56.580
 but you have to code in it.

2:42:56.580 --> 2:42:58.940
 And then like, you have to appreciate assembly

2:42:58.940 --> 2:43:02.020
 in order to appreciate all the great things C gets you.

2:43:02.020 --> 2:43:03.380
 And then you have to code in C

2:43:03.380 --> 2:43:06.340
 in order to appreciate all the great things Python gets you.

2:43:06.340 --> 2:43:07.860
 So I'll just say assembly C and Python,

2:43:07.860 --> 2:43:09.740
 we'll start with those three.

2:43:09.740 --> 2:43:14.700
 The memory allocation of C and the fact that,

2:43:14.700 --> 2:43:16.340
 so assembly gives you a sense

2:43:16.340 --> 2:43:18.460
 of just how many levels of abstraction

2:43:18.460 --> 2:43:20.660
 you get to work on in modern day programming.

2:43:20.660 --> 2:43:22.900
 Yeah, yeah, yeah, yeah, graph coloring for assignment,

2:43:22.900 --> 2:43:24.740
 register assignment and compilers.

2:43:24.740 --> 2:43:25.940
 Like, you know, you gotta do,

2:43:25.940 --> 2:43:26.780
 you know, the compiler,

2:43:26.780 --> 2:43:28.340
 the computer only has a certain number of registers,

2:43:28.340 --> 2:43:31.140
 yet you can have all the variables you want in a C function.

2:43:31.140 --> 2:43:34.460
 So you get to start to build intuition about compilation,

2:43:34.460 --> 2:43:35.960
 like what a compiler gets you.

2:43:37.380 --> 2:43:38.500
 What else?

2:43:38.500 --> 2:43:41.220
 Well, then there's kind of a,

2:43:41.220 --> 2:43:44.220
 so those are all very imperative programming languages.

2:43:45.800 --> 2:43:47.700
 Then there's two other paradigms for programming

2:43:47.700 --> 2:43:49.220
 that everybody should be familiar with.

2:43:49.220 --> 2:43:51.260
 And one of them is functional.

2:43:51.260 --> 2:43:54.140
 You should learn Haskell and take that all the way through,

2:43:54.140 --> 2:43:57.260
 learn a language with dependent types like Coq,

2:43:57.260 --> 2:43:58.980
 learn that whole space,

2:43:58.980 --> 2:44:02.740
 like the very PL theory, heavy languages.

2:44:02.740 --> 2:44:04.860
 And Haskell is your favorite functional?

2:44:04.860 --> 2:44:06.500
 Is that the go to, you'd say?

2:44:06.500 --> 2:44:08.580
 Yeah, I'm not a great Haskell programmer.

2:44:08.580 --> 2:44:10.580
 I wrote a compiler in Haskell once.

2:44:10.580 --> 2:44:11.460
 There's another paradigm,

2:44:11.460 --> 2:44:12.540
 and actually there's one more paradigm

2:44:12.540 --> 2:44:14.180
 that I'll even talk about after that,

2:44:14.180 --> 2:44:15.100
 that I never used to talk about

2:44:15.100 --> 2:44:15.940
 when I would think about this,

2:44:15.940 --> 2:44:18.440
 but the next paradigm is learn Verilog of HDL.

2:44:20.240 --> 2:44:22.280
 Understand this idea of all of the instructions

2:44:22.280 --> 2:44:26.860
 execute at once. If I have a block in Verilog

2:44:26.860 --> 2:44:29.900
 and I write stuff in it, it's not sequential.

2:44:29.900 --> 2:44:31.300
 They all execute at once.

2:44:33.740 --> 2:44:36.700
 And then think like that, that's how hardware works.

2:44:36.700 --> 2:44:40.020
 To be, so I guess assembly doesn't quite get you that.

2:44:40.020 --> 2:44:42.260
 Assembly is more about compilation,

2:44:42.260 --> 2:44:44.260
 and Verilog is more about the hardware,

2:44:44.260 --> 2:44:46.420
 like giving a sense of what actually

2:44:46.420 --> 2:44:48.580
 is the hardware is doing.

2:44:48.580 --> 2:44:50.480
 Assembly, C, Python are straight,

2:44:50.480 --> 2:44:52.420
 like they sit right on top of each other.

2:44:52.420 --> 2:44:55.660
 In fact, C is, well, C is kind of coded in C,

2:44:55.660 --> 2:44:57.900
 but you could imagine the first C was coded in assembly,

2:44:57.900 --> 2:45:00.020
 and Python is actually coded in C.

2:45:00.020 --> 2:45:02.500
 So you can straight up go on that.

2:45:03.620 --> 2:45:06.940
 Got it, and then Verilog gives you, that's brilliant.

2:45:06.940 --> 2:45:07.780
 Okay.

2:45:07.780 --> 2:45:09.820
 And then I think there's another one now.

2:45:09.820 --> 2:45:12.660
 Everyone, Carpathia calls it programming 2.0,

2:45:12.660 --> 2:45:16.500
 which is learn a, I'm not even gonna,

2:45:16.500 --> 2:45:18.620
 don't learn TensorFlow, learn PyTorch.

2:45:18.620 --> 2:45:20.180
 So machine learning.

2:45:20.180 --> 2:45:21.540
 We've got to come up with a better term

2:45:21.540 --> 2:45:26.100
 than programming 2.0, or, but yeah.

2:45:26.100 --> 2:45:28.060
 It's a programming language, learn it.

2:45:29.900 --> 2:45:32.660
 I wonder if it can be formalized a little bit better.

2:45:32.660 --> 2:45:34.900
 It feels like we're in the early days

2:45:34.900 --> 2:45:37.100
 of what that actually entails.

2:45:37.100 --> 2:45:39.220
 Data driven programming?

2:45:39.220 --> 2:45:41.700
 Data driven programming, yeah.

2:45:41.700 --> 2:45:43.080
 But it's so fundamentally different

2:45:43.080 --> 2:45:44.900
 as a paradigm than the others.

2:45:44.900 --> 2:45:49.900
 Like it almost requires a different skillset.

2:45:50.420 --> 2:45:52.020
 But you think it's still, yeah.

2:45:53.740 --> 2:45:56.580
 And PyTorch versus TensorFlow, PyTorch wins.

2:45:56.580 --> 2:45:57.460
 It's the fourth paradigm.

2:45:57.460 --> 2:45:59.380
 It's the fourth paradigm that I've kind of seen.

2:45:59.380 --> 2:46:01.460
 There's like this, you know,

2:46:01.460 --> 2:46:04.840
 imperative functional hardware.

2:46:04.840 --> 2:46:06.340
 I don't know a better word for it.

2:46:06.340 --> 2:46:07.300
 And then ML.

2:46:08.180 --> 2:46:13.180
 Do you have advice for people that wanna,

2:46:13.180 --> 2:46:16.180
 you know, get into programming, wanna learn programming?

2:46:16.180 --> 2:46:18.200
 You have a video,

2:46:19.940 --> 2:46:22.860
 what is programming noob lessons, exclamation point.

2:46:22.860 --> 2:46:24.620
 And I think the top comment is like,

2:46:24.620 --> 2:46:26.520
 warning, this is not for noobs.

2:46:27.820 --> 2:46:32.560
 Do you have a noob, like a TLDW for that video,

2:46:32.560 --> 2:46:37.560
 but also a noob friendly advice

2:46:38.020 --> 2:46:39.540
 on how to get into programming?

2:46:39.540 --> 2:46:41.400
 We're never going to learn programming

2:46:41.400 --> 2:46:44.640
 by watching a video called Learn Programming.

2:46:44.640 --> 2:46:46.660
 The only way to learn programming, I think,

2:46:46.660 --> 2:46:48.060
 and the only one is that the only way

2:46:48.060 --> 2:46:50.140
 everyone I've ever met who can program well,

2:46:50.140 --> 2:46:51.940
 learned it all in the same way.

2:46:51.940 --> 2:46:53.740
 They had something they wanted to do

2:46:54.960 --> 2:46:56.580
 and then they tried to do it.

2:46:56.580 --> 2:47:00.020
 And then they were like, oh, well, okay.

2:47:00.020 --> 2:47:01.380
 This is kind of, you know, it'd be nice

2:47:01.380 --> 2:47:02.740
 if the computer could kind of do this.

2:47:02.740 --> 2:47:04.440
 And then, you know, that's how you learn.

2:47:04.440 --> 2:47:06.520
 You just keep pushing on a project.

2:47:09.020 --> 2:47:10.900
 So the only advice I have for learning programming

2:47:10.900 --> 2:47:12.140
 is go program.

2:47:12.140 --> 2:47:14.680
 Somebody wrote to me a question like,

2:47:14.680 --> 2:47:17.100
 we don't really, they're looking to learn

2:47:17.100 --> 2:47:19.060
 about recurring neural networks.

2:47:19.060 --> 2:47:20.540
 And he's saying, like, my company's thinking

2:47:20.540 --> 2:47:24.140
 of using recurring neural networks for time series data,

2:47:24.140 --> 2:47:27.420
 but we don't really have an idea of where to use it yet.

2:47:27.420 --> 2:47:28.980
 We just want to, like, do you have any advice

2:47:28.980 --> 2:47:31.780
 on how to learn about, these are these kind of

2:47:31.780 --> 2:47:33.340
 general machine learning questions.

2:47:33.340 --> 2:47:36.780
 And I think the answer is, like,

2:47:36.780 --> 2:47:39.020
 actually have a problem that you're trying to solve.

2:47:39.020 --> 2:47:40.220
 And just.

2:47:40.220 --> 2:47:41.220
 I see that stuff.

2:47:41.220 --> 2:47:42.660
 Oh my God, when people talk like that,

2:47:42.660 --> 2:47:45.500
 they're like, I heard machine learning is important.

2:47:45.500 --> 2:47:47.500
 Could you help us integrate machine learning

2:47:47.500 --> 2:47:49.580
 with macaroni and cheese production?

2:47:51.300 --> 2:47:54.000
 You just, I don't even, you can't help these people.

2:47:54.000 --> 2:47:55.980
 Like, who lets you run anything?

2:47:55.980 --> 2:47:58.400
 Who lets that kind of person run anything?

2:47:58.400 --> 2:48:02.620
 I think we're all, we're all beginners at some point.

2:48:02.620 --> 2:48:03.460
 So.

2:48:03.460 --> 2:48:04.860
 It's not like they're a beginner.

2:48:04.860 --> 2:48:07.340
 It's like, my problem is not that they don't know

2:48:07.340 --> 2:48:08.620
 about machine learning.

2:48:08.620 --> 2:48:10.840
 My problem is that they think that machine learning

2:48:10.840 --> 2:48:13.740
 has something to say about macaroni and cheese production.

2:48:14.740 --> 2:48:17.020
 Or like, I heard about this new technology.

2:48:17.020 --> 2:48:19.020
 How can I use it for why?

2:48:19.860 --> 2:48:23.500
 Like, I don't know what it is, but how can I use it for why?

2:48:23.500 --> 2:48:24.340
 That's true.

2:48:24.340 --> 2:48:26.140
 You have to build up an intuition of how,

2:48:26.140 --> 2:48:27.620
 cause you might be able to figure out a way,

2:48:27.620 --> 2:48:29.300
 but like the prerequisites,

2:48:29.300 --> 2:48:32.300
 you should have a macaroni and cheese problem to solve first.

2:48:32.300 --> 2:48:33.460
 Exactly.

2:48:33.460 --> 2:48:36.940
 And then two, you should have more traditional,

2:48:36.940 --> 2:48:39.340
 like the learning process should involve

2:48:39.340 --> 2:48:41.940
 more traditionally applicable problems

2:48:41.940 --> 2:48:44.540
 in the space of whatever that is, machine learning,

2:48:44.540 --> 2:48:47.060
 and then see if it can be applied to mac and cheese.

2:48:47.060 --> 2:48:49.180
 At least start with, tell me about a problem.

2:48:49.180 --> 2:48:50.740
 Like if you have a problem, you're like,

2:48:50.740 --> 2:48:52.500
 you know, some of my boxes aren't getting

2:48:52.500 --> 2:48:54.500
 enough macaroni in them.

2:48:54.500 --> 2:48:56.820
 Can we use machine learning to solve this problem?

2:48:56.820 --> 2:48:59.380
 That's much, much better than how do I apply

2:48:59.380 --> 2:49:01.600
 machine learning to macaroni and cheese?

2:49:01.600 --> 2:49:05.380
 One big thing, maybe this is me talking

2:49:05.380 --> 2:49:07.860
 to the audience a little bit, cause I get these days

2:49:07.860 --> 2:49:12.860
 so many messages, advice on how to like learn stuff, okay?

2:49:15.060 --> 2:49:18.160
 My, this is not me being mean.

2:49:18.160 --> 2:49:20.540
 I think this is quite profound actually,

2:49:20.540 --> 2:49:22.780
 is you should Google it.

2:49:22.780 --> 2:49:23.820
 Oh yeah.

2:49:23.820 --> 2:49:28.820
 Like one of the like skills that you should really acquire

2:49:29.700 --> 2:49:33.100
 as an engineer, as a researcher, as a thinker,

2:49:33.100 --> 2:49:36.660
 like one, there's two complementary skills.

2:49:36.660 --> 2:49:39.100
 Like one is with a blank sheet of paper

2:49:39.100 --> 2:49:41.660
 with no internet to think deeply.

2:49:41.660 --> 2:49:44.740
 And then the other is to Google the crap

2:49:44.740 --> 2:49:45.940
 out of the questions you have.

2:49:45.940 --> 2:49:49.180
 Like that's actually a skill people often talk about,

2:49:49.180 --> 2:49:52.020
 but like doing research, like pulling at the thread,

2:49:52.020 --> 2:49:53.860
 like looking up different words,

2:49:53.860 --> 2:49:58.060
 going into like GitHub repositories with two stars

2:49:58.060 --> 2:49:59.700
 and like looking how they did stuff,

2:49:59.700 --> 2:50:03.460
 like looking at the code or going on Twitter,

2:50:03.460 --> 2:50:05.780
 seeing like there's little pockets of brilliant people

2:50:05.780 --> 2:50:07.620
 that are like having discussions.

2:50:07.620 --> 2:50:09.860
 Like if you're a neuroscientist,

2:50:09.860 --> 2:50:11.620
 go into signal processing community.

2:50:11.620 --> 2:50:15.860
 If you're an AI person going into the psychology community,

2:50:15.860 --> 2:50:18.020
 like switch communities.

2:50:18.020 --> 2:50:19.980
 I keep searching, searching, searching,

2:50:19.980 --> 2:50:23.860
 because it's so much better to invest

2:50:23.860 --> 2:50:27.260
 in like finding somebody else who already solved your problem

2:50:27.260 --> 2:50:30.900
 than it is to try to solve the problem.

2:50:30.900 --> 2:50:34.380
 And because they've often invested years of their life,

2:50:34.380 --> 2:50:37.420
 like entire communities are probably already out there

2:50:37.420 --> 2:50:39.180
 who have tried to solve your problem.

2:50:39.180 --> 2:50:40.980
 I think they're the same thing.

2:50:40.980 --> 2:50:44.180
 I think you go try to solve the problem.

2:50:44.180 --> 2:50:46.180
 And then in trying to solve the problem,

2:50:46.180 --> 2:50:47.740
 if you're good at solving problems,

2:50:47.740 --> 2:50:50.260
 you'll stumble upon the person who solved it already.

2:50:50.260 --> 2:50:52.300
 But the stumbling is really important.

2:50:52.300 --> 2:50:54.260
 I think that's a skill that people should really put,

2:50:54.260 --> 2:50:57.700
 especially in undergrad, like search.

2:50:57.700 --> 2:50:58.700
 If you ask me a question,

2:50:58.700 --> 2:51:02.180
 how should I get started in deep learning, like especially?

2:51:04.100 --> 2:51:07.260
 Like that is just so Googleable.

2:51:07.260 --> 2:51:10.060
 Like the whole point is you Google that

2:51:10.060 --> 2:51:13.980
 and you get a million pages and just start looking at them.

2:51:13.980 --> 2:51:16.420
 Start pulling at the threads, start exploring,

2:51:16.420 --> 2:51:19.300
 start taking notes, start getting advice

2:51:19.300 --> 2:51:22.820
 from a million people that already like spent their life

2:51:22.820 --> 2:51:25.060
 answering that question, actually.

2:51:25.060 --> 2:51:26.460
 Oh, well, yeah, I mean, that's definitely also, yeah,

2:51:26.460 --> 2:51:28.500
 when people like ask me things like that, I'm like, trust me,

2:51:28.500 --> 2:51:30.340
 the top answer on Google is much, much better

2:51:30.340 --> 2:51:32.860
 than anything I'm going to tell you, right?

2:51:32.860 --> 2:51:34.460
 Yeah.

2:51:34.460 --> 2:51:38.100
 People ask, it's an interesting question.

2:51:38.100 --> 2:51:39.940
 Let me know if you have any recommendations.

2:51:39.940 --> 2:51:43.700
 What three books, technical or fiction or philosophical,

2:51:43.700 --> 2:51:47.720
 had an impact on your life or you would recommend perhaps?

2:51:49.180 --> 2:51:51.100
 Maybe we'll start with the least controversial,

2:51:51.100 --> 2:51:56.100
 Infinite Jest, Infinite Jest is a...

2:51:57.500 --> 2:51:58.860
 David Foster Wallace.

2:51:58.860 --> 2:52:01.100
 Yeah, it's a book about wireheading, really.

2:52:03.820 --> 2:52:07.540
 Very enjoyable to read, very well written.

2:52:07.540 --> 2:52:11.180
 You know, you will grow as a person reading this book,

2:52:11.180 --> 2:52:14.740
 its effort, and I'll set that up for the second book,

2:52:14.740 --> 2:52:17.500
 which is pornography, it's called Atlas Shrugged,

2:52:17.500 --> 2:52:21.020
 which...

2:52:21.020 --> 2:52:22.580
 Atlas Shrugged is pornography.

2:52:22.580 --> 2:52:25.540
 I mean, it is, I will not defend the,

2:52:25.540 --> 2:52:28.380
 I will not say Atlas Shrugged is a well written book.

2:52:28.380 --> 2:52:31.380
 It is entertaining to read, certainly, just like pornography.

2:52:31.380 --> 2:52:33.540
 The production value isn't great.

2:52:33.540 --> 2:52:36.220
 You know, there's a 60 page monologue in there

2:52:36.220 --> 2:52:38.740
 that Ann Rand's editor really wanted to take out.

2:52:38.740 --> 2:52:42.580
 And she paid, she paid out of her pocket

2:52:42.580 --> 2:52:45.060
 to keep that 60 page monologue in the book.

2:52:45.060 --> 2:52:50.060
 But it is a great book for a kind of framework

2:52:53.220 --> 2:52:54.660
 of human relations.

2:52:54.660 --> 2:52:55.960
 And I know a lot of people are like,

2:52:55.960 --> 2:52:58.060
 yeah, but it's a terrible framework.

2:52:58.060 --> 2:53:00.500
 Yeah, but it's a framework.

2:53:00.500 --> 2:53:02.360
 Just for context, in a couple of days,

2:53:02.360 --> 2:53:06.260
 I'm speaking for probably four plus hours

2:53:06.260 --> 2:53:10.200
 with Yaron Brook, who's the main living,

2:53:10.200 --> 2:53:13.340
 remaining objectivist, objectivist.

2:53:13.340 --> 2:53:14.580
 Interesting.

2:53:14.580 --> 2:53:19.380
 So I've always found this philosophy quite interesting

2:53:19.380 --> 2:53:20.260
 on many levels.

2:53:20.260 --> 2:53:24.020
 One of how repulsive some percent of,

2:53:24.020 --> 2:53:26.300
 large percent of the population find it,

2:53:26.300 --> 2:53:29.140
 which is always, always funny to me

2:53:29.140 --> 2:53:32.980
 when people are like unable to even read a philosophy

2:53:32.980 --> 2:53:36.700
 because of some, I think that says more

2:53:36.700 --> 2:53:40.580
 about their psychological perspective on it.

2:53:40.580 --> 2:53:45.300
 But there is something about objectivism

2:53:45.300 --> 2:53:48.780
 and Ann Rand's philosophy that's deeply connected

2:53:48.780 --> 2:53:50.740
 to this idea of capitalism,

2:53:50.740 --> 2:53:54.620
 of the ethical life is the productive life

2:53:56.620 --> 2:54:00.740
 that was always compelling to me.

2:54:00.740 --> 2:54:03.160
 It didn't seem as, like I didn't seem to interpret it

2:54:03.160 --> 2:54:05.660
 in the negative sense that some people do.

2:54:05.660 --> 2:54:07.980
 To be fair, I read that book when I was 19.

2:54:07.980 --> 2:54:09.620
 So you had an impact at that point, yeah.

2:54:09.620 --> 2:54:13.700
 Yeah, and the bad guys in the book have this slogan

2:54:13.700 --> 2:54:15.340
 from each according to their ability

2:54:15.340 --> 2:54:17.300
 to each according to their need.

2:54:17.300 --> 2:54:19.740
 And I'm looking at this and I'm like,

2:54:19.740 --> 2:54:20.580
 these are the most cart,

2:54:20.580 --> 2:54:22.940
 this is team rocket level cartoonishness, right?

2:54:22.940 --> 2:54:23.820
 No bad guy.

2:54:23.820 --> 2:54:25.780
 And then when I realized that was actually the slogan

2:54:25.780 --> 2:54:29.940
 of the communist party, I'm like, wait a second.

2:54:29.940 --> 2:54:31.660
 Wait, no, no, no, no, no.

2:54:31.660 --> 2:54:34.100
 You're telling me this really happened?

2:54:34.100 --> 2:54:34.920
 Yeah, it's interesting.

2:54:34.920 --> 2:54:36.660
 I mean, one of the criticisms of her work

2:54:36.660 --> 2:54:39.220
 is she has a cartoonish view of good and evil.

2:54:39.220 --> 2:54:44.220
 Like the reality, as Jordan Peterson says,

2:54:44.300 --> 2:54:47.420
 is that each of us have the capacity for good and evil

2:54:47.420 --> 2:54:49.940
 in us as opposed to like, there's some characters

2:54:49.940 --> 2:54:52.220
 who are purely evil and some characters that are purely good.

2:54:52.220 --> 2:54:55.220
 And that's in a way why it's pornographic.

2:54:55.220 --> 2:54:57.020
 The production value, I love it.

2:54:57.020 --> 2:54:59.540
 Like evil is punished and there's very clearly,

2:55:01.020 --> 2:55:06.020
 there's no, just like porn doesn't have character growth.

2:55:06.020 --> 2:55:09.540
 Well, you know, neither does Alice Shrugged, like.

2:55:09.540 --> 2:55:10.860
 Really, well put.

2:55:10.860 --> 2:55:14.220
 But at 19 year old George Hots, it was good enough.

2:55:14.220 --> 2:55:15.380
 Yeah, yeah, yeah, yeah.

2:55:15.380 --> 2:55:16.940
 What's the third?

2:55:16.940 --> 2:55:18.620
 You have something?

2:55:18.620 --> 2:55:21.520
 I could give, these two I'll just throw out.

2:55:21.520 --> 2:55:22.420
 They're sci fi.

2:55:22.420 --> 2:55:24.280
 Perputation City.

2:55:24.280 --> 2:55:26.620
 Great thing to start thinking about copies of yourself.

2:55:26.620 --> 2:55:27.460
 And then the...

2:55:27.460 --> 2:55:28.280
 Who's that by?

2:55:28.280 --> 2:55:29.120
 Sorry, I didn't catch that.

2:55:29.120 --> 2:55:30.900
 That is Greg Egan.

2:55:31.980 --> 2:55:33.540
 He's a, that might not be his real name.

2:55:33.540 --> 2:55:35.740
 Some Australian guy, might not be Australian.

2:55:35.740 --> 2:55:36.700
 I don't know.

2:55:36.700 --> 2:55:38.740
 And then this one's online.

2:55:38.740 --> 2:55:41.180
 It's called The Metamorphosis of Prime Intellect.

2:55:43.020 --> 2:55:45.420
 It's a story set in a post singularity world.

2:55:45.420 --> 2:55:46.720
 It's interesting.

2:55:46.720 --> 2:55:49.180
 Is there, can you, either of the worlds,

2:55:49.180 --> 2:55:51.540
 do you find something philosophical interesting in them

2:55:51.540 --> 2:55:52.740
 that you can comment on?

2:55:53.660 --> 2:55:55.780
 I mean, it is clear to me that

2:55:57.860 --> 2:56:00.620
 Metamorphosis of Prime Intellect is like written by

2:56:00.620 --> 2:56:03.780
 an engineer, which is,

2:56:03.780 --> 2:56:08.780
 it's very almost a pragmatic take on a utopia, in a way.

2:56:12.620 --> 2:56:13.820
 Positive or negative?

2:56:15.260 --> 2:56:17.940
 That's up to you to decide reading the book.

2:56:17.940 --> 2:56:21.580
 And the ending of it is very interesting as well.

2:56:21.580 --> 2:56:23.660
 And I didn't realize what it was.

2:56:23.660 --> 2:56:25.260
 I first read that when I was 15.

2:56:25.260 --> 2:56:27.540
 I've reread that book several times in my life.

2:56:27.540 --> 2:56:29.220
 And it's short, it's 50 pages.

2:56:29.220 --> 2:56:30.740
 Everyone should go read it.

2:56:30.740 --> 2:56:33.100
 What's, sorry, it's a little tangent.

2:56:33.100 --> 2:56:34.700
 I've been working through the foundation.

2:56:34.700 --> 2:56:37.060
 I've been, I haven't read much sci fi my whole life

2:56:37.060 --> 2:56:40.180
 and I'm trying to fix that the last few months.

2:56:40.180 --> 2:56:42.180
 That's been a little side project.

2:56:42.180 --> 2:56:46.180
 What's to you as the greatest sci fi novel

2:56:46.180 --> 2:56:47.740
 that people should read?

2:56:47.740 --> 2:56:49.220
 Or is that?

2:56:49.220 --> 2:56:51.180
 I mean, I would, yeah, I would say like, yeah,

2:56:51.180 --> 2:56:53.820
 Permutation City, Metamorphosis of Prime Intellect.

2:56:53.820 --> 2:56:54.660
 I don't know.

2:56:54.660 --> 2:56:56.240
 I didn't like Foundation.

2:56:56.240 --> 2:56:58.820
 I thought it was way too modernist.

2:56:58.820 --> 2:57:00.780
 You like Dune and all of those.

2:57:00.780 --> 2:57:01.780
 I've never read Dune.

2:57:01.780 --> 2:57:02.820
 I've never read Dune.

2:57:02.820 --> 2:57:04.580
 I have to read it.

2:57:04.580 --> 2:57:07.380
 Fire Upon the Deep is interesting.

2:57:09.140 --> 2:57:10.540
 Okay, I mean, look, everyone should read,

2:57:10.540 --> 2:57:11.380
 everyone should read Neuromancer.

2:57:11.380 --> 2:57:12.820
 Everyone should read Snow Crash.

2:57:12.820 --> 2:57:15.500
 If you haven't read those, like start there.

2:57:15.500 --> 2:57:16.340
 Yeah, I haven't read Snow Crash.

2:57:16.340 --> 2:57:17.460
 You haven't read Snow Crash?

2:57:17.460 --> 2:57:19.980
 Oh, it's, I mean, it's very entertaining.

2:57:19.980 --> 2:57:20.820
 Go to Lesher Bach.

2:57:20.820 --> 2:57:22.220
 And if you want the controversial one,

2:57:22.220 --> 2:57:23.220
 Bronze Age Mindset.

2:57:25.340 --> 2:57:27.740
 All right, I'll look into that one.

2:57:27.740 --> 2:57:30.360
 Those aren't sci fi, but just to round out books.

2:57:30.360 --> 2:57:34.360
 So a bunch of people asked me on Twitter

2:57:34.360 --> 2:57:36.880
 and Reddit and so on for advice.

2:57:36.880 --> 2:57:39.440
 So what advice would you give a young person today

2:57:39.440 --> 2:57:40.560
 about life?

2:57:40.560 --> 2:57:45.560
 In other words, what, yeah, I mean, looking back,

2:57:47.480 --> 2:57:50.480
 especially when you were younger, you did,

2:57:50.480 --> 2:57:51.560
 and you continued it.

2:57:51.560 --> 2:57:54.840
 You've accomplished a lot of interesting things.

2:57:54.840 --> 2:57:57.840
 Is there some advice from those,

2:57:57.840 --> 2:58:01.880
 from that life of yours that you can pass on?

2:58:01.880 --> 2:58:03.760
 If college ever opens again,

2:58:03.760 --> 2:58:07.640
 I would love to give a graduation speech.

2:58:07.640 --> 2:58:11.360
 At that point, I will put a lot of somewhat satirical effort

2:58:11.360 --> 2:58:12.320
 into this question.

2:58:12.320 --> 2:58:15.880
 Yeah, at this, you haven't written anything at this point.

2:58:15.880 --> 2:58:16.760
 Oh, you know what?

2:58:16.760 --> 2:58:18.240
 Always wear sunscreen.

2:58:18.240 --> 2:58:19.560
 This is water.

2:58:19.560 --> 2:58:21.160
 Pick your plagiarizing.

2:58:21.160 --> 2:58:23.960
 I mean, you know, but that's the,

2:58:23.960 --> 2:58:26.240
 that's the like clean your room.

2:58:26.240 --> 2:58:28.600
 You know, yeah, you can plagiarize from all of this stuff.

2:58:28.600 --> 2:58:33.600
 And it's, there is no,

2:58:35.920 --> 2:58:37.680
 self help books aren't designed to help you.

2:58:37.680 --> 2:58:40.080
 They're designed to make you feel good.

2:58:40.080 --> 2:58:44.120
 Like whatever advice I could give, you already know.

2:58:44.120 --> 2:58:45.840
 Everyone already knows.

2:58:45.840 --> 2:58:47.520
 Sorry, it doesn't feel good.

2:58:50.240 --> 2:58:51.080
 Right?

2:58:51.080 --> 2:58:53.040
 Like, you know, you know,

2:58:53.040 --> 2:58:56.880
 if I tell you that you should, you know,

2:58:56.880 --> 2:59:01.880
 eat well and read more and it's not gonna do anything.

2:59:01.920 --> 2:59:03.480
 I think the whole like genre

2:59:03.480 --> 2:59:07.480
 of those kinds of questions is meaningless.

2:59:07.480 --> 2:59:08.320
 I don't know.

2:59:08.320 --> 2:59:10.560
 If anything, it's don't worry so much about that stuff.

2:59:10.560 --> 2:59:12.440
 Don't be so caught up in your head.

2:59:12.440 --> 2:59:13.280
 Right.

2:59:13.280 --> 2:59:14.280
 I mean, you're, yeah.

2:59:14.280 --> 2:59:16.560
 In a sense that your whole life,

2:59:16.560 --> 2:59:20.680
 your whole existence is like moving version of that advice.

2:59:20.680 --> 2:59:23.840
 I don't know.

2:59:23.840 --> 2:59:25.480
 There's something, I mean,

2:59:25.480 --> 2:59:27.080
 there's something in you that resists

2:59:27.080 --> 2:59:29.800
 that kind of thinking and that in itself is,

2:59:30.880 --> 2:59:34.480
 it's just illustrative of who you are.

2:59:34.480 --> 2:59:36.760
 And there's something to learn from that.

2:59:36.760 --> 2:59:39.760
 I think you're clearly not overthinking stuff.

2:59:41.280 --> 2:59:42.120
 Yeah.

2:59:42.120 --> 2:59:42.960
 And you know what?

2:59:42.960 --> 2:59:43.800
 There's a gut thing.

2:59:43.800 --> 2:59:45.000
 Even when I talk about my advice,

2:59:45.000 --> 2:59:47.400
 I'm like, my advice is only relevant to me.

2:59:47.400 --> 2:59:48.720
 It's not relevant to anybody else.

2:59:48.720 --> 2:59:49.960
 I'm not saying you should go out.

2:59:49.960 --> 2:59:51.640
 If you're the kind of person who overthinks things

2:59:51.640 --> 2:59:54.080
 to stop overthinking things, it's not bad.

2:59:54.080 --> 2:59:54.960
 It doesn't work for me.

2:59:54.960 --> 2:59:55.800
 Maybe it works for you.

2:59:55.800 --> 2:59:57.960
 I don't know.

2:59:57.960 --> 2:59:59.400
 Let me ask you about love.

2:59:59.400 --> 3:00:00.240
 Yeah.

3:00:02.240 --> 3:00:05.120
 I think last time we talked about the meaning of life

3:00:05.120 --> 3:00:08.640
 and it was kind of about winning.

3:00:08.640 --> 3:00:09.480
 Of course.

3:00:10.880 --> 3:00:13.120
 I don't think I've talked to you about love much,

3:00:13.120 --> 3:00:15.000
 whether romantic or just love

3:00:15.000 --> 3:00:18.120
 for the common humanity amongst us all.

3:00:18.120 --> 3:00:21.400
 What role has love played in your life?

3:00:21.400 --> 3:00:26.360
 In this quest for winning, where does love fit in?

3:00:26.360 --> 3:00:29.840
 Well, the word love, I think means several different things.

3:00:29.840 --> 3:00:32.960
 There's love in the sense of, maybe I could just say,

3:00:32.960 --> 3:00:34.400
 there's like love in the sense of opiates

3:00:34.400 --> 3:00:37.880
 and love in the sense of oxytocin

3:00:37.880 --> 3:00:40.080
 and then love in the sense of,

3:00:43.080 --> 3:00:44.480
 maybe like a love for math.

3:00:44.480 --> 3:00:45.640
 I don't think it fits into either

3:00:45.640 --> 3:00:49.160
 of those first two paradigms.

3:00:49.160 --> 3:00:53.480
 So each of those, have they given something to you

3:00:55.560 --> 3:00:56.920
 in your life?

3:00:56.920 --> 3:00:59.120
 I'm not that big of a fan of the first two.

3:01:00.800 --> 3:01:01.640
 Why?

3:01:03.600 --> 3:01:06.360
 The same reason I'm not a fan of,

3:01:06.360 --> 3:01:09.880
 the same reason I don't do opiates and don't take ecstasy.

3:01:09.880 --> 3:01:14.200
 And there were times, look, I've tried both.

3:01:14.200 --> 3:01:17.320
 I liked opiates way more than I liked ecstasy,

3:01:18.400 --> 3:01:23.400
 but they're not, the ethical life is the productive life.

3:01:24.400 --> 3:01:27.080
 So maybe that's my problem with those.

3:01:27.080 --> 3:01:29.440
 And then like, yeah, a sense of, I don't know,

3:01:29.440 --> 3:01:32.200
 like abstract love for humanity.

3:01:32.200 --> 3:01:34.520
 I mean, the abstract love for humanity,

3:01:34.520 --> 3:01:36.200
 I'm like, yeah, I've always felt that.

3:01:36.200 --> 3:01:39.680
 And I guess it's hard for me to imagine

3:01:39.680 --> 3:01:41.560
 not feeling it and maybe there's people who don't.

3:01:41.560 --> 3:01:43.560
 And I don't know.

3:01:43.560 --> 3:01:46.520
 Yeah, that's just like a background thing that's there.

3:01:46.520 --> 3:01:49.560
 I mean, since we brought up drugs, let me ask you,

3:01:51.760 --> 3:01:54.000
 this is becoming more and more a part of my life

3:01:54.000 --> 3:01:55.520
 because I'm talking to a few researchers

3:01:55.520 --> 3:01:57.680
 that are working on psychedelics.

3:01:57.680 --> 3:02:00.440
 I've eaten shrooms a couple of times

3:02:00.440 --> 3:02:04.680
 and it was fascinating to me that like the mind can go,

3:02:04.680 --> 3:02:08.200
 like just fascinating the mind can go to places

3:02:08.200 --> 3:02:09.480
 I didn't imagine it could go.

3:02:09.480 --> 3:02:12.720
 And it was very friendly and positive and exciting

3:02:12.720 --> 3:02:16.160
 and everything was kind of hilarious in the place.

3:02:16.160 --> 3:02:18.200
 Wherever my mind went, that's where I went.

3:02:18.200 --> 3:02:20.960
 Is, what do you think about psychedelics?

3:02:20.960 --> 3:02:24.680
 Do you think they have, where do you think the mind goes?

3:02:24.680 --> 3:02:25.920
 Have you done psychedelics?

3:02:25.920 --> 3:02:28.640
 Where do you think the mind goes?

3:02:28.640 --> 3:02:32.240
 Is there something useful to learn about the places it goes

3:02:32.240 --> 3:02:33.840
 once you come back?

3:02:33.840 --> 3:02:38.040
 I find it interesting that this idea

3:02:38.040 --> 3:02:40.400
 that psychedelics have something to teach

3:02:40.400 --> 3:02:43.800
 is almost unique to psychedelics, right?

3:02:43.800 --> 3:02:46.560
 People don't argue this about amphetamines.

3:02:46.560 --> 3:02:50.240
 And I'm not really sure why.

3:02:50.240 --> 3:02:53.800
 I think all of the drugs have lessons to teach.

3:02:53.800 --> 3:02:55.160
 I think there's things to learn from opiates.

3:02:55.160 --> 3:02:56.600
 I think there's things to learn from amphetamines.

3:02:56.600 --> 3:02:58.160
 I think there's things to learn from psychedelics,

3:02:58.160 --> 3:02:59.720
 things to learn from marijuana.

3:03:02.320 --> 3:03:05.200
 But also at the same time recognize

3:03:05.200 --> 3:03:07.400
 that I don't think you're learning things about the world.

3:03:07.400 --> 3:03:09.160
 I think you're learning things about yourself.

3:03:09.160 --> 3:03:10.680
 Yes.

3:03:10.680 --> 3:03:14.480
 And, you know, what's the, even, it might've even been,

3:03:15.760 --> 3:03:17.240
 might've even been a Timothy Leary quote.

3:03:17.240 --> 3:03:18.200
 I don't wanna misquote him,

3:03:18.200 --> 3:03:20.240
 but the idea is basically like, you know,

3:03:20.240 --> 3:03:21.600
 everybody should look behind the door,

3:03:21.600 --> 3:03:22.760
 but then once you've seen behind the door,

3:03:22.760 --> 3:03:24.400
 you don't need to keep going back.

3:03:26.120 --> 3:03:29.960
 So, I mean, and that's my thoughts on all real drug use too.

3:03:29.960 --> 3:03:31.200
 Except maybe for caffeine.

3:03:32.680 --> 3:03:37.160
 It's a little experience that is good to have, but.

3:03:37.160 --> 3:03:39.240
 Oh yeah, no, I mean, yeah, I guess,

3:03:39.240 --> 3:03:40.880
 yes, psychedelics are definitely.

3:03:41.880 --> 3:03:43.920
 So you're a fan of new experiences, I suppose.

3:03:43.920 --> 3:03:44.760
 Yes.

3:03:44.760 --> 3:03:45.880
 Because they all contain a little,

3:03:45.880 --> 3:03:47.000
 especially the first few times,

3:03:47.000 --> 3:03:49.720
 it contains some lessons that can be picked up.

3:03:49.720 --> 3:03:53.960
 Yeah, and I'll revisit psychedelics maybe once a year.

3:03:55.720 --> 3:03:57.680
 Usually smaller doses.

3:03:58.760 --> 3:04:01.440
 Maybe they turn up the learning rate of your brain.

3:04:01.440 --> 3:04:03.160
 I've heard that, I like that.

3:04:03.160 --> 3:04:04.280
 Yeah, that's cool.

3:04:04.280 --> 3:04:07.360
 Big learning rates have pros and cons.

3:04:07.360 --> 3:04:09.440
 Last question, and this is a little weird one,

3:04:09.440 --> 3:04:11.800
 but you've called yourself crazy in the past.

3:04:14.120 --> 3:04:16.120
 First of all, on a scale of one to 10,

3:04:16.120 --> 3:04:18.040
 how crazy would you say are you?

3:04:18.040 --> 3:04:19.480
 Oh, I mean, it depends how you, you know,

3:04:19.480 --> 3:04:21.680
 when you compare me to Elon Musk and Anthony Levandowski,

3:04:21.680 --> 3:04:22.560
 not so crazy.

3:04:23.520 --> 3:04:24.880
 So like a seven?

3:04:25.920 --> 3:04:27.000
 Let's go with six.

3:04:27.000 --> 3:04:29.320
 Six, six, six.

3:04:29.320 --> 3:04:30.160
 What?

3:04:31.440 --> 3:04:32.960
 Well, I like seven, seven's a good number.

3:04:32.960 --> 3:04:36.320
 Seven, all right, well, I'm sure day by day it changes,

3:04:36.320 --> 3:04:39.640
 right, so, but you're in that area.

3:04:42.440 --> 3:04:43.440
 In thinking about that,

3:04:43.440 --> 3:04:45.760
 what do you think is the role of madness?

3:04:45.760 --> 3:04:48.520
 Is that a feature or a bug

3:04:48.520 --> 3:04:50.640
 if you were to dissect your brain?

3:04:51.680 --> 3:04:56.680
 So, okay, from like a mental health lens on crazy,

3:04:57.040 --> 3:04:59.040
 I'm not sure I really believe in that.

3:04:59.040 --> 3:05:02.840
 I'm not sure I really believe in like a lot of that stuff.

3:05:02.840 --> 3:05:05.080
 Right, this concept of, okay, you know,

3:05:05.080 --> 3:05:09.560
 when you get over to like hardcore bipolar and schizophrenia,

3:05:09.560 --> 3:05:13.160
 these things are clearly real, somewhat biological.

3:05:13.160 --> 3:05:14.480
 And then over here on the spectrum,

3:05:14.480 --> 3:05:18.360
 you have like ADD and oppositional defiance disorder

3:05:18.360 --> 3:05:20.760
 and these things that are like,

3:05:20.760 --> 3:05:22.800
 wait, this is normal spectrum human behavior.

3:05:22.800 --> 3:05:27.800
 Like this isn't, you know, where's the line here

3:05:28.640 --> 3:05:31.320
 and why is this like a problem?

3:05:31.320 --> 3:05:33.280
 So there's this whole, you know,

3:05:33.280 --> 3:05:35.840
 the neurodiversity of humanity is huge.

3:05:35.840 --> 3:05:37.640
 Like people think I'm always on drugs.

3:05:37.640 --> 3:05:38.920
 People are saying this to me on my streams.

3:05:38.920 --> 3:05:39.760
 And I'm like, guys, you know,

3:05:39.760 --> 3:05:41.320
 like I'm real open with my drug use.

3:05:41.320 --> 3:05:44.080
 I'd tell you if I was on drugs and yeah,

3:05:44.080 --> 3:05:45.640
 I had like a cup of coffee this morning,

3:05:45.640 --> 3:05:47.240
 but other than that, this is just me.

3:05:47.240 --> 3:05:49.840
 You're witnessing my brain in action.

3:05:51.400 --> 3:05:55.600
 So the word madness doesn't even make sense

3:05:55.600 --> 3:05:59.680
 in the rich neurodiversity of humans.

3:05:59.680 --> 3:06:04.600
 I think it makes sense, but only for like

3:06:04.600 --> 3:06:07.040
 some insane extremes.

3:06:07.040 --> 3:06:11.720
 Like if you are actually like visibly hallucinating,

3:06:11.720 --> 3:06:15.000
 you know, that's okay.

3:06:15.000 --> 3:06:17.440
 But there is the kind of spectrum on which you stand out.

3:06:17.440 --> 3:06:22.000
 Like that's like, if I were to look, you know,

3:06:22.000 --> 3:06:25.080
 at decorations on a Christmas tree or something like that,

3:06:25.080 --> 3:06:28.760
 like if you were a decoration, that would catch my eye.

3:06:28.760 --> 3:06:33.760
 Like that thing is sparkly, whatever the hell that thing is.

3:06:35.360 --> 3:06:37.360
 There's something to that.

3:06:37.360 --> 3:06:42.120
 Just like refusing to be boring

3:06:42.120 --> 3:06:43.920
 or maybe boring is the wrong word,

3:06:43.920 --> 3:06:48.920
 but to yeah, I mean, be willing to sparkle, you know?

3:06:52.440 --> 3:06:54.200
 It's like somewhat constructed.

3:06:54.200 --> 3:06:57.000
 I mean, I am who I choose to be.

3:06:57.000 --> 3:07:01.000
 I'm gonna say things as true as I can see them.

3:07:01.000 --> 3:07:04.480
 I'm not gonna lie.

3:07:04.480 --> 3:07:06.600
 But that's a really important feature in itself.

3:07:06.600 --> 3:07:09.080
 So like whatever the neurodiversity of your,

3:07:09.080 --> 3:07:13.800
 whatever your brain is, not putting constraints on it

3:07:13.800 --> 3:07:18.720
 that force it to fit into the mold of what society is like,

3:07:18.720 --> 3:07:20.640
 defines what you're supposed to be.

3:07:20.640 --> 3:07:22.360
 So you're one of the specimens

3:07:22.360 --> 3:07:27.360
 that doesn't mind being yourself.

3:07:27.800 --> 3:07:31.720
 Being right is super important,

3:07:31.720 --> 3:07:33.760
 except at the expense of being wrong.

3:07:37.240 --> 3:07:38.480
 Without breaking that apart,

3:07:38.480 --> 3:07:40.400
 I think it's a beautiful way to end it.

3:07:40.400 --> 3:07:43.080
 George, you're one of the most special humans I know.

3:07:43.080 --> 3:07:44.600
 It's truly an honor to talk to you.

3:07:44.600 --> 3:07:45.800
 Thanks so much for doing it.

3:07:45.800 --> 3:07:47.640
 Thank you for having me.

3:07:47.640 --> 3:07:50.360
 Thanks for listening to this conversation with George Hotz

3:07:50.360 --> 3:07:52.320
 and thank you to our sponsors,

3:07:52.320 --> 3:07:54.640
 Four Sigmatic, which is the maker

3:07:54.640 --> 3:07:57.160
 of delicious mushroom coffee,

3:07:57.160 --> 3:07:59.880
 Decoding Digital, which is a tech podcast

3:07:59.880 --> 3:08:02.120
 that I listen to and enjoy,

3:08:02.120 --> 3:08:07.080
 and ExpressVPN, which is the VPN I've used for many years.

3:08:07.080 --> 3:08:09.240
 Please check out these sponsors in the description

3:08:09.240 --> 3:08:13.120
 to get a discount and to support this podcast.

3:08:13.120 --> 3:08:15.520
 If you enjoy this thing, subscribe on YouTube,

3:08:15.520 --> 3:08:17.920
 review it with Five Stars and Apple Podcast,

3:08:17.920 --> 3:08:20.680
 follow on Spotify, support on Patreon,

3:08:20.680 --> 3:08:24.480
 or connect with me on Twitter at Lex Friedman.

3:08:24.480 --> 3:08:27.040
 And now, let me leave you with some words

3:08:27.040 --> 3:08:30.720
 from the great and powerful Linus Torvalds.

3:08:30.720 --> 3:08:33.120
 Talk is cheap, show me the code.

3:08:33.120 --> 3:08:50.120
 Thank you for listening and hope to see you next time.

