WEBVTT

00:00.000 --> 00:07.280
 The following is a conversation with Elon Musk, Part 2, the second time we spoke on the podcast,

00:07.280 --> 00:13.120
 with parallels, if not in quality, than an outfit, to the objectively speaking greatest

00:13.120 --> 00:20.720
 sequel of all time, Godfather Part 2. As many people know, Elon Musk is a leader of Tesla,

00:20.720 --> 00:26.560
 SpaceX, Neuralink, and the Boring Company. What may be less known is that he's a world

00:26.560 --> 00:32.480
 class engineer and designer, constantly emphasizing first principles thinking and taking on big

00:32.480 --> 00:39.600
 engineering problems that many before him will consider impossible. As scientists and engineers,

00:39.600 --> 00:44.160
 most of us don't question the way things are done, we simply follow the momentum of the crowd.

00:44.880 --> 00:51.520
 But revolutionary ideas that change the world on the small and large scales happen when you

00:51.520 --> 00:57.840
 return to the fundamentals and ask, is there a better way? This conversation focuses on the

00:57.840 --> 01:02.960
 incredible engineering and innovation done in brain computer interfaces at Neuralink.

01:04.160 --> 01:09.440
 This work promises to help treat neurobiological diseases to help us further understand the

01:09.440 --> 01:14.400
 connection between the individual neuron to the high level function of the human brain.

01:14.400 --> 01:20.240
 And finally, to one day expand the capacity of the brain through two way communication

01:20.240 --> 01:24.640
 with computational devices, the internet, and artificial intelligence systems.

01:25.440 --> 01:31.040
 This is the Artificial Intelligence Podcast. If you enjoy it, subscribe by YouTube,

01:31.040 --> 01:36.320
 Apple Podcasts, Spotify, support on Patreon, or simply connect with me on Twitter

01:36.320 --> 01:43.520
 at Lex Friedman, spelled F R I D M A N. And now, as an anonymous YouTube commenter referred to

01:43.520 --> 01:49.440
 our previous conversation as the quote, historical first video of two robots conversing without

01:49.440 --> 01:56.560
 supervision, here's the second time, the second conversation with Elon Musk.

01:57.840 --> 02:03.120
 Let's start with an easy question about consciousness. In your view, is consciousness

02:03.120 --> 02:07.600
 something that's unique to humans or is it something that permeates all matter, almost like

02:07.600 --> 02:13.680
 a fundamental force of physics? I don't think consciousness permeates all matter. Panpsychists

02:13.680 --> 02:21.120
 believe that. Yeah. There's a philosophical. How would you tell? That's true. That's a good point.

02:21.120 --> 02:24.240
 I believe in scientific methods. I don't know about your mind or anything, but the scientific

02:24.240 --> 02:28.800
 method is like, if you cannot test the hypothesis, then you cannot reach meaningful conclusion that

02:28.800 --> 02:34.000
 it is true. Do you think consciousness, understanding consciousness is within the

02:34.000 --> 02:40.160
 reach of science of the scientific method? We can dramatically improve our understanding of

02:40.160 --> 02:47.120
 consciousness. You know, hot press to say that we understand anything with complete accuracy,

02:47.120 --> 02:51.360
 but can we dramatically improve our understanding of consciousness? I believe the answer is yes.

02:53.360 --> 02:58.480
 Does an AI system in your view have to have consciousness in order to achieve human level

02:58.480 --> 03:03.360
 or superhuman level intelligence? Does it need to have some of these human qualities that

03:03.360 --> 03:11.120
 consciousness, maybe a body, maybe a fear of mortality, capacity to love those kinds of

03:11.120 --> 03:19.440
 silly human things? There's a different, you know, there's this, there's the scientific method,

03:19.440 --> 03:25.200
 which I very much believe in where something is true to the degree that it is testably. So

03:25.200 --> 03:34.560
 and otherwise, you're really just talking about, you know, preferences or untestable beliefs or

03:34.560 --> 03:41.520
 that, you know, that kind of thing. So it ends up being somewhat of a semantic question, where

03:42.320 --> 03:46.880
 we were conflating a lot of things with the word intelligence. If we parse them out and say,

03:46.880 --> 03:56.240
 you know, are we headed towards the future where an AI will be able to outthink us in every way?

03:57.520 --> 03:59.440
 Then the answer is unequivocally yes.

04:01.440 --> 04:06.400
 In order for an AI system that needs to outthink us in every way, it also needs to have

04:07.360 --> 04:12.320
 a capacity to have consciousness, self awareness, and understanding.

04:12.320 --> 04:18.400
 It will be self aware. Yes, that's different from consciousness. I mean, to me, in terms of what

04:18.400 --> 04:21.840
 what consciousness feels like, it feels like consciousness is in a different dimension.

04:22.640 --> 04:30.480
 But this is this could be just an illusion. You know, if you damage your brain in some way,

04:30.480 --> 04:35.920
 physically, you get you, you damage your consciousness, which implies that consciousness

04:35.920 --> 04:42.720
 is a physical phenomenon. And in my view, the thing is that that I think are really quite,

04:42.720 --> 04:48.880
 quite likely is that digital intelligence will be able to outthink us in every way, and it will

04:48.880 --> 04:54.080
 simply be able to simulate what we consider consciousness. So to the degree that you would

04:54.080 --> 04:58.160
 not be able to tell the difference. And from the from the aspect of the scientific method,

04:58.160 --> 05:01.440
 it's might as well be consciousness, if we can simulate it perfectly.

05:01.440 --> 05:06.800
 If you can't tell the difference, when this is sort of the Turing test, but think of a more

05:06.800 --> 05:13.600
 sort of advanced version of the Turing test. If you're if you're talking to a digital super

05:13.600 --> 05:19.440
 intelligence and can't tell if that is a computer or a human, like let's say you're just having

05:19.440 --> 05:26.480
 conversation over a phone or a video conference or something where you're you think you're talking

05:26.480 --> 05:33.360
 looks like a person makes all of the right inflections and movements and all the small

05:33.360 --> 05:41.360
 subtleties that constitute a human and talks like human makes mistakes like a human like

05:42.400 --> 05:49.120
 and you literally just can't tell is this Are you video conferencing with a person or or an AI

05:49.120 --> 05:54.960
 might as well might as well be human. So on a darker topic, you've expressed serious concern

05:54.960 --> 06:02.400
 about existential threats of AI. It's perhaps one of the greatest challenges our civilization faces,

06:02.400 --> 06:08.480
 but since I would say we're kind of an optimistic descendants of apes, perhaps we can find several

06:08.480 --> 06:16.480
 paths of escaping the harm of AI. So if I can give you an example of an example of an example

06:16.480 --> 06:21.920
 of escaping the harm of AI. So if I can give you three options, maybe you can comment which do you

06:21.920 --> 06:29.040
 think is the most promising. So one is scaling up efforts on AI safety and beneficial AI research

06:29.040 --> 06:35.920
 in hope of finding an algorithmic or maybe a policy solution. Two is becoming a multi planetary

06:35.920 --> 06:44.000
 species as quickly as possible. And three is merging with AI and riding the wave of that

06:44.000 --> 06:49.280
 increasing intelligence as it continuously improves. What do you think is most promising,

06:49.280 --> 06:52.720
 most interesting, as a civilization that we should invest in?

06:54.640 --> 06:59.200
 I think there's a lot of tremendous amount of investment going on in AI, where there's a lack

06:59.200 --> 07:06.640
 of investment is in AI safety. And there should be in my view, a government agency that oversees

07:07.760 --> 07:12.960
 anything related to AI to confirm that it is does not represent a public safety risk,

07:12.960 --> 07:19.120
 just as there is a regulatory authority for the Food and Drug Administration is that's for

07:20.320 --> 07:25.920
 automotive safety, there's the FAA for aircraft safety, which I really come to the conclusion that

07:25.920 --> 07:31.120
 it is important to have a government referee or referee that is serving the public interest

07:31.120 --> 07:37.120
 in ensuring that things are safe when when there's a potential danger to the public.

07:37.120 --> 07:43.920
 I would argue that AI is unequivocally something that has potential to be dangerous to the public,

07:43.920 --> 07:48.480
 and therefore should have a regulatory agency just as other things that are dangerous to the public

07:48.480 --> 07:54.240
 have a regulatory agency. But let me tell you, the problem with this is that the government

07:54.240 --> 08:01.920
 moves very slowly. And the rate of the rate, the usually way a regulatory agency comes into being

08:01.920 --> 08:09.120
 is that something terrible happens. There's a huge public outcry. And years after that,

08:09.680 --> 08:15.120
 there's a regulatory agency or a rule put in place, take something like, like seatbelts,

08:15.120 --> 08:25.840
 it was known for a decade or more that seatbelts would have a massive impact on safety and save so

08:25.840 --> 08:32.000
 many lives in serious injuries. And the car industry fought the requirement to put seatbelts in

08:32.000 --> 08:39.600
 tooth and nail. That's crazy. Yeah. And hundreds of 1000s of people probably died because of that.

08:41.040 --> 08:44.080
 And they said people wouldn't buy cars if they had seatbelts, which is obviously absurd.

08:45.680 --> 08:51.920
 Yeah, or look at the tobacco industry and how long they fought any thing about smoking. That's part

08:51.920 --> 08:58.320
 of why I helped make that movie. Thank you for smoking. You can sort of see just how pernicious

08:58.320 --> 09:09.680
 it can be when you have these companies effectively achieve regulatory capture of government. The bad

09:11.280 --> 09:17.040
 people in the community refer to the advent of digital super intelligence as a singularity.

09:17.040 --> 09:23.680
 That is not to say that it is good or bad, but that it is very difficult to predict what will

09:23.680 --> 09:28.480
 happen after that point. And then there's some probability it will be bad, some probably it'll

09:28.480 --> 09:34.720
 be it will be good. We obviously want to affect that probability and have it be more good than bad.

09:35.920 --> 09:40.960
 Well, let me on the merger with AI question and the incredible work that's being done at Neuralink.

09:40.960 --> 09:47.280
 There's a lot of fascinating innovation here across different disciplines going on. So the flexible

09:47.280 --> 09:52.960
 wires, the robotic sewing machine, that responsive brain movement, everything around ensuring safety

09:52.960 --> 10:02.560
 and so on. So we currently understand very little about the human brain. Do you also hope that the

10:02.560 --> 10:07.840
 work at Neuralink will help us understand more about our about our human brain?

10:07.840 --> 10:13.840
 Yeah, I think the work in Neuralink will definitely shed a lot of insight into how the brain, the mind

10:13.840 --> 10:20.640
 works. Right now, just the data we have regarding how the brain works is very limited. You know,

10:20.640 --> 10:28.480
 we've got fMRI, which is that that's kind of like putting us, you know, stethoscope on the outside

10:28.480 --> 10:33.200
 of a factory wall and then putting it like all over the factory wall and you can sort of hear

10:33.200 --> 10:38.720
 the sounds, but you don't know what the machines are doing, really. It's hard. You can infer a few

10:38.720 --> 10:43.200
 things, but it's very broad brushstroke. In order to really know what's going on in the brain,

10:43.200 --> 10:47.600
 you really need you have to have high precision sensors. And then you want to have stimulus and

10:47.600 --> 10:53.280
 response. Like if you trigger a neuron, what, how do you feel? What do you see? How does it change

10:53.280 --> 10:57.200
 your perception of the world? You're speaking to physically just getting close to the brain,

10:57.200 --> 11:00.400
 being able to measure signals, how do you know what's going on in the brain?

11:00.400 --> 11:04.160
 Physically, just getting close to the brain, being able to measure signals from the brain

11:04.160 --> 11:07.680
 will give us sort of open the door inside the factory.

11:08.480 --> 11:15.280
 Yes, exactly. Being able to have high precision sensors that tell you what individual neurons

11:15.280 --> 11:20.960
 are doing. And then being able to trigger a neuron and see what the response is in the brain.

11:22.000 --> 11:28.720
 So you can see the consequences of if you fire this neuron, what happens? How do you feel? What

11:28.720 --> 11:34.160
 does it change? It'll be really profound to have this in people because people can articulate

11:35.520 --> 11:43.040
 their change. Like if there's a change in mood, or if they can tell you if they can see better,

11:43.040 --> 11:51.040
 or hear better, or be able to form sentences better or worse, or their memories are jogged,

11:51.040 --> 11:56.880
 or that kind of thing. So on the human side, there's this incredible general malleability,

11:56.880 --> 12:01.040
 plasticity of the human brain, the human brain adapts, adjusts, and so on.

12:01.040 --> 12:03.200
 So that's not that plastic, to be totally frank.

12:03.200 --> 12:09.040
 So there's a firm structure, but nevertheless, there's some plasticity. And the open question is,

12:09.040 --> 12:15.120
 sort of, if I could ask a broad question is how much that plasticity can be utilized. Sort of,

12:15.120 --> 12:20.560
 on the human side, there's some plasticity in the human brain. And on the machine side,

12:20.560 --> 12:26.640
 we have neural networks, machine learning, artificial intelligence, it's able to adjust

12:26.640 --> 12:31.760
 and figure out signals. So there's a mysterious language that we don't perfectly understand

12:31.760 --> 12:37.120
 that's within the human brain. And then we're trying to understand that language to communicate

12:37.120 --> 12:42.160
 both directions. So the brain is adjusting a little bit, we don't know how much, and the

12:42.160 --> 12:48.080
 machine is adjusting. Where do you see, as they try to sort of reach together, almost like with

12:48.080 --> 12:53.600
 an alien species, try to find a protocol, communication protocol that works? Where do

12:53.600 --> 12:59.360
 you see the biggest, the biggest benefit arriving from on the machine side or the human side? Do you

12:59.360 --> 13:03.680
 see both of them working together? I think the machine side is far more malleable than the

13:03.680 --> 13:12.480
 biological side, by a huge amount. So it'll be the machine that adapts to the brain. That's the only

13:12.480 --> 13:19.120
 thing that's possible. The brain can't adapt that well to the machine. You can't have neurons start

13:19.120 --> 13:24.960
 to regard an electrode as another neuron, because neurons just, there's like the pulse. And so

13:24.960 --> 13:31.520
 something else is pulsing. So there is that elasticity in the interface, which we believe is

13:32.320 --> 13:37.520
 something that can happen. But the vast majority of the malleability will have to be on the machine

13:37.520 --> 13:42.800
 side. But it's interesting, when you look at that synaptic plasticity at the interface side,

13:43.680 --> 13:48.560
 there might be like an emergent plasticity. Because it's a whole nother, it's not like in the

13:48.560 --> 13:53.840
 brain, it's a whole nother extension of the brain. You know, we might have to redefine what it means

13:53.840 --> 13:59.440
 to be malleable for the brain. So maybe the brain is able to adjust to external interfaces. There

13:59.440 --> 14:03.680
 will be some adjustments to the brain, because there's going to be something reading and simulating

14:03.680 --> 14:12.400
 the brain. And so it will adjust to that thing. But most, the vast majority of the adjustment

14:12.400 --> 14:18.720
 will be on the machine side. This is just, this is just, it has to be that otherwise it will not

14:18.720 --> 14:23.440
 work. Ultimately, like, we currently operate on two layers, we have sort of a limbic, like prime

14:23.440 --> 14:29.680
 primitive brain layer, which is where all of our kind of impulses are coming from. It's sort of

14:29.680 --> 14:34.720
 like we've got, we've got like a monkey brain with a computer stuck on it. That's that's the

14:34.720 --> 14:38.480
 human brain. And a lot of our impulses and everything are driven by the monkey brain.

14:39.360 --> 14:44.720
 And the computer, the cortex is constantly trying to make the monkey brain happy.

14:44.720 --> 14:49.040
 It's not the cortex that's steering the monkey brains, the monkey brain steering the cortex.

14:51.040 --> 14:56.000
 You know, the cortex is the part that tells the story of the whole thing. So we convince ourselves

14:56.000 --> 15:01.360
 it's, it's more interesting than just the monkey brain. The cortex is like what we call like human

15:01.360 --> 15:05.280
 intelligence. You know, it's just like, that's like the advanced computer relative to other

15:05.280 --> 15:11.840
 creatures. The other creatures do not have either. Really, they don't, they don't have the

15:11.840 --> 15:19.840
 computer, or they have a very weak computer relative to humans. But it's, it's like, it sort

15:19.840 --> 15:24.880
 of seems like surely the really smart thing should control the dumb thing. But actually,

15:24.880 --> 15:30.160
 the dumb thing controls the smart thing. So do you think some of the same kind of machine learning

15:30.160 --> 15:35.920
 methods, whether that's natural language processing applications are going to be applied for the

15:35.920 --> 15:43.040
 communication between the machine and the brain to learn how to do certain things like movement

15:43.040 --> 15:50.320
 of the body, how to process visual stimuli, and so on. Do you see the value of using machine

15:50.320 --> 15:55.440
 learning to understand the language of the two way communication with the brain? Sure. Yeah,

15:55.440 --> 16:02.240
 absolutely. I mean, we're neural net. And that, you know, AI is basically neural net.

16:02.800 --> 16:06.000
 So it's like digital neural net will interface with biological neural net.

16:08.160 --> 16:14.320
 And hopefully bring us along for the ride. Yeah. But the vast majority of our intelligence will be

16:14.320 --> 16:23.120
 digital. Like, so like, think of like, the difference in intelligence between your cortex

16:23.120 --> 16:29.840
 and your limbic system is gigantic, your limbic system really has no comprehension of what the

16:29.840 --> 16:40.240
 hell the cortex is doing. It's just literally hungry, you know, or tired or angry or sexy or

16:40.240 --> 16:46.480
 something, you know, that's just and then that communicates that that impulse to the cortex and

16:47.600 --> 16:54.480
 tells the cortex to go satisfy that then love a great deal of like, a massive amount of thinking,

16:54.480 --> 17:00.960
 like truly stupendous amount of thinking has gone into sex without purpose, without procreation,

17:00.960 --> 17:11.440
 without procreation. Which is actually quite a silly action in the absence of procreation. It's

17:11.440 --> 17:16.960
 a bit silly. Why are you doing it? Because it makes the limbic system happy. That's why. That's why.

17:17.840 --> 17:24.880
 But it's pretty absurd, really. Well, the whole of existence is pretty absurd in some kind of sense.

17:24.880 --> 17:32.160
 Yeah. But I mean, this is a lot of computation has gone into how can I do more of that with

17:32.160 --> 17:37.440
 procreation not even being a factor? This is, I think, a very important area of research by NSFW.

17:40.160 --> 17:44.160
 An agency that should receive a lot of funding, especially after this conversation.

17:44.160 --> 17:48.480
 I propose the formation of a new agency. Oh, boy.

17:48.480 --> 17:53.520
 What is the most exciting or some of the most exciting things that you see in the future impact

17:53.520 --> 17:58.080
 of Neuralink, both in the science, the engineering and societal broad impact?

17:59.120 --> 18:05.600
 Neuralink, I think, at first will solve a lot of brain related diseases. So it could be anything

18:05.600 --> 18:11.600
 from like autism, schizophrenia, memory loss, like everyone experiences memory loss at certain points

18:11.600 --> 18:16.480
 in age. Parents can't remember their kids names and that kind of thing. So it could be anything

18:16.480 --> 18:19.280
 from like autism, schizophrenia, memory loss, like everyone experiences memory loss at certain points

18:19.280 --> 18:24.400
 in age. Parents can't remember their kids names and that kind of thing. So there's a tremendous

18:24.400 --> 18:34.480
 amount of good that Neuralink can do in solving critical damage to the brain or the spinal cord.

18:34.480 --> 18:40.720
 There's a lot that can be done to improve quality of life of individuals. And those will be steps

18:40.720 --> 18:48.240
 to address the existential risk associated with digital superintelligence. Like we will not be

18:48.240 --> 18:56.880
 able to be smarter than a digital supercomputer. So therefore, if you cannot beat them, join them.

18:58.240 --> 18:59.680
 And at least we won't have that option.

19:01.520 --> 19:09.200
 So you have hope that Neuralink will be able to be a kind of connection to allow us to merge,

19:09.200 --> 19:14.640
 the wave of the improving AI systems. I think the chance is above zero percent.

19:15.600 --> 19:20.720
 So it's non zero. There's a chance. And that's what I've seen. Dumb and Dumber.

19:21.920 --> 19:26.400
 Yes. So I'm saying there's a chance. He's saying one in a billion or one in a million,

19:26.400 --> 19:30.560
 whatever it was, a dumb and dumber. You know, it went from maybe one in a million to improving.

19:31.120 --> 19:35.040
 Maybe it'll be one in a thousand and then one in a hundred, then one in ten. Depends on the rate

19:35.040 --> 19:40.400
 of improvement of Neuralink and how fast we're able to do make progress.

19:41.040 --> 19:45.440
 Well, I've talked to a few folks here that are quite brilliant engineers, so I'm excited.

19:45.440 --> 19:47.200
 Yeah, I think it's like fundamentally good, you know,

19:48.400 --> 19:52.400
 giving somebody back full motor control after they've had a spinal cord injury.

19:53.840 --> 19:56.240
 You know, restoring brain functionality after a stroke,

19:57.920 --> 20:02.160
 solving debilitating genetically oriented brain diseases. These are all incredibly

20:02.160 --> 20:07.440
 great, I think. And in order to do these, you have to be able to interface with neurons at

20:07.440 --> 20:12.160
 a detailed level and you need to be able to fire the right neurons, read the right neurons, and

20:13.200 --> 20:18.720
 and then effectively you can create a circuit, replace what's broken with

20:19.760 --> 20:26.000
 with silicon and essentially fill in the missing functionality. And then over time,

20:26.000 --> 20:31.120
 we can develop a tertiary layer. So if like the limbic system is the primary layer, then the

20:31.120 --> 20:36.320
 cortex is like the second layer. And as I said, obviously the cortex is vastly more intelligent

20:36.320 --> 20:40.080
 than the limbic system, but people generally like the fact that they have a limbic system

20:40.080 --> 20:44.480
 and a cortex. I haven't met anyone who wants to delete either one of them. They're like,

20:44.480 --> 20:47.440
 okay, I'll keep them both. That's cool. The limbic system is kind of fun.

20:47.440 --> 20:53.360
 That's where the fun is, absolutely. And then people generally don't want to lose their

20:53.360 --> 20:59.360
 cortex either. They're like having the cortex and the limbic system. And then there's a tertiary

20:59.360 --> 21:05.520
 layer, which will be digital superintelligence. And I think there's room for optimism given that

21:05.520 --> 21:11.760
 the cortex, the cortex is very intelligent and limbic system is not, and yet they work together

21:11.760 --> 21:18.560
 well. Perhaps there can be a tertiary layer where digital superintelligence lies, and that will be

21:18.560 --> 21:24.880
 vastly more intelligent than the cortex, but still coexist peacefully and in a benign manner with the

21:24.880 --> 21:30.320
 cortex and limbic system. That's a super exciting future, both in low level engineering that I saw

21:30.320 --> 21:36.080
 as being done here and the actual possibility in the next few decades. It's important that

21:36.080 --> 21:40.880
 Neuralink solve this problem sooner rather than later, because the point at which we have digital

21:40.880 --> 21:45.440
 superintelligence, that's when we pass the singularity and things become just very uncertain.

21:45.440 --> 21:48.640
 It doesn't mean that they're necessarily bad or good. For the point at which we pass singularity,

21:48.640 --> 21:55.440
 things become extremely unstable. So we want to have a human brain interface before the singularity,

21:55.440 --> 22:01.360
 or at least not long after it, to minimize existential risk for humanity and consciousness

22:01.360 --> 22:07.200
 as we know it. So there's a lot of fascinating actual engineering, low level problems here at

22:07.200 --> 22:15.600
 Neuralink that are quite exciting. The problems that we face in Neuralink are material science,

22:15.600 --> 22:21.520
 electrical engineering, software, mechanical engineering, microfabrication. It's a bunch of

22:22.560 --> 22:26.080
 engineering disciplines, essentially. That's what it comes down to, is you have to have a

22:26.080 --> 22:35.520
 tiny electrode, so small it doesn't hurt neurons, but it's got to last for as long as a person. So

22:35.520 --> 22:40.880
 it's going to last for decades. And then you've got to take that signal, you've got to process

22:40.880 --> 22:48.800
 that signal locally at low power. So we need a lot of chip design engineers, because we're going to

22:48.800 --> 22:56.320
 do signal processing, and do so in a very power efficient way, so that we don't heat your brain

22:56.320 --> 23:01.040
 up, because the brain is very heat sensitive. And then we've got to take those signals and

23:01.040 --> 23:10.080
 we're going to do something with them. And then we've got to stimulate the back to bidirectional

23:10.080 --> 23:15.360
 communication. So somebody's good at material science, software, and we've got to do a lot of

23:15.360 --> 23:20.880
 that. So somebody's good at material science, software, mechanical engineering, electrical

23:20.880 --> 23:26.080
 engineering, chip design, microfabrication. Those are the things we need to work on.

23:27.520 --> 23:32.080
 We need to be good at material science, so that we can have tiny electrodes that last a long time.

23:32.080 --> 23:35.760
 And it's a tough thing with the material science problem, it's a tough one, because

23:35.760 --> 23:43.680
 you're trying to read and simulate electrically in an electrically active area. Your brain is

23:43.680 --> 23:49.520
 very electrically active and electrochemically active. So how do you have a coating on the

23:49.520 --> 23:57.200
 electrode that doesn't dissolve over time and is safe in the brain? This is a very hard problem.

23:59.040 --> 24:06.880
 And then how do you collect those signals in a way that is most efficient? Because you really

24:06.880 --> 24:12.720
 just have very tiny amounts of power to process those signals. And then we need to automate the

24:12.720 --> 24:20.960
 whole thing so it's like LASIK. If this is done by neurosurgeons, there's no way it can scale to

24:20.960 --> 24:24.800
 a large number of people. And it needs to scale to a large number of people, because I think

24:24.800 --> 24:32.720
 ultimately we want the future to be determined by a large number of humans. Do you think that

24:32.720 --> 24:39.040
 this has a chance to revolutionize surgery period? So neurosurgery and surgery all across?

24:39.040 --> 24:45.120
 Yeah, for sure. It's got to be like LASIK. If LASIK had to be done by hand by a person,

24:45.680 --> 24:54.320
 that wouldn't be great. It's done by a robot. And the ophthalmologist kind of just needs to make

24:54.320 --> 24:58.480
 sure your head's in the right position, and then they just press a button and go.

25:00.000 --> 25:05.920
 SmartSummon and soon Autopark takes on the full beautiful mess of parking lots and their human

25:05.920 --> 25:13.680
 to human nonverbal communication. I think it has actually the potential to have a profound impact

25:13.680 --> 25:19.440
 in changing how our civilization looks at AI and robotics, because this is the first time human

25:19.440 --> 25:24.080
 beings, people that don't own a Tesla may have never seen a Tesla or heard about a Tesla,

25:24.080 --> 25:30.880
 get to watch hundreds of thousands of cars without a driver. Do you see it this way, almost like an

25:30.880 --> 25:36.080
 education tool for the world about AI? Do you feel the burden of that, the excitement of that,

25:36.080 --> 25:42.160
 or do you just think it's a smart parking feature? I do think you are getting at something

25:42.160 --> 25:47.680
 important, which is most people have never really seen a robot. And what is the car that is

25:47.680 --> 25:53.200
 autonomous? It's a four wheeled robot. Yeah, it communicates a certain sort of message with

25:53.200 --> 25:59.520
 everything from safety to the possibility of what AI could bring to its current limitations,

25:59.520 --> 26:04.000
 its current challenges, it's what's possible. Do you feel the burden of that almost like a

26:04.000 --> 26:09.600
 communicator educator to the world about AI? We were just really trying to make people's

26:09.600 --> 26:15.040
 lives easier with autonomy. But now that you mentioned it, I think it will be an eye opener

26:15.040 --> 26:19.920
 to people about robotics, because they've really never seen most people never seen a robot. And

26:20.960 --> 26:25.440
 there are hundreds of thousands of Tesla's won't be long before there's a million of them that

26:25.440 --> 26:31.760
 have autonomous capability, and the drive without a person in it. And you can see the kind of

26:31.760 --> 26:40.080
 evolution of the car's personality and, and thinking with each iteration of autopilot,

26:40.080 --> 26:47.600
 you can see it's, it's uncertain about this, or it gets it, but now it's more certain. Now it's

26:47.600 --> 26:53.200
 moving in a slightly different way. Like, I can tell immediately if a car is on Tesla autopilot,

26:53.200 --> 26:56.880
 because it's got just little nuances of movement, it just moves in a slightly different way.

26:58.720 --> 27:02.960
 Cars on Tesla autopilot, for example, on the highway are far more precise about being in the

27:02.960 --> 27:08.960
 center of the lane than a person. If you drive down the highway and look at how at where cars

27:08.960 --> 27:13.840
 are, the human driven cars are within their lane, they're like bumper cars. They're like moving all

27:13.840 --> 27:20.720
 over the place. The car in autopilot, dead center. Yeah, so the incredible work that's going into

27:20.720 --> 27:27.040
 that neural network, it's learning fast. Autonomy is still very, very hard. We don't actually know

27:27.040 --> 27:33.840
 how hard it is fully, of course. You look at the most problems you tackle, this one included,

27:34.880 --> 27:39.520
 with an exponential lens, but even with an exponential improvement, things can take longer

27:39.520 --> 27:47.840
 than expected sometimes. So where does Tesla currently stand on its quest for full autonomy?

27:47.840 --> 27:54.720
 What's your sense? When can we see successful deployment of full autonomy?

27:55.840 --> 28:00.160
 Well, on the highway already, the the probability of intervention is extremely low.

28:00.160 --> 28:08.480
 Yes. So for highway autonomy, with the latest release, especially the probability of needing

28:08.480 --> 28:13.200
 to intervene is really quite low. In fact, I'd say for stop and go traffic,

28:13.200 --> 28:18.880
 it's far safer than a person right now. The probability of an injury or impact is much,

28:18.880 --> 28:25.360
 much lower for autopilot than a person. And then with navigating autopilot, you can change lanes,

28:25.360 --> 28:30.320
 take highway interchanges, and then we're coming at it from the other direction, which is low speed,

28:30.320 --> 28:35.920
 full autonomy. And in a way, this is like, how does a person learn to drive? You learn to drive

28:35.920 --> 28:40.720
 in the parking lot. You know, the first time you learn to drive probably wasn't jumping on

28:40.720 --> 28:45.200
 August Street in San Francisco. That'd be crazy. You learn to drive in the parking lot, get things

28:45.200 --> 28:52.400
 get things right at low speed. And then the missing piece that we're working on is traffic

28:52.400 --> 28:59.200
 lights and stop streets. Stop streets, I would say actually also relatively easy, because, you know,

28:59.200 --> 29:04.320
 you kind of know where the stop street is, worst case in geocoders, and then use visualization to

29:04.320 --> 29:10.720
 see where the line is and stop at the line to eliminate the GPS error. So actually, I'd say it's

29:10.720 --> 29:19.680
 probably complex traffic lights and very windy roads are the two things that need to get solved.

29:19.680 --> 29:24.000
 What's harder, perception or control for these problems? So being able to perfectly perceive

29:24.000 --> 29:29.600
 everything, or figuring out a plan once you perceive everything, how to interact with all the

29:29.600 --> 29:35.440
 agents in the environment in your sense, from a learning perspective, is perception or action

29:35.440 --> 29:42.240
 harder? And that giant, beautiful multitask learning neural network, the hottest thing is

29:42.240 --> 29:48.960
 having accurate representation of the physical objects in vector space. So transfer taking the

29:48.960 --> 29:56.880
 visual input, primarily visual input, some sonar and radar, and and then creating the an accurate

29:56.880 --> 30:02.400
 vector space representation of the objects around you. Once you have an accurate vector space

30:02.400 --> 30:08.160
 representation, the planning and control is relatively easier. That is relatively easy.

30:08.160 --> 30:14.560
 Basically, once you have accurate vector space representation, then you're kind of like a video

30:14.560 --> 30:19.600
 game, like cars and like Grand Theft Auto or something like they work pretty well. They drive

30:19.600 --> 30:24.160
 down the road, they don't crash, you know, pretty much unless you crash into them. That's because

30:24.160 --> 30:27.360
 they've they've got an accurate vector space representation of where the cars are, and they're

30:27.360 --> 30:33.520
 just and then they're rendering that as the as the output. Do you have a sense, high level, that

30:33.520 --> 30:42.000
 Tesla's on track on being able to achieve full autonomy? So on the highway? Yeah, absolutely.

30:42.000 --> 30:48.320
 And still no driver state, driver sensing? And we have driver sensing with torque on the wheel.

30:48.320 --> 30:55.120
 That's right. Yeah. By the way, just a quick comment on karaoke. Most people think it's fun,

30:55.120 --> 30:59.040
 but I also think it is a driving feature. I've been saying for a long time, singing in the car

30:59.040 --> 31:02.720
 is really good for attention management and vigilance management. That's right.

31:02.720 --> 31:08.480
 Tesla karaoke is great. It's one of the most fun features of the car. Do you think of a connection

31:08.480 --> 31:12.640
 between fun and safety sometimes? Yeah, you can do both at the same time. That's great.

31:12.640 --> 31:19.760
 I just met with Andrew and wife of Carl Sagan, directed Cosmos. I'm generally a big fan of Carl

31:19.760 --> 31:25.280
 Sagan. He's super cool. And had a great way of putting things. All of our consciousness,

31:25.280 --> 31:29.360
 all civilization, everything we've ever known and done is on this tiny blue dot.

31:29.920 --> 31:34.720
 People also get they get too trapped in there. This is like squabbles amongst humans.

31:34.720 --> 31:39.680
 Let's not think of the big picture. They take civilization and our continued existence for

31:39.680 --> 31:47.120
 granted. I shouldn't do that. Look at the history of civilizations. They rise and they fall. And now

31:47.760 --> 31:56.480
 civilization is all it's globalized. And so civilization, I think now rises and falls together.

31:56.480 --> 32:05.120
 There's no there's not geographic isolation. This is a big risk. Things don't always go up. That

32:05.120 --> 32:12.720
 should be that's an important lesson of history. In 1990, at the request of Carl Sagan, the Voyager

32:12.720 --> 32:18.560
 One spacecraft, which is a spacecraft that's reaching out farther than anything human made

32:18.560 --> 32:24.720
 into space, turned around to take a picture of Earth from 3.6 billion years ago. And that's

32:24.720 --> 32:31.520
 a picture of Earth from 3.7 billion miles away. And as you're talking about the pale blue dot,

32:31.520 --> 32:37.600
 that picture there takes up less than a single pixel in that image. Yes. Appearing as a tiny

32:37.600 --> 32:46.640
 blue dot, as a pale blue dot, as Carl Sagan called it. So he spoke about this dot of ours in 1994.

32:46.640 --> 32:54.160
 And if you could humor me, I was wondering if in the last two minutes you could read the words

32:54.160 --> 33:01.520
 that he wrote describing this pale blue dot. Sure. Yes, it's funny. The universe appears to be 13.8

33:01.520 --> 33:07.920
 billion years old. Earth is like four and a half billion years old.

33:07.920 --> 33:14.320
 In another half billion years or so, the sun will expand and probably evaporate the oceans and make

33:14.320 --> 33:19.200
 life impossible on Earth, which means that if it had taken consciousness 10% longer to evolve,

33:19.200 --> 33:29.680
 it would never have evolved at all. It's 10% longer. And I wonder how many dead one planet

33:29.680 --> 33:31.520
 civilizations there are out there in the cosmos.

33:31.520 --> 33:35.200
 That never made it to the other planet and ultimately extinguished themselves or were destroyed

33:35.200 --> 33:46.080
 by external factors. Probably a few. It's only just possible to travel to Mars. Just barely.

33:46.640 --> 33:50.080
 If G was 10% more, it wouldn't work really.

33:50.080 --> 34:00.240
 If G was 10% lower, it would be easy. Like you can go single stage from the surface of Mars all the

34:00.240 --> 34:08.240
 way to the surface of the Earth. Because Mars is 37% Earth's gravity. We need a giant booster

34:08.240 --> 34:24.240
 to get off the Earth. Channeling Carl Sagan. Look again at that dot. That's here. That's home. That's us.

34:25.360 --> 34:30.960
 On it, everyone you love, everyone you know, everyone you've ever heard of, every human being

34:30.960 --> 34:37.600
 who ever was, lived out their lives. The aggregate of our joy and suffering, thousands of confident

34:37.600 --> 34:42.960
 religions, ideologies and economic doctrines. Every hunter and farger, every hero and coward,

34:42.960 --> 34:49.120
 every creator and destroyer of civilization, every king and peasant, every young couple in love,

34:49.840 --> 34:57.760
 every mother and father, hopeful child, inventor and explorer, every teacher of morals, every

34:57.760 --> 35:06.400
 corrupt politician, every superstar, every supreme leader, every saint and sinner in the history of

35:06.400 --> 35:13.840
 our species lived there on a mode of dust suspended in a sunbeam. Our planet is a lonely speck in the

35:13.840 --> 35:20.640
 great enveloping cosmic dark. In our obscurity, in all this vastness, there is no hint that help

35:20.640 --> 35:25.680
 will come from elsewhere to save us from ourselves. The Earth is the only world known so far to harbor

35:25.680 --> 35:32.080
 life. There is nowhere else, at least in the near future, to which our species could migrate. This

35:32.080 --> 35:39.840
 is not true. This is false. Mars. And I think Carl Sagan would agree with that. He couldn't even

35:39.840 --> 35:45.760
 imagine it at that time. So thank you for making the world dream. And thank you for talking today.

35:45.760 --> 35:59.520
 I really appreciate it. Thank you.

