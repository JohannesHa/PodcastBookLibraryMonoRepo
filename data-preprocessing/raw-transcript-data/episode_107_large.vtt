WEBVTT

00:00.000 --> 00:03.440
 The following is a conversation with Peter Singer,

00:03.440 --> 00:06.200
 professor of bioethics at Princeton University,

00:06.200 --> 00:10.280
 best known for his 1975 book, Animal Liberation,

00:10.280 --> 00:14.240
 that makes an ethical case against eating meat.

00:14.240 --> 00:17.680
 He has written brilliantly from an ethical perspective

00:17.680 --> 00:21.480
 on extreme poverty, euthanasia, human genetic selection,

00:21.480 --> 00:23.720
 sports doping, the sale of kidneys,

00:23.720 --> 00:28.520
 and generally happiness, including in his books,

00:28.520 --> 00:32.920
 Ethics in the Real World, and The Life You Can Save.

00:32.920 --> 00:36.320
 He was a key popularizer of the effective altruism movement

00:36.320 --> 00:39.240
 and is generally considered one of the most influential

00:39.240 --> 00:42.200
 philosophers in the world.

00:42.200 --> 00:43.760
 Quick summary of the ads.

00:43.760 --> 00:47.080
 Two sponsors, Cash App and Masterclass.

00:47.080 --> 00:48.840
 Please consider supporting the podcast

00:48.840 --> 00:52.200
 by downloading Cash App and using code LexPodcast

00:52.200 --> 00:55.920
 and signing up at masterclass.com slash Lex.

00:55.920 --> 00:57.800
 Click the links, buy the stuff.

00:57.800 --> 01:00.080
 It really is the best way to support the podcast

01:00.080 --> 01:02.400
 and the journey I'm on.

01:02.400 --> 01:07.480
 As you may know, I primarily eat a ketogenic or carnivore diet,

01:07.480 --> 01:10.320
 which means that most of my diet is made up of meat.

01:10.320 --> 01:15.280
 I do not hunt the food I eat, though one day I hope to.

01:15.280 --> 01:17.800
 I love fishing, for example.

01:17.800 --> 01:19.680
 Fishing and eating the fish I catch

01:19.680 --> 01:23.640
 has always felt much more honest than participating

01:23.640 --> 01:26.400
 in the supply chain of factory farming.

01:26.400 --> 01:29.360
 From an ethics perspective, this part of my life

01:29.360 --> 01:31.920
 has always had a cloud over it.

01:31.920 --> 01:33.600
 It makes me think.

01:33.600 --> 01:35.960
 I've tried a few times in my life

01:35.960 --> 01:37.920
 to reduce the amount of meat I eat.

01:37.920 --> 01:41.240
 But for some reason, whatever the makeup of my body,

01:41.240 --> 01:44.040
 whatever the way I practice the dieting I have,

01:44.040 --> 01:48.040
 I get a lot of mental and physical energy

01:48.040 --> 01:50.600
 and performance from eating meat.

01:50.600 --> 01:53.960
 So both intellectually and physically,

01:53.960 --> 01:56.080
 it's a continued journey for me.

01:56.080 --> 02:00.320
 I return to Peter's work often to reevaluate the ethics

02:00.320 --> 02:03.360
 of how I live this aspect of my life.

02:03.360 --> 02:06.160
 Let me also say that you may be a vegan

02:06.160 --> 02:09.840
 or you may be a meat eater and may be upset by the words I say

02:09.840 --> 02:13.680
 or Peter says, but I ask for this podcast

02:13.680 --> 02:16.000
 and other episodes of this podcast

02:16.000 --> 02:18.240
 that you keep an open mind.

02:18.240 --> 02:21.640
 I may and probably will talk with people you disagree with.

02:21.640 --> 02:25.360
 Please try to really listen, especially

02:25.360 --> 02:27.400
 to people you disagree with.

02:27.400 --> 02:29.800
 And give me and the world the gift

02:29.800 --> 02:33.080
 of being a participant in a patient, intelligent,

02:33.080 --> 02:34.840
 and nuanced discourse.

02:34.840 --> 02:38.640
 If your instinct and desire is to be a voice of mockery

02:38.640 --> 02:42.520
 towards those you disagree with, please unsubscribe.

02:42.520 --> 02:44.840
 My source of joy and inspiration here

02:44.840 --> 02:48.040
 has been to be a part of a community that thinks deeply

02:48.040 --> 02:51.000
 and speaks with empathy and compassion.

02:51.000 --> 02:53.880
 That is what I hope to continue being a part of

02:53.880 --> 02:56.200
 and I hope you join as well.

02:56.200 --> 02:58.960
 If you enjoy this podcast, subscribe on YouTube,

02:58.960 --> 03:01.360
 review it with five stars on Apple Podcast,

03:01.360 --> 03:04.280
 follow on Spotify, support on Patreon,

03:04.280 --> 03:07.920
 or connect with me on Twitter at Lex Friedman.

03:07.920 --> 03:09.960
 As usual, I'll do a few minutes of ads now

03:09.960 --> 03:11.320
 and never any ads in the middle

03:11.320 --> 03:14.040
 that can break the flow of the conversation.

03:14.040 --> 03:16.560
 This show is presented by Cash App,

03:16.560 --> 03:18.960
 the number one finance app in the App Store.

03:18.960 --> 03:22.000
 When you get it, use code LEXPODCAST.

03:22.000 --> 03:24.280
 Cash App lets you send money to friends,

03:24.280 --> 03:27.320
 buy Bitcoin, and invest in the stock market

03:27.320 --> 03:29.520
 with as little as one dollar.

03:29.520 --> 03:31.800
 Since Cash App allows you to buy Bitcoin,

03:31.800 --> 03:34.600
 let me mention that cryptocurrency in the context

03:34.600 --> 03:37.400
 of the history of money is fascinating.

03:37.400 --> 03:39.520
 I recommend Ascent of Money

03:39.520 --> 03:41.480
 as a great book on this history.

03:41.480 --> 03:43.160
 Debits and credits on ledgers

03:43.160 --> 03:45.960
 started around 30,000 years ago.

03:45.960 --> 03:48.560
 The US dollar created over 200 years ago

03:48.560 --> 03:51.080
 and the first decentralized cryptocurrency

03:51.080 --> 03:53.760
 released just over 10 years ago.

03:53.760 --> 03:57.000
 So given that history, cryptocurrency is still very much

03:57.000 --> 03:58.720
 in its early days of development,

03:58.720 --> 04:01.280
 but it's still aiming to and just might

04:01.280 --> 04:04.320
 redefine the nature of money.

04:04.320 --> 04:07.000
 So again, if you get Cash App from the App Store

04:07.000 --> 04:10.480
 or Google Play and use the code LEXPODCAST,

04:10.480 --> 04:14.920
 you get $10 and Cash App will also donate $10 to FIRST,

04:14.920 --> 04:16.720
 an organization that is helping to advance

04:16.720 --> 04:20.880
 robotic system education for young people around the world.

04:20.880 --> 04:23.440
 This show is sponsored by Masterclass.

04:23.440 --> 04:26.080
 Sign up at masterclass.com slash LEX

04:26.080 --> 04:29.640
 to get a discount and to support this podcast.

04:29.640 --> 04:31.320
 When I first heard about Masterclass,

04:31.320 --> 04:33.160
 I thought it was too good to be true.

04:33.160 --> 04:36.680
 For $180 a year, you get an all access pass

04:36.680 --> 04:40.400
 to watch courses from, to list some of my favorites,

04:40.400 --> 04:42.920
 Chris Hadfield on space exploration,

04:42.920 --> 04:46.200
 Neil Gauss Tyson on scientific thinking and communication,

04:46.200 --> 04:50.400
 Will Wright, creator of SimCity and Sims on game design.

04:50.400 --> 04:53.880
 I promise I'll start streaming games at some point soon.

04:53.880 --> 04:57.520
 Carlos Santana on guitar, Gary Kasparov on chess,

04:57.520 --> 05:01.600
 Daniel Lagrano on poker and many more.

05:01.600 --> 05:04.240
 Chris Hadfield explaining how rockets work

05:04.240 --> 05:07.280
 and the experience of being launched into space alone

05:07.280 --> 05:08.720
 is worth the money.

05:08.720 --> 05:12.820
 By the way, you can watch it on basically any device.

05:12.820 --> 05:16.600
 Once again, sign up at masterclass.com slash LEX

05:16.600 --> 05:19.360
 to get a discount and to support this podcast.

05:19.360 --> 05:24.080
 And now, here's my conversation with Peter Singer.

05:25.080 --> 05:27.640
 When did you first become conscious of the fact

05:27.640 --> 05:30.340
 that there is much suffering in the world?

05:32.280 --> 05:33.760
 I think I was conscious of the fact

05:33.760 --> 05:35.760
 that there's a lot of suffering in the world

05:35.760 --> 05:38.520
 pretty much as soon as I was able to understand

05:38.520 --> 05:40.960
 anything about my family and its background

05:40.960 --> 05:44.720
 because I lost three of my four grandparents

05:44.720 --> 05:48.720
 in the Holocaust and obviously I knew

05:48.720 --> 05:51.080
 why I only had one grandparent

05:52.160 --> 05:54.560
 and she herself had been in the camps and survived,

05:54.560 --> 05:58.120
 so I think I knew a lot about that pretty early.

05:58.120 --> 06:01.200
 My entire family comes from the Soviet Union.

06:01.200 --> 06:03.360
 I was born in the Soviet Union.

06:05.400 --> 06:07.920
 World War II has deep roots in the culture

06:07.920 --> 06:10.360
 and the suffering that the war brought

06:10.360 --> 06:14.000
 the millions of people who died is in the music,

06:14.000 --> 06:16.900
 is in the literature, is in the culture.

06:16.900 --> 06:18.960
 What do you think was the impact

06:18.960 --> 06:22.420
 of the war broadly on our society?

06:25.080 --> 06:26.840
 The war had many impacts.

06:28.160 --> 06:31.440
 I think one of them, a beneficial impact,

06:31.440 --> 06:34.300
 is that it showed what racism

06:34.300 --> 06:37.960
 and authoritarian government can do

06:37.960 --> 06:41.080
 and at least as far as the West was concerned,

06:41.080 --> 06:43.200
 I think that meant that I grew up in an era

06:43.200 --> 06:48.000
 in which there wasn't the kind of overt racism

06:48.000 --> 06:52.160
 and antisemitism that had existed for my parents in Europe.

06:52.160 --> 06:53.800
 I was growing up in Australia

06:53.800 --> 06:57.560
 and certainly that was clearly seen

06:57.560 --> 06:59.400
 as something completely unacceptable.

07:00.560 --> 07:05.560
 There was also, though, a fear of a further outbreak of war

07:05.800 --> 07:08.920
 which this time we expected would be nuclear

07:08.920 --> 07:11.720
 because of the way the Second World War had ended,

07:11.720 --> 07:16.200
 so there was this overshadowing of my childhood

07:16.200 --> 07:19.880
 about the possibility that I would not live to grow up

07:19.880 --> 07:23.780
 and be an adult because of a catastrophic nuclear war.

07:25.620 --> 07:28.100
 The film On the Beach was made

07:28.100 --> 07:29.860
 in which the city that I was living,

07:29.860 --> 07:32.080
 Melbourne, was the last place on Earth

07:32.080 --> 07:34.320
 to have living human beings

07:34.320 --> 07:36.420
 because of the nuclear cloud

07:36.420 --> 07:38.120
 that was spreading from the North,

07:38.120 --> 07:41.840
 so that certainly gave us a bit of that sense.

07:42.840 --> 07:45.400
 There were many, there were clearly many other legacies

07:45.400 --> 07:47.560
 that we got of the war as well

07:47.560 --> 07:49.440
 and the whole setup of the world

07:49.440 --> 07:51.600
 and the Cold War that followed.

07:51.600 --> 07:55.320
 All of that has its roots in the Second World War.

07:55.320 --> 07:58.120
 There is much beauty that comes from war.

07:58.120 --> 08:01.400
 Sort of, I had a conversation with Eric Weinstein.

08:01.400 --> 08:03.960
 He said everything is great about war

08:03.960 --> 08:08.200
 except all the death and suffering.

08:08.200 --> 08:10.120
 Do you think there's something positive

08:11.060 --> 08:13.640
 that came from the war,

08:13.640 --> 08:16.840
 the mirror that it put to our society,

08:16.840 --> 08:20.320
 sort of the ripple effects on it, ethically speaking?

08:20.320 --> 08:24.540
 Do you think there are positive aspects to war?

08:24.540 --> 08:27.540
 I find it hard to see positive aspects in war

08:27.540 --> 08:30.440
 and some of the things that other people think of

08:30.440 --> 08:35.440
 as positive and beautiful may be questioning.

08:35.640 --> 08:38.280
 So there's a certain kind of patriotism.

08:38.280 --> 08:41.040
 People say during wartime, we all pull together,

08:41.040 --> 08:44.080
 we all work together against a common enemy

08:44.080 --> 08:45.300
 and that's true.

08:45.300 --> 08:47.380
 An outside enemy does unite a country

08:47.380 --> 08:49.920
 and in general, it's good for countries to be united

08:49.920 --> 08:51.080
 and have common purposes

08:51.080 --> 08:55.360
 but it also engenders a kind of a nationalism

08:55.360 --> 08:57.760
 and a patriotism that can't be questioned

08:57.760 --> 09:01.960
 and that I'm more skeptical about.

09:01.960 --> 09:04.560
 What about the brotherhood

09:04.560 --> 09:08.240
 that people talk about from soldiers?

09:08.240 --> 09:12.960
 The sort of counterintuitive, sad idea

09:12.960 --> 09:16.240
 that the closest that people feel to each other

09:16.240 --> 09:17.880
 is in those moments of suffering,

09:17.880 --> 09:20.360
 of being at the sort of the edge

09:20.360 --> 09:24.980
 of seeing your comrades dying in your arms.

09:24.980 --> 09:27.440
 That somehow brings people extremely closely together.

09:27.440 --> 09:29.600
 Suffering brings people closer together.

09:29.600 --> 09:31.920
 How do you make sense of that?

09:31.920 --> 09:33.520
 It may bring people close together

09:33.520 --> 09:36.440
 but there are other ways of bonding

09:36.440 --> 09:37.840
 and being close to people I think

09:37.840 --> 09:41.120
 without the suffering and death that war entails.

09:42.840 --> 09:44.560
 Perhaps you could see, you could already hear

09:44.560 --> 09:46.660
 the romanticized Russian in me.

09:48.160 --> 09:50.280
 We tend to romanticize suffering just a little bit

09:50.280 --> 09:53.440
 in our literature and culture and so on.

09:53.440 --> 09:54.880
 Could you take a step back

09:54.880 --> 09:57.560
 and I apologize if it's a ridiculous question

09:57.560 --> 09:59.640
 but what is suffering?

09:59.640 --> 10:03.760
 If you would try to define what suffering is,

10:03.760 --> 10:05.560
 how would you go about it?

10:05.560 --> 10:08.720
 Suffering is a conscious state.

10:09.640 --> 10:11.360
 There can be no suffering for a being

10:11.360 --> 10:13.040
 who is completely unconscious

10:14.520 --> 10:17.940
 and it's distinguished from other conscious states

10:17.940 --> 10:22.940
 in terms of being one that considered just in itself.

10:22.940 --> 10:25.500
 We would rather be without.

10:25.500 --> 10:27.500
 It's a conscious state that we want to stop

10:27.500 --> 10:31.780
 if we're experiencing or we want to avoid having again

10:31.780 --> 10:34.500
 if we've experienced it in the past.

10:34.500 --> 10:37.400
 And that's, as I say, emphasized for its own sake

10:37.400 --> 10:39.340
 because of course people will say,

10:39.340 --> 10:41.580
 well, suffering strengthens the spirit.

10:41.580 --> 10:43.140
 It has good consequences.

10:44.340 --> 10:47.100
 And sometimes it does have those consequences

10:47.100 --> 10:50.780
 and of course sometimes we might undergo suffering.

10:50.780 --> 10:53.700
 We set ourselves a challenge to run a marathon

10:53.700 --> 10:57.260
 or climb a mountain or even just to go to the dentist

10:57.260 --> 10:59.100
 so that the toothache doesn't get worse

10:59.100 --> 11:00.900
 even though we know the dentist is gonna hurt us

11:00.900 --> 11:01.940
 to some extent.

11:01.940 --> 11:04.520
 So I'm not saying that we never choose suffering

11:04.520 --> 11:07.260
 but I am saying that other things being equal,

11:07.260 --> 11:10.660
 we would rather not be in that state of consciousness.

11:10.660 --> 11:12.380
 Is the ultimate goal sort of,

11:12.380 --> 11:15.820
 you have the new 10 year anniversary release

11:15.820 --> 11:18.860
 of the Life You Can Save book, really influential book.

11:18.860 --> 11:20.700
 We'll talk about it a bunch of times

11:20.700 --> 11:21.780
 throughout this conversation

11:21.780 --> 11:25.340
 but do you think it's possible

11:25.340 --> 11:29.820
 to eradicate suffering or is that the goal

11:29.820 --> 11:34.820
 or do we want to achieve a kind of minimum threshold

11:36.820 --> 11:41.500
 of suffering and then keeping a little drop of poison

11:43.860 --> 11:46.160
 to keep things interesting in the world?

11:46.160 --> 11:50.120
 In practice, I don't think we ever will eliminate suffering

11:50.120 --> 11:53.000
 so I think that little drop of poison as you put it

11:53.000 --> 11:58.000
 or if you like the contrasting dash of an unpleasant color

11:58.680 --> 11:59.680
 perhaps something like that

11:59.680 --> 12:04.040
 in a otherwise harmonious and beautiful composition,

12:04.040 --> 12:05.880
 that is gonna always be there.

12:07.240 --> 12:09.140
 If you ask me whether in theory

12:09.140 --> 12:12.640
 if we could get rid of it, we should.

12:12.640 --> 12:14.680
 I think the answer is whether in fact

12:14.680 --> 12:17.760
 we would be better off

12:17.760 --> 12:20.240
 or whether in terms of by eliminating the suffering

12:20.240 --> 12:22.520
 we would also eliminate some of the highs,

12:22.520 --> 12:24.880
 the positive highs and if that's so

12:24.880 --> 12:27.360
 then we might be prepared to say

12:27.360 --> 12:30.600
 it's worth having a minimum of suffering

12:30.600 --> 12:34.560
 in order to have the best possible experiences as well.

12:34.560 --> 12:37.680
 Is there a relative aspect to suffering?

12:37.680 --> 12:42.680
 So when you talk about eradicating poverty in the world,

12:42.680 --> 12:44.920
 is this the more you succeed,

12:44.920 --> 12:47.760
 the more the bar of what defines poverty raises

12:47.760 --> 12:51.360
 or is there at the basic human ethical level

12:51.360 --> 12:55.000
 a bar that's absolute that once you get above it

12:55.000 --> 13:00.000
 then we can morally converge

13:00.000 --> 13:02.720
 to feeling like we have eradicated poverty?

13:04.280 --> 13:08.160
 I think they're both and I think this is true for poverty

13:08.160 --> 13:09.000
 as well as suffering.

13:09.000 --> 13:14.000
 There's an objective level of suffering or of poverty

13:14.280 --> 13:17.720
 where we're talking about objective indicators

13:17.720 --> 13:20.360
 like you're constantly hungry,

13:22.360 --> 13:24.000
 you can't get enough food,

13:24.000 --> 13:28.600
 you're constantly cold, you can't get warm,

13:28.600 --> 13:32.600
 you have some physical pains that you're never rid of.

13:32.600 --> 13:35.080
 I think those things are objective

13:35.080 --> 13:38.520
 but it may also be true that if you do get rid of it

13:38.520 --> 13:40.840
 if you do get rid of that and you get to the stage

13:40.840 --> 13:43.840
 where all of those basic needs have been met,

13:45.280 --> 13:48.760
 there may still be then new forms of suffering that develop

13:48.760 --> 13:50.400
 and perhaps that's what we're seeing

13:50.400 --> 13:52.720
 in the affluent societies we have

13:52.720 --> 13:55.680
 that people get bored for example,

13:55.680 --> 13:58.920
 they don't need to spend so many hours a day earning money

13:58.920 --> 14:01.400
 to get enough to eat and shelter.

14:01.400 --> 14:04.240
 So now they're bored, they lack a sense of purpose.

14:05.120 --> 14:06.360
 That can happen.

14:06.360 --> 14:09.480
 And that then is a kind of a relative suffering

14:10.440 --> 14:14.320
 that is distinct from the objective forms of suffering.

14:14.320 --> 14:17.520
 But in your focus on eradicating suffering,

14:17.520 --> 14:19.960
 you don't think about that kind of,

14:19.960 --> 14:22.520
 the kind of interesting challenges and suffering

14:22.520 --> 14:24.400
 that emerges in affluent societies,

14:24.400 --> 14:28.800
 that's just not, in your ethical philosophical brain,

14:28.800 --> 14:31.240
 is that of interest at all?

14:31.240 --> 14:34.120
 It would be of interest to me if we had eliminated

14:34.120 --> 14:36.480
 all of the objective forms of suffering,

14:36.480 --> 14:40.240
 which I think of as generally more severe

14:40.240 --> 14:43.200
 and also perhaps easier at this stage anyway

14:43.200 --> 14:45.000
 to know how to eliminate.

14:45.000 --> 14:49.160
 So yes, in some future state when we've eliminated

14:49.160 --> 14:50.560
 those objective forms of suffering,

14:50.560 --> 14:53.080
 I would be interested in trying to eliminate

14:53.080 --> 14:55.920
 the relative forms as well.

14:55.920 --> 14:59.920
 But that's not a practical need for me at the moment.

14:59.920 --> 15:02.400
 Sorry to linger on it because you kind of said it,

15:02.400 --> 15:07.400
 but just is elimination the goal for the affluent society?

15:07.640 --> 15:12.640
 So is there, do you see suffering as a creative force?

15:14.400 --> 15:17.120
 Suffering can be a creative force.

15:17.120 --> 15:20.560
 I think repeating what I said about the highs

15:20.560 --> 15:22.240
 and whether we need some of the lows

15:22.240 --> 15:24.120
 to experience the highs.

15:24.120 --> 15:26.560
 So it may be that suffering makes us more creative

15:26.560 --> 15:29.840
 and we regard that as worthwhile.

15:29.840 --> 15:32.920
 Maybe that brings some of those highs with it

15:32.920 --> 15:35.520
 that we would not have had if we'd had no suffering.

15:36.680 --> 15:37.720
 I don't really know.

15:37.720 --> 15:39.520
 Many people have suggested that

15:39.520 --> 15:43.080
 and I certainly can't have no basis for denying it.

15:44.840 --> 15:47.800
 And if it's true, then I would not want

15:47.800 --> 15:49.440
 to eliminate suffering completely.

15:50.920 --> 15:54.000
 But the focus is on the absolute,

15:54.000 --> 15:56.840
 not to be cold, not to be hungry.

15:56.840 --> 15:59.800
 Yes, that's at the present stage

15:59.800 --> 16:03.040
 of where the world's population is, that's the focus.

16:03.920 --> 16:06.360
 Talking about human nature for a second,

16:06.360 --> 16:08.440
 do you think people are inherently good

16:08.440 --> 16:11.000
 or do we all have good and evil in us

16:11.000 --> 16:14.880
 that basically everyone is capable of evil

16:14.880 --> 16:16.160
 based on the environment?

16:17.400 --> 16:21.480
 Certainly most of us have potential for both good and evil.

16:21.480 --> 16:24.280
 I'm not prepared to say that everyone is capable of evil.

16:24.280 --> 16:27.160
 Maybe some people who even in the worst of circumstances

16:27.160 --> 16:28.880
 would not be capable of it,

16:28.880 --> 16:32.400
 but most of us are very susceptible

16:32.400 --> 16:34.520
 to environmental influences.

16:34.520 --> 16:36.520
 So when we look at things

16:36.520 --> 16:37.880
 that we were talking about previously,

16:37.880 --> 16:42.400
 let's say what the Nazis did during the Holocaust,

16:43.640 --> 16:46.600
 I think it's quite difficult to say,

16:46.600 --> 16:50.200
 I know that I would not have done those things

16:50.200 --> 16:52.640
 even if I were in the same circumstances

16:52.640 --> 16:54.480
 as those who did them.

16:54.480 --> 16:58.280
 Even if let's say I had grown up under the Nazi regime

16:58.280 --> 17:02.480
 and had been indoctrinated with racist ideas,

17:02.480 --> 17:07.160
 had also had the idea that I must obey orders,

17:07.160 --> 17:09.880
 follow the commands of the Fuhrer,

17:11.040 --> 17:12.480
 plus of course perhaps the threat

17:12.480 --> 17:14.520
 that if I didn't do certain things,

17:14.520 --> 17:16.560
 I might get sent to the Russian front

17:16.560 --> 17:19.200
 and that would be a pretty grim fate.

17:19.200 --> 17:22.720
 I think it's really hard for anybody to say,

17:22.720 --> 17:26.720
 nevertheless, I know I would not have killed those Jews

17:26.720 --> 17:28.440
 or whatever else it was that they were.

17:28.440 --> 17:29.400
 Well, what's your intuition?

17:29.400 --> 17:31.440
 How many people will be able to say that?

17:32.440 --> 17:33.840
 Truly to be able to say it,

17:34.920 --> 17:37.680
 I think very few, less than 10%.

17:37.680 --> 17:39.680
 To me, it seems a very interesting

17:39.680 --> 17:42.080
 and powerful thing to meditate on.

17:42.080 --> 17:45.800
 So I've read a lot about the war, World War II,

17:45.800 --> 17:47.880
 and I can't escape the thought

17:47.880 --> 17:51.640
 that I would have not been one of the 10%.

17:51.640 --> 17:55.440
 Right, I have to say, I simply don't know.

17:55.440 --> 17:59.000
 I would like to hope that I would have been one of the 10%,

17:59.000 --> 18:00.920
 but I don't really have any basis

18:00.920 --> 18:04.280
 for claiming that I would have been different

18:04.280 --> 18:05.240
 from the majority.

18:06.160 --> 18:08.400
 Is it a worthwhile thing to contemplate?

18:09.520 --> 18:11.360
 It would be interesting if we could find a way

18:11.360 --> 18:13.920
 of really finding these answers.

18:13.920 --> 18:16.600
 There obviously is quite a bit of research

18:16.600 --> 18:19.760
 on people during the Holocaust,

18:19.760 --> 18:24.760
 on how ordinary Germans got led to do terrible things,

18:24.840 --> 18:28.160
 and there are also studies of the resistance,

18:28.160 --> 18:32.400
 some heroic people in the White Rose group, for example,

18:32.400 --> 18:34.720
 who resisted even though they knew

18:34.720 --> 18:36.280
 they were likely to die for it.

18:37.960 --> 18:40.080
 But I don't know whether these studies

18:40.080 --> 18:43.160
 really can answer your larger question

18:43.160 --> 18:47.720
 of how many people would have been capable of doing that.

18:47.720 --> 18:50.360
 Well, sort of the reason I think is interesting

18:50.360 --> 18:53.320
 is in the world, as you described,

18:55.120 --> 18:59.920
 when there are things that you'd like to do that are good,

18:59.920 --> 19:01.400
 that are objectively good,

19:02.280 --> 19:04.800
 it's useful to think about whether

19:04.800 --> 19:06.720
 I'm not willing to do something,

19:06.720 --> 19:09.000
 or I'm not willing to acknowledge something

19:09.000 --> 19:10.760
 as good and the right thing to do

19:10.760 --> 19:15.760
 because I'm simply scared of putting my life,

19:15.920 --> 19:18.920
 of damaging my life in some kind of way.

19:18.920 --> 19:20.720
 And that kind of thought exercise is helpful

19:20.720 --> 19:23.400
 to understand what is the right thing

19:23.400 --> 19:27.400
 in my current skill set and the capacity to do.

19:27.400 --> 19:30.000
 Sort of there's things that are convenient,

19:30.000 --> 19:31.920
 and I wonder if there are things

19:31.920 --> 19:33.640
 that are highly inconvenient,

19:33.640 --> 19:35.560
 where I would have to experience derision,

19:35.560 --> 19:39.640
 or hatred, or death, or all those kinds of things,

19:39.640 --> 19:41.200
 but it's truly the right thing to do.

19:41.200 --> 19:42.680
 And that kind of balance is,

19:43.800 --> 19:46.560
 I feel like in America, we don't have,

19:46.560 --> 19:50.000
 it's difficult to think in the current times,

19:50.000 --> 19:53.360
 it seems easier to put yourself back in history,

19:53.360 --> 19:56.280
 where you can sort of objectively contemplate

19:56.280 --> 19:59.880
 whether, how willing you are to do the right thing

19:59.880 --> 20:01.160
 when the cost is high.

20:03.000 --> 20:06.080
 True, but I think we do face those challenges today,

20:06.080 --> 20:09.960
 and I think we can still ask ourselves those questions.

20:09.960 --> 20:13.480
 So one stand that I took more than 40 years ago now

20:13.480 --> 20:17.520
 was to stop eating meat, become a vegetarian at a time

20:17.520 --> 20:21.360
 when you hardly met anybody who was a vegetarian,

20:21.360 --> 20:23.760
 or if you did, they might've been a Hindu,

20:23.760 --> 20:27.560
 or they might've had some weird theories

20:27.560 --> 20:28.960
 about meat and health.

20:30.160 --> 20:33.240
 And I know thinking about making that decision,

20:33.240 --> 20:35.280
 I was convinced that it was the right thing to do,

20:35.280 --> 20:37.240
 but I still did have to think,

20:37.240 --> 20:40.080
 are all my friends gonna think that I'm a crank

20:40.080 --> 20:42.120
 because I'm now refusing to eat meat?

20:43.960 --> 20:47.760
 So I'm not saying there were any terrible sanctions,

20:47.760 --> 20:50.000
 obviously, but I thought about that,

20:50.000 --> 20:51.600
 and I guess I decided,

20:51.600 --> 20:54.080
 well, I still think this is the right thing to do,

20:54.080 --> 20:56.320
 and I'll put up with that if it happens.

20:56.320 --> 20:59.080
 And one or two friends were clearly uncomfortable

20:59.080 --> 21:03.480
 with that decision, but that was pretty minor

21:03.480 --> 21:05.840
 compared to the historical examples

21:05.840 --> 21:08.040
 that we've been talking about.

21:08.040 --> 21:09.840
 But other issues that we have around too,

21:09.840 --> 21:13.800
 like global poverty and what we ought to be doing about that

21:13.800 --> 21:16.880
 is another question where people, I think,

21:16.880 --> 21:19.080
 can have the opportunity to take a stand

21:19.080 --> 21:21.040
 on what's the right thing to do now.

21:21.040 --> 21:23.200
 Climate change would be a third question

21:23.200 --> 21:25.680
 where, again, people are taking a stand.

21:25.680 --> 21:29.120
 I can look at Greta Thunberg there and say,

21:29.120 --> 21:32.360
 well, I think it must've taken a lot of courage

21:32.360 --> 21:35.240
 for a schoolgirl to say,

21:35.240 --> 21:37.160
 I'm gonna go on strike about climate change

21:37.160 --> 21:39.440
 and see what happens.

21:41.200 --> 21:42.960
 Yeah, especially in this divisive world,

21:42.960 --> 21:45.560
 she gets exceptionally huge amounts of support

21:45.560 --> 21:47.400
 and hatred, both.

21:47.400 --> 21:48.240
 That's right.

21:48.240 --> 21:50.560
 Which is very difficult for a teenager to operate in.

21:53.920 --> 21:56.080
 In your book, Ethics in the Real World,

21:56.080 --> 21:57.880
 amazing book, people should check it out.

21:57.880 --> 21:59.600
 Very easy read.

21:59.600 --> 22:02.800
 82 brief essays on things that matter.

22:02.800 --> 22:06.920
 One of the essays asks, should robots have rights?

22:06.920 --> 22:07.920
 You've written about this,

22:07.920 --> 22:10.600
 so let me ask, should robots have rights?

22:11.560 --> 22:16.560
 If we ever develop robots capable of consciousness,

22:17.080 --> 22:20.520
 capable of having their own internal perspective

22:20.520 --> 22:22.080
 on what's happening to them

22:22.080 --> 22:25.600
 so that their lives can go well or badly for them,

22:25.600 --> 22:27.720
 then robots should have rights.

22:27.720 --> 22:30.040
 Until that happens, they shouldn't.

22:31.000 --> 22:36.000
 So is consciousness essentially a prerequisite to suffering?

22:36.160 --> 22:40.400
 So everything that possesses consciousness

22:41.520 --> 22:43.920
 is capable of suffering, put another way.

22:43.920 --> 22:47.000
 And if so, what is consciousness?

22:48.440 --> 22:51.320
 I certainly think that consciousness

22:51.320 --> 22:53.080
 is a prerequisite for suffering.

22:53.080 --> 22:58.040
 You can't suffer if you're not conscious.

22:58.040 --> 23:01.160
 But is it true that every being that is conscious

23:02.160 --> 23:05.400
 will suffer or has to be capable of suffering?

23:05.400 --> 23:08.200
 I suppose you could imagine a kind of consciousness,

23:08.200 --> 23:10.920
 especially if we can construct it artificially,

23:10.920 --> 23:13.840
 that's capable of experiencing pleasure

23:13.840 --> 23:16.720
 but just automatically cuts out the consciousness

23:16.720 --> 23:18.240
 when they're suffering.

23:18.240 --> 23:20.400
 So they're like an instant anesthesia

23:20.400 --> 23:22.520
 as soon as something is gonna cause you suffering.

23:22.520 --> 23:23.880
 So that's possible.

23:25.120 --> 23:29.840
 But doesn't exist as far as we know on this planet yet.

23:31.280 --> 23:32.880
 You asked what is consciousness.

23:34.680 --> 23:36.440
 Philosophers often talk about it

23:36.440 --> 23:39.520
 as there being a subject of experiences.

23:39.520 --> 23:42.920
 So you and I and everybody listening to this

23:42.920 --> 23:44.680
 is a subject of experience.

23:44.680 --> 23:48.640
 There is a conscious subject who is taking things in,

23:48.640 --> 23:51.320
 responding to it in various ways,

23:51.320 --> 23:53.480
 feeling good about it, feeling bad about it.

23:54.720 --> 23:57.400
 And that's different from the kinds

23:57.400 --> 24:00.600
 of artificial intelligence we have now.

24:00.600 --> 24:03.000
 I take out my phone.

24:03.000 --> 24:06.840
 I ask Google directions to where I'm going.

24:06.840 --> 24:08.680
 Google gives me the directions

24:08.680 --> 24:10.840
 and I choose to take a different way.

24:10.840 --> 24:11.840
 Google doesn't care.

24:11.840 --> 24:14.080
 It's not like I'm offending Google or anything like that.

24:14.080 --> 24:16.520
 There is no subject of experiences there.

24:16.520 --> 24:19.360
 And I think that's the indication

24:19.360 --> 24:24.360
 that Google AI we have now is not conscious

24:24.480 --> 24:27.560
 or at least that level of AI is not conscious.

24:27.560 --> 24:28.880
 And that's the way to think about it.

24:28.880 --> 24:31.040
 Now, it may be difficult to tell, of course,

24:31.040 --> 24:34.080
 whether a certain AI is or isn't conscious.

24:34.080 --> 24:35.280
 It may mimic consciousness

24:35.280 --> 24:37.360
 and we can't tell if it's only mimicking it

24:37.360 --> 24:39.120
 or if it's the real thing.

24:39.120 --> 24:40.600
 But that's what we're looking for.

24:40.600 --> 24:43.480
 Is there a subject of experience,

24:43.480 --> 24:47.080
 a perspective on the world from which things can go well

24:47.080 --> 24:50.160
 or badly from that perspective?

24:50.160 --> 24:54.200
 So our idea of what suffering looks like

24:54.200 --> 24:59.200
 comes from just watching ourselves when we're in pain.

25:01.200 --> 25:03.360
 Or when we're experiencing pleasure, it's not only.

25:03.360 --> 25:04.600
 Pleasure and pain.

25:04.600 --> 25:07.880
 Yes, so and then you could actually,

25:07.880 --> 25:09.400
 you could push back on us, but I would say

25:09.400 --> 25:14.280
 that's how we kind of build an intuition about animals

25:14.280 --> 25:18.520
 is we can infer the similarities between humans and animals

25:18.520 --> 25:21.000
 and so infer that they're suffering or not

25:21.000 --> 25:24.320
 based on certain things and they're conscious or not.

25:24.320 --> 25:29.320
 So what if robots, you mentioned Google Maps

25:31.040 --> 25:32.520
 and I've done this experiment.

25:32.520 --> 25:35.080
 So I work in robotics just for my own self

25:35.080 --> 25:37.640
 or I have several Roomba robots

25:37.640 --> 25:40.960
 and I play with different speech interaction,

25:40.960 --> 25:42.160
 voice based interaction.

25:42.160 --> 25:45.880
 And if the Roomba or the robot or Google Maps

25:47.120 --> 25:50.360
 shows any signs of pain, like screaming or moaning

25:50.360 --> 25:54.240
 or being displeased by something you've done,

25:54.240 --> 25:58.240
 that in my mind, I can't help but immediately upgrade it.

25:59.440 --> 26:02.520
 And even when I myself programmed it in,

26:02.520 --> 26:06.040
 just having another entity that's now for the moment

26:06.040 --> 26:09.080
 disjoint from me showing signs of pain

26:09.080 --> 26:11.120
 makes me feel like it is conscious.

26:11.120 --> 26:13.880
 Like I immediately, then the whatever,

26:15.440 --> 26:17.800
 I immediately realize that it's not obviously,

26:17.800 --> 26:19.640
 but that feeling is there.

26:19.640 --> 26:24.640
 So sort of, I guess, what do you think about a world

26:26.400 --> 26:31.400
 where Google Maps and Roombas are pretending to be conscious

26:32.080 --> 26:35.360
 and we descendants of apes are not smart enough

26:35.360 --> 26:39.080
 to realize they're not or whatever, or that is conscious,

26:39.080 --> 26:40.720
 they appear to be conscious.

26:40.720 --> 26:44.000
 And so you then have to give them rights.

26:44.000 --> 26:47.120
 The reason I'm asking that is that kind of capability

26:47.120 --> 26:51.120
 may be closer than we realize.

26:52.280 --> 26:55.840
 Yes, that kind of capability may be closer,

26:58.400 --> 26:59.720
 but I don't think it follows

26:59.720 --> 27:00.920
 that we have to give them rights.

27:00.920 --> 27:05.400
 I suppose the argument for saying that in those circumstances

27:05.400 --> 27:07.800
 we should give them rights is that if we don't,

27:07.800 --> 27:11.920
 we'll harden ourselves against other beings

27:11.920 --> 27:14.240
 who are not robots and who really do suffer.

27:15.200 --> 27:17.880
 That's a possibility that, you know,

27:17.880 --> 27:20.880
 if we get used to looking at a being suffering

27:20.880 --> 27:23.440
 and saying, yeah, we don't have to do anything about that,

27:23.440 --> 27:25.000
 that being doesn't have any rights,

27:25.000 --> 27:28.280
 maybe we'll feel the same about animals, for instance.

27:29.240 --> 27:34.240
 And interestingly, among philosophers and thinkers

27:34.240 --> 27:39.240
 who denied that we have any direct duties to animals,

27:39.720 --> 27:41.840
 and this includes people like Thomas Aquinas

27:41.840 --> 27:46.640
 and Immanuel Kant, they did say, yes,

27:46.640 --> 27:48.960
 but still it's better not to be cruel to them,

27:48.960 --> 27:50.880
 not because of the suffering we're inflicting

27:50.880 --> 27:54.280
 on the animals, but because if we are,

27:54.280 --> 27:56.440
 we may develop a cruel disposition

27:56.440 --> 28:00.000
 and this will be bad for humans, you know,

28:00.000 --> 28:02.080
 because we're more likely to be cruel to other humans

28:02.080 --> 28:03.760
 and that would be wrong.

28:03.760 --> 28:06.080
 So.

28:06.080 --> 28:07.760
 But you don't accept that kind of.

28:07.760 --> 28:10.160
 I don't accept that as the basis of the argument

28:10.160 --> 28:11.600
 for why we shouldn't be cruel to animals.

28:11.600 --> 28:12.680
 I think the basis of the argument

28:12.680 --> 28:14.000
 for why we shouldn't be cruel to animals

28:14.000 --> 28:16.440
 is just that we're inflicting suffering on them

28:16.440 --> 28:18.120
 and the suffering is a bad thing.

28:19.160 --> 28:23.000
 But possibly I might accept some sort of parallel

28:23.000 --> 28:26.040
 of that argument as a reason why you shouldn't be cruel

28:26.040 --> 28:30.880
 to these robots that mimic the symptoms of pain

28:30.880 --> 28:33.520
 if it's gonna be harder for us to distinguish.

28:33.520 --> 28:36.760
 I would venture to say, I'd like to disagree with you

28:36.760 --> 28:38.440
 and with most people, I think,

28:39.680 --> 28:42.240
 at the risk of sounding crazy,

28:42.240 --> 28:46.880
 I would like to say that if that Roomba is dedicated

28:47.840 --> 28:50.840
 to faking the consciousness and the suffering,

28:50.840 --> 28:54.640
 I think it will be impossible for us.

28:55.920 --> 28:58.440
 I would like to apply the same argument

28:58.440 --> 29:00.480
 as with animals to robots,

29:00.480 --> 29:02.880
 that they deserve rights in that sense.

29:02.880 --> 29:05.880
 Now we might outlaw the addition

29:05.880 --> 29:07.600
 of those kinds of features into Roombas,

29:07.600 --> 29:12.600
 but once you do, I think I'm quite surprised

29:13.000 --> 29:16.800
 by the upgrade in consciousness

29:16.800 --> 29:19.720
 that the display of suffering creates.

29:20.640 --> 29:22.360
 It's a totally open world,

29:22.360 --> 29:25.600
 but I'd like to just sort of the difference

29:25.600 --> 29:29.480
 between animals and other humans is that in the robot case,

29:29.480 --> 29:32.440
 we've added it in ourselves.

29:32.440 --> 29:37.440
 Therefore, we can say something about how real it is.

29:37.560 --> 29:40.160
 But I would like to say that the display of it

29:40.160 --> 29:41.920
 is what makes it real.

29:41.920 --> 29:45.560
 And I'm not a philosopher, I'm not making that argument,

29:45.560 --> 29:48.120
 but I'd at least like to add that as a possibility.

29:49.080 --> 29:50.920
 And I've been surprised by it

29:50.920 --> 29:55.160
 is all I'm trying to sort of articulate poorly, I suppose.

29:55.160 --> 29:57.880
 So there is a philosophical view

29:59.080 --> 30:00.760
 has been held about humans,

30:00.760 --> 30:02.480
 which is rather like what you're talking about,

30:02.480 --> 30:04.760
 and that's behaviorism.

30:04.760 --> 30:07.480
 So behaviorism was employed both in psychology,

30:07.480 --> 30:10.240
 people like BF Skinner was a famous behaviorist,

30:10.240 --> 30:14.760
 but in psychology, it was more a kind of a,

30:14.760 --> 30:16.360
 what is it that makes this science?

30:16.360 --> 30:17.480
 Well, you need to have behavior

30:17.480 --> 30:18.680
 because that's what you can observe,

30:18.680 --> 30:21.200
 you can't observe consciousness.

30:21.200 --> 30:23.440
 But in philosophy, the view just defended

30:23.440 --> 30:24.800
 by people like Gilbert Ryle,

30:24.800 --> 30:26.440
 who was a professor of philosophy at Oxford,

30:26.440 --> 30:28.480
 wrote a book called The Concept of Mind,

30:28.480 --> 30:32.000
 in which in this kind of phase,

30:32.000 --> 30:35.280
 this is in the 40s of linguistic philosophy,

30:35.280 --> 30:38.920
 he said, well, the meaning of a term is its use,

30:38.920 --> 30:42.440
 and we use terms like so and so is in pain

30:42.440 --> 30:44.840
 when we see somebody writhing or screaming

30:44.840 --> 30:47.080
 or trying to escape some stimulus,

30:47.080 --> 30:48.400
 and that's the meaning of the term.

30:48.400 --> 30:50.440
 So that's what it is to be in pain,

30:50.440 --> 30:52.840
 and you point to the behavior.

30:54.720 --> 30:58.400
 And Norman Malcolm, who was another philosopher

30:58.400 --> 31:02.920
 in the school from Cornell, had the view that,

31:02.920 --> 31:04.600
 so what is it to dream?

31:04.600 --> 31:07.960
 After all, we can't see other people's dreams.

31:07.960 --> 31:10.040
 Well, when people wake up and say,

31:10.880 --> 31:14.080
 I've just had a dream of, here I was,

31:14.080 --> 31:15.720
 undressed, walking down the main street

31:15.720 --> 31:17.760
 or whatever it is you've dreamt,

31:17.760 --> 31:19.040
 that's what it is to have a dream.

31:19.040 --> 31:21.760
 It's basically to wake up and recall something.

31:22.720 --> 31:25.640
 So you could apply this to what you're talking about

31:25.640 --> 31:28.480
 and say, so what it is to be in pain

31:28.480 --> 31:31.040
 is to exhibit these symptoms of pain behavior,

31:31.040 --> 31:34.920
 and therefore, these robots are in pain.

31:34.920 --> 31:36.840
 That's what the word means.

31:36.840 --> 31:38.520
 But nowadays, not many people think

31:38.520 --> 31:40.880
 that Ryle's kind of philosophical behaviorism

31:40.880 --> 31:42.320
 is really very plausible,

31:42.320 --> 31:45.080
 so I think they would say the same about your view.

31:45.080 --> 31:48.600
 So, yes, I just spoke with Noam Chomsky,

31:48.600 --> 31:52.760
 who basically was part of dismantling

31:52.760 --> 31:54.800
 the behaviorist movement.

31:54.800 --> 31:59.800
 But, and I'm with that 100% for studying human behavior,

32:00.600 --> 32:04.080
 but I am one of the few people in the world

32:04.080 --> 32:08.240
 who has made Roombas scream in pain.

32:09.480 --> 32:12.200
 And I just don't know what to do

32:12.200 --> 32:14.520
 with that empirical evidence,

32:14.520 --> 32:18.720
 because it's hard, sort of philosophically, I agree.

32:19.760 --> 32:23.240
 But the only reason I philosophically agree in that case

32:23.240 --> 32:25.040
 is because I was the programmer.

32:25.040 --> 32:26.760
 But if somebody else was a programmer,

32:26.760 --> 32:29.280
 I'm not sure I would be able to interpret that well.

32:29.280 --> 32:31.760
 So I think it's a new world

32:34.320 --> 32:37.480
 that I was just curious what your thoughts are.

32:37.480 --> 32:42.280
 For now, you feel that the display

32:42.280 --> 32:46.400
 of what we can kind of intellectually say

32:46.400 --> 32:50.120
 is a fake display of suffering is not suffering.

32:50.120 --> 32:53.240
 That's right, that would be my view.

32:53.240 --> 32:54.480
 But that's consistent, of course,

32:54.480 --> 32:56.920
 with the idea that it's part of our nature

32:56.920 --> 32:58.680
 to respond to this display

32:58.680 --> 33:01.120
 if it's reasonably authentically done.

33:02.600 --> 33:04.800
 And therefore it's understandable

33:04.800 --> 33:06.240
 that people would feel this,

33:06.240 --> 33:09.880
 and maybe, as I said, it's even a good thing

33:09.880 --> 33:10.720
 that they do feel it,

33:10.720 --> 33:12.640
 and you wouldn't want to harden yourself against it

33:12.640 --> 33:14.440
 because then you might harden yourself

33:14.440 --> 33:17.240
 against being sort of really suffering.

33:17.240 --> 33:20.160
 But there's this line, so you said,

33:20.160 --> 33:22.880
 once artificial general intelligence system,

33:22.880 --> 33:25.760
 a human level intelligence system become conscious,

33:25.760 --> 33:28.480
 I guess if I could just linger on it,

33:28.480 --> 33:30.720
 now I've wrote really dumb programs

33:30.720 --> 33:33.760
 that just say things that I told them to say,

33:33.760 --> 33:38.320
 but how do you know when a system like Alexa,

33:38.320 --> 33:39.720
 which is sufficiently complex

33:39.720 --> 33:42.040
 that you can't introspect to how it works,

33:42.040 --> 33:46.200
 starts giving you signs of consciousness

33:46.200 --> 33:48.000
 through natural language?

33:48.000 --> 33:49.800
 That there's a feeling,

33:49.800 --> 33:52.560
 there's another entity there that's self aware,

33:52.560 --> 33:55.080
 that has a fear of death, a mortality,

33:55.080 --> 33:57.840
 that has awareness of itself

33:57.840 --> 34:00.600
 that we kind of associate with other living creatures.

34:03.160 --> 34:05.680
 I guess I'm sort of trying to do the slippery slope

34:05.680 --> 34:07.880
 from the very naive thing where I started

34:07.880 --> 34:12.120
 into something where it's sufficiently a black box

34:12.120 --> 34:16.120
 to where it's starting to feel like it's conscious.

34:16.120 --> 34:17.960
 Where's that threshold

34:17.960 --> 34:20.240
 where you would start getting uncomfortable

34:20.240 --> 34:23.960
 with the idea of robot suffering, do you think?

34:25.080 --> 34:27.640
 I don't know enough about the programming

34:27.640 --> 34:31.600
 that we're going to this really to answer this question.

34:31.600 --> 34:34.880
 But I presume that somebody who does know more about this

34:34.880 --> 34:37.360
 could look at the program

34:37.360 --> 34:41.480
 and see whether we can explain the behaviors

34:41.480 --> 34:45.360
 in a parsimonious way that doesn't require us

34:45.360 --> 34:50.080
 to suggest that some sort of consciousness has emerged.

34:50.080 --> 34:52.400
 Or alternatively, whether you're in a situation

34:52.400 --> 34:56.280
 where you say, I don't know how this is happening,

34:56.280 --> 35:00.160
 the program does generate a kind of artificial

35:00.160 --> 35:04.200
 general intelligence which is autonomous,

35:04.200 --> 35:06.360
 starts to do things itself and is autonomous

35:06.360 --> 35:10.400
 of the basics programming that set it up.

35:10.400 --> 35:13.400
 And so it's quite possible that actually

35:13.400 --> 35:15.800
 we have achieved consciousness

35:15.800 --> 35:18.600
 in a system of artificial intelligence.

35:18.600 --> 35:20.640
 Sort of the approach that I work with,

35:20.640 --> 35:22.680
 most of the community is really excited about now

35:22.680 --> 35:26.000
 is with learning methods, so machine learning.

35:26.000 --> 35:27.960
 And the learning methods are unfortunately

35:27.960 --> 35:31.440
 are not capable of revealing,

35:31.440 --> 35:34.120
 which is why somebody like Noam Chomsky criticizes them.

35:34.120 --> 35:36.080
 You create powerful systems that are able

35:36.080 --> 35:38.240
 to do certain things without understanding

35:38.240 --> 35:42.160
 the theory, the physics, the science of how it works.

35:42.160 --> 35:44.840
 And so it's possible if those are the kinds

35:44.840 --> 35:46.760
 of methods that succeed, we won't be able

35:46.760 --> 35:51.760
 to know exactly, sort of try to reduce,

35:53.000 --> 35:56.200
 try to find whether this thing is conscious or not,

35:56.200 --> 35:58.120
 this thing is intelligent or not.

35:58.120 --> 36:01.760
 It's simply giving, when we talk to it,

36:01.760 --> 36:05.800
 it displays wit and humor and cleverness

36:05.800 --> 36:10.200
 and emotion and fear, and then we won't be able

36:10.200 --> 36:13.920
 to say where in the billions of nodes,

36:13.920 --> 36:16.400
 neurons in this artificial neural network

36:16.400 --> 36:19.080
 is the fear coming from.

36:20.020 --> 36:22.440
 So in that case, that's a really interesting place

36:22.440 --> 36:26.420
 where we do now start to return to behaviorism and say.

36:28.480 --> 36:32.300
 Yeah, that is an interesting issue.

36:33.860 --> 36:36.960
 I would say that if we have serious doubts

36:36.960 --> 36:39.440
 and think it might be conscious,

36:39.440 --> 36:41.840
 then we ought to try to give it the benefit

36:41.840 --> 36:45.360
 of the doubt, just as I would say with animals.

36:45.360 --> 36:46.880
 I think we can be highly confident

36:46.880 --> 36:50.460
 that vertebrates are conscious,

36:50.460 --> 36:53.480
 but when we get down, and some invertebrates

36:53.480 --> 36:56.920
 like the octopus, but with insects,

36:56.920 --> 37:01.480
 it's much harder to be confident of that.

37:01.480 --> 37:02.760
 I think we should give them the benefit

37:02.760 --> 37:06.300
 of the doubt where we can, which means,

37:06.300 --> 37:09.000
 I think it would be wrong to torture an insect,

37:09.000 --> 37:11.800
 but it doesn't necessarily mean it's wrong

37:11.800 --> 37:14.800
 to slap a mosquito that's about to bite you

37:14.800 --> 37:16.300
 and stop you getting to sleep.

37:16.300 --> 37:20.100
 So I think you try to achieve some balance

37:20.100 --> 37:22.000
 in these circumstances of uncertainty.

37:22.960 --> 37:26.440
 If it's okay with you, if we can go back just briefly.

37:26.440 --> 37:29.640
 So 44 years ago, like you mentioned, 40 plus years ago,

37:29.640 --> 37:31.200
 you've written Animal Liberation,

37:31.200 --> 37:33.560
 the classic book that started,

37:33.560 --> 37:36.440
 that launched, that was the foundation

37:36.440 --> 37:39.380
 of the movement of Animal Liberation.

37:40.640 --> 37:42.440
 Can you summarize the key set of ideas

37:42.440 --> 37:44.360
 that underpin that book?

37:44.360 --> 37:49.000
 Certainly, the key idea that underlies that book

37:49.000 --> 37:52.200
 is the concept of speciesism,

37:52.200 --> 37:54.760
 which I did not invent that term.

37:54.760 --> 37:56.720
 I took it from a man called Richard Rider,

37:56.720 --> 37:58.600
 who was in Oxford when I was,

37:58.600 --> 38:00.240
 and I saw a pamphlet that he'd written

38:00.240 --> 38:04.060
 about experiments on chimpanzees that used that term.

38:05.240 --> 38:06.240
 But I think I contributed

38:06.240 --> 38:08.800
 to making it philosophically more precise

38:08.800 --> 38:12.040
 and to getting it into a broader audience.

38:12.040 --> 38:16.760
 And the idea is that we have a bias or a prejudice

38:16.760 --> 38:20.400
 against taking seriously the interests of beings

38:20.400 --> 38:23.440
 who are not members of our species.

38:23.440 --> 38:26.920
 Just as in the past, Europeans, for example,

38:26.920 --> 38:28.600
 had a bias against taking seriously

38:28.600 --> 38:31.600
 the interests of Africans, racism.

38:31.600 --> 38:34.080
 And men have had a bias against taking seriously

38:34.080 --> 38:37.320
 the interests of women, sexism.

38:37.320 --> 38:41.320
 So I think something analogous, not completely identical,

38:41.320 --> 38:44.320
 but something analogous goes on

38:44.320 --> 38:46.640
 and has gone on for a very long time

38:46.640 --> 38:50.440
 with the way humans see themselves vis a vis animals.

38:50.440 --> 38:53.920
 We see ourselves as more important.

38:55.000 --> 38:58.320
 We see animals as existing to serve our needs

38:58.320 --> 38:59.380
 in various ways.

38:59.380 --> 39:00.760
 And you're gonna find this very explicit

39:00.760 --> 39:04.480
 in earlier philosophers from Aristotle

39:04.480 --> 39:06.060
 through to Kant and others.

39:07.080 --> 39:12.040
 And either we don't need to take their interests

39:12.040 --> 39:14.080
 into account at all,

39:14.080 --> 39:17.800
 or we can discount it because they're not humans.

39:17.800 --> 39:18.800
 They can a little bit,

39:18.800 --> 39:21.160
 but they don't count nearly as much as humans do.

39:22.840 --> 39:25.760
 My book argues that that attitude is responsible

39:25.760 --> 39:29.360
 for a lot of the things that we do to animals

39:29.360 --> 39:32.120
 that are wrong, confining them indoors

39:32.120 --> 39:36.260
 in very crowded, cramped conditions in factory farms

39:36.260 --> 39:39.720
 to produce meat or eggs or milk more cheaply,

39:39.720 --> 39:44.000
 using them in some research that's by no means essential

39:44.000 --> 39:48.320
 for survival or wellbeing, and a whole lot,

39:48.320 --> 39:51.340
 some of the sports and things that we do to animals.

39:52.460 --> 39:55.000
 So I think that's unjustified

39:55.000 --> 40:00.000
 because I think the significance of pain and suffering

40:01.280 --> 40:03.520
 does not depend on the species of the being

40:03.520 --> 40:04.880
 who is in pain or suffering

40:04.880 --> 40:08.200
 any more than it depends on the race or sex of the being

40:08.200 --> 40:09.920
 who is in pain or suffering.

40:11.000 --> 40:14.760
 And I think we ought to rethink our treatment of animals

40:14.760 --> 40:16.800
 along the lines of saying,

40:16.800 --> 40:19.040
 if the pain is just as great in an animal,

40:19.040 --> 40:23.580
 then it's just as bad that it happens as if it were a human.

40:23.580 --> 40:27.980
 Maybe if I could ask, I apologize,

40:27.980 --> 40:29.540
 hopefully it's not a ridiculous question,

40:29.540 --> 40:32.420
 but so as far as we know,

40:32.420 --> 40:35.420
 we cannot communicate with animals through natural language,

40:36.420 --> 40:40.260
 but we would be able to communicate with robots.

40:40.260 --> 40:43.060
 So I'm returning to sort of a small parallel

40:43.060 --> 40:45.420
 between perhaps animals and the future of AI.

40:46.420 --> 40:48.140
 If we do create an AGI system

40:48.140 --> 40:53.140
 or as we approach creating that AGI system,

40:53.620 --> 40:56.980
 what kind of questions would you ask her

40:56.980 --> 41:01.980
 to try to intuit whether there is consciousness

41:06.500 --> 41:09.420
 or more importantly, whether there's capacity to suffer?

41:12.840 --> 41:17.840
 I might ask the AGI what she was feeling

41:17.840 --> 41:19.840
 or does she have feelings?

41:19.840 --> 41:22.680
 And if she says yes, to describe those feelings,

41:22.680 --> 41:24.560
 to describe what they were like,

41:24.560 --> 41:29.100
 to see what the phenomenal account of consciousness is like.

41:30.800 --> 41:32.080
 That's one question.

41:33.540 --> 41:37.840
 I might also try to find out if the AGI

41:37.840 --> 41:40.200
 has a sense of itself.

41:41.360 --> 41:45.080
 So for example, the idea would you,

41:45.080 --> 41:46.360
 we often ask people,

41:46.360 --> 41:48.680
 so suppose you were in a car accident

41:48.680 --> 41:51.880
 and your brain were transplanted into someone else's body,

41:51.880 --> 41:53.280
 do you think you would survive

41:53.280 --> 41:56.200
 or would it be the person whose body was still surviving,

41:56.200 --> 41:58.000
 your body having been destroyed?

41:58.000 --> 42:00.320
 And most people say, I think I would,

42:00.320 --> 42:02.480
 if my brain was transplanted along with my memories

42:02.480 --> 42:04.120
 and so on, I would survive.

42:04.120 --> 42:07.960
 So we could ask AGI those kinds of questions.

42:07.960 --> 42:11.680
 If they were transferred to a different piece of hardware,

42:11.680 --> 42:12.880
 would they survive?

42:12.880 --> 42:13.960
 What would survive?

42:13.960 --> 42:15.320
 And get at that sort of concept.

42:15.320 --> 42:19.380
 Sort of on that line, another perhaps absurd question,

42:19.380 --> 42:22.640
 but do you think having a body

42:22.640 --> 42:24.840
 is necessary for consciousness?

42:24.840 --> 42:29.680
 So do you think digital beings can suffer?

42:31.080 --> 42:33.320
 Presumably digital beings need to be

42:34.740 --> 42:36.960
 running on some kind of hardware, right?

42:36.960 --> 42:38.760
 Yeah, that ultimately boils down to,

42:38.760 --> 42:40.440
 but this is exactly what you just said,

42:40.440 --> 42:42.360
 is moving the brain from one place to another.

42:42.360 --> 42:44.800
 So you could move it to a different kind of hardware.

42:44.800 --> 42:49.280
 And I could say, look, your hardware is getting worn out.

42:49.280 --> 42:52.080
 We're going to transfer you to a fresh piece of hardware.

42:52.080 --> 42:55.120
 So we're gonna shut you down for a time,

42:55.120 --> 42:58.180
 but don't worry, you'll be running very soon

42:58.180 --> 43:00.260
 on a nice fresh piece of hardware.

43:00.260 --> 43:03.200
 And you could imagine this conscious AGI saying,

43:03.200 --> 43:05.320
 that's fine, I don't mind having a little rest.

43:05.320 --> 43:08.780
 Just make sure you don't lose me or something like that.

43:08.780 --> 43:10.380
 Yeah, I mean, that's an interesting thought

43:10.380 --> 43:14.920
 that even with us humans, the suffering is in the software.

43:14.920 --> 43:19.320
 We right now don't know how to repair the hardware,

43:19.320 --> 43:23.200
 but we're getting better at it and better in the idea.

43:23.200 --> 43:26.580
 I mean, some people dream about one day being able

43:26.580 --> 43:30.800
 to transfer certain aspects of the software

43:30.800 --> 43:33.000
 to another piece of hardware.

43:33.000 --> 43:35.720
 What do you think, just on that topic,

43:35.720 --> 43:39.200
 there's been a lot of exciting innovation

43:39.200 --> 43:41.180
 in brain computer interfaces.

43:42.120 --> 43:43.680
 I don't know if you're familiar with the companies

43:43.680 --> 43:45.960
 like Neuralink, with Elon Musk,

43:45.960 --> 43:48.200
 communicating both ways from a computer,

43:48.200 --> 43:51.520
 being able to send, activate neurons

43:51.520 --> 43:54.840
 and being able to read spikes from neurons.

43:54.840 --> 43:58.900
 With the dream of being able to expand,

43:58.900 --> 44:02.460
 sort of increase the bandwidth at which your brain

44:02.460 --> 44:05.240
 can like look up articles on Wikipedia kind of thing,

44:05.240 --> 44:08.360
 sort of expand the knowledge capacity of the brain.

44:08.360 --> 44:13.160
 Do you think that notion, is that interesting to you

44:13.160 --> 44:15.520
 as the expansion of the human mind?

44:15.520 --> 44:17.280
 Yes, that's very interesting.

44:17.280 --> 44:20.000
 I'd love to be able to have that increased bandwidth.

44:20.000 --> 44:23.680
 And I want better access to my memory, I have to say too,

44:23.680 --> 44:28.280
 as I get older, I talk to my wife about things

44:28.280 --> 44:30.280
 that we did 20 years ago or something.

44:30.280 --> 44:32.660
 Her memory is often better about particular events.

44:32.660 --> 44:33.500
 Where were we?

44:33.500 --> 44:35.180
 Who was at that event?

44:35.180 --> 44:36.680
 What did he or she wear even?

44:36.680 --> 44:39.040
 She may know and I have not the faintest idea about this,

44:39.040 --> 44:40.880
 but perhaps it's somewhere in my memory.

44:40.880 --> 44:42.560
 And if I had this extended memory,

44:42.560 --> 44:46.580
 I could search that particular year and rerun those things.

44:46.580 --> 44:47.920
 I think that would be great.

44:49.540 --> 44:51.120
 In some sense, we already have that

44:51.120 --> 44:53.220
 by storing so much of our data online,

44:53.220 --> 44:54.720
 like pictures of different events.

44:54.720 --> 44:56.520
 Yes, well, Gmail is fantastic for that

44:56.520 --> 44:59.760
 because people email me as if they know me well

44:59.760 --> 45:01.440
 and I haven't got a clue who they are,

45:01.440 --> 45:02.760
 but then I search for their name.

45:02.760 --> 45:05.240
 Ah yes, they emailed me in 2007

45:05.240 --> 45:07.040
 and I know who they are now.

45:07.040 --> 45:11.080
 Yeah, so we're taking the first steps already.

45:11.080 --> 45:13.320
 So on the flip side of AI,

45:13.320 --> 45:14.920
 people like Stuart Russell and others

45:14.920 --> 45:19.000
 focus on the control problem, value alignment in AI,

45:19.000 --> 45:21.400
 which is the problem of making sure we build systems

45:21.400 --> 45:25.480
 that align to our own values, our ethics.

45:25.480 --> 45:28.440
 Do you think sort of high level,

45:28.440 --> 45:31.160
 how do we go about building systems?

45:31.160 --> 45:34.640
 Do you think is it possible that align with our values,

45:34.640 --> 45:39.360
 align with our human ethics or living being ethics?

45:39.360 --> 45:42.360
 Presumably, it's possible to do that.

45:43.900 --> 45:46.120
 I know that a lot of people who think

45:46.120 --> 45:48.000
 that there's a real danger that we won't,

45:48.000 --> 45:51.840
 that we'll more or less accidentally lose control of AGI.

45:51.840 --> 45:54.080
 Do you have that fear yourself personally?

45:56.880 --> 45:58.600
 I'm not quite sure what to think.

45:58.600 --> 46:01.880
 I talk to philosophers like Nick Bostrom and Toby Ord

46:01.880 --> 46:05.000
 and they think that this is a real problem

46:05.000 --> 46:07.240
 we need to worry about.

46:07.240 --> 46:11.200
 Then I talk to people who work for Microsoft

46:11.200 --> 46:13.640
 or DeepMind or somebody and they say,

46:13.640 --> 46:18.320
 no, we're not really that close to producing AGI,

46:18.320 --> 46:19.600
 super intelligence.

46:19.600 --> 46:21.280
 So if you look at Nick Bostrom,

46:21.280 --> 46:25.000
 sort of the arguments, it's very hard to defend.

46:25.000 --> 46:28.040
 So I'm of course, I am a self engineer AI system,

46:28.040 --> 46:29.920
 so I'm more with the DeepMind folks

46:29.920 --> 46:32.360
 where it seems that we're really far away,

46:32.360 --> 46:34.840
 but then the counter argument is,

46:34.840 --> 46:38.280
 is there any fundamental reason that we'll never achieve it?

46:39.160 --> 46:42.160
 And if not, then eventually there'll be

46:42.160 --> 46:44.360
 a dire existential risk.

46:44.360 --> 46:46.440
 So we should be concerned about it.

46:46.440 --> 46:50.700
 And do you find that argument at all appealing

46:50.700 --> 46:53.120
 in this domain or any domain that eventually

46:53.120 --> 46:55.760
 this will be a problem so we should be worried about it?

46:56.880 --> 46:58.720
 Yes, I think it's a problem.

46:58.720 --> 47:02.320
 I think that's a valid point.

47:03.760 --> 47:06.100
 Of course, when you say eventually,

47:08.960 --> 47:11.440
 that raises the question, how far off is that?

47:11.440 --> 47:13.840
 And is there something that we can do about it now?

47:13.840 --> 47:15.440
 Because if we're talking about

47:15.440 --> 47:17.720
 this is gonna be 100 years in the future

47:17.720 --> 47:20.080
 and you consider how rapidly our knowledge

47:20.080 --> 47:22.080
 of artificial intelligence has grown

47:22.080 --> 47:24.000
 in the last 10 or 20 years,

47:24.000 --> 47:26.920
 it seems unlikely that there's anything much

47:26.920 --> 47:29.640
 we could do now that would influence

47:29.640 --> 47:33.440
 whether this is going to happen 100 years in the future.

47:33.440 --> 47:35.120
 People in 80 years in the future

47:35.120 --> 47:37.300
 would be in a much better position to say,

47:37.300 --> 47:39.740
 this is what we need to do to prevent this happening

47:39.740 --> 47:41.520
 than we are now.

47:41.520 --> 47:44.560
 So to some extent I find that reassuring,

47:44.560 --> 47:48.640
 but I'm all in favor of some people doing research

47:48.640 --> 47:51.480
 into this to see if indeed it is that far off

47:51.480 --> 47:55.440
 or if we are in a position to do something about it sooner.

47:55.440 --> 47:58.760
 I'm very much of the view that extinction

47:58.760 --> 48:02.760
 is a terrible thing and therefore,

48:02.760 --> 48:05.960
 even if the risk of extinction is very small,

48:05.960 --> 48:09.040
 if we can reduce that risk,

48:09.040 --> 48:11.240
 that's something that we ought to do.

48:11.240 --> 48:12.760
 My disagreement with some of these people

48:12.760 --> 48:16.360
 who talk about longterm risks, extinction risks,

48:16.360 --> 48:18.820
 is only about how much priority that should have

48:18.820 --> 48:20.520
 as compared to present questions.

48:20.520 --> 48:22.680
 So essentially, if you look at the math of it

48:22.680 --> 48:25.000
 from a utilitarian perspective,

48:25.000 --> 48:28.920
 if it's existential risk, so everybody dies,

48:28.920 --> 48:33.160
 that it feels like an infinity in the math equation,

48:33.160 --> 48:36.880
 that that makes the math

48:36.880 --> 48:39.380
 with the priorities difficult to do.

48:39.380 --> 48:42.720
 That if we don't know the time scale

48:42.720 --> 48:43.960
 and you can legitimately argue

48:43.960 --> 48:46.760
 that it's nonzero probability that it'll happen tomorrow,

48:48.160 --> 48:52.080
 that how do you deal with these kinds of existential risks

48:52.080 --> 48:55.720
 like from nuclear war, from nuclear weapons,

48:55.720 --> 48:58.640
 from biological weapons, from,

48:58.640 --> 49:01.960
 I'm not sure if global warming falls into that category

49:01.960 --> 49:04.760
 because global warming is a lot more gradual.

49:04.760 --> 49:06.880
 And people say it's not an existential risk

49:06.880 --> 49:08.280
 because there'll always be possibilities

49:08.280 --> 49:11.200
 of some humans existing, farming Antarctica

49:11.200 --> 49:14.260
 or northern Siberia or something of that sort, yeah.

49:14.260 --> 49:18.360
 But you don't find the complete existential risks

49:18.360 --> 49:23.080
 as a fundamental, like an overriding part

49:23.080 --> 49:26.280
 of the equations of ethics, of what we should do.

49:26.280 --> 49:29.000
 You know, certainly if you treat it as an infinity,

49:29.000 --> 49:32.040
 then it plays havoc with any calculations.

49:32.040 --> 49:34.480
 But arguably, we shouldn't.

49:34.480 --> 49:37.380
 I mean, one of the ethical assumptions that goes into this

49:37.380 --> 49:40.680
 is that the loss of future lives,

49:40.680 --> 49:43.280
 that is of merely possible lives of beings

49:43.280 --> 49:44.920
 who may never exist at all,

49:44.920 --> 49:49.920
 is in some way comparable to the sufferings or deaths

49:51.240 --> 49:53.700
 of people who do exist at some point.

49:54.680 --> 49:57.380
 And that's not clear to me.

49:57.380 --> 49:59.320
 I think there's a case for saying that,

49:59.320 --> 50:01.800
 but I also think there's a case for taking the other view.

50:01.800 --> 50:04.560
 So that has some impact on it.

50:04.560 --> 50:05.940
 Of course, you might say, ah, yes,

50:05.940 --> 50:08.920
 but still, if there's some uncertainty about this

50:08.920 --> 50:12.560
 and the costs of extinction are infinite,

50:12.560 --> 50:15.360
 then still, it's gonna overwhelm everything else.

50:16.680 --> 50:20.880
 But I suppose I'm not convinced of that.

50:20.880 --> 50:23.440
 I'm not convinced that it's really infinite here.

50:23.440 --> 50:27.240
 And even Nick Bostrom, in his discussion of this,

50:27.240 --> 50:28.560
 doesn't claim that there'll be

50:28.560 --> 50:31.280
 an infinite number of lives lived.

50:31.280 --> 50:33.360
 What is it, 10 to the 56th or something?

50:33.360 --> 50:36.040
 It's a vast number that I think he calculates.

50:36.040 --> 50:38.220
 This is assuming we can upload consciousness

50:38.220 --> 50:43.220
 onto these digital forms,

50:43.560 --> 50:45.280
 and therefore, they'll be much more energy efficient,

50:45.280 --> 50:47.640
 but he calculates the amount of energy in the universe

50:47.640 --> 50:48.660
 or something like that.

50:48.660 --> 50:50.480
 So the numbers are vast but not infinite,

50:50.480 --> 50:52.520
 which gives you some prospect maybe

50:52.520 --> 50:55.640
 of resisting some of the argument.

50:55.640 --> 50:57.360
 The beautiful thing with Nick's arguments

50:57.360 --> 50:59.780
 is he quickly jumps from the individual scale

50:59.780 --> 51:01.080
 to the universal scale,

51:01.080 --> 51:04.480
 which is just awe inspiring to think of

51:04.480 --> 51:06.200
 when you think about the entirety

51:06.200 --> 51:08.880
 of the span of time of the universe.

51:08.880 --> 51:11.400
 It's both interesting from a computer science perspective,

51:11.400 --> 51:13.760
 AI perspective, and from an ethical perspective,

51:13.760 --> 51:16.000
 the idea of utilitarianism.

51:16.000 --> 51:18.600
 Could you say what is utilitarianism?

51:19.720 --> 51:22.060
 Utilitarianism is the ethical view

51:22.060 --> 51:25.440
 that the right thing to do is the act

51:25.440 --> 51:28.740
 that has the greatest expected utility,

51:28.740 --> 51:32.320
 where what that means is it's the act

51:32.320 --> 51:34.860
 that will produce the best consequences,

51:34.860 --> 51:37.680
 discounted by the odds that you won't be able

51:37.680 --> 51:38.940
 to produce those consequences,

51:38.940 --> 51:40.400
 that something will go wrong.

51:40.400 --> 51:43.880
 But in simple case, let's assume we have certainty

51:43.880 --> 51:46.140
 about what the consequences of our actions will be,

51:46.140 --> 51:47.600
 then the right action is the action

51:47.600 --> 51:50.500
 that will produce the best consequences.

51:50.500 --> 51:52.080
 Is that always, and by the way,

51:52.080 --> 51:53.400
 there's a bunch of nuanced stuff

51:53.400 --> 51:56.000
 that you talk with Sam Harris on this podcast

51:56.000 --> 51:57.960
 on that people should go listen to.

51:57.960 --> 51:58.800
 It's great.

51:58.800 --> 52:02.940
 That's like two hours of moral philosophy discussion.

52:02.940 --> 52:05.520
 But is that an easy calculation?

52:05.520 --> 52:07.360
 No, it's a difficult calculation.

52:07.360 --> 52:10.000
 And actually, there's one thing that I need to add,

52:10.000 --> 52:14.240
 and that is utilitarians, certainly the classical

52:14.240 --> 52:16.760
 utilitarians, think that by best consequences,

52:16.760 --> 52:18.840
 we're talking about happiness

52:18.840 --> 52:21.020
 and the absence of pain and suffering.

52:21.020 --> 52:22.920
 There are other consequentialists

52:22.920 --> 52:25.920
 who are not really utilitarians who say

52:27.320 --> 52:29.740
 there are different things that could be good consequences.

52:29.740 --> 52:32.800
 Justice, freedom, human dignity,

52:32.800 --> 52:35.840
 knowledge, they all count as good consequences too.

52:35.840 --> 52:38.080
 And that makes the calculations even more difficult

52:38.080 --> 52:38.920
 because then you need to know

52:38.920 --> 52:40.840
 how to balance these things off.

52:40.840 --> 52:44.580
 If you are just talking about wellbeing,

52:44.580 --> 52:46.560
 using that term to express happiness

52:46.560 --> 52:48.060
 and the absence of suffering,

52:49.040 --> 52:54.040
 I think the calculation becomes more manageable

52:54.280 --> 52:56.400
 in a philosophical sense.

52:56.400 --> 52:58.180
 It's still in practice.

52:58.180 --> 52:59.280
 We don't know how to do it.

52:59.280 --> 53:01.040
 We don't know how to measure quantities

53:01.040 --> 53:02.740
 of happiness and misery.

53:02.740 --> 53:04.960
 We don't know how to calculate the probabilities

53:04.960 --> 53:07.760
 that different actions will produce, this or that.

53:08.800 --> 53:13.080
 So at best, we can use it as a rough guide

53:13.080 --> 53:16.520
 to different actions and one where we have to focus

53:16.520 --> 53:20.120
 on the short term consequences

53:20.120 --> 53:22.800
 because we just can't really predict

53:22.800 --> 53:25.360
 all of the longer term ramifications.

53:25.360 --> 53:33.240
 So what about the extreme suffering of very small groups?

53:33.240 --> 53:36.920
 Utilitarianism is focused on the overall aggregate, right?

53:38.320 --> 53:41.040
 Would you say you yourself are a utilitarian?

53:41.040 --> 53:42.440
 Yes, I'm a utilitarian.

53:45.540 --> 53:50.280
 What do you make of the difficult, ethical,

53:50.280 --> 53:54.960
 maybe poetic suffering of very few individuals?

53:54.960 --> 53:57.040
 I think it's possible that that gets overridden

53:57.040 --> 54:00.080
 by benefits to very large numbers of individuals.

54:00.080 --> 54:02.880
 I think that can be the right answer.

54:02.880 --> 54:05.440
 But before we conclude that it is the right answer,

54:05.440 --> 54:08.960
 we have to know how severe the suffering is

54:08.960 --> 54:12.320
 and how that compares with the benefits.

54:12.320 --> 54:17.320
 So I tend to think that extreme suffering is worse than

54:19.680 --> 54:23.480
 or is further, if you like, below the neutral level

54:23.480 --> 54:27.320
 than extreme happiness or bliss is above it.

54:27.320 --> 54:30.720
 So when I think about the worst experiences possible

54:30.720 --> 54:33.160
 and the best experiences possible,

54:33.160 --> 54:36.200
 I don't think of them as equidistant from neutral.

54:36.200 --> 54:39.640
 So like it's a scale that goes from minus 100 through zero

54:39.640 --> 54:41.840
 as a neutral level to plus 100.

54:43.480 --> 54:46.880
 Because I know that I would not exchange an hour

54:46.880 --> 54:49.620
 of my most pleasurable experiences

54:49.620 --> 54:52.400
 for an hour of my most painful experiences,

54:52.400 --> 54:54.440
 even I wouldn't have an hour

54:54.440 --> 54:57.360
 of my most painful experiences even for two hours

54:57.360 --> 55:01.760
 or 10 hours of my most painful experiences.

55:01.760 --> 55:02.600
 Did I say that correctly?

55:02.600 --> 55:03.720
 Yeah, yeah, yeah, yeah.

55:03.720 --> 55:07.080
 Maybe 20 hours then, it's 21, what's the exchange rate?

55:07.080 --> 55:08.700
 So that's the question, what is the exchange rate?

55:08.700 --> 55:10.940
 But I think it can be quite high.

55:10.940 --> 55:13.760
 So that's why you shouldn't just assume that

55:15.480 --> 55:18.480
 it's okay to make one person suffer extremely

55:18.480 --> 55:21.520
 in order to make two people much better off.

55:21.520 --> 55:23.520
 It might be a much larger number.

55:23.520 --> 55:27.520
 But at some point I do think you should aggregate

55:27.520 --> 55:30.560
 and the result will be,

55:30.560 --> 55:33.840
 even though it violates our intuitions of justice

55:33.840 --> 55:36.560
 and fairness, whatever it might be,

55:36.560 --> 55:39.560
 giving priority to those who are worse off,

55:39.560 --> 55:41.660
 at some point I still think

55:41.660 --> 55:43.040
 that will be the right thing to do.

55:43.040 --> 55:45.440
 Yeah, it's some complicated nonlinear function.

55:46.960 --> 55:49.000
 Can I ask a sort of out there question is,

55:49.000 --> 55:51.080
 the more and more we put our data out there,

55:51.080 --> 55:53.200
 the more we're able to measure a bunch of factors

55:53.200 --> 55:55.680
 of each of our individual human lives.

55:55.680 --> 55:59.940
 And I could foresee the ability to estimate wellbeing

55:59.940 --> 56:03.940
 of whatever we together collectively agree

56:03.940 --> 56:05.960
 and is in a good objective function

56:05.960 --> 56:07.900
 from a utilitarian perspective.

56:07.900 --> 56:11.360
 Do you think it'll be possible

56:11.360 --> 56:15.960
 and is a good idea to push that kind of analysis

56:15.960 --> 56:19.920
 to make then public decisions perhaps with the help of AI

56:19.920 --> 56:23.560
 that here's a tax rate,

56:24.560 --> 56:28.280
 here's a tax rate at which wellbeing will be optimized.

56:28.280 --> 56:31.040
 Yeah, that would be great if we really knew that,

56:31.040 --> 56:32.360
 if we really could calculate that.

56:32.360 --> 56:33.600
 No, but do you think it's possible

56:33.600 --> 56:36.640
 to converge towards an agreement amongst humans,

56:36.640 --> 56:39.720
 towards an objective function

56:39.720 --> 56:42.020
 or is it just a hopeless pursuit?

56:42.020 --> 56:43.080
 I don't think it's hopeless.

56:43.080 --> 56:44.800
 I think it would be difficult

56:44.800 --> 56:47.880
 to get converged towards agreement, at least at present,

56:47.880 --> 56:49.920
 because some people would say,

56:49.920 --> 56:52.040
 I've got different views about justice

56:52.040 --> 56:54.180
 and I think you ought to give priority

56:54.180 --> 56:55.860
 to those who are worse off,

56:55.860 --> 56:58.720
 even though I acknowledge that the gains

56:58.720 --> 57:01.460
 that the worst off are making are less than the gains

57:01.460 --> 57:05.740
 that those who are sort of medium badly off could be making.

57:05.740 --> 57:10.240
 So we still have all of these intuitions that we argue about.

57:10.240 --> 57:11.700
 So I don't think we would get agreement,

57:11.700 --> 57:14.280
 but the fact that we wouldn't get agreement

57:14.280 --> 57:17.840
 doesn't show that there isn't a right answer there.

57:17.840 --> 57:21.320
 Do you think, who gets to say what is right and wrong?

57:21.320 --> 57:23.600
 Do you think there's place for ethics oversight

57:23.600 --> 57:26.360
 from the government?

57:26.360 --> 57:29.320
 So I'm thinking in the case of AI,

57:29.320 --> 57:33.900
 overseeing what kind of decisions AI can make or not,

57:33.900 --> 57:36.700
 but also if you look at animal rights

57:36.700 --> 57:39.560
 or rather not rights or perhaps rights,

57:39.560 --> 57:43.000
 but the ideas you've explored in animal liberation,

57:43.000 --> 57:46.480
 who gets to, so you eloquently and beautifully write

57:46.480 --> 57:50.480
 in your book that this, you know, we shouldn't do this,

57:50.480 --> 57:53.600
 but is there some harder rules that should be imposed

57:53.600 --> 57:56.680
 or is this a collective thing we converse towards the society

57:56.680 --> 57:59.680
 and thereby make the better and better ethical decisions?

58:02.080 --> 58:04.320
 Politically, I'm still a Democrat

58:04.320 --> 58:07.880
 despite looking at the flaws in democracy

58:07.880 --> 58:10.160
 and the way it doesn't work always very well.

58:10.160 --> 58:11.880
 So I don't see a better option

58:11.880 --> 58:16.880
 than allowing the public to vote for governments

58:18.520 --> 58:20.040
 in accordance with their policies.

58:20.040 --> 58:24.800
 And I hope that they will vote for policies

58:24.800 --> 58:27.800
 that reduce the suffering of animals

58:27.800 --> 58:30.600
 and reduce the suffering of distant humans,

58:30.600 --> 58:32.600
 whether geographically distant or distant

58:32.600 --> 58:35.160
 because they're future humans.

58:35.160 --> 58:36.520
 But I recognise that democracy

58:36.520 --> 58:38.440
 isn't really well set up to do that.

58:38.440 --> 58:43.440
 And in a sense, you could imagine a wise and benevolent,

58:45.540 --> 58:48.740
 you know, omnibenevolent leader

58:48.740 --> 58:51.820
 who would do that better than democracies could.

58:51.820 --> 58:54.660
 But in the world in which we live,

58:54.660 --> 58:57.420
 it's difficult to imagine that this leader

58:57.420 --> 59:01.300
 isn't gonna be corrupted by a variety of influences.

59:01.300 --> 59:04.100
 You know, we've had so many examples

59:04.100 --> 59:08.540
 of people who've taken power with good intentions

59:08.540 --> 59:10.260
 and then have ended up being corrupt

59:10.260 --> 59:11.620
 and favouring themselves.

59:12.780 --> 59:16.540
 So I don't know, you know, that's why, as I say,

59:16.540 --> 59:17.960
 I don't know that we have a better system

59:17.960 --> 59:20.060
 than democracy to make these decisions.

59:20.060 --> 59:23.460
 Well, so you also discuss effective altruism,

59:23.460 --> 59:27.220
 which is a mechanism for going around government

59:27.220 --> 59:29.540
 for putting the power in the hands of the people

59:29.540 --> 59:32.460
 to donate money towards causes to help, you know,

59:32.460 --> 59:37.460
 remove the middleman and give it directly

59:37.940 --> 59:41.540
 to the causes that they care about.

59:41.540 --> 59:45.220
 Sort of, maybe this is a good time to ask,

59:45.220 --> 59:48.180
 you've, 10 years ago, wrote The Life You Can Save,

59:48.180 --> 59:51.300
 that's now, I think, available for free online?

59:51.300 --> 59:53.820
 That's right, you can download either the ebook

59:53.820 --> 59:57.480
 or the audiobook free from the lifeyoucansave.org.

59:58.420 --> 1:00:01.520
 And what are the key ideas that you present

1:00:01.520 --> 1:00:03.820
 in the book?

1:00:03.820 --> 1:00:05.140
 The main thing I wanna do in the book

1:00:05.140 --> 1:00:10.140
 is to make people realise that it's not difficult

1:00:10.320 --> 1:00:13.700
 to help people in extreme poverty,

1:00:13.700 --> 1:00:16.780
 that there are highly effective organisations now

1:00:16.780 --> 1:00:20.300
 that are doing this, that they've been independently assessed

1:00:20.300 --> 1:00:25.300
 and verified by research teams that are expert in this area

1:00:25.300 --> 1:00:28.180
 and that it's a fulfilling thing to do

1:00:28.180 --> 1:00:30.860
 to, for at least part of your life, you know,

1:00:30.860 --> 1:00:33.500
 we can't all be saints, but at least one of your goals

1:00:33.500 --> 1:00:36.060
 should be to really make a positive contribution

1:00:36.060 --> 1:00:38.260
 to the world and to do something to help people

1:00:38.260 --> 1:00:40.940
 who through no fault of their own

1:00:40.940 --> 1:00:45.820
 are in very dire circumstances and living a life

1:00:45.820 --> 1:00:49.540
 that is barely or perhaps not at all

1:00:49.540 --> 1:00:51.920
 a decent life for a human being to live.

1:00:51.920 --> 1:00:56.920
 So you describe a minimum ethical standard of giving.

1:00:56.920 --> 1:01:01.380
 What advice would you give to people

1:01:01.380 --> 1:01:06.380
 that want to be effectively altruistic in their life,

1:01:06.500 --> 1:01:09.340
 like live an effective altruism life?

1:01:09.340 --> 1:01:12.060
 There are many different kinds of ways of living

1:01:12.060 --> 1:01:13.540
 as an effective altruist.

1:01:14.440 --> 1:01:16.660
 And if you're at the point where you're thinking

1:01:16.660 --> 1:01:20.060
 about your long term career, I'd recommend you take a look

1:01:20.060 --> 1:01:24.660
 at a website called 80,000Hours, 80,000Hours.org,

1:01:24.660 --> 1:01:27.180
 which looks at ethical career choices.

1:01:27.180 --> 1:01:29.740
 And they range from, for example,

1:01:29.740 --> 1:01:31.060
 going to work on Wall Street

1:01:31.060 --> 1:01:33.340
 so that you can earn a huge amount of money

1:01:33.340 --> 1:01:36.980
 and then donate most of it to effective charities

1:01:36.980 --> 1:01:40.860
 to going to work for a really good nonprofit organization

1:01:40.860 --> 1:01:44.060
 so that you can directly use your skills and ability

1:01:44.060 --> 1:01:48.620
 and hard work to further a good cause,

1:01:48.620 --> 1:01:52.640
 or perhaps going into politics, maybe small chances,

1:01:52.640 --> 1:01:55.140
 but big payoffs in politics,

1:01:55.140 --> 1:01:56.520
 go to work in the public service

1:01:56.520 --> 1:01:59.180
 where if you're talented, you might rise to a high level

1:01:59.180 --> 1:02:01.700
 where you can influence decisions,

1:02:01.700 --> 1:02:05.160
 do research in an area where the payoffs could be great.

1:02:05.160 --> 1:02:07.220
 There are a lot of different opportunities,

1:02:07.220 --> 1:02:11.340
 but too few people are even thinking about those questions.

1:02:11.340 --> 1:02:14.720
 They're just going along in some sort of preordained rut

1:02:14.720 --> 1:02:15.780
 to particular careers.

1:02:15.780 --> 1:02:17.420
 Maybe they think they'll earn a lot of money

1:02:17.420 --> 1:02:19.180
 and have a comfortable life,

1:02:19.180 --> 1:02:20.940
 but they may not find that as fulfilling

1:02:20.940 --> 1:02:23.500
 as actually knowing that they're making

1:02:23.500 --> 1:02:25.100
 a positive difference to the world.

1:02:25.100 --> 1:02:27.020
 What about in terms of,

1:02:27.020 --> 1:02:30.100
 so that's like long term, 80,000 hours,

1:02:30.100 --> 1:02:33.100
 sort of shorter term giving part of,

1:02:33.100 --> 1:02:34.340
 well, actually it's a part of that.

1:02:34.340 --> 1:02:37.100
 You go to work at Wall Street,

1:02:37.100 --> 1:02:40.060
 if you would like to give a percentage of your income

1:02:40.060 --> 1:02:42.420
 that you talk about and life you can save that.

1:02:42.420 --> 1:02:45.820
 I mean, I was looking through, it's quite a compelling,

1:02:48.100 --> 1:02:50.440
 I mean, I'm just a dumb engineer,

1:02:50.440 --> 1:02:53.740
 so I like, there's simple rules, there's a nice percentage.

1:02:53.740 --> 1:02:57.540
 Okay, so I do actually set out suggested levels of giving

1:02:57.540 --> 1:03:00.220
 because people often ask me about this.

1:03:00.220 --> 1:03:04.140
 A popular answer is give 10%, the traditional tithe

1:03:04.140 --> 1:03:08.500
 that's recommended in Christianity and also Judaism.

1:03:08.500 --> 1:03:11.820
 But why should it be the same percentage

1:03:11.820 --> 1:03:13.640
 irrespective of your income?

1:03:13.640 --> 1:03:16.280
 Tax scales reflect the idea that the more income you have,

1:03:16.280 --> 1:03:18.040
 the more you can pay tax.

1:03:18.040 --> 1:03:20.400
 And I think the same is true in what you can give.

1:03:20.400 --> 1:03:25.400
 So I do set out a progressive donor scale,

1:03:25.500 --> 1:03:28.940
 which starts out at 1% for people on modest incomes

1:03:28.940 --> 1:03:31.900
 and rises to 33 and a third percent

1:03:31.900 --> 1:03:34.320
 for people who are really earning a lot.

1:03:34.320 --> 1:03:38.620
 And my idea is that I don't think any of these amounts

1:03:38.620 --> 1:03:42.120
 really impose real hardship on people

1:03:42.120 --> 1:03:44.780
 because they are progressive and geared to income.

1:03:45.660 --> 1:03:48.660
 So I think anybody can do this

1:03:48.660 --> 1:03:51.940
 and can know that they're doing something significant

1:03:51.940 --> 1:03:56.060
 to play their part in reducing the huge gap

1:03:56.060 --> 1:03:58.780
 between people in extreme poverty in the world

1:03:58.780 --> 1:04:01.180
 and people living affluent lives.

1:04:02.180 --> 1:04:05.780
 And aside from it being an ethical life,

1:04:05.780 --> 1:04:07.540
 it's one that you find more fulfilling

1:04:07.540 --> 1:04:10.980
 because there's something about our human nature that,

1:04:11.940 --> 1:04:13.740
 or some of our human natures,

1:04:13.740 --> 1:04:18.580
 maybe most of our human nature that enjoys doing

1:04:18.580 --> 1:04:21.660
 the ethical thing.

1:04:21.660 --> 1:04:23.140
 Yes, I make both those arguments,

1:04:23.140 --> 1:04:25.460
 that it is an ethical requirement

1:04:25.460 --> 1:04:27.220
 in the kind of world we live in today

1:04:27.220 --> 1:04:30.480
 to help people in great need when we can easily do so,

1:04:30.480 --> 1:04:33.000
 but also that it is a rewarding thing

1:04:33.000 --> 1:04:35.700
 and there's good psychological research showing

1:04:35.700 --> 1:04:39.440
 that people who give more tend to be more satisfied

1:04:39.440 --> 1:04:40.580
 with their lives.

1:04:40.580 --> 1:04:41.940
 And I think this has something to do

1:04:41.940 --> 1:04:44.900
 with having a purpose that's larger than yourself

1:04:44.900 --> 1:04:49.620
 and therefore never being, if you like,

1:04:49.620 --> 1:04:51.180
 never being bored sitting around,

1:04:51.180 --> 1:04:52.800
 oh, you know, what will I do next?

1:04:52.800 --> 1:04:54.260
 I've got nothing to do.

1:04:54.260 --> 1:04:56.440
 In a world like this, there are many good things

1:04:56.440 --> 1:04:59.420
 that you can do and enjoy doing them.

1:04:59.420 --> 1:05:02.380
 Plus you're working with other people

1:05:02.380 --> 1:05:03.940
 in the effective altruism movement

1:05:03.940 --> 1:05:06.280
 who are forming a community of other people

1:05:06.280 --> 1:05:09.300
 with similar ideas and they tend to be interesting,

1:05:09.300 --> 1:05:11.100
 thoughtful and good people as well.

1:05:11.100 --> 1:05:14.180
 And having friends of that sort is another big contribution

1:05:14.180 --> 1:05:16.020
 to having a good life.

1:05:16.020 --> 1:05:20.340
 So we talked about big things that are beyond ourselves,

1:05:20.340 --> 1:05:24.600
 but we're also just human and mortal.

1:05:24.600 --> 1:05:27.420
 Do you ponder your own mortality?

1:05:27.420 --> 1:05:29.660
 Is there insights about your philosophy,

1:05:29.660 --> 1:05:33.800
 the ethics that you gain from pondering your own mortality?

1:05:35.780 --> 1:05:37.940
 Clearly, you know, as you get into your 70s,

1:05:37.940 --> 1:05:40.380
 you can't help thinking about your own mortality.

1:05:40.380 --> 1:05:44.780
 Uh, but I don't know that I have great insights

1:05:44.780 --> 1:05:47.140
 into that from my philosophy.

1:05:47.140 --> 1:05:50.460
 I don't think there's anything after the death of my body,

1:05:50.460 --> 1:05:53.500
 you know, assuming that we won't be able to upload my mind

1:05:53.500 --> 1:05:55.380
 into anything at the time when I die.

1:05:56.860 --> 1:05:58.460
 So I don't think there's any afterlife

1:05:58.460 --> 1:06:00.940
 or anything to look forward to in that sense.

1:06:00.940 --> 1:06:01.900
 Do you fear death?

1:06:01.900 --> 1:06:04.140
 So if you look at Ernest Becker

1:06:04.140 --> 1:06:08.060
 and describing the motivating aspects

1:06:08.060 --> 1:06:13.060
 of our ability to be cognizant of our mortality,

1:06:14.820 --> 1:06:17.460
 do you have any of those elements

1:06:17.460 --> 1:06:19.680
 in your drive and your motivation in life?

1:06:21.020 --> 1:06:23.500
 I suppose the fact that you have only a limited time

1:06:23.500 --> 1:06:25.840
 to achieve the things that you want to achieve

1:06:25.840 --> 1:06:27.320
 gives you some sort of motivation

1:06:27.320 --> 1:06:29.700
 to get going and achieving them.

1:06:29.700 --> 1:06:31.020
 And if we thought we were immortal,

1:06:31.020 --> 1:06:32.600
 we might say, ah, you know,

1:06:32.600 --> 1:06:35.000
 I can put that off for another decade or two.

1:06:36.080 --> 1:06:37.740
 So there's that about it.

1:06:37.740 --> 1:06:40.020
 But otherwise, you know, no,

1:06:40.020 --> 1:06:42.060
 I'd rather have more time to do more.

1:06:42.060 --> 1:06:45.860
 I'd also like to be able to see how things go

1:06:45.860 --> 1:06:47.500
 that I'm interested in, you know.

1:06:47.500 --> 1:06:49.940
 Is climate change gonna turn out to be as dire

1:06:49.940 --> 1:06:53.500
 as a lot of scientists say that it is going to be?

1:06:53.500 --> 1:06:55.500
 Will we somehow scrape through

1:06:55.500 --> 1:06:57.860
 with less damage than we thought?

1:06:57.860 --> 1:06:59.840
 I'd really like to know the answers to those questions,

1:06:59.840 --> 1:07:02.180
 but I guess I'm not going to.

1:07:02.180 --> 1:07:05.780
 Well, you said there's nothing afterwards.

1:07:05.780 --> 1:07:08.100
 So let me ask the even more absurd question.

1:07:08.100 --> 1:07:10.100
 What do you think is the meaning of it all?

1:07:11.120 --> 1:07:14.120
 I think the meaning of life is the meaning we give to it.

1:07:14.120 --> 1:07:18.100
 I don't think that we were brought into the universe

1:07:18.100 --> 1:07:21.860
 for any kind of larger purpose.

1:07:21.860 --> 1:07:24.100
 But given that we exist,

1:07:24.100 --> 1:07:26.460
 I think we can recognize that some things

1:07:26.460 --> 1:07:29.500
 are objectively bad.

1:07:30.820 --> 1:07:32.620
 Extreme suffering is an example,

1:07:32.620 --> 1:07:35.060
 and other things are objectively good,

1:07:35.060 --> 1:07:38.020
 like having a rich, fulfilling, enjoyable,

1:07:38.020 --> 1:07:42.780
 pleasurable life, and we can try to do our part

1:07:42.780 --> 1:07:45.820
 in reducing the bad things and increasing the good things.

1:07:47.220 --> 1:07:50.540
 So one way, the meaning is to do a little bit more

1:07:50.540 --> 1:07:52.660
 of the good things, objectively good things,

1:07:52.660 --> 1:07:55.460
 and a little bit less of the bad things.

1:07:55.460 --> 1:07:58.940
 Yes, so do as much of the good things as you can

1:07:58.940 --> 1:08:00.580
 and as little of the bad things.

1:08:00.580 --> 1:08:03.020
 You beautifully put, I don't think there's a better place

1:08:03.020 --> 1:08:04.900
 to end it, thank you so much for talking today.

1:08:04.900 --> 1:08:05.740
 Thanks very much, Lex.

1:08:05.740 --> 1:08:08.780
 It's been really interesting talking to you.

1:08:08.780 --> 1:08:10.260
 Thanks for listening to this conversation

1:08:10.260 --> 1:08:13.420
 with Peter Singer, and thank you to our sponsors,

1:08:13.420 --> 1:08:15.940
 Cash App and Masterclass.

1:08:15.940 --> 1:08:17.660
 Please consider supporting the podcast

1:08:17.660 --> 1:08:21.620
 by downloading Cash App and using the code LexPodcast,

1:08:21.620 --> 1:08:26.140
 and signing up at masterclass.com slash Lex.

1:08:26.140 --> 1:08:28.900
 Click the links, buy all the stuff.

1:08:28.900 --> 1:08:30.960
 It's the best way to support this podcast

1:08:30.960 --> 1:08:35.220
 and the journey I'm on in my research and startup.

1:08:35.220 --> 1:08:38.020
 If you enjoy this thing, subscribe on YouTube,

1:08:38.020 --> 1:08:41.660
 review it with 5,000 Apple Podcast, support on Patreon,

1:08:41.660 --> 1:08:43.980
 or connect with me on Twitter at Lex Friedman,

1:08:43.980 --> 1:08:48.940
 spelled without the E, just F R I D M A N.

1:08:48.940 --> 1:08:50.860
 And now, let me leave you with some words

1:08:50.860 --> 1:08:54.940
 from Peter Singer, what one generation finds ridiculous,

1:08:54.940 --> 1:08:59.020
 the next accepts, and the third shudders

1:08:59.020 --> 1:09:01.100
 when looks back at what the first did.

1:09:01.100 --> 1:09:28.100
 Thank you for listening, and hope to see you next time.

