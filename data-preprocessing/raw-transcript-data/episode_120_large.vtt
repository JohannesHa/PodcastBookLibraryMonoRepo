WEBVTT

00:00.000 --> 00:03.220
 The following is a conversation with Francois Chollet,

00:03.220 --> 00:05.260
 his second time on the podcast.

00:05.260 --> 00:09.580
 He's both a world class engineer and a philosopher

00:09.580 --> 00:13.180
 in the realm of deep learning and artificial intelligence.

00:13.180 --> 00:16.200
 This time, we talk a lot about his paper titled

00:16.200 --> 00:19.040
 on the measure of intelligence that discusses

00:19.040 --> 00:22.440
 how we might define and measure general intelligence

00:22.440 --> 00:24.640
 in our computing machinery.

00:24.640 --> 00:26.420
 Quick summary of the sponsors,

00:26.420 --> 00:29.460
 Babbel, Masterclass, and Cash App.

00:29.460 --> 00:31.240
 Click the sponsor links in the description

00:31.240 --> 00:34.500
 to get a discount and to support this podcast.

00:34.500 --> 00:36.880
 As a side note, let me say that the serious,

00:36.880 --> 00:38.720
 rigorous scientific study

00:38.720 --> 00:42.220
 of artificial general intelligence is a rare thing.

00:42.220 --> 00:44.080
 The mainstream machine learning community works

00:44.080 --> 00:47.740
 on very narrow AI with very narrow benchmarks.

00:47.740 --> 00:49.920
 This is very good for incremental

00:49.920 --> 00:53.200
 and sometimes big incremental progress.

00:53.200 --> 00:56.060
 On the other hand, the outside the mainstream,

00:56.060 --> 01:00.020
 renegade, you could say, AGI community works

01:00.020 --> 01:03.000
 on approaches that verge on the philosophical

01:03.000 --> 01:07.300
 and even the literary without big public benchmarks.

01:07.300 --> 01:10.640
 Walking the line between the two worlds is a rare breed,

01:10.640 --> 01:12.360
 but it doesn't have to be.

01:12.360 --> 01:15.320
 I ran the AGI series at MIT as an attempt

01:15.320 --> 01:17.700
 to inspire more people to walk this line.

01:17.700 --> 01:20.020
 Deep mind and open AI for a time

01:20.020 --> 01:23.180
 and still on occasion walk this line.

01:23.180 --> 01:25.860
 Francois Chollet does as well.

01:25.860 --> 01:27.620
 I hope to also.

01:27.620 --> 01:29.880
 It's a beautiful dream to work towards

01:29.880 --> 01:32.480
 and to make real one day.

01:32.480 --> 01:34.580
 If you enjoy this thing, subscribe on YouTube,

01:34.580 --> 01:36.760
 review it with five stars on Apple Podcast,

01:36.760 --> 01:39.020
 follow on Spotify, support on Patreon,

01:39.020 --> 01:42.020
 or connect with me on Twitter at Lex Friedman.

01:42.020 --> 01:44.240
 As usual, I'll do a few minutes of ads now

01:44.240 --> 01:45.780
 and no ads in the middle.

01:45.780 --> 01:47.440
 I try to make these interesting,

01:47.440 --> 01:50.620
 but I give you timestamps so you can skip.

01:50.620 --> 01:52.660
 But still, please do check out the sponsors

01:52.660 --> 01:54.580
 by clicking the links in the description.

01:54.580 --> 01:57.900
 It's the best way to support this podcast.

01:57.900 --> 02:00.100
 This show is sponsored by Babbel,

02:00.100 --> 02:02.460
 an app and website that gets you speaking

02:02.460 --> 02:04.360
 in a new language within weeks.

02:04.360 --> 02:08.200
 Go to babbel.com and use code Lex to get three months free.

02:08.200 --> 02:11.460
 They offer 14 languages, including Spanish, French,

02:11.460 --> 02:15.220
 Italian, German, and yes, Russian.

02:15.220 --> 02:17.340
 Daily lessons are 10 to 15 minutes,

02:17.340 --> 02:19.060
 super easy, effective,

02:19.060 --> 02:22.240
 designed by over 100 language experts.

02:22.240 --> 02:24.700
 Let me read a few lines from the Russian poem

02:24.700 --> 02:29.020
 Noch, ulitsa, fanar, apteka, by Alexander Bloch,

02:29.020 --> 02:32.580
 that you'll start to understand if you sign up to Babbel.

02:32.580 --> 02:35.220
 Noch, ulitsa, fanar, apteka,

02:35.220 --> 02:38.100
 Bessmysliny, ituskly, svet,

02:38.100 --> 02:41.140
 Zhevi esho, khod chetvert veka,

02:41.140 --> 02:44.700
 Vse budet tak, ishoda, net.

02:44.700 --> 02:48.500
 Now, I say that you'll start to understand this poem

02:48.500 --> 02:51.420
 because Russian starts with a language

02:51.420 --> 02:54.020
 and ends with vodka.

02:54.020 --> 02:56.600
 Now, the latter part is definitely not endorsed

02:56.600 --> 02:58.020
 or provided by Babbel.

02:58.020 --> 03:00.340
 It will probably lose me this sponsorship,

03:00.340 --> 03:02.460
 although it hasn't yet.

03:02.460 --> 03:04.460
 But once you graduate with Babbel,

03:04.460 --> 03:06.120
 you can enroll in my advanced course

03:06.120 --> 03:09.200
 of late night Russian conversation over vodka.

03:09.200 --> 03:11.260
 No app for that yet.

03:11.260 --> 03:13.740
 So get started by visiting babbel.com

03:13.740 --> 03:17.080
 and use code Lex to get three months free.

03:18.180 --> 03:20.980
 This show is also sponsored by Masterclass.

03:20.980 --> 03:23.380
 Sign up at masterclass.com slash Lex

03:23.380 --> 03:26.580
 to get a discount and to support this podcast.

03:26.580 --> 03:28.060
 When I first heard about Masterclass,

03:28.060 --> 03:29.980
 I thought it was too good to be true.

03:29.980 --> 03:32.340
 I still think it's too good to be true.

03:32.340 --> 03:35.420
 For $180 a year, you get an all access pass

03:35.420 --> 03:38.740
 to watch courses from, to list some of my favorites.

03:38.740 --> 03:41.340
 Chris Hatfield on space exploration,

03:41.340 --> 03:43.500
 hope to have him in this podcast one day.

03:43.500 --> 03:46.660
 Neil Dugras Tyson on scientific thinking and communication,

03:46.660 --> 03:47.900
 Neil too.

03:47.900 --> 03:50.140
 Will Wright, creator of SimCity and Sims

03:50.140 --> 03:52.780
 on game design, Carlos Santana on guitar,

03:52.780 --> 03:55.980
 Kary Kasparov on chess, Daniel Nagrano on poker,

03:55.980 --> 03:57.240
 and many more.

03:57.240 --> 03:59.700
 Chris Hatfield explaining how rockets work

03:59.700 --> 04:01.740
 and the experience of being watched at the space

04:01.740 --> 04:03.300
 alone is worth the money.

04:03.300 --> 04:06.540
 By the way, you can watch it on basically any device.

04:06.540 --> 04:09.380
 Once again, sign up at masterclass.com slash Lex

04:09.380 --> 04:12.460
 to get a discount and to support this podcast.

04:13.340 --> 04:16.460
 This show finally is presented by Cash App,

04:16.460 --> 04:18.720
 the number one finance app in the App Store.

04:18.720 --> 04:21.220
 When you get it, use code LexPodcast.

04:21.220 --> 04:23.300
 Cash App lets you send money to friends,

04:23.300 --> 04:25.460
 buy Bitcoin, and invest in the stock market

04:25.460 --> 04:27.260
 with as little as $1.

04:27.260 --> 04:28.980
 Since Cash App allows you to send

04:28.980 --> 04:30.540
 and receive money digitally,

04:30.540 --> 04:33.860
 let me mention a surprising fact related to physical money.

04:33.860 --> 04:35.700
 Of all the currency in the world,

04:35.700 --> 04:39.300
 roughly 8% of it is actually physical money.

04:39.300 --> 04:42.820
 The other 92% of the money only exists digitally,

04:42.820 --> 04:45.280
 and that's only going to increase.

04:45.280 --> 04:47.400
 So again, if you get Cash App from the App Store

04:47.400 --> 04:50.660
 through Google Play and use code LexPodcast,

04:50.660 --> 04:51.740
 you get 10 bucks,

04:51.740 --> 04:54.420
 and Cash App will also donate $10 to FIRST,

04:54.420 --> 04:57.000
 an organization that is helping to advance robotics

04:57.000 --> 05:00.500
 and STEM education for young people around the world.

05:00.500 --> 05:03.900
 And now here's my conversation with Francois Chalet.

05:05.060 --> 05:07.360
 What philosophers, thinkers, or ideas

05:07.360 --> 05:10.700
 had a big impact on you growing up and today?

05:10.700 --> 05:14.860
 So one author that had a big impact on me

05:14.860 --> 05:18.820
 when I read his books as a teenager was Jean Piaget,

05:18.820 --> 05:21.380
 who is a Swiss psychologist,

05:21.380 --> 05:25.540
 is considered to be the father of developmental psychology.

05:25.540 --> 05:27.620
 And he has a large body of work about

05:28.700 --> 05:33.380
 basically how intelligence develops in children.

05:33.380 --> 05:35.500
 And so it's very old work,

05:35.500 --> 05:39.140
 like most of it is from the 1930s, 1940s.

05:39.140 --> 05:40.900
 So it's not quite up to date.

05:40.900 --> 05:43.820
 It's actually superseded by many newer developments

05:43.820 --> 05:45.660
 in developmental psychology.

05:45.660 --> 05:49.600
 But to me, it was very interesting, very striking,

05:49.600 --> 05:51.340
 and actually shaped the early ways

05:51.340 --> 05:53.820
 in which I started thinking about the mind

05:53.820 --> 05:56.220
 and the development of intelligence as a teenager.

05:56.220 --> 05:58.460
 His actual ideas or the way he thought about it

05:58.460 --> 05:59.840
 or just the fact that you could think

05:59.840 --> 06:01.600
 about the developing mind at all?

06:01.600 --> 06:02.500
 I guess both.

06:02.500 --> 06:04.940
 Jean Piaget is the author that really introduced me

06:04.940 --> 06:07.980
 to the notion that intelligence and the mind

06:07.980 --> 06:11.120
 is something that you construct throughout your life

06:11.120 --> 06:15.780
 and that children construct it in stages.

06:15.780 --> 06:17.460
 And I thought that was a very interesting idea,

06:17.460 --> 06:20.460
 which is, of course, very relevant to AI,

06:20.460 --> 06:22.020
 to building artificial minds.

06:23.180 --> 06:25.860
 Another book that I read around the same time

06:25.860 --> 06:27.260
 that had a big impact on me,

06:28.900 --> 06:32.100
 and there was actually a little bit of overlap

06:32.100 --> 06:32.980
 with Jean Piaget as well,

06:32.980 --> 06:35.340
 and I read it around the same time,

06:35.340 --> 06:39.860
 is Geoff Hawking's On Intelligence, which is a classic.

06:39.860 --> 06:42.500
 And he has this vision of the mind

06:42.500 --> 06:47.500
 as a multi scale hierarchy of temporal prediction modules.

06:47.820 --> 06:50.020
 And these ideas really resonated with me,

06:50.020 --> 06:53.940
 like the notion of a modular hierarchy

06:55.440 --> 07:00.100
 of potentially compression functions

07:00.100 --> 07:01.700
 or prediction functions.

07:01.700 --> 07:03.980
 I thought it was really, really interesting,

07:03.980 --> 07:07.100
 and it shaped the way I started thinking

07:07.100 --> 07:09.760
 about how to build minds.

07:09.760 --> 07:13.740
 The hierarchical nature, which aspect?

07:13.740 --> 07:17.520
 Also, he's a neuroscientist, so he was thinking actual,

07:17.520 --> 07:20.580
 he was basically talking about how our mind works.

07:20.580 --> 07:23.260
 Yeah, the notion that cognition is prediction

07:23.260 --> 07:25.460
 was an idea that was kind of new to me at the time

07:25.460 --> 07:27.840
 and that I really loved at the time.

07:27.840 --> 07:31.900
 And yeah, and the notion that there are multiple scales

07:31.900 --> 07:34.020
 of processing in the brain.

07:35.320 --> 07:36.260
 The hierarchy.

07:36.260 --> 07:37.100
 Yes.

07:37.100 --> 07:38.600
 This was before deep learning.

07:38.600 --> 07:41.140
 These ideas of hierarchies in AI

07:41.140 --> 07:43.180
 have been around for a long time,

07:43.180 --> 07:45.020
 even before on intelligence.

07:45.020 --> 07:47.100
 They've been around since the 1980s.

07:48.980 --> 07:50.500
 And yeah, that was before deep learning.

07:50.500 --> 07:53.500
 But of course, I think these ideas really found

07:53.500 --> 07:58.100
 their practical implementation in deep learning.

07:58.100 --> 07:59.740
 What about the memory side of things?

07:59.740 --> 08:02.860
 I think he was talking about knowledge representation.

08:02.860 --> 08:04.420
 Do you think about memory a lot?

08:04.420 --> 08:06.340
 One way you can think of neural networks

08:06.340 --> 08:10.780
 as a kind of memory, you're memorizing things,

08:10.780 --> 08:14.260
 but it doesn't seem to be the kind of memory

08:14.260 --> 08:15.980
 that's in our brains,

08:16.880 --> 08:18.660
 or it doesn't have the same rich complexity,

08:18.660 --> 08:20.660
 long term nature that's in our brains.

08:20.660 --> 08:23.980
 Yes, the brain is more of a sparse access memory

08:23.980 --> 08:27.740
 so that you can actually retrieve very precisely

08:27.740 --> 08:30.100
 like bits of your experience.

08:30.100 --> 08:33.500
 The retrieval aspect, you can like introspect,

08:33.500 --> 08:35.300
 you can ask yourself questions.

08:35.300 --> 08:38.260
 I guess you can program your own memory

08:38.260 --> 08:41.700
 and language is actually the tool you use to do that.

08:41.700 --> 08:46.360
 I think language is a kind of operating system for the mind

08:46.360 --> 08:47.820
 and use language.

08:47.820 --> 08:51.800
 Well, one of the uses of language is as a query

08:51.800 --> 08:53.860
 that you run over your own memory,

08:53.860 --> 08:57.940
 use words as keys to retrieve specific experiences

08:57.940 --> 09:00.140
 or specific concepts, specific thoughts.

09:00.140 --> 09:02.380
 Like language is a way you store thoughts,

09:02.380 --> 09:04.740
 not just in writing, in the physical world,

09:04.740 --> 09:06.100
 but also in your own mind.

09:06.100 --> 09:07.580
 And it's also how you retrieve them.

09:07.580 --> 09:10.000
 Like, imagine if you didn't have language,

09:10.000 --> 09:11.740
 then you would have to,

09:11.740 --> 09:14.340
 you would not really have a self,

09:14.340 --> 09:18.620
 internally triggered way of retrieving past thoughts.

09:18.620 --> 09:21.300
 You would have to rely on external experiences.

09:21.300 --> 09:24.020
 For instance, you see a specific site,

09:24.020 --> 09:26.780
 you smell a specific smell and that brings up memories,

09:26.780 --> 09:28.700
 but you would not really have a way

09:28.700 --> 09:32.740
 to deliberately access these memories without language.

09:32.740 --> 09:33.980
 Well, the interesting thing you mentioned

09:33.980 --> 09:37.420
 is you can also program the memory.

09:37.420 --> 09:39.980
 You can change it probably with language.

09:39.980 --> 09:41.500
 Yeah, using language, yes.

09:41.500 --> 09:44.100
 Well, let me ask you a Chomsky question,

09:44.100 --> 09:45.980
 which is like, first of all,

09:45.980 --> 09:49.100
 do you think language is like fundamental,

09:49.100 --> 09:54.100
 like there's turtles, what's at the bottom of the turtles?

09:54.460 --> 09:57.260
 They don't go, it can't be turtles all the way down.

09:57.260 --> 10:00.260
 Is language at the bottom of cognition of everything?

10:00.260 --> 10:05.260
 Is like language, the fundamental aspect

10:05.300 --> 10:10.300
 of like what it means to be a thinking thing?

10:10.700 --> 10:12.100
 No, I don't think so.

10:12.100 --> 10:12.940
 I think language is.

10:12.940 --> 10:14.620
 You disagree with Norm Chomsky?

10:14.620 --> 10:17.900
 Yes, I think language is a layer on top of cognition.

10:17.900 --> 10:21.740
 So it is fundamental to cognition in the sense that

10:21.740 --> 10:23.380
 to use a computing metaphor,

10:23.380 --> 10:28.060
 I see language as the operating system of the brain,

10:28.060 --> 10:29.500
 of the human mind.

10:29.500 --> 10:33.180
 And the operating system is a layer on top of the computer.

10:33.180 --> 10:36.140
 The computer exists before the operating system,

10:36.140 --> 10:39.500
 but the operating system is how you make it truly useful.

10:39.500 --> 10:43.940
 And the operating system is most likely Windows, not Linux,

10:43.940 --> 10:45.860
 because language is messy.

10:45.860 --> 10:49.460
 Yeah, it's messy and it's pretty difficult

10:49.460 --> 10:53.140
 to inspect it, introspect it.

10:53.140 --> 10:55.100
 How do you think about language?

10:55.100 --> 11:00.060
 Like we use actually sort of human interpretable language,

11:00.060 --> 11:03.100
 but is there something like a deeper,

11:03.100 --> 11:07.900
 that's closer to like logical type of statements?

11:08.860 --> 11:13.860
 Like, yeah, what is the nature of language, do you think?

11:16.140 --> 11:18.540
 Like is there something deeper than like the syntactic rules

11:18.540 --> 11:19.380
 we construct?

11:19.380 --> 11:22.860
 Is there something that doesn't require utterances

11:22.860 --> 11:25.580
 or writing or so on?

11:25.580 --> 11:27.460
 Are you asking about the possibility

11:27.460 --> 11:30.900
 that there could exist languages for thinking

11:30.900 --> 11:32.820
 that are not made of words?

11:32.820 --> 11:33.660
 Yeah.

11:33.660 --> 11:34.500
 Yeah, I think so.

11:34.500 --> 11:38.580
 I think, so the mind is layers, right?

11:38.580 --> 11:41.780
 And language is almost like the outermost,

11:41.780 --> 11:43.140
 the uppermost layer.

11:44.620 --> 11:46.780
 But before we think in words,

11:46.780 --> 11:51.100
 I think we think in terms of emotion in space

11:51.100 --> 11:54.180
 and we think in terms of physical actions.

11:54.180 --> 11:56.860
 And I think babies in particular,

11:56.860 --> 12:01.380
 probably expresses thoughts in terms of the actions

12:01.380 --> 12:03.700
 that they've seen or that they can perform

12:03.700 --> 12:08.020
 and in terms of motions of objects in their environment

12:08.020 --> 12:10.860
 before they start thinking in terms of words.

12:10.860 --> 12:13.900
 It's amazing to think about that

12:13.900 --> 12:16.780
 as the building blocks of language.

12:16.780 --> 12:21.780
 So like the kind of actions and ways the babies see the world

12:21.820 --> 12:23.260
 as like more fundamental

12:23.260 --> 12:26.220
 than the beautiful Shakespearean language

12:26.220 --> 12:27.540
 you construct on top of it.

12:28.620 --> 12:30.500
 And we probably don't have any idea

12:30.500 --> 12:31.700
 what that looks like, right?

12:31.700 --> 12:34.020
 Like what, because it's important

12:34.020 --> 12:37.460
 for them trying to engineer it into AI systems.

12:38.460 --> 12:42.060
 I think visual analogies and motion

12:42.060 --> 12:45.380
 is a fundamental building block of the mind.

12:45.380 --> 12:48.540
 And you actually see it reflected in language.

12:48.540 --> 12:51.820
 Like language is full of special metaphors.

12:51.820 --> 12:53.820
 And when you think about things,

12:53.820 --> 12:57.380
 I consider myself very much as a visual thinker.

12:57.380 --> 13:01.140
 You often express these thoughts

13:01.140 --> 13:05.260
 by using things like visualizing concepts

13:06.500 --> 13:09.940
 in 2D space or like you solve problems

13:09.940 --> 13:14.940
 by imagining yourself navigating a concept space.

13:14.940 --> 13:17.940
 So I don't know if you have this sort of experience.

13:17.940 --> 13:19.860
 You said visualizing concept space.

13:19.860 --> 13:22.300
 So like, so I certainly think about,

13:24.820 --> 13:27.980
 I certainly visualize mathematical concepts,

13:27.980 --> 13:31.420
 but you mean like in concept space,

13:32.340 --> 13:34.860
 visually you're embedding ideas

13:34.860 --> 13:36.940
 into a three dimensional space

13:36.940 --> 13:38.820
 you can explore with your mind essentially?

13:38.820 --> 13:40.340
 You should be more like 2D, but yeah.

13:40.340 --> 13:41.180
 2D?

13:41.180 --> 13:42.100
 Yeah.

13:42.100 --> 13:43.180
 You're a flatlander.

13:43.180 --> 13:45.700
 You're, okay.

13:45.700 --> 13:49.660
 No, I do not.

13:49.660 --> 13:52.780
 I always have to, before I jump from concept to concept,

13:52.780 --> 13:57.100
 I have to put it back down on paper.

13:57.100 --> 13:58.060
 It has to be on paper.

13:58.060 --> 14:03.060
 I can only travel on 2D paper, not inside my mind.

14:03.340 --> 14:05.340
 You're able to move inside your mind.

14:05.340 --> 14:07.900
 But even if you're writing like a paper, for instance,

14:07.900 --> 14:11.020
 don't you have like a spatial representation of your paper?

14:11.020 --> 14:16.020
 Like you visualize where ideas lie topologically

14:16.660 --> 14:18.980
 in relationship to other ideas,

14:18.980 --> 14:22.500
 kind of like a subway map of the ideas in your paper.

14:22.500 --> 14:23.380
 Yeah, that's true.

14:23.380 --> 14:27.900
 I mean, there is, in papers, I don't know about you,

14:27.900 --> 14:30.540
 but it feels like there's a destination.

14:32.540 --> 14:36.220
 There's a key idea that you want to arrive at.

14:36.220 --> 14:39.340
 And a lot of it is in the fog

14:39.340 --> 14:40.820
 and you're trying to kind of,

14:40.820 --> 14:45.820
 it's almost like, what's that called

14:46.180 --> 14:49.900
 when you do a path planning search from both directions,

14:49.900 --> 14:51.500
 from the start and from the end.

14:52.700 --> 14:54.740
 And then you find, you do like shortest path,

14:54.740 --> 14:57.380
 but like, you know, in game playing,

14:57.380 --> 15:01.020
 you do this with like A star from both sides.

15:01.020 --> 15:03.420
 And you see where we're on the join.

15:03.420 --> 15:05.740
 Yeah, so you kind of do, at least for me,

15:05.740 --> 15:07.100
 I think like, first of all,

15:07.100 --> 15:10.800
 just exploring from the start from like first principles,

15:10.800 --> 15:15.620
 what do I know, what can I start proving from that, right?

15:15.620 --> 15:18.060
 And then from the destination,

15:18.060 --> 15:20.460
 if you start backtracking,

15:20.460 --> 15:25.400
 like if I want to show some kind of sets of ideas,

15:25.400 --> 15:28.300
 what would it take to show them and you kind of backtrack,

15:28.300 --> 15:29.140
 but like, yeah,

15:29.140 --> 15:31.260
 I don't think I'm doing all that in my mind though.

15:31.260 --> 15:33.180
 Like I'm putting it down on paper.

15:33.180 --> 15:35.500
 Do you use mind maps to organize your ideas?

15:35.500 --> 15:37.740
 Yeah, I like mind maps.

15:37.740 --> 15:38.580
 Let's get into this,

15:38.580 --> 15:41.180
 because I've been so jealous of people.

15:41.180 --> 15:42.120
 I haven't really tried it.

15:42.120 --> 15:45.500
 I've been jealous of people that seem to like,

15:45.500 --> 15:48.140
 they get like this fire of passion in their eyes

15:48.140 --> 15:50.020
 because everything starts making sense.

15:50.020 --> 15:51.940
 It's like Tom Cruise in the movie

15:51.940 --> 15:53.820
 was like moving stuff around.

15:53.820 --> 15:55.900
 Some of the most brilliant people I know use mind maps.

15:55.900 --> 15:57.660
 I haven't tried really.

15:57.660 --> 16:01.240
 Can you explain what the hell a mind map is?

16:01.240 --> 16:03.700
 I guess mind map is a way to make

16:03.700 --> 16:05.940
 kind of like the mess inside your mind

16:05.940 --> 16:10.020
 to just put it on paper so that you gain more control over it.

16:10.020 --> 16:13.020
 It's a way to organize things on paper

16:13.020 --> 16:16.420
 and as kind of like a consequence

16:16.420 --> 16:17.940
 of organizing things on paper,

16:17.940 --> 16:20.300
 they start being more organized inside your own mind.

16:20.300 --> 16:21.540
 So what does that look like?

16:21.540 --> 16:23.980
 You put, like, do you have an example?

16:23.980 --> 16:27.360
 Like what's the first thing you write on paper?

16:27.360 --> 16:28.980
 What's the second thing you write?

16:28.980 --> 16:31.660
 I mean, typically you draw a mind map

16:31.660 --> 16:34.860
 to organize the way you think about a topic.

16:34.860 --> 16:37.340
 So you would start by writing down

16:37.340 --> 16:39.580
 like the key concept about that topic.

16:39.580 --> 16:42.220
 Like you would write intelligence or something,

16:42.220 --> 16:45.660
 and then you would start adding associative connections.

16:45.660 --> 16:46.860
 Like what do you think about

16:46.860 --> 16:48.100
 when you think about intelligence?

16:48.100 --> 16:50.460
 What do you think are the key elements of intelligence?

16:50.460 --> 16:52.340
 So maybe you would have language, for instance,

16:52.340 --> 16:53.420
 and you'd have motion.

16:53.420 --> 16:55.460
 And so you would start drawing notes with these things.

16:55.460 --> 16:57.220
 And then you would see what do you think about

16:57.220 --> 16:59.140
 when you think about motion and so on.

16:59.140 --> 17:00.620
 And you would go like that, like a tree.

17:00.620 --> 17:05.620
 Is it a tree mostly or is it a graph too, like a tree?

17:05.660 --> 17:07.980
 Oh, it's more of a graph than a tree.

17:07.980 --> 17:12.980
 And it's not limited to just writing down words.

17:13.260 --> 17:15.940
 You can also draw things.

17:15.940 --> 17:19.660
 And it's not supposed to be purely hierarchical, right?

17:21.660 --> 17:24.540
 The point is that once you start writing it down,

17:24.540 --> 17:27.500
 you can start reorganizing it so that it makes more sense,

17:27.500 --> 17:29.940
 so that it's connected in a more effective way.

17:29.940 --> 17:34.460
 See, but I'm so OCD that you just mentioned

17:34.460 --> 17:37.060
 intelligence and language and motion.

17:37.060 --> 17:39.100
 I would start becoming paranoid

17:39.100 --> 17:41.980
 that the categorization isn't perfect.

17:41.980 --> 17:46.980
 Like that I would become paralyzed with the mind map

17:47.860 --> 17:49.660
 that like this may not be.

17:49.660 --> 17:52.660
 So like the, even though you're just doing

17:52.660 --> 17:55.380
 associative kind of connections,

17:55.380 --> 17:58.460
 there's an implied hierarchy that's emerging.

17:58.460 --> 17:59.900
 And I would start becoming paranoid

17:59.900 --> 18:02.340
 that it's not the proper hierarchy.

18:02.340 --> 18:04.940
 So you're not just, one way to see mind maps

18:04.940 --> 18:07.060
 is you're putting thoughts on paper.

18:07.060 --> 18:10.580
 It's like a stream of consciousness,

18:10.580 --> 18:12.220
 but then you can also start getting paranoid.

18:12.220 --> 18:15.140
 Well, is this the right hierarchy?

18:15.140 --> 18:17.780
 Sure, which it's mind maps, your mind map.

18:17.780 --> 18:19.420
 You're free to draw anything you want.

18:19.420 --> 18:20.860
 You're free to draw any connection you want.

18:20.860 --> 18:23.420
 And you can just make a different mind map

18:23.420 --> 18:26.260
 if you think the central node is not the right node.

18:26.260 --> 18:29.700
 Yeah, I suppose there's a fear of being wrong.

18:29.700 --> 18:32.660
 If you want to organize your ideas

18:32.660 --> 18:35.540
 by writing down what you think,

18:35.540 --> 18:37.380
 which I think is very effective.

18:37.380 --> 18:40.140
 Like how do you know what you think about something

18:40.140 --> 18:42.940
 if you don't write it down, right?

18:42.940 --> 18:46.180
 If you do that, the thing is that it imposes

18:46.180 --> 18:49.980
 much more syntactic structure over your ideas,

18:49.980 --> 18:51.540
 which is not required with mind maps.

18:51.540 --> 18:54.180
 So mind map is kind of like a lower level,

18:54.180 --> 18:57.900
 more freehand way of organizing your thoughts.

18:57.900 --> 18:59.580
 And once you've drawn it,

18:59.580 --> 19:03.620
 then you can start actually voicing your thoughts

19:03.620 --> 19:05.380
 in terms of, you know, paragraphs.

19:05.380 --> 19:08.780
 It's a two dimensional aspect of layout too, right?

19:08.780 --> 19:09.620
 Yeah.

19:09.620 --> 19:12.860
 It's a kind of flower, I guess, you start.

19:12.860 --> 19:15.820
 There's usually, you want to start with a central concept?

19:15.820 --> 19:16.660
 Yes.

19:16.660 --> 19:17.500
 Then you move out.

19:17.500 --> 19:19.140
 Typically it ends up more like a subway map.

19:19.140 --> 19:20.660
 So it ends up more like a graph,

19:20.660 --> 19:23.500
 a topological graph without a root node.

19:23.500 --> 19:25.020
 Yeah, so like in a subway map,

19:25.020 --> 19:27.300
 there are some nodes that are more connected than others.

19:27.300 --> 19:30.940
 And there are some nodes that are more important than others.

19:30.940 --> 19:32.380
 So there are destinations,

19:32.380 --> 19:36.420
 but it's not going to be purely like a tree, for instance.

19:36.420 --> 19:38.540
 Yeah, it's fascinating to think that

19:38.540 --> 19:42.420
 if there's something to that about the way our mind thinks.

19:42.420 --> 19:45.820
 By the way, I just kind of remembered obvious thing

19:45.820 --> 19:49.020
 that I have probably thousands of documents

19:49.020 --> 19:53.620
 in Google Doc at this point, that are bullet point lists,

19:53.620 --> 19:57.860
 which is, you can probably map a mind map

19:57.860 --> 19:59.660
 to a bullet point list.

20:01.460 --> 20:05.060
 It's the same, it's a, no, it's not, it's a tree.

20:05.060 --> 20:06.220
 It's a tree, yeah.

20:06.220 --> 20:07.900
 So I create trees,

20:07.900 --> 20:10.740
 but also they don't have the visual element.

20:10.740 --> 20:13.460
 Like, I guess I'm comfortable with the structure.

20:13.460 --> 20:15.740
 It feels like the narrowness,

20:15.740 --> 20:18.260
 the constraints feel more comforting.

20:18.260 --> 20:20.300
 If you have thousands of documents

20:20.300 --> 20:23.100
 with your own thoughts in Google Docs,

20:23.100 --> 20:26.580
 why don't you write some kind of search engine,

20:26.580 --> 20:30.900
 like maybe a mind map, a piece of software,

20:30.900 --> 20:33.980
 mind mapping software, where you write down a concept

20:33.980 --> 20:37.500
 and then it gives you sentences or paragraphs

20:37.500 --> 20:39.700
 from your thousand Google Docs document

20:39.700 --> 20:41.220
 that match this concept.

20:41.220 --> 20:45.300
 The problem is it's so deeply, unlike mind maps,

20:45.300 --> 20:48.460
 it's so deeply rooted in natural language.

20:48.460 --> 20:53.460
 So it's not, it's not semantically searchable,

20:54.420 --> 20:57.220
 I would say, because the categories are very,

20:57.220 --> 21:00.700
 you kind of mentioned intelligence, language, and motion.

21:00.700 --> 21:02.580
 They're very strong, semantic.

21:02.580 --> 21:05.020
 Like, it feels like the mind map forces you

21:05.020 --> 21:09.780
 to be semantically clear and specific.

21:09.780 --> 21:13.860
 The bullet points list I have are sparse,

21:13.860 --> 21:18.860
 desperate thoughts that poetically represent

21:20.340 --> 21:24.220
 a category like motion, as opposed to saying motion.

21:25.260 --> 21:28.980
 So unfortunately, that's the same problem with the internet.

21:28.980 --> 21:32.340
 That's why the idea of semantic web is difficult to get.

21:32.340 --> 21:37.340
 It's, most language on the internet is a giant mess

21:37.980 --> 21:42.500
 of natural language that's hard to interpret, which,

21:42.500 --> 21:46.180
 so do you think there's something to mind maps as,

21:46.180 --> 21:48.100
 you actually originally brought it up

21:48.100 --> 21:53.100
 as we were talking about kind of cognition and language.

21:53.580 --> 21:55.300
 Do you think there's something to mind maps

21:55.300 --> 21:58.100
 about how our brain actually deals,

21:58.100 --> 22:00.300
 like think reasons about things?

22:01.740 --> 22:02.580
 It's possible.

22:02.580 --> 22:05.660
 I think it's reasonable to assume that there is

22:07.100 --> 22:10.620
 some level of topological processing in the brain,

22:10.620 --> 22:15.140
 that the brain is very associative in nature.

22:15.140 --> 22:20.140
 And I also believe that a topological space

22:20.660 --> 22:25.420
 is a better medium to encode thoughts

22:25.420 --> 22:27.540
 than a geometric space.

22:27.540 --> 22:28.380
 So I think...

22:28.380 --> 22:29.740
 What's the difference in a topological

22:29.740 --> 22:31.060
 and a geometric space?

22:31.060 --> 22:34.100
 Well, if you're talking about topologies,

22:34.100 --> 22:36.220
 then points are either connected or not.

22:36.220 --> 22:38.660
 So a topology is more like a subway map.

22:38.660 --> 22:41.660
 And geometry is when you're interested

22:41.660 --> 22:43.900
 in the distance between things.

22:43.900 --> 22:44.740
 And in a subway map,

22:44.740 --> 22:46.340
 you don't really have the concept of distance.

22:46.340 --> 22:48.420
 You only have the concept of whether there is a train

22:48.420 --> 22:51.500
 going from station A to station B.

22:52.820 --> 22:55.620
 And what we do in deep learning is that we're actually

22:55.620 --> 22:57.740
 dealing with geometric spaces.

22:57.740 --> 23:00.700
 We are dealing with concept vectors, word vectors,

23:01.540 --> 23:03.300
 that have a distance between them

23:03.300 --> 23:05.340
 to express in terms of that product.

23:05.340 --> 23:10.340
 So we are not really building topological models usually.

23:10.780 --> 23:11.820
 I think you're absolutely right.

23:11.820 --> 23:16.540
 Like distance is a fundamental importance in deep learning.

23:16.540 --> 23:19.300
 I mean, it's the continuous aspect of it.

23:19.300 --> 23:21.180
 Yes, because everything is a vector

23:21.180 --> 23:22.500
 and everything has to be a vector

23:22.500 --> 23:24.500
 because everything has to be differentiable.

23:24.500 --> 23:26.860
 If your space is discrete, it's no longer differentiable.

23:26.860 --> 23:29.660
 You cannot do deep learning in it anymore.

23:29.660 --> 23:32.420
 Well, you could, but you can only do it by embedding it

23:32.420 --> 23:35.620
 in a bigger continuous space.

23:35.620 --> 23:39.380
 So if you do topology in the context of deep learning,

23:39.380 --> 23:41.100
 you have to do it by embedding your topology

23:41.100 --> 23:41.940
 in the geometry.

23:42.820 --> 23:46.220
 Well, let me zoom out for a second.

23:46.220 --> 23:49.060
 Let's get into your paper on the measure of intelligence

23:50.180 --> 23:52.860
 that you put out in 2019.

23:52.860 --> 23:53.700
 Yes.

23:53.700 --> 23:54.540
 Okay.

23:54.540 --> 23:55.380
 November.

23:55.380 --> 23:56.220
 November.

23:57.700 --> 23:59.420
 Yeah, remember 2019?

23:59.420 --> 24:01.100
 That was a different time.

24:01.100 --> 24:02.780
 Yeah, I remember.

24:02.780 --> 24:03.700
 I still remember.

24:06.500 --> 24:09.620
 It feels like a different world.

24:09.620 --> 24:12.620
 You could travel, you could actually go outside

24:12.620 --> 24:14.060
 and see friends.

24:15.100 --> 24:16.260
 Yeah.

24:16.260 --> 24:18.940
 Let me ask the most absurd question.

24:18.940 --> 24:21.740
 I think there's some nonzero probability

24:21.740 --> 24:25.220
 there'll be a textbook one day, like 200 years from now

24:25.220 --> 24:27.740
 on artificial intelligence,

24:27.740 --> 24:30.660
 or it'll be called like just intelligence

24:30.660 --> 24:32.460
 cause humans will already be gone.

24:32.460 --> 24:35.220
 It'll be your picture with a quote.

24:35.220 --> 24:39.020
 This is, you know, one of the early biological systems

24:39.020 --> 24:41.580
 would consider the nature of intelligence

24:41.580 --> 24:43.180
 and there'll be like a definition

24:43.180 --> 24:45.180
 of how they thought about intelligence.

24:45.180 --> 24:46.860
 Which is one of the things you do in your paper

24:46.860 --> 24:50.100
 on measure intelligence is to ask like,

24:51.060 --> 24:52.620
 well, what is intelligence

24:52.620 --> 24:55.540
 and how to test for intelligence and so on.

24:55.540 --> 25:00.540
 So is there a spiffy quote about what is intelligence?

25:01.860 --> 25:03.900
 What is the definition of intelligence

25:03.900 --> 25:05.420
 according to Francois Chollet?

25:06.740 --> 25:10.740
 Yeah, so do you think the super intelligent AIs

25:10.740 --> 25:13.900
 of the future will want to remember us

25:13.900 --> 25:16.060
 the way we remember humans from the past?

25:16.060 --> 25:18.500
 And do you think they will be, you know,

25:18.500 --> 25:21.300
 they won't be ashamed of having a biological origin?

25:22.340 --> 25:24.660
 No, I think it would be a niche topic.

25:24.660 --> 25:25.820
 It won't be that interesting,

25:25.820 --> 25:29.420
 but it'll be like the people that study

25:29.420 --> 25:33.100
 in certain contexts like historical civilization

25:33.100 --> 25:36.340
 that no longer exists, the Aztecs and so on.

25:36.340 --> 25:38.260
 That's how it'll be seen.

25:38.260 --> 25:42.340
 And it'll be study in also the context on social media.

25:42.340 --> 25:46.700
 There'll be hashtags about the atrocity

25:46.700 --> 25:48.140
 committed to human beings

25:49.340 --> 25:52.500
 when the robots finally got rid of them.

25:52.500 --> 25:55.180
 Like it was a mistake.

25:55.180 --> 25:57.020
 You'll be seen as a giant mistake,

25:57.020 --> 26:00.060
 but ultimately in the name of progress

26:00.060 --> 26:01.540
 and it created a better world

26:01.540 --> 26:05.220
 because humans were over consuming the resources

26:05.220 --> 26:07.260
 and they were not very rational

26:07.260 --> 26:11.060
 and were destructive in the end in terms of productivity

26:11.060 --> 26:13.820
 and putting more love in the world.

26:13.820 --> 26:15.300
 And so within that context,

26:15.300 --> 26:17.420
 there'll be a chapter about these biological systems.

26:17.420 --> 26:20.380
 It seems to have a very detailed vision of that hit here.

26:20.380 --> 26:22.340
 You should write a sci fi novel about it.

26:22.340 --> 26:26.460
 I'm working on a sci fi novel currently, yes.

26:28.100 --> 26:29.460
 Self published, yeah.

26:29.460 --> 26:30.740
 The definition of intelligence.

26:30.740 --> 26:34.660
 So intelligence is the efficiency

26:34.660 --> 26:39.380
 with which you acquire new skills at tasks

26:39.380 --> 26:41.940
 that you did not previously know about,

26:41.940 --> 26:44.700
 that you did not prepare for, right?

26:44.700 --> 26:47.780
 So intelligence is not skill itself.

26:47.780 --> 26:50.740
 It's not what you know, it's not what you can do.

26:50.740 --> 26:52.900
 It's how well and how efficiently

26:52.900 --> 26:54.580
 you can learn new things.

26:54.580 --> 26:55.580
 New things.

26:55.580 --> 26:56.420
 Yes.

26:56.420 --> 26:58.100
 The idea of newness there

26:58.100 --> 27:00.100
 seems to be fundamentally important.

27:01.180 --> 27:02.020
 Yes.

27:02.020 --> 27:05.780
 So you would see intelligence on display, for instance.

27:05.780 --> 27:09.980
 Whenever you see a human being or an AI creature

27:09.980 --> 27:13.900
 adapt to a new environment that it does not see before,

27:13.900 --> 27:16.620
 that its creators did not anticipate.

27:16.620 --> 27:19.340
 When you see adaptation, when you see improvisation,

27:19.340 --> 27:22.500
 when you see generalization, that's intelligence.

27:22.500 --> 27:24.460
 In reverse, if you have a system

27:24.460 --> 27:27.100
 that when you put it in a slightly new environment,

27:27.100 --> 27:30.060
 it cannot adapt, it cannot improvise,

27:30.060 --> 27:33.380
 it cannot deviate from what it's hard coded to do

27:33.380 --> 27:37.660
 or what it has been trained to do,

27:38.700 --> 27:41.060
 that is a system that is not intelligent.

27:41.060 --> 27:43.580
 There's actually a quote from Einstein

27:43.580 --> 27:46.780
 that captures this idea, which is,

27:46.780 --> 27:50.740
 the measure of intelligence is the ability to change.

27:50.740 --> 27:51.740
 I like that quote.

27:51.740 --> 27:54.940
 I think it captures at least part of this idea.

27:54.940 --> 27:56.460
 You know, there might be something interesting

27:56.460 --> 27:59.500
 about the difference between your definition and Einstein's.

27:59.500 --> 28:03.700
 I mean, he's just being Einstein and clever,

28:04.740 --> 28:09.740
 but acquisition of new ability to deal with new things

28:09.740 --> 28:14.100
 versus ability to just change.

28:14.100 --> 28:16.820
 What's the difference between those two things?

28:16.820 --> 28:19.260
 So just change in itself.

28:19.260 --> 28:21.300
 Do you think there's something to that?

28:21.300 --> 28:23.780
 Just being able to change.

28:23.780 --> 28:25.540
 Yes, being able to adapt.

28:25.540 --> 28:30.060
 So not change, but certainly change its direction.

28:30.060 --> 28:33.460
 Being able to adapt yourself to your environment.

28:34.420 --> 28:35.660
 Whatever the environment is.

28:35.660 --> 28:37.460
 That's a big part of intelligence.

28:37.460 --> 28:40.020
 And intelligence is more precisely, you know,

28:40.020 --> 28:42.460
 how efficiently you're able to adapt,

28:42.460 --> 28:45.740
 how efficiently you're able to basically master your environment,

28:45.740 --> 28:49.140
 how efficiently you can acquire new skills.

28:49.140 --> 28:52.300
 And I think there's a big distinction to be drawn

28:52.300 --> 28:56.220
 between intelligence, which is a process,

28:56.220 --> 28:59.740
 and the output of that process, which is skill.

29:01.420 --> 29:04.900
 So for instance, if you have a very smart human brain,

29:04.900 --> 29:08.980
 so for instance, if you have a very smart human programmer

29:08.980 --> 29:10.780
 that considers the game of chess,

29:10.780 --> 29:15.780
 and that writes down a static program that can play chess,

29:16.180 --> 29:19.140
 then the intelligence is the process

29:19.140 --> 29:20.660
 of developing that program.

29:20.660 --> 29:24.460
 But the program itself is just encoding

29:25.660 --> 29:28.100
 the output artifact of that process.

29:28.100 --> 29:30.020
 The program itself is not intelligent.

29:30.020 --> 29:31.860
 And the way you tell it's not intelligent

29:31.860 --> 29:34.020
 is that if you put it in a different context,

29:34.020 --> 29:36.060
 you ask it to play Go or something,

29:36.060 --> 29:37.780
 it's not going to be able to perform well

29:37.780 --> 29:38.900
 without human involvement,

29:38.900 --> 29:41.100
 because the source of intelligence,

29:41.100 --> 29:43.140
 the entity that is capable of that process

29:43.140 --> 29:44.380
 is the human programmer.

29:44.380 --> 29:47.940
 So we should be able to tell the difference

29:47.940 --> 29:50.100
 between the process and its output.

29:50.100 --> 29:53.260
 We should not confuse the output and the process.

29:53.260 --> 29:54.860
 It's the same as, you know,

29:54.860 --> 29:58.780
 do not confuse a road building company

29:58.780 --> 30:00.180
 and one specific road,

30:00.180 --> 30:03.460
 because one specific road takes you from point A to point B,

30:03.460 --> 30:06.180
 but a road building company can take you from,

30:06.180 --> 30:08.980
 can make a path from anywhere to anywhere else.

30:08.980 --> 30:10.140
 Yeah, that's beautifully put,

30:10.140 --> 30:15.140
 but it's also to play devil's advocate a little bit.

30:15.460 --> 30:18.740
 You know, it's possible that there's something

30:18.740 --> 30:21.260
 more fundamental than us humans.

30:21.260 --> 30:24.660
 So you kind of said the programmer creates

30:25.860 --> 30:28.940
 the difference between the choir,

30:28.940 --> 30:31.340
 the skill and the skill itself.

30:31.340 --> 30:32.780
 There could be something like,

30:32.780 --> 30:36.420
 you could argue the universe is more intelligent.

30:36.420 --> 30:41.420
 Like the base intelligence that we should be trying

30:43.020 --> 30:45.380
 to measure is something that created humans.

30:46.500 --> 30:51.500
 We should be measuring God or the source of the universe

30:51.540 --> 30:55.140
 as opposed to, like there could be a deeper intelligence.

30:55.140 --> 30:55.980
 Sure.

30:55.980 --> 30:57.140
 There's always deeper intelligence, I guess.

30:57.140 --> 30:58.020
 You can argue that,

30:58.020 --> 31:00.100
 but that does not take anything away

31:00.100 --> 31:01.900
 from the fact that humans are intelligent.

31:01.900 --> 31:03.260
 And you can tell that

31:03.260 --> 31:07.020
 because they are capable of adaptation and generality.

31:07.020 --> 31:07.860
 Got it.

31:07.860 --> 31:09.700
 And you see that in particular in the fact

31:09.700 --> 31:14.700
 that humans are capable of handling situations and tasks

31:16.780 --> 31:19.780
 that are quite different from anything

31:19.780 --> 31:22.940
 that any of our evolutionary ancestors

31:22.940 --> 31:24.540
 has ever encountered.

31:24.540 --> 31:27.140
 So we are capable of generalizing very much

31:27.140 --> 31:28.100
 out of distribution,

31:28.100 --> 31:30.260
 if you consider our evolutionary history

31:30.260 --> 31:32.220
 as being in a way our training data.

31:33.260 --> 31:35.060
 Of course, evolutionary biologists would argue

31:35.060 --> 31:37.660
 that we're not going too far out of the distribution.

31:37.660 --> 31:41.380
 We're like mapping the skills we've learned previously,

31:41.380 --> 31:43.540
 desperately trying to like jam them

31:43.540 --> 31:47.060
 into like these new situations.

31:47.060 --> 31:49.460
 I mean, there's definitely a little bit of that,

31:49.460 --> 31:52.220
 but it's pretty clear to me that we're able to,

31:53.660 --> 31:56.580
 most of the things we do any given day

31:56.580 --> 31:58.060
 in our modern civilization

31:58.060 --> 32:00.860
 are things that are very, very different

32:00.860 --> 32:03.900
 from what our ancestors a million years ago

32:03.900 --> 32:05.900
 would have been doing in a given day.

32:05.900 --> 32:07.540
 And your environment is very different.

32:07.540 --> 32:12.180
 So I agree that everything we do,

32:12.180 --> 32:14.220
 we do it with cognitive building blocks

32:14.220 --> 32:17.820
 that we acquired over the course of evolution, right?

32:17.820 --> 32:22.180
 And that anchors our cognition to a certain context,

32:22.180 --> 32:25.260
 which is the human condition very much.

32:25.260 --> 32:29.500
 But still our mind is capable of a pretty remarkable degree

32:29.500 --> 32:32.700
 of generality far beyond anything we can create

32:32.700 --> 32:34.100
 in artificial systems today.

32:34.100 --> 32:37.740
 Like the degree in which the mind can generalize

32:37.740 --> 32:40.420
 from its evolutionary history,

32:41.620 --> 32:43.940
 can generalize away from its evolutionary history

32:43.940 --> 32:46.500
 is much greater than the degree

32:46.500 --> 32:48.860
 to which a deep learning system today

32:48.860 --> 32:51.020
 can generalize away from its training data.

32:51.020 --> 32:52.380
 And like the key point you're making,

32:52.380 --> 32:54.220
 which I think is quite beautiful is like,

32:54.220 --> 32:58.660
 we shouldn't measure, if we're talking about measurement,

32:58.660 --> 33:00.340
 we shouldn't measure the skill.

33:01.620 --> 33:04.340
 We should measure like the creation of the new skill,

33:04.340 --> 33:06.780
 the ability to create that new skill.

33:06.780 --> 33:10.940
 But it's tempting, like it's weird

33:10.940 --> 33:13.620
 because the skill is a little bit of a small window

33:13.620 --> 33:16.380
 into the system.

33:16.380 --> 33:18.300
 So whenever you have a lot of skills,

33:19.420 --> 33:21.900
 it's tempting to measure the skills.

33:21.900 --> 33:25.820
 I mean, the skill is the only thing you can objectively

33:25.820 --> 33:27.540
 measure, but yeah.

33:27.540 --> 33:30.780
 So the thing to keep in mind is that

33:30.780 --> 33:33.460
 when you see skill in the human,

33:35.060 --> 33:39.220
 it gives you a strong signal that that human is intelligent

33:39.220 --> 33:42.740
 because you know they weren't born with that skill typically.

33:42.740 --> 33:45.220
 Like you see a very strong chess player,

33:45.220 --> 33:47.540
 maybe you're a very strong chess player yourself.

33:47.540 --> 33:51.020
 I think you're saying that because I'm Russian

33:51.020 --> 33:53.860
 and now you're prejudiced, you assume.

33:53.860 --> 33:54.700
 All Russians are good at chess.

33:54.700 --> 33:55.540
 I'm biased, exactly.

33:55.540 --> 33:56.900
 I'm biased, yeah.

33:56.900 --> 34:00.020
 Well, you're definitely biased.

34:00.020 --> 34:01.900
 So if you see a very strong chess player,

34:01.900 --> 34:05.460
 you know they weren't born knowing how to play chess.

34:05.460 --> 34:07.780
 So they had to acquire that skill

34:07.780 --> 34:10.940
 with their limited resources, with their limited lifetime.

34:10.940 --> 34:15.420
 And they did that because they are generally intelligent.

34:15.420 --> 34:18.980
 And so they may as well have acquired any other skill.

34:18.980 --> 34:21.180
 You know they have this potential.

34:21.180 --> 34:25.700
 And on the other hand, if you see a computer playing chess,

34:25.700 --> 34:27.860
 you cannot make the same assumptions

34:27.860 --> 34:29.380
 because you cannot just assume

34:29.380 --> 34:30.860
 the computer is generally intelligent.

34:30.860 --> 34:35.300
 The computer may be born knowing how to play chess

34:35.300 --> 34:38.220
 in the sense that it may have been programmed by a human

34:38.220 --> 34:40.900
 that has understood chess for the computer

34:40.900 --> 34:44.180
 and that has just encoded the output

34:44.180 --> 34:46.020
 of that understanding in a static program.

34:46.020 --> 34:49.420
 And that program is not intelligent.

34:49.420 --> 34:52.380
 So let's zoom out just for a second and say like,

34:52.380 --> 34:57.380
 what is the goal on the measure of intelligence paper?

34:57.460 --> 34:59.020
 Like what do you hope to achieve with it?

34:59.020 --> 35:01.700
 So the goal of the paper is to clear up

35:01.700 --> 35:04.580
 some longstanding misunderstandings

35:04.580 --> 35:08.380
 about the way we've been conceptualizing intelligence

35:08.380 --> 35:12.500
 in the AI community and in the way we've been

35:12.500 --> 35:16.780
 evaluating progress in AI.

35:16.780 --> 35:19.060
 There's been a lot of progress recently in machine learning

35:19.060 --> 35:22.140
 and people are extrapolating from that progress

35:22.140 --> 35:26.380
 that we are about to solve general intelligence.

35:26.380 --> 35:30.500
 And if you want to be able to evaluate these statements,

35:30.500 --> 35:33.820
 you need to precisely define what you're talking about

35:33.820 --> 35:35.580
 when you're talking about general intelligence.

35:35.580 --> 35:40.580
 And you need a formal way, a reliable way to measure

35:40.580 --> 35:42.380
 how much intelligence,

35:42.380 --> 35:45.900
 how much general intelligence a system processes.

35:45.900 --> 35:48.420
 And ideally this measure of intelligence

35:48.420 --> 35:50.260
 should be actionable.

35:50.260 --> 35:54.620
 So it should not just describe what intelligence is.

35:54.620 --> 35:56.860
 It should not just be a binary indicator

35:56.860 --> 36:00.540
 that tells you the system is intelligent or it isn't.

36:01.620 --> 36:03.060
 It should be actionable.

36:03.060 --> 36:05.740
 It should have explanatory power, right?

36:05.740 --> 36:08.580
 So you could use it as a feedback signal.

36:08.580 --> 36:10.980
 It would show you the way

36:10.980 --> 36:13.100
 towards building more intelligent systems.

36:13.100 --> 36:16.500
 So at the first level, you draw a distinction

36:16.500 --> 36:19.020
 between two divergent views of intelligence.

36:21.780 --> 36:22.860
 As we just talked about,

36:22.860 --> 36:26.820
 intelligence is a collection of task specific skills

36:26.820 --> 36:29.900
 and a general learning ability.

36:29.900 --> 36:31.420
 So what's the difference between

36:32.300 --> 36:35.580
 kind of this memorization of skills

36:35.580 --> 36:37.820
 and a general learning ability?

36:37.820 --> 36:39.580
 We've talked about it a little bit,

36:39.580 --> 36:43.060
 but can you try to linger on this topic for a bit?

36:43.060 --> 36:45.460
 Yeah, so the first part of the paper

36:45.460 --> 36:49.100
 is an assessment of the different ways

36:49.100 --> 36:50.500
 we've been thinking about intelligence

36:50.500 --> 36:54.540
 and the different ways we've been evaluating progress in AI.

36:54.540 --> 36:57.700
 And this tree of cognitive sciences

36:57.700 --> 37:01.220
 has been shaped by two views of the human mind.

37:01.220 --> 37:04.740
 And one view is the evolutionary psychology view

37:04.740 --> 37:09.740
 in which the mind is a collection of fairly static

37:10.660 --> 37:14.220
 special purpose ad hoc mechanisms

37:14.220 --> 37:17.620
 that have been hard coded by evolution

37:17.620 --> 37:22.500
 over our history as a species for a very long time.

37:22.500 --> 37:27.500
 And early AI researchers,

37:27.940 --> 37:30.340
 people like Marvin Minsky, for instance,

37:30.340 --> 37:33.300
 they clearly subscribed to this view.

37:33.300 --> 37:36.860
 And they saw the mind as a kind of

37:36.860 --> 37:38.740
 collection of static programs

37:39.820 --> 37:42.140
 similar to the programs they would run

37:42.140 --> 37:43.580
 on like mainframe computers.

37:43.580 --> 37:48.060
 And in fact, I think they very much understood the mind

37:48.060 --> 37:50.540
 through the metaphor of the mainframe computer

37:50.540 --> 37:53.580
 because that was the tool they were working with, right?

37:53.580 --> 37:55.100
 And so you had these static programs,

37:55.100 --> 37:57.180
 this collection of very different static programs

37:57.180 --> 38:00.060
 operating over a database like memory.

38:00.060 --> 38:03.580
 And in this picture, learning was not very important.

38:03.580 --> 38:05.660
 Learning was considered to be just memorization.

38:05.660 --> 38:10.380
 And in fact, learning is basically not featured

38:10.380 --> 38:14.620
 in AI textbooks until the 1980s

38:14.620 --> 38:16.940
 with the rise of machine learning.

38:16.940 --> 38:18.780
 It's kind of fun to think about

38:18.780 --> 38:20.500
 that learning was the outcast.

38:21.500 --> 38:24.060
 Like the weird people working on learning,

38:24.060 --> 38:28.100
 like the mainstream AI world was,

38:28.100 --> 38:31.780
 I mean, I don't know what the best term is,

38:31.780 --> 38:32.980
 but it's non learning.

38:33.900 --> 38:37.940
 It was seen as like reasoning would not be learning based.

38:37.940 --> 38:40.620
 Yes, it was considered that the mind

38:40.620 --> 38:43.180
 was a collection of programs

38:43.180 --> 38:46.620
 that were primarily logical in nature.

38:46.620 --> 38:49.140
 And that's all you needed to do to create a mind

38:49.140 --> 38:50.860
 was to write down these programs

38:50.860 --> 38:52.860
 and they would operate over knowledge,

38:52.860 --> 38:55.100
 which would be stored in some kind of database.

38:55.100 --> 38:57.300
 And as long as your database would encompass,

38:57.300 --> 38:59.380
 you know, everything about the world

38:59.380 --> 39:03.340
 and your logical rules were comprehensive,

39:03.340 --> 39:04.940
 then you would have a mind.

39:04.940 --> 39:06.420
 So the other view of the mind

39:06.420 --> 39:11.420
 is the brain as a sort of blank slate, right?

39:11.940 --> 39:13.180
 This is a very old idea.

39:13.180 --> 39:16.140
 You find it in John Locke's writings.

39:16.140 --> 39:17.580
 This is the tabula rasa.

39:19.220 --> 39:21.140
 And this is this idea that the mind

39:21.140 --> 39:23.340
 is some kind of like information sponge

39:23.340 --> 39:27.340
 that starts empty, that starts blank.

39:27.340 --> 39:32.340
 And that absorbs knowledge and skills from experience, right?

39:34.340 --> 39:38.700
 So it's a sponge that reflects the complexity of the world,

39:38.700 --> 39:41.780
 the complexity of your life experience, essentially.

39:41.780 --> 39:44.340
 That everything you know and everything you can do

39:44.340 --> 39:47.740
 is a reflection of something you found

39:47.740 --> 39:49.580
 in the outside world, essentially.

39:49.580 --> 39:51.580
 So this is an idea that's very old.

39:51.580 --> 39:56.580
 That was not very popular, for instance, in the 1970s.

39:56.780 --> 39:58.820
 But that gained a lot of vitality recently

39:58.820 --> 40:02.300
 with the rise of connectionism, in particular deep learning.

40:02.300 --> 40:03.780
 And so today, deep learning

40:03.780 --> 40:06.540
 is the dominant paradigm in AI.

40:06.540 --> 40:10.420
 And I feel like lots of AI researchers

40:10.420 --> 40:14.980
 are conceptualizing the mind via a deep learning metaphor.

40:14.980 --> 40:17.820
 Like they see the mind as a kind of

40:17.820 --> 40:21.660
 randomly initialized neural network that starts blank

40:21.660 --> 40:22.500
 when you're born.

40:22.500 --> 40:26.100
 And then that gets trained via exposure to trained data

40:26.100 --> 40:27.740
 that acquires knowledge and skills

40:27.740 --> 40:29.220
 via exposure to trained data.

40:29.220 --> 40:31.740
 By the way, it's a small tangent.

40:32.700 --> 40:36.700
 I feel like people who are thinking about intelligence

40:36.700 --> 40:39.700
 are not conceptualizing it that way.

40:39.700 --> 40:41.820
 I actually haven't met too many people

40:41.820 --> 40:44.700
 who believe that a neural network

40:44.700 --> 40:49.700
 will be able to reason, who seriously think that rigorously.

40:51.660 --> 40:54.260
 Because I think it's actually an interesting worldview.

40:54.260 --> 40:56.420
 And we'll talk about it more,

40:56.420 --> 41:00.420
 but it's been impressive what neural networks

41:00.420 --> 41:02.100
 have been able to accomplish.

41:02.100 --> 41:04.540
 And to me, I don't know, you might disagree,

41:04.540 --> 41:09.540
 but it's an open question whether like scaling size

41:09.820 --> 41:13.660
 eventually might lead to incredible results

41:13.660 --> 41:17.060
 to us mere humans will appear as if it's general.

41:17.060 --> 41:19.860
 I mean, if you ask people who are seriously thinking

41:19.860 --> 41:22.660
 about intelligence, they will definitely not say

41:22.660 --> 41:24.900
 that all you need to do is,

41:24.900 --> 41:27.420
 like the mind is just a neural network.

41:27.420 --> 41:30.420
 However, it's actually a view that's very popular,

41:30.420 --> 41:31.780
 I think, in the deep learning community

41:31.780 --> 41:35.460
 that many people are kind of conceptually

41:35.460 --> 41:37.140
 intellectually lazy about it.

41:37.140 --> 41:40.500
 Right, it's a, but I guess what I'm saying exactly right,

41:40.500 --> 41:44.740
 it's, I mean, I haven't met many people

41:44.740 --> 41:47.740
 and I think it would be interesting to meet a person

41:47.740 --> 41:50.260
 who is not intellectually lazy about this particular topic

41:50.260 --> 41:54.460
 and still believes that neural networks will go all the way.

41:54.460 --> 41:56.820
 I think Yama is probably closest to that

41:56.820 --> 41:57.660
 with self supervised.

41:57.660 --> 41:59.660
 There are definitely people who argue

41:59.660 --> 42:03.100
 that current deep learning techniques

42:03.100 --> 42:06.860
 are already the way to general artificial intelligence.

42:06.860 --> 42:09.460
 And that all you need to do is to scale it up

42:09.460 --> 42:12.780
 to all the available trained data.

42:12.780 --> 42:16.300
 And that's, if you look at the waves

42:16.300 --> 42:19.500
 that OpenAI's GPT3 model has made,

42:19.500 --> 42:22.700
 you see echoes of this idea.

42:22.700 --> 42:27.700
 So on that topic, GPT3, similar to GPT2 actually,

42:28.980 --> 42:33.060
 have captivated some part of the imagination of the public.

42:33.060 --> 42:35.580
 There's just a bunch of hype of different kind.

42:35.580 --> 42:37.940
 That's, I would say it's emergent.

42:37.940 --> 42:39.820
 It's not artificially manufactured.

42:39.820 --> 42:42.580
 It's just like people just get excited

42:42.580 --> 42:43.780
 for some strange reason.

42:43.780 --> 42:46.500
 And in the case of GPT3, which is funny,

42:46.500 --> 42:49.100
 that there's, I believe, a couple months delay

42:49.100 --> 42:51.580
 from release to hype.

42:51.580 --> 42:56.580
 Maybe I'm not historically correct on that,

42:56.780 --> 43:01.260
 but it feels like there was a little bit of a lack of hype

43:01.260 --> 43:04.780
 and then there's a phase shift into hype.

43:04.780 --> 43:07.460
 But nevertheless, there's a bunch of cool applications

43:07.460 --> 43:10.380
 that seem to captivate the imagination of the public

43:10.380 --> 43:12.140
 about what this language model

43:12.140 --> 43:15.180
 that's trained in unsupervised way

43:15.180 --> 43:19.500
 without any fine tuning is able to achieve.

43:19.500 --> 43:20.900
 So what do you make of that?

43:20.900 --> 43:22.940
 What are your thoughts about GPT3?

43:22.940 --> 43:25.700
 Yeah, so I think what's interesting about GPT3

43:25.700 --> 43:29.900
 is the idea that it may be able to learn new tasks

43:31.180 --> 43:33.580
 after just being shown a few examples.

43:33.580 --> 43:35.620
 So I think if it's actually capable of doing that,

43:35.620 --> 43:37.580
 that's novel and that's very interesting

43:37.580 --> 43:39.900
 and that's something we should investigate.

43:39.900 --> 43:43.140
 That said, I must say, I'm not entirely convinced

43:43.140 --> 43:47.300
 that we have shown it's capable of doing that.

43:47.300 --> 43:50.980
 It's very likely, given the amount of data

43:50.980 --> 43:52.220
 that the model is trained on,

43:52.220 --> 43:55.700
 that what it's actually doing is pattern matching

43:55.700 --> 43:58.060
 a new task you give it with a task

43:58.060 --> 44:00.100
 that it's been exposed to in its trained data.

44:00.100 --> 44:01.620
 It's just recognizing the task

44:01.620 --> 44:05.540
 instead of just developing a model of the task, right?

44:05.540 --> 44:07.660
 But there's, sorry to interrupt,

44:07.660 --> 44:10.020
 there's a parallel as to what you said before,

44:10.020 --> 44:14.620
 which is it's possible to see GPT3 as like the prompts

44:14.620 --> 44:17.780
 it's given as a kind of SQL query

44:17.780 --> 44:19.580
 into this thing that it's learned,

44:19.580 --> 44:20.860
 similar to what you said before,

44:20.860 --> 44:23.340
 which is language is used to query the memory.

44:23.340 --> 44:24.180
 Yes.

44:24.180 --> 44:26.940
 So is it possible that neural network

44:26.940 --> 44:29.300
 is a giant memorization thing,

44:29.300 --> 44:32.260
 but then if it gets sufficiently giant,

44:32.260 --> 44:35.100
 it'll memorize sufficiently large amounts

44:35.100 --> 44:37.860
 of things in the world or it becomes,

44:37.860 --> 44:40.580
 or intelligence becomes a querying machine?

44:40.580 --> 44:44.180
 I think it's possible that a significant chunk

44:44.180 --> 44:47.500
 of intelligence is this giant associative memory.

44:48.740 --> 44:51.340
 I definitely don't believe that intelligence

44:51.340 --> 44:53.740
 is just a giant associative memory,

44:53.740 --> 44:56.300
 but it may well be a big component.

44:57.660 --> 45:02.660
 So do you think GPT3, 4, 5,

45:02.660 --> 45:07.140
 GPT10 will eventually, like, what do you think,

45:07.140 --> 45:08.340
 where's the ceiling?

45:08.340 --> 45:10.540
 Do you think you'll be able to reason?

45:11.980 --> 45:13.420
 No, that's a bad question.

45:14.620 --> 45:17.340
 Like, what is the ceiling is the better question.

45:17.340 --> 45:18.500
 How well is it gonna scale?

45:18.500 --> 45:21.180
 How good is GPTN going to be?

45:21.180 --> 45:22.020
 Yeah.

45:22.020 --> 45:25.420
 So I believe GPTN is gonna.

45:25.420 --> 45:26.860
 GPTN.

45:26.860 --> 45:30.940
 Is gonna improve on the strength of GPT2 and 3,

45:30.940 --> 45:33.980
 which is it will be able to generate, you know,

45:33.980 --> 45:37.660
 ever more plausible text in context.

45:37.660 --> 45:39.900
 Just monotonically increasing performance.

45:41.260 --> 45:44.340
 Yes, if you train a bigger model on more data,

45:44.340 --> 45:49.340
 then your text will be increasingly more context aware

45:49.340 --> 45:51.220
 and increasingly more plausible

45:51.220 --> 45:54.700
 in the same way that GPT3 is much better

45:54.700 --> 45:57.500
 at generating plausible text compared to GPT2.

45:57.500 --> 46:01.940
 But that said, I don't think just scaling up the model

46:01.940 --> 46:04.180
 to more transformer layers and more trained data

46:04.180 --> 46:07.020
 is gonna address the flaws of GPT3,

46:07.020 --> 46:09.900
 which is that it can generate plausible text,

46:09.900 --> 46:13.620
 but that text is not constrained by anything else

46:13.620 --> 46:15.180
 other than plausibility.

46:15.180 --> 46:19.180
 So in particular, it's not constrained by factualness

46:19.180 --> 46:21.820
 or even consistency, which is why it's very easy

46:21.820 --> 46:23.860
 to get GPT3 to generate statements

46:23.860 --> 46:26.260
 that are factually untrue.

46:26.260 --> 46:29.580
 Or to generate statements that are even self contradictory.

46:29.580 --> 46:30.420
 Right?

46:30.420 --> 46:35.420
 Because it's only goal is plausibility,

46:35.420 --> 46:37.620
 and it has no other constraints.

46:37.620 --> 46:40.300
 It's not constrained to be self consistent, for instance.

46:40.300 --> 46:41.140
 Right?

46:41.140 --> 46:43.540
 And so for this reason, one thing that I thought

46:43.540 --> 46:46.780
 was very interesting with GPT3 is that you can

46:46.780 --> 46:49.780
 predetermine the answer it will give you

46:49.780 --> 46:52.020
 by asking the question in a specific way,

46:52.020 --> 46:55.260
 because it's very responsive to the way you ask the question.

46:55.260 --> 47:00.260
 Since it has no understanding of the content of the question.

47:00.260 --> 47:01.100
 Right.

47:01.100 --> 47:05.620
 And if you have the same question in two different ways

47:05.620 --> 47:09.020
 that are basically adversarially engineered

47:09.020 --> 47:10.260
 to produce certain answer,

47:10.260 --> 47:12.740
 you will get two different answers,

47:12.740 --> 47:14.180
 two contradictory answers.

47:14.180 --> 47:16.660
 It's very susceptible to adversarial attacks, essentially.

47:16.660 --> 47:17.780
 Potentially, yes.

47:17.780 --> 47:20.820
 So in general, the problem with these models,

47:20.820 --> 47:24.180
 these generative models, is that they are very good

47:24.180 --> 47:27.220
 at generating plausible text,

47:27.220 --> 47:29.660
 but that's just not enough.

47:29.660 --> 47:30.500
 Right?

47:33.620 --> 47:36.500
 I think one avenue that would be very interesting

47:36.500 --> 47:39.460
 to make progress is to make it possible

47:40.780 --> 47:43.860
 to write programs over the latent space

47:43.860 --> 47:45.620
 that these models operate on.

47:45.620 --> 47:49.460
 That you would rely on these self supervised models

47:49.460 --> 47:54.340
 to generate a sort of like pool of knowledge and concepts

47:54.340 --> 47:55.260
 and common sense.

47:55.260 --> 47:57.180
 And then you would be able to write

47:57.180 --> 48:01.460
 explicit reasoning programs over it.

48:01.460 --> 48:03.660
 Because the current problem with GPT3 is that

48:03.660 --> 48:08.660
 it can be quite difficult to get it to do what you want to do.

48:09.420 --> 48:12.420
 If you want to turn GPT3 into products,

48:12.420 --> 48:14.780
 you need to put constraints on it.

48:14.780 --> 48:19.500
 You need to force it to obey certain rules.

48:19.500 --> 48:22.540
 So you need a way to program it explicitly.

48:22.540 --> 48:24.220
 Yeah, so if you look at its ability

48:24.220 --> 48:26.140
 to do program synthesis,

48:26.140 --> 48:29.060
 it generates, like you said, something that's plausible.

48:29.060 --> 48:32.580
 Yeah, so if you try to make it generate programs,

48:32.580 --> 48:35.940
 it will perform well for any program

48:35.940 --> 48:38.700
 that it has seen in its training data.

48:38.700 --> 48:42.940
 But because program space is not interpretive, right?

48:42.940 --> 48:46.740
 It's not going to be able to generalize to problems

48:46.740 --> 48:48.700
 it hasn't seen before.

48:48.700 --> 48:53.700
 Now that's currently, do you think sort of an absurd,

48:54.980 --> 48:59.980
 but I think useful, I guess, intuition builder is,

49:00.340 --> 49:05.340
 you know, the GPT3 has 175 billion parameters.

49:07.340 --> 49:11.740
 Human brain has 100, has about a thousand times that

49:11.740 --> 49:14.820
 or more in terms of number of synapses.

49:16.380 --> 49:21.180
 Do you think, obviously, very different kinds of things,

49:21.180 --> 49:26.180
 but there is some degree of similarity.

49:26.380 --> 49:30.700
 Do you think, what do you think GPT will look like

49:30.700 --> 49:34.180
 when it has 100 trillion parameters?

49:34.180 --> 49:39.100
 You think our conversation might be in nature different?

49:39.100 --> 49:42.940
 Like, because you've criticized GPT3 very effectively now.

49:42.940 --> 49:43.900
 Do you think?

49:45.420 --> 49:46.940
 No, I don't think so.

49:46.940 --> 49:51.020
 So to begin with, the bottleneck with scaling up GPT3,

49:51.020 --> 49:54.860
 GPT models, generative pre trained transformer models,

49:54.860 --> 49:57.620
 is not going to be the size of the model

49:57.620 --> 49:59.580
 or how long it takes to train it.

49:59.580 --> 50:01.860
 The bottleneck is going to be the trained data

50:01.860 --> 50:05.540
 because OpenAI is already training GPT3

50:05.540 --> 50:08.860
 on a core of basically the entire web, right?

50:08.860 --> 50:09.820
 And that's a lot of data.

50:09.820 --> 50:12.140
 So you could imagine training on more data than that,

50:12.140 --> 50:14.460
 like Google could train on more data than that,

50:14.460 --> 50:17.500
 but it would still be only incrementally more data.

50:17.500 --> 50:21.340
 And I don't recall exactly how much more data GPT3

50:21.340 --> 50:22.820
 was trained on compared to GPT2,

50:22.820 --> 50:25.100
 but it's probably at least like a hundred,

50:25.100 --> 50:26.620
 maybe even a thousand X.

50:26.620 --> 50:28.460
 I don't have the exact number.

50:28.460 --> 50:30.140
 You're not going to be able to train a model

50:30.140 --> 50:34.180
 on a hundred more data than what you're already doing.

50:34.180 --> 50:35.300
 So that's brilliant.

50:35.300 --> 50:38.940
 So it's easier to think of compute as a bottleneck

50:38.940 --> 50:41.380
 and then arguing that we can remove that bottleneck.

50:41.380 --> 50:43.060
 But we can remove the compute bottleneck.

50:43.060 --> 50:44.580
 I don't think it's a big problem.

50:44.580 --> 50:48.500
 If you look at the pace at which we've improved

50:48.500 --> 50:51.340
 the efficiency of deep learning models

50:51.340 --> 50:54.060
 in the past few years,

50:54.060 --> 50:57.180
 I'm not worried about train time bottlenecks

50:57.180 --> 50:58.740
 or model size bottlenecks.

50:59.580 --> 51:01.140
 The bottleneck in the case

51:01.140 --> 51:03.420
 of these generative transformer models

51:03.420 --> 51:05.540
 is absolutely the trained data.

51:05.540 --> 51:07.740
 What about the quality of the data?

51:07.740 --> 51:08.580
 So, yeah.

51:08.580 --> 51:10.900
 So the quality of the data is an interesting point.

51:10.900 --> 51:11.900
 The thing is,

51:11.900 --> 51:14.460
 if you're going to want to use these models

51:14.460 --> 51:15.820
 in real products,

51:16.900 --> 51:20.060
 then you want to feed them data

51:20.060 --> 51:23.460
 that's as high quality, as factual,

51:23.460 --> 51:25.620
 I would say as unbiased as possible,

51:25.620 --> 51:27.340
 that there's not really such a thing

51:27.340 --> 51:30.500
 as unbiased data in the first place.

51:30.500 --> 51:34.020
 But you probably don't want to train it on Reddit,

51:34.020 --> 51:34.860
 for instance.

51:34.860 --> 51:37.060
 It sounds like a bad plan.

51:37.060 --> 51:38.620
 So from my personal experience,

51:38.620 --> 51:42.740
 working with large scale deep learning models.

51:42.740 --> 51:46.580
 So at some point I was working on a model at Google

51:46.580 --> 51:51.580
 that's trained on 350 million labeled images.

51:52.340 --> 51:53.660
 It's an image classification model.

51:53.660 --> 51:54.660
 That's a lot of images.

51:54.660 --> 51:58.140
 That's like probably most publicly available images

51:58.140 --> 52:00.980
 on the web at the time.

52:00.980 --> 52:03.900
 And it was a very noisy data set

52:03.900 --> 52:07.820
 because the labels were not originally annotated by hand,

52:07.820 --> 52:08.660
 by humans.

52:08.660 --> 52:12.420
 They were automatically derived from like tags

52:12.420 --> 52:14.300
 on social media,

52:14.300 --> 52:16.820
 or just keywords in the same page

52:16.820 --> 52:18.220
 as the image was found and so on.

52:18.220 --> 52:19.140
 So it was very noisy.

52:19.140 --> 52:24.140
 And it turned out that you could easily get a better model,

52:25.340 --> 52:26.500
 not just by training,

52:26.500 --> 52:29.980
 like if you train on more of the noisy data,

52:29.980 --> 52:31.540
 you get an incrementally better model,

52:31.540 --> 52:35.500
 but you very quickly hit diminishing returns.

52:35.500 --> 52:36.660
 On the other hand,

52:36.660 --> 52:38.420
 if you train on smaller data set

52:38.420 --> 52:40.020
 with higher quality annotations,

52:40.020 --> 52:45.020
 quality annotations that are actually made by humans,

52:45.380 --> 52:47.340
 you get a better model.

52:47.340 --> 52:49.860
 And it also takes less time to train it.

52:49.860 --> 52:51.580
 Yeah, that's fascinating.

52:51.580 --> 52:53.500
 It's the self supervised learning.

52:53.500 --> 52:58.500
 There's a way to get better doing the automated labeling.

52:58.780 --> 53:03.780
 Yeah, so you can enrich or refine your labels

53:04.620 --> 53:05.860
 in an automated way.

53:05.860 --> 53:07.460
 That's correct.

53:07.460 --> 53:08.700
 Do you have a hope for,

53:08.700 --> 53:09.580
 I don't know if you're familiar

53:09.580 --> 53:11.980
 with the idea of a semantic web.

53:11.980 --> 53:15.620
 Is a semantic web just for people who are not familiar

53:15.620 --> 53:20.620
 and is the idea of being able to convert the internet

53:20.620 --> 53:25.620
 or be able to attach like semantic meaning

53:25.700 --> 53:27.940
 to the words on the internet,

53:27.940 --> 53:29.780
 the sentences, the paragraphs,

53:29.780 --> 53:33.940
 to be able to convert information on the internet

53:33.940 --> 53:35.660
 or some fraction of the internet

53:35.660 --> 53:38.180
 into something that's interpretable by machines.

53:39.140 --> 53:42.980
 That was kind of a dream for,

53:44.260 --> 53:47.020
 I think the semantic web papers in the nineties,

53:47.020 --> 53:49.740
 it's kind of the dream that, you know,

53:49.740 --> 53:52.340
 the internet is full of rich, exciting information.

53:52.340 --> 53:54.420
 Even just looking at Wikipedia,

53:54.420 --> 53:57.780
 we should be able to use that as data for machines.

53:57.780 --> 53:58.980
 And so far it's not,

53:58.980 --> 54:01.220
 it's not really in a format that's available to machines.

54:01.220 --> 54:04.540
 So no, I don't think the semantic web will ever work

54:04.540 --> 54:08.020
 simply because it would be a lot of work, right?

54:08.020 --> 54:12.020
 To make, to provide that information in structured form.

54:12.020 --> 54:13.820
 And there is not really any incentive

54:13.820 --> 54:16.340
 for anyone to provide that work.

54:16.340 --> 54:21.180
 So I think the way forward to make the knowledge

54:21.180 --> 54:22.820
 on the web available to machines

54:22.820 --> 54:26.620
 is actually something closer to unsupervised deep learning.

54:29.140 --> 54:32.220
 So GPT3 is actually a bigger step in the direction

54:32.220 --> 54:34.940
 of making the knowledge of the web available to machines

54:34.940 --> 54:36.660
 than the semantic web was.

54:36.660 --> 54:40.140
 Yeah, perhaps in a human centric sense,

54:40.140 --> 54:47.300
 it feels like GPT3 hasn't learned anything

54:47.300 --> 54:50.340
 that could be used to reason.

54:50.340 --> 54:52.820
 But that might be just the early days.

54:52.820 --> 54:54.300
 Yeah, I think that's correct.

54:54.300 --> 54:57.340
 I think the forms of reasoning that you see it perform

54:57.340 --> 55:00.660
 are basically just reproducing patterns

55:00.660 --> 55:02.380
 that it has seen in string data.

55:02.380 --> 55:06.580
 So of course, if you're trained on the entire web,

55:06.580 --> 55:09.340
 then you can produce an illusion of reasoning

55:09.340 --> 55:10.740
 in many different situations.

55:10.740 --> 55:13.100
 But it will break down if it's presented

55:13.100 --> 55:15.260
 with a novel situation.

55:15.260 --> 55:17.660
 That's the open question between the illusion of reasoning

55:17.660 --> 55:18.700
 and actual reasoning, yeah.

55:18.700 --> 55:19.660
 Yes.

55:19.660 --> 55:22.780
 The power to adapt to something that is genuinely new.

55:22.780 --> 55:28.020
 Because the thing is, even imagine you had,

55:28.020 --> 55:31.100
 you could train on every bit of data

55:31.100 --> 55:35.500
 ever generated in the history of humanity.

55:35.500 --> 55:38.540
 It remains, that model would be capable

55:38.540 --> 55:43.220
 of anticipating many different possible situations.

55:43.220 --> 55:45.660
 But it remains that the future is

55:45.660 --> 55:48.940
 going to be something different.

55:48.940 --> 55:52.940
 For instance, if you train a GPT3 model on data

55:52.940 --> 55:55.700
 from the year 2002, for instance,

55:55.700 --> 55:58.260
 and then use it today, it's going to be missing many things.

55:58.260 --> 56:00.740
 It's going to be missing many common sense

56:00.740 --> 56:02.620
 facts about the world.

56:02.620 --> 56:05.820
 It's even going to be missing vocabulary and so on.

56:05.820 --> 56:09.580
 Yeah, it's interesting that GPT3 even doesn't have,

56:09.580 --> 56:13.580
 I think, any information about the coronavirus.

56:13.580 --> 56:14.980
 Yes.

56:14.980 --> 56:19.620
 Which is why a system that's, you

56:19.620 --> 56:21.300
 tell that the system is intelligent

56:21.300 --> 56:22.860
 when it's capable to adapt.

56:22.860 --> 56:25.580
 So intelligence is going to require

56:25.580 --> 56:28.140
 some amount of continuous learning.

56:28.140 --> 56:31.020
 It's also going to require some amount of improvisation.

56:31.020 --> 56:33.980
 It's not enough to assume that what you're

56:33.980 --> 56:36.780
 going to be asked to do is something

56:36.780 --> 56:39.300
 that you've seen before, or something

56:39.300 --> 56:42.700
 that is a simple interpolation of things you've seen before.

56:42.700 --> 56:43.340
 Yeah.

56:43.340 --> 56:49.060
 In fact, that model breaks down for even very

56:49.060 --> 56:52.300
 tasks that look relatively simple from a distance,

56:52.300 --> 56:55.660
 like L5 self driving, for instance.

56:55.660 --> 56:58.420
 Google had a paper a couple of years

56:58.420 --> 57:04.540
 back showing that something like 30 million different road

57:04.540 --> 57:07.180
 situations were actually completely insufficient

57:07.180 --> 57:09.780
 to train a driving model.

57:09.780 --> 57:11.740
 It wasn't even L2, right?

57:11.740 --> 57:12.820
 And that's a lot of data.

57:12.820 --> 57:16.940
 That's a lot more data than the 20 or 30 hours of driving

57:16.940 --> 57:19.580
 that a human needs to learn to drive,

57:19.580 --> 57:21.900
 given the knowledge they've already accumulated.

57:21.900 --> 57:25.540
 Well, let me ask you on that topic.

57:25.540 --> 57:31.100
 Elon Musk, Tesla Autopilot, one of the only companies,

57:31.100 --> 57:34.660
 I believe, is really pushing for a learning based approach.

57:34.660 --> 57:37.020
 Are you skeptical that that kind of network

57:37.020 --> 57:39.460
 can achieve level 4?

57:39.460 --> 57:42.660
 L4 is probably achievable.

57:42.660 --> 57:44.420
 L5 probably not.

57:44.420 --> 57:45.860
 What's the distinction there?

57:45.860 --> 57:49.340
 Is L5 is completely you can just fall asleep?

57:49.340 --> 57:51.060
 Yeah, L5 is basically human level.

57:51.060 --> 57:53.740
 Well, with driving, we have to be careful saying human level,

57:53.740 --> 57:57.180
 because that's the most of the drivers.

57:57.180 --> 58:00.620
 Yeah, that's the clearest example of cars

58:00.620 --> 58:05.020
 will most likely be much safer than humans in many situations

58:05.020 --> 58:06.540
 where humans fail.

58:06.540 --> 58:09.860
 It's the vice versa question.

58:09.860 --> 58:13.820
 I'll tell you, the thing is the amount of trained data

58:13.820 --> 58:17.020
 you would need to anticipate for pretty much every possible

58:17.020 --> 58:20.460
 situation you learn content in the real world

58:20.460 --> 58:23.500
 is such that it's not entirely unrealistic

58:23.500 --> 58:25.540
 to think that at some point in the future,

58:25.540 --> 58:27.700
 we'll develop a system that's trained on enough data,

58:27.700 --> 58:32.340
 especially provided that we can simulate a lot of that data.

58:32.340 --> 58:34.500
 We don't necessarily need actual cars

58:34.500 --> 58:37.620
 on the road for everything.

58:37.620 --> 58:39.780
 But it's a massive effort.

58:39.780 --> 58:42.100
 And it turns out you can create a system that's

58:42.100 --> 58:45.180
 much more adaptive, that can generalize much better

58:45.180 --> 58:52.060
 if you just add explicit models of the surroundings

58:52.060 --> 58:53.580
 of the car.

58:53.580 --> 58:55.180
 And if you use deep learning for what

58:55.180 --> 58:57.460
 it's good at, which is to provide

58:57.460 --> 58:59.500
 perceptive information.

58:59.500 --> 59:02.460
 So in general, deep learning is a way

59:02.460 --> 59:05.740
 to encode perception and a way to encode intuition.

59:05.740 --> 59:11.100
 But it is not a good medium for any sort of explicit reasoning.

59:11.100 --> 59:15.940
 And in AI systems today, strong generalization

59:15.940 --> 59:21.020
 tends to come from explicit models,

59:21.020 --> 59:24.540
 tend to come from abstractions in the human mind that

59:24.540 --> 59:29.540
 are encoded in program form by a human engineer.

59:29.540 --> 59:31.580
 These are the abstractions you can actually generalize, not

59:31.580 --> 59:33.380
 the sort of weak abstraction that

59:33.380 --> 59:34.860
 is learned by a neural network.

59:34.860 --> 59:38.540
 Yeah, and the question is how much reasoning,

59:38.540 --> 59:41.940
 how much strong abstractions are required

59:41.940 --> 59:44.620
 to solve particular tasks like driving.

59:44.620 --> 59:46.540
 That's the question.

59:46.540 --> 59:48.860
 Or human life existence.

59:48.860 --> 59:53.340
 How much strong abstractions does existence require?

59:53.340 --> 59:58.100
 But more specifically on driving,

59:58.100 --> 1:00:02.180
 that seems to be a coupled question about intelligence.

1:00:02.180 --> 1:00:05.740
 How much intelligence, how do you

1:00:05.740 --> 1:00:07.140
 build an intelligent system?

1:00:07.140 --> 1:00:11.420
 And the coupled problem, how hard is this problem?

1:00:11.420 --> 1:00:14.380
 How much intelligence does this problem actually require?

1:00:14.380 --> 1:00:18.460
 So we get to cheat because we get

1:00:18.460 --> 1:00:20.700
 to look at the problem.

1:00:20.700 --> 1:00:22.860
 It's not like you get to close our eyes

1:00:22.860 --> 1:00:24.740
 and completely new to driving.

1:00:24.740 --> 1:00:27.020
 We get to do what we do as human beings, which

1:00:27.020 --> 1:00:31.100
 is for the majority of our life before we ever

1:00:31.100 --> 1:00:32.460
 learn, quote unquote, to drive.

1:00:32.460 --> 1:00:35.460
 We get to watch other cars and other people drive.

1:00:35.460 --> 1:00:36.540
 We get to be in cars.

1:00:36.540 --> 1:00:37.540
 We get to watch.

1:00:37.540 --> 1:00:39.500
 We get to see movies about cars.

1:00:39.500 --> 1:00:42.700
 We get to observe all this stuff.

1:00:42.700 --> 1:00:45.060
 And that's similar to what neural networks are doing.

1:00:45.060 --> 1:00:50.340
 It's getting a lot of data, and the question

1:00:50.340 --> 1:00:55.740
 is, yeah, how many leaps of reasoning genius

1:00:55.740 --> 1:00:59.420
 is required to be able to actually effectively drive?

1:00:59.420 --> 1:01:01.260
 I think it's a good example of driving.

1:01:01.260 --> 1:01:06.260
 I mean, sure, you've seen a lot of cars in your life

1:01:06.260 --> 1:01:07.700
 before you learned to drive.

1:01:07.700 --> 1:01:10.620
 But let's say you've learned to drive in Silicon Valley,

1:01:10.620 --> 1:01:14.100
 and now you rent a car in Tokyo.

1:01:14.100 --> 1:01:16.820
 Well, now everyone is driving on the other side of the road,

1:01:16.820 --> 1:01:19.220
 and the signs are different, and the roads

1:01:19.220 --> 1:01:20.500
 are more narrow and so on.

1:01:20.500 --> 1:01:22.660
 So it's a very, very different environment.

1:01:22.660 --> 1:01:26.780
 And a smart human, even an average human,

1:01:26.780 --> 1:01:29.300
 should be able to just zero shot it,

1:01:29.300 --> 1:01:34.260
 to just be operational in this very different environment

1:01:34.260 --> 1:01:40.500
 right away, despite having had no contact with the novel

1:01:40.500 --> 1:01:44.140
 complexity that is contained in this environment.

1:01:44.140 --> 1:01:49.780
 And that novel complexity is not just an interpolation

1:01:49.780 --> 1:01:52.420
 over the situations that you've encountered previously,

1:01:52.420 --> 1:01:55.060
 like learning to drive in the US.

1:01:55.060 --> 1:01:57.300
 I would say the reason I ask is one

1:01:57.300 --> 1:01:59.940
 of the most interesting tests of intelligence

1:01:59.940 --> 1:02:04.460
 we have today actively, which is driving,

1:02:04.460 --> 1:02:06.740
 in terms of having an impact on the world.

1:02:06.740 --> 1:02:09.900
 When do you think we'll pass that test of intelligence?

1:02:09.900 --> 1:02:13.380
 So I don't think driving is that much of a test of intelligence,

1:02:13.380 --> 1:02:18.500
 because again, there is no task for which skill at that task

1:02:18.500 --> 1:02:21.980
 demonstrates intelligence, unless it's

1:02:21.980 --> 1:02:26.540
 a kind of meta task that involves acquiring new skills.

1:02:26.540 --> 1:02:28.260
 So I don't think, I think you can actually

1:02:28.260 --> 1:02:35.060
 solve driving without having any real amount of intelligence.

1:02:35.060 --> 1:02:39.540
 For instance, if you did have infinite trained data,

1:02:39.540 --> 1:02:42.660
 you could just literally train an end to end deep learning

1:02:42.660 --> 1:02:45.700
 model that does driving, provided infinite trained data.

1:02:45.700 --> 1:02:48.940
 The only problem with the whole idea

1:02:48.940 --> 1:02:53.500
 is collecting a data set that's sufficiently comprehensive,

1:02:53.500 --> 1:02:56.380
 that covers the very long tail of possible situations

1:02:56.380 --> 1:02:57.260
 you might encounter.

1:02:57.260 --> 1:02:59.380
 And it's really just a scale problem.

1:02:59.380 --> 1:03:04.500
 So I think there's nothing fundamentally wrong

1:03:04.500 --> 1:03:06.500
 with this plan, with this idea.

1:03:06.500 --> 1:03:11.260
 It's just that it strikes me as a fairly inefficient thing

1:03:11.260 --> 1:03:17.340
 to do, because you run into this scaling issue with diminishing

1:03:17.340 --> 1:03:17.860
 returns.

1:03:17.860 --> 1:03:21.980
 Whereas if instead you took a more manual engineering

1:03:21.980 --> 1:03:29.020
 approach, where you use deep learning modules in combination

1:03:29.020 --> 1:03:33.220
 with engineering an explicit model of the surrounding

1:03:33.220 --> 1:03:36.100
 of the cars, and you bridge the two in a clever way,

1:03:36.100 --> 1:03:38.900
 your model will actually start generalizing

1:03:38.900 --> 1:03:40.900
 much earlier and more effectively

1:03:40.900 --> 1:03:42.540
 than the end to end deep learning model.

1:03:42.540 --> 1:03:46.500
 So why would you not go with the more manual engineering

1:03:46.500 --> 1:03:47.900
 oriented approach?

1:03:47.900 --> 1:03:50.620
 Even if you created that system, either the end

1:03:50.620 --> 1:03:52.500
 to end deep learning model system that's

1:03:52.500 --> 1:03:58.500
 running infinite data, or the slightly more human system,

1:03:58.500 --> 1:04:02.740
 I don't think achieving L5 would demonstrate

1:04:02.740 --> 1:04:04.540
 general intelligence or intelligence

1:04:04.540 --> 1:04:05.740
 of any generality at all.

1:04:05.740 --> 1:04:10.580
 Again, the only possible test of generality in AI

1:04:10.580 --> 1:04:12.740
 would be a test that looks at skill acquisition

1:04:12.740 --> 1:04:14.500
 over unknown tasks.

1:04:14.500 --> 1:04:17.380
 For instance, you could take your L5 driver

1:04:17.380 --> 1:04:21.540
 and ask it to learn to pilot a commercial airplane,

1:04:21.540 --> 1:04:22.420
 for instance.

1:04:22.420 --> 1:04:25.180
 And then you would look at how much human involvement is

1:04:25.180 --> 1:04:26.740
 required and how much strength data

1:04:26.740 --> 1:04:29.860
 is required for the system to learn to pilot an airplane.

1:04:29.860 --> 1:04:35.020
 And that gives you a measure of how intelligent

1:04:35.020 --> 1:04:35.860
 that system really is.

1:04:35.860 --> 1:04:37.540
 Yeah, well, I mean, that's a big leap.

1:04:37.540 --> 1:04:38.060
 I get you.

1:04:38.060 --> 1:04:42.820
 But I'm more interested, as a problem, I would see,

1:04:42.820 --> 1:04:47.380
 to me, driving is a black box that

1:04:47.380 --> 1:04:51.180
 can generate novel situations at some rate,

1:04:51.180 --> 1:04:53.500
 what people call edge cases.

1:04:53.500 --> 1:04:56.380
 So it does have newness that keeps being like,

1:04:56.380 --> 1:04:59.460
 we're confronted, let's say, once a month.

1:04:59.460 --> 1:05:00.660
 It is a very long tail, yes.

1:05:00.660 --> 1:05:01.460
 It's a long tail.

1:05:01.460 --> 1:05:05.620
 That doesn't mean you cannot solve it just

1:05:05.620 --> 1:05:08.740
 by training a statistical model and a lot of data.

1:05:08.740 --> 1:05:09.820
 Huge amount of data.

1:05:09.820 --> 1:05:11.900
 It's really a matter of scale.

1:05:11.900 --> 1:05:16.020
 But I guess what I'm saying is if you have a vehicle that

1:05:16.020 --> 1:05:21.580
 achieves level 5, it is going to be able to deal

1:05:21.580 --> 1:05:23.980
 with new situations.

1:05:23.980 --> 1:05:30.860
 Or, I mean, the data is so large that the rate of new situations

1:05:30.860 --> 1:05:32.100
 is very low.

1:05:32.100 --> 1:05:33.140
 Yes.

1:05:33.140 --> 1:05:34.220
 That's not intelligent.

1:05:34.220 --> 1:05:37.780
 So if we go back to your kind of definition of intelligence,

1:05:37.780 --> 1:05:39.460
 it's the efficiency.

1:05:39.460 --> 1:05:42.380
 With which you can adapt to new situations,

1:05:42.380 --> 1:05:45.700
 to truly new situations, not situations you've seen before.

1:05:45.700 --> 1:05:48.460
 Not situations that could be anticipated by your creators,

1:05:48.460 --> 1:05:51.740
 by the creators of the system, but truly new situations.

1:05:51.740 --> 1:05:54.940
 The efficiency with which you acquire new skills.

1:05:54.940 --> 1:05:58.260
 If you require, if in order to pick up a new skill,

1:05:58.260 --> 1:06:03.180
 you require a very extensive training

1:06:03.180 --> 1:06:05.900
 data set of most possible situations

1:06:05.900 --> 1:06:08.940
 that can occur in the practice of that skill,

1:06:08.940 --> 1:06:10.620
 then the system is not intelligent.

1:06:10.620 --> 1:06:15.060
 It is mostly just a lookup table.

1:06:15.060 --> 1:06:16.140
 Yeah.

1:06:16.140 --> 1:06:20.100
 Well, likewise, if in order to acquire a skill,

1:06:20.100 --> 1:06:23.300
 you need a human engineer to write down

1:06:23.300 --> 1:06:26.940
 a bunch of rules that cover most or every possible situation.

1:06:26.940 --> 1:06:29.620
 Likewise, the system is not intelligent.

1:06:29.620 --> 1:06:33.100
 The system is merely the output artifact

1:06:33.100 --> 1:06:39.300
 of a process that happens in the minds of the engineers that

1:06:39.300 --> 1:06:40.820
 are creating it.

1:06:40.820 --> 1:06:44.700
 It is encoding an abstraction that's

1:06:44.700 --> 1:06:46.420
 produced by the human mind.

1:06:46.420 --> 1:06:51.500
 And intelligence would actually be

1:06:51.500 --> 1:06:56.260
 the process of autonomously producing this abstraction.

1:06:56.260 --> 1:06:57.180
 Yeah.

1:06:57.180 --> 1:06:59.260
 Not like if you take an abstraction

1:06:59.260 --> 1:07:02.900
 and you encode it on a piece of paper or in a computer program,

1:07:02.900 --> 1:07:05.940
 the abstraction itself is not intelligent.

1:07:05.940 --> 1:07:09.220
 What's intelligent is the agent that's

1:07:09.220 --> 1:07:11.780
 capable of producing these abstractions.

1:07:11.780 --> 1:07:16.500
 Yeah, it feels like there's a little bit of a gray area.

1:07:16.500 --> 1:07:18.860
 Because you're basically saying that deep learning forms

1:07:18.860 --> 1:07:21.500
 abstractions, too.

1:07:21.500 --> 1:07:24.660
 But those abstractions do not seem

1:07:24.660 --> 1:07:29.140
 to be effective for generalizing far outside of the things

1:07:29.140 --> 1:07:30.100
 that it's already seen.

1:07:30.100 --> 1:07:31.620
 But generalize a little bit.

1:07:31.620 --> 1:07:32.620
 Yeah, absolutely.

1:07:32.620 --> 1:07:34.820
 No, deep learning does generalize a little bit.

1:07:34.820 --> 1:07:36.980
 Generalization is not binary.

1:07:36.980 --> 1:07:38.140
 It's more like a spectrum.

1:07:38.140 --> 1:07:38.740
 Yeah.

1:07:38.740 --> 1:07:40.860
 And there's a certain point, it's a gray area,

1:07:40.860 --> 1:07:42.500
 but there's a certain point where

1:07:42.500 --> 1:07:47.340
 there's an impressive degree of generalization that happens.

1:07:47.340 --> 1:07:50.420
 No, I guess exactly what you were saying

1:07:50.420 --> 1:07:56.420
 is intelligence is how efficiently you're

1:07:56.420 --> 1:08:02.300
 able to generalize far outside of the distribution of things

1:08:02.300 --> 1:08:03.260
 you've seen already.

1:08:03.260 --> 1:08:03.780
 Yes.

1:08:03.780 --> 1:08:07.180
 So it's both the distance of how far you can,

1:08:07.180 --> 1:08:10.180
 how new, how radically new something is,

1:08:10.180 --> 1:08:12.740
 and how efficiently you're able to deal with that.

1:08:12.740 --> 1:08:17.420
 So you can think of intelligence as a measure of an information

1:08:17.420 --> 1:08:19.140
 conversion ratio.

1:08:19.140 --> 1:08:23.420
 Imagine a space of possible situations.

1:08:23.420 --> 1:08:27.860
 And you've covered some of them.

1:08:27.860 --> 1:08:30.180
 So you have some amount of information

1:08:30.180 --> 1:08:32.020
 about your space of possible situations

1:08:32.020 --> 1:08:34.420
 that's provided by the situations you already know.

1:08:34.420 --> 1:08:36.540
 And that's, on the other hand, also provided

1:08:36.540 --> 1:08:40.420
 by the prior knowledge that the system brings

1:08:40.420 --> 1:08:42.340
 to the table, the prior knowledge embedded

1:08:42.340 --> 1:08:43.660
 in the system.

1:08:43.660 --> 1:08:46.420
 So the system starts with some information

1:08:46.420 --> 1:08:48.860
 about the problem, about the task.

1:08:48.860 --> 1:08:52.500
 And it's about going from that information

1:08:52.500 --> 1:08:55.340
 to a program, what we would call a skill program,

1:08:55.340 --> 1:08:58.860
 a behavioral program, that can cover a large area

1:08:58.860 --> 1:09:01.660
 of possible situation space.

1:09:01.660 --> 1:09:04.100
 And essentially, the ratio between that area

1:09:04.100 --> 1:09:09.740
 and the amount of information you start with is intelligence.

1:09:09.740 --> 1:09:14.180
 So a very smart agent can make efficient use

1:09:14.180 --> 1:09:17.580
 of very little information about a new problem

1:09:17.580 --> 1:09:19.580
 and very little prior knowledge as well

1:09:19.580 --> 1:09:23.380
 to cover a very large area of potential situations

1:09:23.380 --> 1:09:28.500
 in that problem without knowing what these future new situations

1:09:28.500 --> 1:09:31.140
 are going to be.

1:09:31.140 --> 1:09:34.540
 So one of the other big things you talk about in the paper,

1:09:34.540 --> 1:09:36.300
 we've talked about a little bit already,

1:09:36.300 --> 1:09:37.860
 but let's talk about it some more,

1:09:37.860 --> 1:09:41.020
 is the actual tests of intelligence.

1:09:41.020 --> 1:09:45.980
 So if we look at human and machine intelligence,

1:09:45.980 --> 1:09:48.100
 do you think tests of intelligence

1:09:48.100 --> 1:09:50.340
 should be different for humans and machines,

1:09:50.340 --> 1:09:54.420
 or how we think about testing of intelligence?

1:09:54.420 --> 1:09:59.740
 Are these fundamentally the same kind of intelligences

1:09:59.740 --> 1:10:03.780
 that we're after, and therefore, the tests should be similar?

1:10:03.780 --> 1:10:10.540
 So if your goal is to create AIs that are more humanlike,

1:10:10.540 --> 1:10:12.540
 then it would be super valuable, obviously,

1:10:12.540 --> 1:10:18.500
 to have a test that's universal, that applies to both AIs

1:10:18.500 --> 1:10:20.820
 and humans, so that you could establish

1:10:20.820 --> 1:10:23.260
 a comparison between the two, that you

1:10:23.260 --> 1:10:27.340
 could tell exactly how intelligent,

1:10:27.340 --> 1:10:30.420
 in terms of human intelligence, a given system is.

1:10:30.420 --> 1:10:34.260
 So that said, the constraints that

1:10:34.260 --> 1:10:37.620
 apply to artificial intelligence and to human intelligence

1:10:37.620 --> 1:10:39.340
 are very different.

1:10:39.340 --> 1:10:44.860
 And your test should account for this difference.

1:10:44.860 --> 1:10:47.140
 Because if you look at artificial systems,

1:10:47.140 --> 1:10:50.420
 it's always possible for an experimenter

1:10:50.420 --> 1:10:55.580
 to buy arbitrary levels of skill at arbitrary tasks,

1:10:55.580 --> 1:11:01.100
 either by injecting hardcoded prior knowledge

1:11:01.100 --> 1:11:05.660
 into the system via rules and so on that

1:11:05.660 --> 1:11:08.660
 come from the human mind, from the minds of the programmers,

1:11:08.660 --> 1:11:12.980
 and also buying higher levels of skill

1:11:12.980 --> 1:11:15.620
 just by training on more data.

1:11:15.620 --> 1:11:17.860
 For instance, you could generate an infinity

1:11:17.860 --> 1:11:21.660
 of different Go games, and you could train a Go playing

1:11:21.660 --> 1:11:26.820
 system that way, but you could not directly compare it

1:11:26.820 --> 1:11:28.620
 to human Go playing skills.

1:11:28.620 --> 1:11:31.100
 Because a human that plays Go had

1:11:31.100 --> 1:11:34.660
 to develop that skill in a very constrained environment.

1:11:34.660 --> 1:11:36.580
 They had a limited amount of time.

1:11:36.580 --> 1:11:38.940
 They had a limited amount of energy.

1:11:38.940 --> 1:11:42.620
 And of course, this started from a different set of priors.

1:11:42.620 --> 1:11:48.540
 This started from innate human priors.

1:11:48.540 --> 1:11:49.860
 So I think if you want to compare

1:11:49.860 --> 1:11:53.260
 the intelligence of two systems, like the intelligence of an AI

1:11:53.260 --> 1:11:59.780
 and the intelligence of a human, you have to control for priors.

1:11:59.780 --> 1:12:04.500
 You have to start from the same set of knowledge priors

1:12:04.500 --> 1:12:06.940
 about the task, and you have to control

1:12:06.940 --> 1:12:11.140
 for experience, that is to say, for training data.

1:12:11.140 --> 1:12:14.980
 So what's priors?

1:12:14.980 --> 1:12:18.340
 So prior is whatever information you

1:12:18.340 --> 1:12:21.020
 have about a given task before you

1:12:21.020 --> 1:12:23.100
 start learning about this task.

1:12:23.100 --> 1:12:25.780
 And how's that different from experience?

1:12:25.780 --> 1:12:28.020
 Well, experience is acquired.

1:12:28.020 --> 1:12:31.100
 So for instance, if you're trying to play Go,

1:12:31.100 --> 1:12:33.900
 your experience with Go is all the Go games

1:12:33.900 --> 1:12:37.060
 you've played, or you've seen, or you've simulated

1:12:37.060 --> 1:12:38.500
 in your mind, let's say.

1:12:38.500 --> 1:12:42.740
 And your priors are things like, well,

1:12:42.740 --> 1:12:45.860
 Go is a game on the 2D grid.

1:12:45.860 --> 1:12:48.780
 And we have lots of hardcoded priors

1:12:48.780 --> 1:12:53.180
 about the organization of 2D space.

1:12:53.180 --> 1:12:58.340
 And the rules of how the dynamics of the physics

1:12:58.340 --> 1:12:59.980
 of this game in this 2D space?

1:12:59.980 --> 1:13:00.580
 Yes.

1:13:00.580 --> 1:13:04.300
 And the idea that you have what winning is.

1:13:04.300 --> 1:13:05.580
 Yes, exactly.

1:13:05.580 --> 1:13:09.660
 And other board games can also share some similarities with Go.

1:13:09.660 --> 1:13:12.060
 And if you've played these board games, then,

1:13:12.060 --> 1:13:13.860
 with respect to the game of Go, that

1:13:13.860 --> 1:13:16.300
 would be part of your priors about the game.

1:13:16.300 --> 1:13:18.500
 Well, it's interesting to think about the game of Go

1:13:18.500 --> 1:13:22.620
 is how many priors are actually brought to the table.

1:13:22.620 --> 1:13:27.500
 When you look at self play, reinforcement learning based

1:13:27.500 --> 1:13:29.300
 mechanisms that do learning, it seems

1:13:29.300 --> 1:13:31.020
 like the number of priors is pretty low.

1:13:31.020 --> 1:13:31.380
 Yes.

1:13:31.380 --> 1:13:32.980
 But you're saying you should be expec...

1:13:32.980 --> 1:13:35.700
 There is a 2D special priors in the carbonate.

1:13:35.700 --> 1:13:36.460
 Right.

1:13:36.460 --> 1:13:39.020
 But you should be clear at making

1:13:39.020 --> 1:13:40.460
 those priors explicit.

1:13:40.460 --> 1:13:41.820
 Yes.

1:13:41.820 --> 1:13:44.060
 So in particular, I think if your goal

1:13:44.060 --> 1:13:47.700
 is to measure a humanlike form of intelligence,

1:13:47.700 --> 1:13:49.700
 then you should clearly establish

1:13:49.700 --> 1:13:52.820
 that you want the AI you're testing

1:13:52.820 --> 1:13:57.500
 to start from the same set of priors that humans start with.

1:13:57.500 --> 1:13:58.820
 Right.

1:13:58.820 --> 1:14:02.740
 So I mean, to me personally, but I think to a lot of people,

1:14:02.740 --> 1:14:05.300
 the human side of things is very interesting.

1:14:05.300 --> 1:14:08.020
 So testing intelligence for humans.

1:14:08.020 --> 1:14:14.420
 What do you think is a good test of human intelligence?

1:14:14.420 --> 1:14:19.820
 Well, that's the question that psychometrics is interested in.

1:14:19.820 --> 1:14:22.420
 There's an entire subfield of psychology

1:14:22.420 --> 1:14:23.860
 that deals with this question.

1:14:23.860 --> 1:14:25.180
 So what's psychometrics?

1:14:25.180 --> 1:14:27.980
 The psychometrics is the subfield of psychology

1:14:27.980 --> 1:14:33.940
 that tries to measure, quantify aspects of the human mind.

1:14:33.940 --> 1:14:36.940
 So in particular, our cognitive abilities, intelligence,

1:14:36.940 --> 1:14:39.660
 and personality traits as well.

1:14:39.660 --> 1:14:43.620
 So what are, it might be a weird question,

1:14:43.620 --> 1:14:49.700
 but what are the first principles of psychometrics

1:14:49.700 --> 1:14:52.100
 this operates on?

1:14:52.100 --> 1:14:55.340
 What are the priors it brings to the table?

1:14:55.340 --> 1:14:58.500
 So it's a field with a fairly long history.

1:15:01.940 --> 1:15:05.500
 So psychology sometimes gets a bad reputation

1:15:05.500 --> 1:15:09.020
 for not having very reproducible results.

1:15:09.020 --> 1:15:12.420
 And psychometrics has actually some fairly solidly

1:15:12.420 --> 1:15:14.180
 reproducible results.

1:15:14.180 --> 1:15:17.980
 So the ideal goals of the field is a test

1:15:17.980 --> 1:15:23.060
 should be reliable, which is a notion tied to reproducibility.

1:15:23.060 --> 1:15:26.540
 It should be valid, meaning that it should actually

1:15:26.540 --> 1:15:30.860
 measure what you say it measures.

1:15:30.860 --> 1:15:32.780
 So for instance, if you're saying

1:15:32.780 --> 1:15:34.140
 that you're measuring intelligence,

1:15:34.140 --> 1:15:36.620
 then your test results should be correlated

1:15:36.620 --> 1:15:39.140
 with things that you expect to be correlated

1:15:39.140 --> 1:15:41.500
 with intelligence like success in school

1:15:41.500 --> 1:15:43.580
 or success in the workplace and so on.

1:15:43.580 --> 1:15:46.540
 Should be standardized, meaning that you

1:15:46.540 --> 1:15:48.980
 can administer your tests to many different people

1:15:48.980 --> 1:15:50.780
 in some conditions.

1:15:50.780 --> 1:15:52.860
 And it should be free from bias.

1:15:52.860 --> 1:15:57.140
 Meaning that, for instance, if your test involves

1:15:57.140 --> 1:15:59.100
 the English language, then you have

1:15:59.100 --> 1:16:02.500
 to be aware that this creates a bias against people

1:16:02.500 --> 1:16:04.340
 who have English as their second language

1:16:04.340 --> 1:16:07.300
 or people who can't speak English at all.

1:16:07.300 --> 1:16:10.100
 So of course, these principles for creating

1:16:10.100 --> 1:16:13.420
 psychometric tests are very much an ideal.

1:16:13.420 --> 1:16:17.540
 I don't think every psychometric test is really either

1:16:17.540 --> 1:16:22.060
 reliable, valid, or free from bias.

1:16:22.060 --> 1:16:25.740
 But at least the field is aware of these weaknesses

1:16:25.740 --> 1:16:27.380
 and is trying to address them.

1:16:27.380 --> 1:16:30.100
 So it's kind of interesting.

1:16:30.100 --> 1:16:31.820
 Ultimately, you're only able to measure,

1:16:31.820 --> 1:16:34.420
 like you said previously, the skill.

1:16:34.420 --> 1:16:36.420
 But you're trying to do a bunch of measures

1:16:36.420 --> 1:16:38.820
 of different skills that correlate,

1:16:38.820 --> 1:16:41.780
 as you mentioned, strongly with some general concept

1:16:41.780 --> 1:16:43.340
 of cognitive ability.

1:16:43.340 --> 1:16:44.060
 Yes, yes.

1:16:44.060 --> 1:16:46.620
 So what's the G factor?

1:16:46.620 --> 1:16:48.140
 So right, there are many different kinds

1:16:48.140 --> 1:16:50.620
 of tests of intelligence.

1:16:50.620 --> 1:16:55.340
 And each of them is interesting in different aspects

1:16:55.340 --> 1:16:56.060
 of intelligence.

1:16:56.060 --> 1:16:57.580
 Some of them will deal with language.

1:16:57.580 --> 1:17:00.940
 Some of them will deal with spatial vision,

1:17:00.940 --> 1:17:04.420
 maybe mental rotations, numbers, and so on.

1:17:04.420 --> 1:17:08.580
 When you run these very different tests at scale,

1:17:08.580 --> 1:17:10.940
 what you start seeing is that there

1:17:10.940 --> 1:17:14.220
 are clusters of correlations among test results.

1:17:14.220 --> 1:17:19.300
 So for instance, if you look at homework at school,

1:17:19.300 --> 1:17:21.780
 you will see that people who do well at math

1:17:21.780 --> 1:17:25.500
 are also likely statistically to do well in physics.

1:17:25.500 --> 1:17:30.060
 And what's more, people who do well at math and physics

1:17:30.060 --> 1:17:32.620
 are also statistically likely to do well

1:17:32.620 --> 1:17:35.580
 in things that sound completely unrelated,

1:17:35.580 --> 1:17:38.420
 like writing an English essay, for instance.

1:17:38.420 --> 1:17:42.700
 And so when you see clusters of correlations

1:17:42.700 --> 1:17:46.140
 in statistical terms, you would explain them

1:17:46.140 --> 1:17:47.540
 with the latent variable.

1:17:47.540 --> 1:17:51.100
 And the latent variable that would, for instance, explain

1:17:51.100 --> 1:17:53.020
 the relationship between being good at math

1:17:53.020 --> 1:17:57.020
 and being good at physics would be cognitive ability.

1:17:57.020 --> 1:18:00.780
 And the G factor is the latent variable

1:18:00.780 --> 1:18:05.540
 that explains the fact that every test of intelligence

1:18:05.540 --> 1:18:09.340
 that you can come up with results on this test

1:18:09.340 --> 1:18:10.540
 end up being correlated.

1:18:10.540 --> 1:18:16.180
 So there is some single unique variable

1:18:16.180 --> 1:18:17.820
 that explains these correlations.

1:18:17.820 --> 1:18:18.820
 That's the G factor.

1:18:18.820 --> 1:18:20.380
 So it's a statistical construct.

1:18:20.380 --> 1:18:23.060
 It's not really something you can directly measure,

1:18:23.060 --> 1:18:25.540
 for instance, in a person.

1:18:25.540 --> 1:18:26.540
 But it's there.

1:18:26.540 --> 1:18:27.220
 But it's there.

1:18:27.220 --> 1:18:27.740
 It's there.

1:18:27.740 --> 1:18:28.740
 It's there at scale.

1:18:28.740 --> 1:18:33.460
 And that's also one thing I want to mention about psychometrics.

1:18:33.460 --> 1:18:36.620
 Like when you talk about measuring intelligence

1:18:36.620 --> 1:18:38.660
 in humans, for instance, some people

1:18:38.660 --> 1:18:40.060
 get a little bit worried.

1:18:40.060 --> 1:18:41.940
 They will say, that sounds dangerous.

1:18:41.940 --> 1:18:44.340
 Maybe that sounds potentially discriminatory, and so on.

1:18:44.340 --> 1:18:46.460
 And they're not wrong.

1:18:46.460 --> 1:18:48.220
 And the thing is, personally, I'm

1:18:48.220 --> 1:18:51.100
 not interested in psychometrics as a way

1:18:51.100 --> 1:18:54.740
 to characterize one individual person.

1:18:54.740 --> 1:18:59.180
 Like if I get your psychometric personality

1:18:59.180 --> 1:19:01.780
 assessments or your IQ, I don't think that actually

1:19:01.780 --> 1:19:05.020
 tells me much about you as a person.

1:19:05.020 --> 1:19:10.300
 I think psychometrics is most useful as a statistical tool.

1:19:10.300 --> 1:19:12.500
 So it's most useful at scale.

1:19:12.500 --> 1:19:15.420
 It's most useful when you start getting test results

1:19:15.420 --> 1:19:17.420
 for a large number of people.

1:19:17.420 --> 1:19:20.580
 And you start cross correlating these test results.

1:19:20.580 --> 1:19:23.620
 Because that gives you information

1:19:23.620 --> 1:19:26.420
 about the structure of the human mind,

1:19:26.420 --> 1:19:28.300
 in particular about the structure

1:19:28.300 --> 1:19:29.780
 of human cognitive abilities.

1:19:29.780 --> 1:19:34.860
 So at scale, psychometrics paints a certain picture

1:19:34.860 --> 1:19:35.620
 of the human mind.

1:19:35.620 --> 1:19:37.220
 And that's interesting.

1:19:37.220 --> 1:19:39.540
 And that's what's relevant to AI, the structure

1:19:39.540 --> 1:19:41.060
 of human cognitive abilities.

1:19:41.060 --> 1:19:42.860
 Yeah, it gives you an insight into it.

1:19:42.860 --> 1:19:45.820
 I mean, to me, I remember when I learned about G factor,

1:19:45.820 --> 1:19:52.820
 it seemed like it would be impossible for it

1:19:52.820 --> 1:19:55.500
 to be real, even as a statistical variable.

1:19:55.500 --> 1:19:59.020
 Like it felt kind of like astrology.

1:19:59.020 --> 1:20:01.980
 Like it's like wishful thinking among psychologists.

1:20:01.980 --> 1:20:05.420
 But the more I learned, I realized that there's some.

1:20:05.420 --> 1:20:07.620
 I mean, I'm not sure what to make about human beings,

1:20:07.620 --> 1:20:10.580
 the fact that the G factor is a thing.

1:20:10.580 --> 1:20:13.260
 There's a commonality across all of human species,

1:20:13.260 --> 1:20:15.340
 that there does seem to be a strong correlation

1:20:15.340 --> 1:20:17.140
 between cognitive abilities.

1:20:17.140 --> 1:20:19.140
 That's kind of fascinating, actually.

1:20:19.140 --> 1:20:22.780
 So human cognitive abilities have a structure.

1:20:22.780 --> 1:20:25.380
 Like the most mainstream theory of the structure

1:20:25.380 --> 1:20:28.780
 of cognitive abilities is called CHC theory.

1:20:28.780 --> 1:20:30.660
 It's Cattell, Horn, Carroll.

1:20:30.660 --> 1:20:33.180
 It's named after the three psychologists who

1:20:33.180 --> 1:20:35.340
 contributed key pieces of it.

1:20:35.340 --> 1:20:38.620
 And it describes cognitive abilities

1:20:38.620 --> 1:20:41.060
 as a hierarchy with three levels.

1:20:41.060 --> 1:20:43.140
 And at the top, you have the G factor.

1:20:43.140 --> 1:20:46.140
 Then you have broad cognitive abilities,

1:20:46.140 --> 1:20:49.340
 for instance fluid intelligence, that

1:20:49.340 --> 1:20:54.940
 encompass a broad set of possible kinds of tasks

1:20:54.940 --> 1:20:57.100
 that are all related.

1:20:57.100 --> 1:20:59.900
 And then you have narrow cognitive abilities

1:20:59.900 --> 1:21:04.340
 at the last level, which is closer to task specific skill.

1:21:04.340 --> 1:21:09.100
 And there are actually different theories of the structure

1:21:09.100 --> 1:21:10.700
 of cognitive abilities that just emerge

1:21:10.700 --> 1:21:14.500
 from different statistical analysis of IQ test results.

1:21:14.500 --> 1:21:18.500
 But they all describe a hierarchy with a kind of G

1:21:18.500 --> 1:21:21.140
 factor at the top.

1:21:21.140 --> 1:21:23.740
 And you're right that the G factor,

1:21:23.740 --> 1:21:27.620
 it's not quite real in the sense that it's not something

1:21:27.620 --> 1:21:29.660
 you can observe and measure, like your height,

1:21:29.660 --> 1:21:30.340
 for instance.

1:21:30.340 --> 1:21:32.940
 But it's real in the sense that you

1:21:32.940 --> 1:21:37.780
 see it in a statistical analysis of the data.

1:21:37.780 --> 1:21:39.700
 One thing I want to mention is that the fact

1:21:39.700 --> 1:21:41.540
 that there is a G factor does not really

1:21:41.540 --> 1:21:45.740
 mean that human intelligence is general in a strong sense.

1:21:45.740 --> 1:21:47.220
 It does not mean human intelligence

1:21:47.220 --> 1:21:50.340
 can be applied to any problem at all,

1:21:50.340 --> 1:21:52.140
 and that someone who has a high IQ

1:21:52.140 --> 1:21:54.100
 is going to be able to solve any problem at all.

1:21:54.100 --> 1:21:55.260
 That's not quite what it means.

1:21:55.260 --> 1:22:00.420
 I think one popular analogy to understand it

1:22:00.420 --> 1:22:03.340
 is the sports analogy.

1:22:03.340 --> 1:22:06.660
 If you consider the concept of physical fitness,

1:22:06.660 --> 1:22:09.220
 it's a concept that's very similar to intelligence

1:22:09.220 --> 1:22:11.340
 because it's a useful concept.

1:22:11.340 --> 1:22:14.460
 It's something you can intuitively understand.

1:22:14.460 --> 1:22:17.620
 Some people are fit, maybe like you.

1:22:17.620 --> 1:22:20.540
 Some people are not as fit, maybe like me.

1:22:20.540 --> 1:22:22.980
 But none of us can fly.

1:22:22.980 --> 1:22:23.700
 Absolutely.

1:22:23.700 --> 1:22:25.460
 It's constrained to a specific set of skills.

1:22:25.460 --> 1:22:27.060
 Even if you're very fit, that doesn't

1:22:27.060 --> 1:22:31.020
 mean you can do anything at all in any environment.

1:22:31.020 --> 1:22:32.420
 You obviously cannot fly.

1:22:32.420 --> 1:22:36.020
 You cannot survive at the bottom of the ocean and so on.

1:22:36.020 --> 1:22:38.540
 And if you were a scientist and you

1:22:38.540 --> 1:22:42.820
 wanted to precisely define and measure physical fitness

1:22:42.820 --> 1:22:47.500
 in humans, then you would come up with a battery of tests.

1:22:47.500 --> 1:22:51.580
 You would have running 100 meter, playing soccer,

1:22:51.580 --> 1:22:54.260
 playing table tennis, swimming, and so on.

1:22:54.260 --> 1:22:58.420
 And if you ran these tests over many different people,

1:22:58.420 --> 1:23:01.220
 you would start seeing correlations in test results.

1:23:01.220 --> 1:23:03.020
 For instance, people who are good at soccer

1:23:03.020 --> 1:23:05.620
 are also good at sprinting.

1:23:05.620 --> 1:23:08.580
 And you would explain these correlations

1:23:08.580 --> 1:23:11.660
 with physical abilities that are strictly

1:23:11.660 --> 1:23:14.020
 analogous to cognitive abilities.

1:23:14.020 --> 1:23:17.060
 And then you would start also observing correlations

1:23:17.060 --> 1:23:21.220
 between biological characteristics,

1:23:21.220 --> 1:23:24.900
 like maybe lung volume is correlated with being

1:23:24.900 --> 1:23:27.820
 a fast runner, for instance, in the same way

1:23:27.820 --> 1:23:32.500
 that there are neurophysical correlates of cognitive

1:23:32.500 --> 1:23:33.940
 abilities.

1:23:33.940 --> 1:23:38.620
 And at the top of the hierarchy of physical abilities

1:23:38.620 --> 1:23:39.980
 that you would be able to observe,

1:23:39.980 --> 1:23:43.340
 you would have a G factor, a physical G factor, which

1:23:43.340 --> 1:23:45.740
 would map to physical fitness.

1:23:45.740 --> 1:23:47.980
 And as you just said, that doesn't

1:23:47.980 --> 1:23:51.340
 mean that people with high physical fitness can't fly.

1:23:51.340 --> 1:23:54.500
 It doesn't mean human morphology and human physiology

1:23:54.500 --> 1:23:55.660
 is universal.

1:23:55.660 --> 1:23:57.860
 It's actually super specialized.

1:23:57.860 --> 1:24:04.100
 We can only do the things that we were evolved to do.

1:24:04.100 --> 1:24:08.340
 We are not appropriate to, you could not

1:24:08.340 --> 1:24:11.100
 exist on Venus or Mars or in the void of space

1:24:11.100 --> 1:24:12.460
 or the bottom of the ocean.

1:24:12.460 --> 1:24:17.740
 So that said, one thing that's really striking and remarkable

1:24:17.740 --> 1:24:23.060
 is that our morphology generalizes

1:24:23.060 --> 1:24:27.260
 far beyond the environments that we evolved for.

1:24:27.260 --> 1:24:31.180
 Like in a way, you could say we evolved to run after prey

1:24:31.180 --> 1:24:32.900
 in the savanna, right?

1:24:32.900 --> 1:24:36.820
 That's very much where our human morphology comes from.

1:24:36.820 --> 1:24:40.220
 And that said, we can do a lot of things

1:24:40.220 --> 1:24:42.980
 that are completely unrelated to that.

1:24:42.980 --> 1:24:44.260
 We can climb mountains.

1:24:44.260 --> 1:24:47.260
 We can swim across lakes.

1:24:47.260 --> 1:24:48.980
 We can play table tennis.

1:24:48.980 --> 1:24:51.060
 I mean, table tennis is very different from what

1:24:51.060 --> 1:24:53.100
 we were evolved to do, right?

1:24:53.100 --> 1:24:56.300
 So our morphology, our bodies, our sense and motor

1:24:56.300 --> 1:24:59.500
 affordances have a degree of generality

1:24:59.500 --> 1:25:02.180
 that is absolutely remarkable, right?

1:25:02.180 --> 1:25:05.300
 And I think cognition is very similar to that.

1:25:05.300 --> 1:25:08.260
 Our cognitive abilities have a degree of generality

1:25:08.260 --> 1:25:11.180
 that goes far beyond what the mind was initially

1:25:11.180 --> 1:25:14.540
 supposed to do, which is why we can play music and write

1:25:14.540 --> 1:25:18.580
 novels and go to Mars and do all kinds of crazy things.

1:25:18.580 --> 1:25:20.780
 But it's not universal in the same way

1:25:20.780 --> 1:25:23.420
 that human morphology and our body

1:25:23.420 --> 1:25:27.500
 is not appropriate for actually most of the universe by volume.

1:25:27.500 --> 1:25:29.940
 In the same way, you could say that the human mind is not

1:25:29.940 --> 1:25:32.620
 really appropriate for most of problem space,

1:25:32.620 --> 1:25:35.460
 potential problem space by volume.

1:25:35.460 --> 1:25:39.660
 So we have very strong cognitive biases, actually,

1:25:39.660 --> 1:25:42.620
 that mean that there are certain types of problems

1:25:42.620 --> 1:25:45.380
 that we handle very well and certain types of problems

1:25:45.380 --> 1:25:48.260
 that we are completely in adapted for.

1:25:48.260 --> 1:25:52.420
 So that's really how we'd interpret the G factor.

1:25:52.420 --> 1:25:56.340
 It's not a sign of strong generality.

1:25:56.340 --> 1:26:01.020
 It's really just the broadest cognitive ability.

1:26:01.020 --> 1:26:03.020
 But our abilities, whether we are

1:26:03.020 --> 1:26:05.820
 talking about sensory motor abilities or cognitive

1:26:05.820 --> 1:26:09.460
 abilities, they still remain very specialized

1:26:09.460 --> 1:26:12.420
 in the human condition, right?

1:26:12.420 --> 1:26:16.300
 Within the constraints of the human cognition,

1:26:16.300 --> 1:26:18.300
 they're general.

1:26:18.300 --> 1:26:19.500
 Yes, absolutely.

1:26:19.500 --> 1:26:22.140
 But the constraints, as you're saying, are very limited.

1:26:22.140 --> 1:26:23.860
 I think what's limiting.

1:26:23.860 --> 1:26:26.980
 So we evolved our cognition and our body

1:26:26.980 --> 1:26:29.420
 evolved in very specific environments.

1:26:29.420 --> 1:26:32.740
 Because our environment was so variable, fast changing,

1:26:32.740 --> 1:26:35.740
 and so unpredictable, part of the constraints

1:26:35.740 --> 1:26:39.540
 that drove our evolution is generality itself.

1:26:39.540 --> 1:26:42.780
 So we were, in a way, evolved to be able to improvise

1:26:42.780 --> 1:26:47.540
 in all kinds of physical or cognitive environments.

1:26:47.540 --> 1:26:49.900
 And for this reason, it turns out

1:26:49.900 --> 1:26:55.060
 that the minds and bodies that we ended up with

1:26:55.060 --> 1:26:58.020
 can be applied to much, much broader scope

1:26:58.020 --> 1:27:00.060
 than what they were evolved for.

1:27:00.060 --> 1:27:01.740
 And that's truly remarkable.

1:27:01.740 --> 1:27:03.940
 And that's a degree of generalization

1:27:03.940 --> 1:27:07.540
 that is far beyond anything you can see in artificial systems

1:27:07.540 --> 1:27:10.300
 today.

1:27:10.300 --> 1:27:14.500
 That said, it does not mean that human intelligence

1:27:14.500 --> 1:27:16.380
 is anywhere universal.

1:27:16.380 --> 1:27:18.900
 Yeah, it's not general.

1:27:18.900 --> 1:27:21.140
 It's a kind of exciting topic for people,

1:27:21.140 --> 1:27:24.060
 even outside of artificial intelligence, is IQ tests.

1:27:27.580 --> 1:27:29.220
 I think it's Mensa, whatever.

1:27:29.220 --> 1:27:32.420
 There's different degrees of difficulty for questions.

1:27:32.420 --> 1:27:34.700
 We talked about this offline a little bit, too,

1:27:34.700 --> 1:27:37.500
 about difficult questions.

1:27:37.500 --> 1:27:42.300
 What makes a question on an IQ test more difficult or less

1:27:42.300 --> 1:27:43.700
 difficult, do you think?

1:27:43.700 --> 1:27:46.500
 So the thing to keep in mind is that there's

1:27:46.500 --> 1:27:51.540
 no such thing as a question that's intrinsically difficult.

1:27:51.540 --> 1:27:54.580
 It has to be difficult to suspect to the things you

1:27:54.580 --> 1:27:58.540
 already know and the things you can already do, right?

1:27:58.540 --> 1:28:02.740
 So in terms of an IQ test question,

1:28:02.740 --> 1:28:05.980
 typically it would be structured, for instance,

1:28:05.980 --> 1:28:11.860
 as a set of demonstration input and output pairs, right?

1:28:11.860 --> 1:28:15.420
 And then you would be given a test input, a prompt,

1:28:15.420 --> 1:28:18.700
 and you would need to recognize or produce

1:28:18.700 --> 1:28:20.300
 the corresponding output.

1:28:20.300 --> 1:28:26.060
 And in that narrow context, you could say a difficult question

1:28:26.060 --> 1:28:31.580
 is a question where the input prompt is

1:28:31.580 --> 1:28:36.540
 very surprising and unexpected, given the training examples.

1:28:36.540 --> 1:28:38.340
 Just even the nature of the patterns

1:28:38.340 --> 1:28:40.180
 that you're observing in the input prompt.

1:28:40.180 --> 1:28:43.260
 For instance, let's say you have a rotation problem.

1:28:43.260 --> 1:28:46.660
 You must relate the shape by 90 degrees.

1:28:46.660 --> 1:28:50.500
 If I give you two examples and then I give you one prompt,

1:28:50.500 --> 1:28:53.020
 which is actually one of the two training examples,

1:28:53.020 --> 1:28:55.700
 then there is zero generalization difficulty

1:28:55.700 --> 1:28:56.380
 for the task.

1:28:56.380 --> 1:28:57.500
 It's actually a trivial task.

1:28:57.500 --> 1:29:00.780
 You just recognize that it's one of the training examples,

1:29:00.780 --> 1:29:02.300
 and you produce the same answer.

1:29:02.300 --> 1:29:05.580
 Now, if it's a more complex shape,

1:29:05.580 --> 1:29:07.700
 there is a little bit more generalization,

1:29:07.700 --> 1:29:09.860
 but it remains that you are still

1:29:09.860 --> 1:29:12.060
 doing the same thing at this time,

1:29:12.060 --> 1:29:15.060
 as you were being demonstrated at training time.

1:29:15.060 --> 1:29:20.300
 A difficult task starts to require some amount of test

1:29:20.300 --> 1:29:25.100
 time adaptation, some amount of improvisation, right?

1:29:25.100 --> 1:29:29.260
 So consider, I don't know, you're

1:29:29.260 --> 1:29:34.020
 teaching a class on quantum physics or something.

1:29:34.020 --> 1:29:40.460
 If you wanted to test the understanding that students

1:29:40.460 --> 1:29:42.220
 have of the material, you would come up

1:29:42.220 --> 1:29:47.740
 with an exam that's very different from anything

1:29:47.740 --> 1:29:51.940
 they've seen on the internet when they were cramming.

1:29:51.940 --> 1:29:54.780
 On the other hand, if you wanted to make it easy,

1:29:54.780 --> 1:29:56.340
 you would just give them something

1:29:56.340 --> 1:30:00.420
 that's very similar to the mock exams

1:30:00.420 --> 1:30:03.060
 that they've taken, something that's

1:30:03.060 --> 1:30:05.220
 just a simple interpolation of questions

1:30:05.220 --> 1:30:07.260
 that they've already seen.

1:30:07.260 --> 1:30:09.220
 And so that would be an easy exam.

1:30:09.220 --> 1:30:11.940
 It's very similar to what you've been trained on.

1:30:11.940 --> 1:30:15.460
 And a difficult exam is one that really probes your understanding

1:30:15.460 --> 1:30:18.980
 because it forces you to improvise.

1:30:18.980 --> 1:30:22.180
 It forces you to do things that are

1:30:22.180 --> 1:30:24.780
 different from what you were exposed to before.

1:30:24.780 --> 1:30:29.100
 So that said, it doesn't mean that the exam that

1:30:29.100 --> 1:30:32.700
 requires improvisation is intrinsically hard, right?

1:30:32.700 --> 1:30:35.820
 Because maybe you're a quantum physics expert.

1:30:35.820 --> 1:30:37.780
 So when you take the exam, this is actually

1:30:37.780 --> 1:30:40.300
 stuff that, despite being new to the students,

1:30:40.300 --> 1:30:42.900
 it's not new to you, right?

1:30:42.900 --> 1:30:46.020
 So it can only be difficult with respect

1:30:46.020 --> 1:30:49.380
 to what the test taker already knows

1:30:49.380 --> 1:30:51.780
 and with respect to the information

1:30:51.780 --> 1:30:54.700
 that the test taker has about the task.

1:30:54.700 --> 1:30:57.860
 So that's what I mean by controlling for priors

1:30:57.860 --> 1:30:59.900
 what the information you bring to the table.

1:30:59.900 --> 1:31:00.660
 And the experience.

1:31:00.660 --> 1:31:02.660
 And the experience, which is to train data.

1:31:02.660 --> 1:31:05.580
 So in the case of the quantum physics exam,

1:31:05.580 --> 1:31:09.740
 that would be all the course material itself

1:31:09.740 --> 1:31:11.500
 and all the mock exams that students

1:31:11.500 --> 1:31:12.820
 might have taken online.

1:31:12.820 --> 1:31:17.700
 Yeah, it's interesting because I've also sent you an email.

1:31:17.700 --> 1:31:21.820
 I asked you, I've been in just this curious question

1:31:21.820 --> 1:31:26.820
 of what's a really hard IQ test question.

1:31:27.300 --> 1:31:30.580
 And I've been talking to also people

1:31:30.580 --> 1:31:32.540
 who have designed IQ tests.

1:31:32.540 --> 1:31:34.420
 There's a few folks on the internet, it's like a thing.

1:31:34.420 --> 1:31:36.180
 People are really curious about it.

1:31:36.180 --> 1:31:39.460
 First of all, most of the IQ tests they designed,

1:31:39.460 --> 1:31:44.460
 they like religiously protect against the correct answers.

1:31:45.620 --> 1:31:48.380
 Like you can't find the correct answers anywhere.

1:31:48.380 --> 1:31:50.620
 In fact, the question is ruined once you know,

1:31:50.620 --> 1:31:53.700
 even like the approach you're supposed to take.

1:31:53.700 --> 1:31:54.540
 So they're very...

1:31:54.540 --> 1:31:58.420
 That said, the approach is implicit in the training examples.

1:31:58.420 --> 1:32:00.860
 So if you release the training examples, it's over.

1:32:02.740 --> 1:32:04.980
 Which is why in Arc, for instance,

1:32:04.980 --> 1:32:07.940
 there is a test set that is private and no one has seen it.

1:32:09.140 --> 1:32:13.580
 No, for really tough IQ questions, it's not obvious.

1:32:13.580 --> 1:32:17.100
 It's not because the ambiguity.

1:32:17.100 --> 1:32:20.780
 Like it's, I mean, we'll have to look through them,

1:32:20.780 --> 1:32:22.860
 but like some number sequences and so on,

1:32:22.860 --> 1:32:25.060
 it's not completely clear.

1:32:25.060 --> 1:32:29.380
 So like you can get a sense, but there's like some,

1:32:30.540 --> 1:32:33.540
 you know, when you look at a number sequence, I don't know,

1:32:36.140 --> 1:32:37.620
 like your Fibonacci number sequence,

1:32:37.620 --> 1:32:39.580
 if you look at the first few numbers,

1:32:39.580 --> 1:32:42.980
 that sequence could be completed in a lot of different ways.

1:32:42.980 --> 1:32:45.620
 And you know, some are, if you think deeply,

1:32:45.620 --> 1:32:46.900
 are more correct than others.

1:32:46.900 --> 1:32:51.300
 Like there's a kind of intuitive simplicity

1:32:51.300 --> 1:32:53.020
 and elegance to the correct solution.

1:32:53.020 --> 1:32:53.860
 Yes.

1:32:53.860 --> 1:32:56.420
 I am personally not a fan of ambiguity

1:32:56.420 --> 1:32:58.660
 in test questions actually,

1:32:58.660 --> 1:33:01.140
 but I think you can have difficulty

1:33:01.140 --> 1:33:05.620
 without requiring ambiguity simply by making the test

1:33:05.620 --> 1:33:09.500
 require a lot of extrapolation over the training examples.

1:33:09.500 --> 1:33:13.340
 But the beautiful question is difficult,

1:33:13.340 --> 1:33:14.500
 but gives away everything

1:33:14.500 --> 1:33:17.180
 when you give the training example.

1:33:17.180 --> 1:33:18.460
 Basically, yes.

1:33:18.460 --> 1:33:23.460
 Meaning that, so the tests I'm interested in creating

1:33:24.020 --> 1:33:27.740
 are not necessarily difficult for humans

1:33:27.740 --> 1:33:31.580
 because human intelligence is the benchmark.

1:33:31.580 --> 1:33:34.380
 They're supposed to be difficult for machines

1:33:34.380 --> 1:33:36.300
 in ways that are easy for humans.

1:33:36.300 --> 1:33:40.820
 Like I think an ideal test of human and machine intelligence

1:33:40.820 --> 1:33:44.380
 is a test that is actionable,

1:33:44.380 --> 1:33:48.260
 that highlights the need for progress,

1:33:48.260 --> 1:33:50.060
 and that highlights the direction

1:33:50.060 --> 1:33:51.500
 in which you should be making progress.

1:33:51.500 --> 1:33:54.340
 I think we'll talk about the ARC challenge

1:33:54.340 --> 1:33:55.580
 and the test you've constructed

1:33:55.580 --> 1:33:58.100
 and you have these elegant examples.

1:33:58.100 --> 1:33:59.180
 I think that highlight,

1:33:59.180 --> 1:34:01.820
 like this is really easy for us humans,

1:34:01.820 --> 1:34:04.580
 but it's really hard for machines.

1:34:04.580 --> 1:34:09.220
 But on the, you know, the designing an IQ test

1:34:09.220 --> 1:34:13.380
 for IQs of like higher than 160 and so on,

1:34:13.380 --> 1:34:15.220
 you have to say, you have to take that

1:34:15.220 --> 1:34:16.500
 and put it on steroids, right?

1:34:16.500 --> 1:34:19.540
 You have to think like, what is hard for humans?

1:34:19.540 --> 1:34:23.900
 And that's a fascinating exercise in itself, I think.

1:34:25.940 --> 1:34:27.740
 And it was an interesting question

1:34:27.740 --> 1:34:32.300
 of what it takes to create a really hard question for humans

1:34:32.300 --> 1:34:36.340
 because you again have to do the same process

1:34:36.340 --> 1:34:39.900
 as you mentioned, which is, you know,

1:34:39.900 --> 1:34:44.900
 something basically where the experience

1:34:45.100 --> 1:34:46.900
 that you have likely to have encountered

1:34:46.900 --> 1:34:48.740
 throughout your whole life,

1:34:48.740 --> 1:34:51.780
 even if you've prepared for IQ tests,

1:34:51.780 --> 1:34:53.380
 which is a big challenge,

1:34:53.380 --> 1:34:55.820
 that this will still be novel for you.

1:34:55.820 --> 1:34:57.900
 Yeah, I mean, novelty is a requirement.

1:34:58.900 --> 1:35:02.100
 You should not be able to practice for the questions

1:35:02.100 --> 1:35:03.780
 that you're gonna be tested on.

1:35:03.780 --> 1:35:06.700
 That's important because otherwise what you're doing

1:35:06.700 --> 1:35:08.180
 is not exhibiting intelligence.

1:35:08.180 --> 1:35:10.900
 What you're doing is just retrieving

1:35:10.900 --> 1:35:12.380
 what you've been exposed before.

1:35:12.380 --> 1:35:14.500
 It's the same thing as deep learning model.

1:35:14.500 --> 1:35:15.900
 If you train a deep learning model

1:35:15.900 --> 1:35:20.100
 on all the possible answers, then it will ace your test

1:35:20.100 --> 1:35:22.860
 in the same way that, you know,

1:35:24.420 --> 1:35:28.100
 a stupid student can still ace the test

1:35:28.100 --> 1:35:30.140
 if they cram for it.

1:35:30.140 --> 1:35:32.500
 They memorize, you know,

1:35:32.500 --> 1:35:34.980
 a hundred different possible mock exams.

1:35:34.980 --> 1:35:37.180
 And then they hope that the actual exam

1:35:37.180 --> 1:35:41.180
 will be a very simple interpolation of the mock exams.

1:35:41.180 --> 1:35:43.180
 And that student could just be a deep learning model

1:35:43.180 --> 1:35:44.020
 at that point.

1:35:44.020 --> 1:35:45.900
 But you can actually do that

1:35:45.900 --> 1:35:48.180
 without any understanding of the material.

1:35:48.180 --> 1:35:50.540
 And in fact, many students pass their exams

1:35:50.540 --> 1:35:51.940
 in exactly this way.

1:35:51.940 --> 1:35:53.140
 And if you want to avoid that,

1:35:53.140 --> 1:35:56.660
 you need an exam that's unlike anything they've seen

1:35:56.660 --> 1:36:00.020
 that really probes their understanding.

1:36:00.020 --> 1:36:05.020
 So how do we design an IQ test for machines,

1:36:05.020 --> 1:36:07.860
 an intelligent test for machines?

1:36:07.860 --> 1:36:10.300
 All right, so in the paper I outline

1:36:10.300 --> 1:36:14.780
 a number of requirements that you expect of such a test.

1:36:14.780 --> 1:36:19.620
 And in particular, we should start by acknowledging

1:36:19.620 --> 1:36:23.300
 the priors that we expect to be required

1:36:23.300 --> 1:36:25.260
 in order to perform the test.

1:36:25.260 --> 1:36:28.100
 So we should be explicit about the priors, right?

1:36:28.100 --> 1:36:31.780
 And if the goal is to compare machine intelligence

1:36:31.780 --> 1:36:32.740
 and human intelligence,

1:36:32.740 --> 1:36:36.980
 then we should assume human cognitive priors, right?

1:36:36.980 --> 1:36:41.980
 And secondly, we should make sure that we are testing

1:36:42.020 --> 1:36:44.820
 for skill acquisition ability,

1:36:44.820 --> 1:36:46.740
 skill acquisition efficiency in particular,

1:36:46.740 --> 1:36:48.580
 and not for skill itself.

1:36:48.580 --> 1:36:51.860
 Meaning that every task featured in your test

1:36:51.860 --> 1:36:54.420
 should be novel and should not be something

1:36:54.420 --> 1:36:55.980
 that you can anticipate.

1:36:55.980 --> 1:36:57.980
 So for instance, it should not be possible

1:36:57.980 --> 1:37:02.860
 to brute force the space of possible questions, right?

1:37:02.860 --> 1:37:05.900
 To pre generate every possible question and answer.

1:37:06.940 --> 1:37:10.660
 So it should be tasks that cannot be anticipated,

1:37:10.660 --> 1:37:12.460
 not just by the system itself,

1:37:12.460 --> 1:37:15.940
 but by the creators of the system, right?

1:37:15.940 --> 1:37:17.660
 Yeah, you know what's fascinating?

1:37:17.660 --> 1:37:20.820
 I mean, one of my favorite aspects of the paper

1:37:20.820 --> 1:37:22.860
 and the work you do with the ARC challenge

1:37:22.860 --> 1:37:27.860
 is the process of making priors explicit.

1:37:28.940 --> 1:37:33.420
 Just even that act alone is a really powerful one

1:37:33.420 --> 1:37:38.420
 of like, what are, it's a really powerful question

1:37:39.260 --> 1:37:40.500
 asked of us humans.

1:37:40.500 --> 1:37:42.860
 What are the priors that we bring to the table?

1:37:44.180 --> 1:37:46.900
 So the next step is like, once you have those priors,

1:37:46.900 --> 1:37:50.060
 how do you use them to solve a novel task?

1:37:50.060 --> 1:37:52.940
 But like, just even making the priors explicit

1:37:52.940 --> 1:37:56.140
 is a really difficult and really powerful step.

1:37:56.140 --> 1:37:58.940
 And that's like visually beautiful

1:37:58.940 --> 1:38:01.340
 and conceptually philosophically beautiful part

1:38:01.340 --> 1:38:06.020
 of the work you did with, and I guess continue to do

1:38:06.020 --> 1:38:08.460
 probably with the paper and the ARC challenge.

1:38:08.460 --> 1:38:10.740
 Can you talk about some of the priors

1:38:10.740 --> 1:38:12.380
 that we're talking about here?

1:38:12.380 --> 1:38:15.380
 Yes, so a researcher has done a lot of work

1:38:15.380 --> 1:38:19.460
 on what exactly are the knowledge priors

1:38:19.460 --> 1:38:24.460
 that are innate to humans is Elizabeth Spelke from Harvard.

1:38:26.500 --> 1:38:30.580
 So she developed the core knowledge theory,

1:38:30.580 --> 1:38:35.580
 which outlines four different core knowledge systems.

1:38:36.500 --> 1:38:39.180
 So systems of knowledge that we are basically

1:38:39.180 --> 1:38:43.660
 either born with or that we are hardwired

1:38:43.660 --> 1:38:47.180
 to acquire very early on in our development.

1:38:47.180 --> 1:38:52.060
 And there's no strong distinction between the two.

1:38:52.060 --> 1:38:57.060
 Like if you are primed to acquire

1:38:57.060 --> 1:39:01.220
 a certain type of knowledge in just a few weeks,

1:39:01.220 --> 1:39:03.500
 you might as well just be born with it.

1:39:03.500 --> 1:39:06.460
 It's just part of who you are.

1:39:06.460 --> 1:39:09.500
 And so there are four different core knowledge systems.

1:39:09.500 --> 1:39:13.460
 Like the first one is the notion of objectness

1:39:13.460 --> 1:39:16.340
 and basic physics.

1:39:16.340 --> 1:39:20.700
 Like you recognize that something that moves

1:39:20.700 --> 1:39:23.220
 coherently, for instance, is an object.

1:39:23.220 --> 1:39:28.220
 So we intuitively, naturally, innately divide the world

1:39:28.260 --> 1:39:31.260
 into objects based on this notion of coherence,

1:39:31.260 --> 1:39:32.740
 physical coherence.

1:39:32.740 --> 1:39:34.700
 And in terms of elementary physics,

1:39:34.700 --> 1:39:39.700
 there's the fact that objects can bump against each other

1:39:41.620 --> 1:39:44.460
 and the fact that they can occlude each other.

1:39:44.460 --> 1:39:48.300
 So these are things that we are essentially born with

1:39:48.300 --> 1:39:52.500
 or at least that we are going to be acquiring extremely early

1:39:52.500 --> 1:39:55.620
 because we're really hardwired to acquire them.

1:39:55.620 --> 1:39:59.940
 So a bunch of points, pixels that move together

1:39:59.940 --> 1:40:02.820
 on objects are partly the same object.

1:40:02.820 --> 1:40:03.660
 Yes.

1:40:07.660 --> 1:40:11.260
 I don't smoke weed, but if I did,

1:40:11.260 --> 1:40:13.100
 that's something I could sit all night

1:40:13.100 --> 1:40:15.700
 and just think about, remember what I wrote in your paper,

1:40:15.700 --> 1:40:19.700
 just objectness, I wasn't self aware, I guess,

1:40:19.700 --> 1:40:23.180
 of that particular prior.

1:40:23.180 --> 1:40:28.180
 That's such a fascinating prior that like...

1:40:28.500 --> 1:40:30.940
 That's the most basic one, but actually...

1:40:30.940 --> 1:40:34.420
 Objectness, just identity, just objectness.

1:40:34.420 --> 1:40:39.060
 It's very basic, I suppose, but it's so fundamental.

1:40:39.060 --> 1:40:41.380
 It is fundamental to human cognition.

1:40:41.380 --> 1:40:42.220
 Yeah.

1:40:42.220 --> 1:40:46.660
 The second prior that's also fundamental is agentness,

1:40:46.660 --> 1:40:50.740
 which is not a real world, a real world, so agentness.

1:40:50.740 --> 1:40:53.340
 The fact that some of these objects

1:40:53.340 --> 1:40:56.540
 that you segment your environment into,

1:40:56.540 --> 1:40:58.940
 some of these objects are agents.

1:40:58.940 --> 1:41:00.300
 So what's an agent?

1:41:00.300 --> 1:41:04.460
 It's basically, it's an object that has goals.

1:41:05.380 --> 1:41:06.340
 That has what?

1:41:06.340 --> 1:41:09.420
 That has goals, that is capable of pursuing goals.

1:41:09.420 --> 1:41:12.580
 So for instance, if you see two dots

1:41:12.580 --> 1:41:16.300
 moving in roughly synchronized fashion,

1:41:16.300 --> 1:41:19.820
 you will intuitively infer that one of the dots

1:41:19.820 --> 1:41:21.620
 is pursuing the other.

1:41:21.620 --> 1:41:24.980
 So that one of the dots is...

1:41:24.980 --> 1:41:27.380
 And one of the dots is an agent

1:41:27.380 --> 1:41:29.460
 and its goal is to avoid the other dot.

1:41:29.460 --> 1:41:32.740
 And one of the dots, the other dot is also an agent

1:41:32.740 --> 1:41:35.860
 and its goal is to catch the first dot.

1:41:35.860 --> 1:41:40.540
 Belke has shown that babies as young as three months

1:41:40.540 --> 1:41:45.220
 identify agentness and goal directedness

1:41:45.220 --> 1:41:46.420
 in their environment.

1:41:46.420 --> 1:41:51.420
 Another prior is basic geometry and topology,

1:41:52.140 --> 1:41:53.660
 like the notion of distance,

1:41:53.660 --> 1:41:57.620
 the ability to navigate in your environment and so on.

1:41:57.620 --> 1:42:01.380
 This is something that is fundamentally hardwired

1:42:01.380 --> 1:42:02.700
 into our brain.

1:42:02.700 --> 1:42:07.100
 It's in fact backed by very specific neural mechanisms,

1:42:07.100 --> 1:42:10.820
 like for instance, grid cells and place cells.

1:42:10.820 --> 1:42:15.260
 So it's something that's literally hard coded

1:42:15.260 --> 1:42:19.940
 at the neural level in our hippocampus.

1:42:19.940 --> 1:42:23.580
 And the last prior would be the notion of numbers.

1:42:23.580 --> 1:42:26.460
 Like numbers are not actually a cultural construct.

1:42:26.460 --> 1:42:31.460
 We are intuitively, innately able to do some basic counting

1:42:31.460 --> 1:42:34.100
 and to compare quantities.

1:42:34.100 --> 1:42:36.660
 So it doesn't mean we can do arbitrary arithmetic.

1:42:37.660 --> 1:42:39.020
 Counting, the actual counting.

1:42:39.020 --> 1:42:41.500
 Counting, like counting one, two, three ish,

1:42:41.500 --> 1:42:43.700
 then maybe more than three.

1:42:43.700 --> 1:42:45.140
 You can also compare quantities.

1:42:45.140 --> 1:42:48.580
 If I give you three dots and five dots,

1:42:48.580 --> 1:42:52.500
 you can tell the side with five dots has more dots.

1:42:52.500 --> 1:42:56.580
 So this is actually an innate prior.

1:42:56.580 --> 1:43:00.020
 So that said, the list may not be exhaustive.

1:43:00.020 --> 1:43:02.580
 So SpellKey is still, you know,

1:43:02.580 --> 1:43:07.580
 passing the potential existence of new knowledge systems.

1:43:08.500 --> 1:43:12.100
 For instance, knowledge systems that we deal

1:43:12.100 --> 1:43:14.340
 with social relationships.

1:43:15.940 --> 1:43:17.700
 Yeah, I mean, and there could be...

1:43:17.700 --> 1:43:22.060
 Which is much less relevant to something like ARC

1:43:22.060 --> 1:43:22.900
 or IQ test and so on.

1:43:22.900 --> 1:43:23.740
 Right.

1:43:23.740 --> 1:43:26.740
 There could be stuff that's like you said,

1:43:26.740 --> 1:43:29.020
 rotation, symmetry, is there like...

1:43:29.020 --> 1:43:31.060
 Symmetry is really interesting.

1:43:31.060 --> 1:43:34.380
 It's very likely that there is, speaking about rotation,

1:43:34.380 --> 1:43:38.900
 that there is in the brain, a hard coded system

1:43:38.900 --> 1:43:40.940
 that is capable of performing rotations.

1:43:42.060 --> 1:43:45.660
 One famous experiment that people did in the...

1:43:45.660 --> 1:43:48.180
 I don't remember which was exactly,

1:43:48.180 --> 1:43:53.180
 but in the 70s was that people found that

1:43:53.180 --> 1:43:57.580
 if you asked people, if you give them two different shapes

1:43:57.580 --> 1:44:01.420
 and one of the shapes is a rotated version

1:44:01.420 --> 1:44:03.340
 of the first shape, and you ask them,

1:44:03.340 --> 1:44:07.060
 is that shape a rotated version of the first shape or not?

1:44:07.060 --> 1:44:11.140
 What you see is that the time it takes people to answer

1:44:11.140 --> 1:44:16.140
 is linearly proportional, right, to the angle of rotation.

1:44:16.140 --> 1:44:19.660
 So it's almost like you have somewhere in your brain

1:44:19.660 --> 1:44:24.020
 like a turntable with a fixed speed.

1:44:24.020 --> 1:44:28.620
 And if you want to know if two objects are a rotated version

1:44:28.620 --> 1:44:31.700
 of each other, you put the object on the turntable,

1:44:31.700 --> 1:44:34.740
 you let it move around a little bit,

1:44:34.740 --> 1:44:37.580
 and then you stop when you have a match.

1:44:37.580 --> 1:44:40.140
 And that's really interesting.

1:44:40.140 --> 1:44:42.740
 So what's the ARC challenge?

1:44:42.740 --> 1:44:47.380
 So in the paper, I outline all these principles

1:44:47.380 --> 1:44:50.140
 that a good test of machine intelligence

1:44:50.140 --> 1:44:51.940
 and human intelligence should follow.

1:44:51.940 --> 1:44:55.300
 And the ARC challenge is one attempt

1:44:55.300 --> 1:44:58.540
 to embody as many of these principles as possible.

1:44:58.540 --> 1:45:03.780
 So I don't think it's anywhere near a perfect attempt, right?

1:45:03.780 --> 1:45:06.060
 It does not actually follow every principle,

1:45:06.060 --> 1:45:10.700
 but it is what I was able to do given the constraints.

1:45:10.700 --> 1:45:15.540
 So the format of ARC is very similar to classic IQ tests,

1:45:15.540 --> 1:45:18.020
 in particular Raven's Progressive Metrices.

1:45:18.020 --> 1:45:18.980
 Raven's?

1:45:18.980 --> 1:45:20.580
 Yeah, Raven's Progressive Metrices.

1:45:20.580 --> 1:45:22.820
 I mean, if you've done IQ tests in the past,

1:45:22.820 --> 1:45:24.220
 you know what that is, probably.

1:45:24.220 --> 1:45:25.620
 Or at least you've seen it, even if you

1:45:25.620 --> 1:45:26.980
 don't know what it's called.

1:45:26.980 --> 1:45:32.300
 And so you have a set of tasks, that's what they're called.

1:45:32.300 --> 1:45:37.180
 And for each task, you have training data,

1:45:37.180 --> 1:45:40.260
 which is a set of input and output pairs.

1:45:40.260 --> 1:45:45.540
 So an input or output pair is a grid of colors, basically.

1:45:45.540 --> 1:45:48.500
 The grid, the size of the grid is variables.

1:45:48.500 --> 1:45:51.380
 The size of the grid is variable.

1:45:51.380 --> 1:45:56.100
 And you're given an input, and you must transform it

1:45:56.100 --> 1:45:59.020
 into the proper output.

1:45:59.020 --> 1:46:02.060
 And so you're shown a few demonstrations

1:46:02.060 --> 1:46:05.100
 of a task in the form of existing input output pairs,

1:46:05.100 --> 1:46:06.860
 and then you're given a new input.

1:46:06.860 --> 1:46:12.620
 And you must provide, you must produce the correct output.

1:46:12.620 --> 1:46:22.860
 And the assumptions in Arc is that every task should only

1:46:22.860 --> 1:46:27.660
 require core knowledge priors, should not

1:46:27.660 --> 1:46:30.460
 require any outside knowledge.

1:46:30.460 --> 1:46:36.900
 So for instance, no language, no English, nothing like this.

1:46:36.900 --> 1:46:41.540
 No concepts taken from our human experience,

1:46:41.540 --> 1:46:44.340
 like trees, dogs, cats, and so on.

1:46:44.340 --> 1:46:49.700
 So only reasoning tasks that are built on top

1:46:49.700 --> 1:46:52.060
 of core knowledge priors.

1:46:52.060 --> 1:46:56.260
 And some of the tasks are actually explicitly

1:46:56.260 --> 1:47:02.220
 trying to probe specific forms of abstraction.

1:47:02.220 --> 1:47:05.500
 Part of the reason why I wanted to create Arc

1:47:05.500 --> 1:47:11.740
 is I'm a big believer in when you're

1:47:11.740 --> 1:47:18.340
 faced with a problem as murky as understanding

1:47:18.340 --> 1:47:22.380
 how to autonomously generate abstraction in a machine,

1:47:22.380 --> 1:47:27.180
 you have to coevolve the solution and the problem.

1:47:27.180 --> 1:47:29.380
 And so part of the reason why I designed Arc

1:47:29.380 --> 1:47:34.660
 was to clarify my ideas about the nature of abstraction.

1:47:34.660 --> 1:47:36.220
 And some of the tasks are actually

1:47:36.220 --> 1:47:39.900
 designed to probe bits of that theory.

1:47:39.900 --> 1:47:42.340
 And there are things that turn out

1:47:42.340 --> 1:47:46.740
 to be very easy for humans to perform, including young kids,

1:47:46.740 --> 1:47:50.500
 but turn out to be near impossible for machines.

1:47:50.500 --> 1:47:53.780
 So what have you learned from the nature of abstraction

1:47:53.780 --> 1:47:58.380
 from designing that?

1:47:58.380 --> 1:47:59.620
 Can you clarify what you mean?

1:47:59.620 --> 1:48:02.300
 One of the things you wanted to try to understand

1:48:02.300 --> 1:48:06.020
 was this idea of abstraction.

1:48:06.020 --> 1:48:10.380
 Yes, so clarifying my own ideas about abstraction

1:48:10.380 --> 1:48:13.700
 by forcing myself to produce tasks that

1:48:13.700 --> 1:48:17.020
 would require the ability to produce

1:48:17.020 --> 1:48:19.900
 that form of abstraction in order to solve them.

1:48:19.900 --> 1:48:20.860
 Got it.

1:48:20.860 --> 1:48:24.060
 OK, so and by the way, just the people should check out.

1:48:24.060 --> 1:48:26.380
 I'll probably overlay if you're watching the video part.

1:48:26.380 --> 1:48:32.180
 But the grid input output with the different colors

1:48:32.180 --> 1:48:34.340
 on the grid, that's it.

1:48:34.340 --> 1:48:36.300
 I mean, it's a very simple world,

1:48:36.300 --> 1:48:37.460
 but it's kind of beautiful.

1:48:37.460 --> 1:48:39.740
 It's very similar to classic IQ tests.

1:48:39.740 --> 1:48:41.620
 It's not very original in that sense.

1:48:41.620 --> 1:48:43.260
 The main difference with IQ tests

1:48:43.260 --> 1:48:46.860
 is that we make the priors explicit, which is not

1:48:46.860 --> 1:48:48.580
 usually the case in IQ tests.

1:48:48.580 --> 1:48:50.820
 So you make it explicit that everything should only

1:48:50.820 --> 1:48:53.860
 be built on top of core knowledge priors.

1:48:53.860 --> 1:48:58.620
 I also think it's generally more diverse than IQ tests

1:48:58.620 --> 1:49:00.300
 in general.

1:49:00.300 --> 1:49:03.820
 And it perhaps requires a bit more manual work

1:49:03.820 --> 1:49:05.460
 to produce solutions, because you

1:49:05.460 --> 1:49:08.500
 have to click around on a grid for a while.

1:49:08.500 --> 1:49:12.020
 Sometimes the grids can be as large as 30 by 30 cells.

1:49:12.020 --> 1:49:18.020
 So how did you come up, if you can reveal, with the questions?

1:49:18.020 --> 1:49:19.580
 What's the process of the questions?

1:49:19.580 --> 1:49:23.380
 Was it mostly you that came up with the questions?

1:49:23.380 --> 1:49:25.780
 How difficult is it to come up with a question?

1:49:25.780 --> 1:49:30.700
 Is this scalable to a much larger number?

1:49:30.700 --> 1:49:33.740
 If we think, with IQ tests, you might not necessarily

1:49:33.740 --> 1:49:36.460
 want it to or need it to be scalable.

1:49:36.460 --> 1:49:39.580
 With machines, it's possible, you

1:49:39.580 --> 1:49:41.620
 could argue, that it needs to be scalable.

1:49:41.620 --> 1:49:46.500
 So there are 1,000 questions, 1,000 tasks,

1:49:46.500 --> 1:49:49.140
 including the test set, the prior test set.

1:49:49.140 --> 1:49:51.060
 I think it's fairly difficult in the sense

1:49:51.060 --> 1:49:54.500
 that a big requirement is that every task should

1:49:54.500 --> 1:50:00.140
 be novel and unique and unpredictable.

1:50:00.140 --> 1:50:04.460
 You don't want to create your own little world that

1:50:04.460 --> 1:50:08.860
 is simple enough that it would be possible for a human

1:50:08.860 --> 1:50:12.580
 to reverse and generate and write down

1:50:12.580 --> 1:50:15.940
 an algorithm that could generate every possible arc

1:50:15.940 --> 1:50:17.060
 task and their solution.

1:50:17.060 --> 1:50:19.340
 So that would completely invalidate the test.

1:50:19.340 --> 1:50:21.700
 So you're constantly coming up with new stuff.

1:50:21.700 --> 1:50:24.820
 Yeah, you need a source of novelty,

1:50:24.820 --> 1:50:27.860
 of unfakeable novelty.

1:50:27.860 --> 1:50:32.020
 And one thing I found is that, as a human,

1:50:32.020 --> 1:50:36.460
 you are not a very good source of unfakeable novelty.

1:50:36.460 --> 1:50:40.580
 And so you have to base the creation of these tasks

1:50:40.580 --> 1:50:41.100
 quite a bit.

1:50:41.100 --> 1:50:42.980
 There are only so many unique tasks

1:50:42.980 --> 1:50:45.580
 that you can do in a given day.

1:50:45.580 --> 1:50:49.860
 So that means coming up with truly original new ideas.

1:50:49.860 --> 1:50:52.380
 Did psychedelics help you at all?

1:50:52.380 --> 1:50:53.780
 No, I'm just kidding.

1:50:53.780 --> 1:50:55.820
 But I mean, that's fascinating to think about.

1:50:55.820 --> 1:50:58.780
 So you would be walking or something like that.

1:50:58.780 --> 1:51:02.860
 Are you constantly thinking of something totally new?

1:51:02.860 --> 1:51:03.380
 Yes.

1:51:06.020 --> 1:51:06.980
 This is hard.

1:51:06.980 --> 1:51:07.620
 This is hard.

1:51:07.620 --> 1:51:10.980
 Yeah, I mean, I'm not saying you've done anywhere

1:51:10.980 --> 1:51:12.380
 near a perfect job at it.

1:51:12.380 --> 1:51:14.540
 There is some amount of redundancy,

1:51:14.540 --> 1:51:16.740
 and there are many imperfections in ARC.

1:51:16.740 --> 1:51:18.540
 So that said, you should consider

1:51:18.540 --> 1:51:19.820
 ARC as a work in progress.

1:51:19.820 --> 1:51:25.180
 It is not the definitive state.

1:51:25.180 --> 1:51:29.300
 The ARC tasks today are not the definitive state of the test.

1:51:29.300 --> 1:51:32.780
 I want to keep refining it in the future.

1:51:32.780 --> 1:51:36.180
 I also think it should be possible to open up

1:51:36.180 --> 1:51:38.660
 the creation of tasks to a broad audience

1:51:38.660 --> 1:51:40.860
 to do crowdsourcing.

1:51:40.860 --> 1:51:43.180
 That would involve several levels of filtering,

1:51:43.180 --> 1:51:44.140
 obviously.

1:51:44.140 --> 1:51:46.260
 But I think it's possible to apply crowdsourcing

1:51:46.260 --> 1:51:51.140
 to develop a much bigger and much more diverse ARC data set.

1:51:51.140 --> 1:51:54.020
 That would also be free of potentially some

1:51:54.020 --> 1:51:56.700
 of my own personal biases.

1:51:56.700 --> 1:51:59.220
 Is there always need to be a part of ARC

1:51:59.220 --> 1:52:02.900
 that the test is hidden?

1:52:02.900 --> 1:52:04.140
 Yes, absolutely.

1:52:04.140 --> 1:52:08.900
 It is imperative that the tests that you're

1:52:08.900 --> 1:52:11.900
 using to actually benchmark algorithms

1:52:11.900 --> 1:52:15.220
 is not accessible to the people developing these algorithms.

1:52:15.220 --> 1:52:16.860
 Because otherwise, what's going to happen

1:52:16.860 --> 1:52:19.100
 is that the human engineers are just

1:52:19.100 --> 1:52:21.820
 going to solve the tasks themselves

1:52:21.820 --> 1:52:24.820
 and encode their solution in program form.

1:52:24.820 --> 1:52:27.420
 But that, again, what you're seeing here

1:52:27.420 --> 1:52:30.100
 is the process of intelligence happening

1:52:30.100 --> 1:52:31.180
 in the mind of the human.

1:52:31.180 --> 1:52:35.460
 And then you're just capturing its crystallized output.

1:52:35.460 --> 1:52:38.260
 But that crystallized output is not the same thing

1:52:38.260 --> 1:52:40.020
 as the process it generated.

1:52:40.020 --> 1:52:41.340
 It's not intelligent in itself.

1:52:41.340 --> 1:52:43.980
 So what, by the way, the idea of crowdsourcing it

1:52:43.980 --> 1:52:45.860
 is fascinating.

1:52:45.860 --> 1:52:49.860
 I think the creation of questions

1:52:49.860 --> 1:52:51.460
 is really exciting for people.

1:52:51.460 --> 1:52:53.980
 I think there's a lot of really brilliant people

1:52:53.980 --> 1:52:56.220
 out there that love to create these kinds of stuff.

1:52:56.220 --> 1:52:59.060
 Yeah, one thing that kind of surprised me

1:52:59.060 --> 1:53:01.620
 that I wasn't expecting is that lots of people

1:53:01.620 --> 1:53:05.980
 seem to actually enjoy ARC as a kind of game.

1:53:05.980 --> 1:53:08.820
 And I was releasing it as a test,

1:53:08.820 --> 1:53:14.100
 as a benchmark of fluid general intelligence.

1:53:14.100 --> 1:53:17.100
 And lots of people just, including kids,

1:53:17.100 --> 1:53:18.900
 just started enjoying it as a game.

1:53:18.900 --> 1:53:20.980
 So I think that's encouraging.

1:53:20.980 --> 1:53:22.300
 Yeah, I'm fascinated by it.

1:53:22.300 --> 1:53:25.940
 There's a world of people who create IQ questions.

1:53:25.940 --> 1:53:32.660
 I think that's a cool activity for machines and for humans.

1:53:32.660 --> 1:53:35.420
 And humans are themselves fascinated

1:53:35.420 --> 1:53:40.220
 by taking the questions, like measuring

1:53:40.220 --> 1:53:42.300
 their own intelligence.

1:53:42.300 --> 1:53:44.420
 I mean, that's just really compelling.

1:53:44.420 --> 1:53:47.020
 It's really interesting to me, too.

1:53:47.020 --> 1:53:48.740
 One of the cool things about ARC, you said,

1:53:48.740 --> 1:53:51.620
 is kind of inspired by IQ tests or whatever

1:53:51.620 --> 1:53:53.460
 follows a similar process.

1:53:53.460 --> 1:53:56.060
 But because of its nature, because of the context

1:53:56.060 --> 1:53:59.020
 in which it lives, it immediately

1:53:59.020 --> 1:54:01.660
 forces you to think about the nature of intelligence

1:54:01.660 --> 1:54:04.220
 as opposed to just the test of your own.

1:54:04.220 --> 1:54:06.020
 It forces you to really think.

1:54:06.020 --> 1:54:09.900
 I don't know if it's within the question,

1:54:09.900 --> 1:54:11.860
 inherent in the question, or just the fact

1:54:11.860 --> 1:54:13.780
 that it lives in the test that's supposed

1:54:13.780 --> 1:54:15.340
 to be a test of machine intelligence.

1:54:15.340 --> 1:54:15.900
 Absolutely.

1:54:15.900 --> 1:54:20.660
 As you solve ARC tasks as a human,

1:54:20.660 --> 1:54:24.700
 you will be forced to basically introspect

1:54:24.700 --> 1:54:27.060
 how you come up with solutions.

1:54:27.060 --> 1:54:32.660
 And that forces you to reflect on the human problem solving

1:54:32.660 --> 1:54:33.820
 process.

1:54:33.820 --> 1:54:38.780
 And the way your own mind generates

1:54:38.780 --> 1:54:44.780
 abstract representations of the problems it's exposed to.

1:54:44.780 --> 1:54:48.860
 I think it's due to the fact that the set of core knowledge

1:54:48.860 --> 1:54:52.460
 priors that ARC is built upon is so small.

1:54:52.460 --> 1:54:58.660
 It's all a recombination of a very, very small set

1:54:58.660 --> 1:55:00.460
 of assumptions.

1:55:00.460 --> 1:55:02.900
 OK, so what's the future of ARC?

1:55:02.900 --> 1:55:05.420
 So you held ARC as a challenge, as part

1:55:05.420 --> 1:55:06.700
 of like a Kaggle competition.

1:55:06.700 --> 1:55:07.180
 Yes.

1:55:07.180 --> 1:55:08.420
 Kaggle competition.

1:55:08.420 --> 1:55:11.860
 And what do you think?

1:55:11.860 --> 1:55:13.060
 Do you think that's something that

1:55:13.060 --> 1:55:16.060
 continues for five years, 10 years,

1:55:16.060 --> 1:55:17.820
 like just continues growing?

1:55:17.820 --> 1:55:18.940
 Yes, absolutely.

1:55:18.940 --> 1:55:21.340
 So ARC itself will keep evolving.

1:55:21.340 --> 1:55:22.780
 So I've talked about crowdsourcing.

1:55:22.780 --> 1:55:26.180
 I think that's a good avenue.

1:55:26.180 --> 1:55:29.340
 Another thing I'm starting is I'll

1:55:29.340 --> 1:55:32.700
 be collaborating with folks from the psychology department

1:55:32.700 --> 1:55:36.660
 at NYU to do human testing on ARC.

1:55:36.660 --> 1:55:38.940
 And I think there are lots of interesting questions

1:55:38.940 --> 1:55:43.940
 you can start asking, especially as you start correlating

1:55:43.940 --> 1:55:49.420
 machine solutions to ARC tasks and the human characteristics

1:55:49.420 --> 1:55:50.060
 of solutions.

1:55:50.060 --> 1:55:52.020
 Like for instance, you can try to see

1:55:52.020 --> 1:55:55.660
 if there's a relationship between the human perceived

1:55:55.660 --> 1:55:59.420
 difficulty of a task and the machine perceived.

1:55:59.420 --> 1:56:01.940
 Yes, and exactly some measure of machine

1:56:01.940 --> 1:56:02.780
 perceived difficulty.

1:56:02.780 --> 1:56:04.900
 Yeah, it's a nice playground in which

1:56:04.900 --> 1:56:06.340
 to explore this very difference.

1:56:06.340 --> 1:56:09.260
 It's the same thing as we talked about the autonomous vehicles.

1:56:09.260 --> 1:56:10.900
 The things that could be difficult for humans

1:56:10.900 --> 1:56:13.100
 might be very different than the things that are difficult.

1:56:13.100 --> 1:56:17.300
 And formalizing or making explicit that difference

1:56:17.300 --> 1:56:21.020
 in difficulty may teach us something fundamental

1:56:21.020 --> 1:56:22.340
 about intelligence.

1:56:22.340 --> 1:56:26.420
 So one thing I think we did well with ARC

1:56:26.420 --> 1:56:33.060
 is that it's proving to be a very actionable test in the sense

1:56:33.060 --> 1:56:37.700
 that machine performance on ARC started at very much zero

1:56:37.700 --> 1:56:43.340
 initially, while humans found actually the task very easy.

1:56:43.340 --> 1:56:48.180
 And that alone was like a big red flashing light saying

1:56:48.180 --> 1:56:52.380
 that something is going on and that we are missing something.

1:56:52.380 --> 1:56:55.420
 And at the same time, machine performance

1:56:55.420 --> 1:56:57.660
 did not stay at zero for very long.

1:56:57.660 --> 1:57:00.260
 Actually, within two weeks of the Kaggle competition,

1:57:00.260 --> 1:57:03.220
 we started having a nonzero number.

1:57:03.220 --> 1:57:06.460
 And now the state of the art is around 20%

1:57:06.460 --> 1:57:10.260
 of the test set solved.

1:57:10.260 --> 1:57:12.500
 And so ARC is actually a challenge

1:57:12.500 --> 1:57:16.860
 where our capabilities start at zero, which indicates

1:57:16.860 --> 1:57:18.180
 the need for progress.

1:57:18.180 --> 1:57:20.580
 But it's also not an impossible challenge.

1:57:20.580 --> 1:57:21.500
 It's not accessible.

1:57:21.500 --> 1:57:25.260
 You can start making progress basically right away.

1:57:25.260 --> 1:57:28.380
 At the same time, we are still very far

1:57:28.380 --> 1:57:29.420
 from having solved it.

1:57:29.420 --> 1:57:32.820
 And that's actually a very positive outcome

1:57:32.820 --> 1:57:35.900
 of the competition is that the competition has proven

1:57:35.900 --> 1:57:41.740
 that there was no obvious shortcut to solve these tasks.

1:57:41.740 --> 1:57:43.180
 Yeah, so the test held up.

1:57:43.180 --> 1:57:44.340
 Yeah, exactly.

1:57:44.340 --> 1:57:46.900
 That was the primary reason to use the Kaggle competition

1:57:46.900 --> 1:57:51.540
 is to check if some clever person was

1:57:51.540 --> 1:57:56.380
 going to hack the benchmark that did not happen.

1:57:56.380 --> 1:58:01.060
 People who are solving the task are essentially doing it.

1:58:01.060 --> 1:58:05.580
 Well, in a way, they're actually exploring some flaws of ARC

1:58:05.580 --> 1:58:07.380
 that we will need to address in the future,

1:58:07.380 --> 1:58:09.900
 especially they're essentially anticipating

1:58:09.900 --> 1:58:13.780
 what sort of tasks may be contained in the test set.

1:58:13.780 --> 1:58:18.460
 Right, which is kind of, yeah, that's the kind of hacking.

1:58:18.460 --> 1:58:20.180
 It's human hacking of the test.

1:58:20.180 --> 1:58:23.380
 Yes, that said, with the state of the art,

1:58:23.380 --> 1:58:28.220
 it's like 20% we're still very, very far from human level,

1:58:28.220 --> 1:58:30.940
 which is closer to 100%.

1:58:30.940 --> 1:58:35.540
 And I do believe that it will take a while

1:58:35.540 --> 1:58:40.500
 until we reach human parity on ARC.

1:58:40.500 --> 1:58:43.540
 And that by the time we have human parity,

1:58:43.540 --> 1:58:47.020
 we will have AI systems that are probably

1:58:47.020 --> 1:58:50.740
 pretty close to human level in terms of general fluid

1:58:50.740 --> 1:58:53.260
 intelligence, which is, I mean, they are not

1:58:53.260 --> 1:58:54.940
 going to be necessarily human like.

1:58:54.940 --> 1:58:58.780
 They're not necessarily, you would not necessarily

1:58:58.780 --> 1:59:01.860
 recognize them as being an AGI.

1:59:01.860 --> 1:59:06.860
 But they would be capable of a degree of generalization

1:59:06.860 --> 1:59:09.820
 that matches the generalization performed

1:59:09.820 --> 1:59:11.300
 by human fluid intelligence.

1:59:11.300 --> 1:59:11.860
 Sure.

1:59:11.860 --> 1:59:13.380
 I mean, this is a good point in terms

1:59:13.380 --> 1:59:17.700
 of general fluid intelligence to mention in your paper.

1:59:17.700 --> 1:59:21.060
 You describe different kinds of generalizations,

1:59:21.060 --> 1:59:23.460
 local, broad, extreme.

1:59:23.460 --> 1:59:25.660
 And there's a kind of a hierarchy that you form.

1:59:25.660 --> 1:59:31.820
 So when we say generalizations, what are we talking about?

1:59:31.820 --> 1:59:33.180
 What kinds are there?

1:59:33.180 --> 1:59:37.020
 Right, so generalization is a very old idea.

1:59:37.020 --> 1:59:39.420
 I mean, it's even older than machine learning.

1:59:39.420 --> 1:59:40.980
 In the context of machine learning,

1:59:40.980 --> 1:59:47.140
 you say a system generalizes if it can make sense of an input

1:59:47.140 --> 1:59:49.580
 it has not yet seen.

1:59:49.580 --> 1:59:54.940
 And that's what I would call system centric generalization,

1:59:54.940 --> 2:00:00.380
 generalization with respect to novelty

2:00:00.380 --> 2:00:02.980
 for the specific system you're considering.

2:00:02.980 --> 2:00:05.060
 So I think a good test of intelligence

2:00:05.060 --> 2:00:09.900
 should actually deal with developer aware generalization,

2:00:09.900 --> 2:00:13.500
 which is slightly stronger than system centric generalization.

2:00:13.500 --> 2:00:16.020
 So developer aware generalization

2:00:16.020 --> 2:00:19.860
 would be the ability to generalize

2:00:19.860 --> 2:00:24.220
 to novelty or uncertainty that not only the system itself has

2:00:24.220 --> 2:00:26.660
 not access to, but the developer of the system

2:00:26.660 --> 2:00:29.380
 could not have access to either.

2:00:29.380 --> 2:00:32.380
 That's a fascinating meta definition.

2:00:32.380 --> 2:00:37.700
 So the system is basically the edge case thing

2:00:37.700 --> 2:00:39.780
 we're talking about with autonomous vehicles.

2:00:39.780 --> 2:00:41.620
 Neither the developer nor the system

2:00:41.620 --> 2:00:44.420
 know about the edge cases in my encounter.

2:00:44.420 --> 2:00:47.020
 So it's up to the system should be

2:00:47.020 --> 2:00:51.660
 able to generalize the thing that nobody expected,

2:00:51.660 --> 2:00:54.860
 neither the designer of the training data,

2:00:54.860 --> 2:00:59.060
 nor obviously the contents of the training data.

2:00:59.060 --> 2:01:00.580
 That's a fascinating definition.

2:01:00.580 --> 2:01:04.540
 So you can see degrees of generalization as a spectrum.

2:01:04.540 --> 2:01:08.060
 And the lowest level is what machine learning

2:01:08.060 --> 2:01:10.780
 is trying to do is the assumption

2:01:10.780 --> 2:01:15.220
 that any new situation is going to be sampled

2:01:15.220 --> 2:01:18.340
 from a static distribution of possible situations

2:01:18.340 --> 2:01:21.500
 and that you already have a representative sample

2:01:21.500 --> 2:01:22.420
 of the distribution.

2:01:22.420 --> 2:01:23.860
 That's your training data.

2:01:23.860 --> 2:01:26.700
 And so in machine learning, you generalize to a new sample

2:01:26.700 --> 2:01:28.780
 from a known distribution.

2:01:28.780 --> 2:01:34.020
 And the ways in which your new sample will be new or different

2:01:34.020 --> 2:01:38.140
 are ways that are already understood by the developers

2:01:38.140 --> 2:01:39.420
 of the system.

2:01:39.420 --> 2:01:43.020
 So you are generalizing to known unknowns

2:01:43.020 --> 2:01:45.100
 for one specific task.

2:01:45.100 --> 2:01:47.500
 That's what you would call robustness.

2:01:47.500 --> 2:01:50.180
 You are robust to things like noise, small variations,

2:01:50.180 --> 2:01:56.620
 and so on for one fixed known distribution

2:01:56.620 --> 2:01:59.300
 that you know through your training data.

2:01:59.300 --> 2:02:05.060
 And the higher degree would be flexibility

2:02:05.060 --> 2:02:06.380
 in machine intelligence.

2:02:06.380 --> 2:02:08.620
 So flexibility would be something

2:02:08.620 --> 2:02:12.500
 like an L5 cell driving car or maybe a robot that

2:02:12.500 --> 2:02:16.820
 can pass the coffee cup test, which

2:02:16.820 --> 2:02:21.460
 is the notion that you'd be given a random kitchen

2:02:21.460 --> 2:02:22.460
 somewhere in the country.

2:02:22.460 --> 2:02:28.460
 And you would have to go make a cup of coffee in that kitchen.

2:02:28.460 --> 2:02:30.820
 So flexibility would be the ability

2:02:30.820 --> 2:02:35.300
 to deal with unknown unknowns, so things that could not,

2:02:35.300 --> 2:02:37.180
 dimensions of viability that could not

2:02:37.180 --> 2:02:41.100
 have been possibly foreseen by the creators of the system

2:02:41.100 --> 2:02:42.860
 within one specific task.

2:02:42.860 --> 2:02:47.020
 So generalizing to the long tail of situations in self driving,

2:02:47.020 --> 2:02:48.540
 for instance, would be flexibility.

2:02:48.540 --> 2:02:51.700
 So you have robustness, flexibility, and finally,

2:02:51.700 --> 2:02:53.700
 you would have extreme generalization,

2:02:53.700 --> 2:02:57.740
 which is basically flexibility, but instead

2:02:57.740 --> 2:03:01.180
 of just considering one specific domain,

2:03:01.180 --> 2:03:03.340
 like driving or domestic robotics,

2:03:03.340 --> 2:03:07.740
 you're considering an open ended range of possible domains.

2:03:07.740 --> 2:03:12.620
 So a robot would be capable of extreme generalization

2:03:12.620 --> 2:03:18.060
 if, let's say, it's designed and trained for cooking,

2:03:18.060 --> 2:03:19.820
 for instance.

2:03:19.820 --> 2:03:24.580
 And if I buy the robot and if it's

2:03:24.580 --> 2:03:28.780
 able to teach itself gardening in a couple of weeks,

2:03:28.780 --> 2:03:32.300
 it would be capable of extreme generalization, for instance.

2:03:32.300 --> 2:03:34.300
 So the ultimate goal is extreme generalization.

2:03:34.300 --> 2:03:34.820
 Yes.

2:03:34.820 --> 2:03:40.020
 So creating a system that is so general that it could

2:03:40.020 --> 2:03:46.140
 essentially achieve human skill parity over arbitrary tasks

2:03:46.140 --> 2:03:50.820
 and arbitrary domains with the same level of improvisation

2:03:50.820 --> 2:03:53.740
 and adaptation power as humans when

2:03:53.740 --> 2:03:55.380
 it encounters new situations.

2:03:55.380 --> 2:03:59.780
 And it would do so over basically the same range

2:03:59.780 --> 2:04:02.780
 of possible domains and tasks as humans

2:04:02.780 --> 2:04:05.500
 and using essentially the same amount of training

2:04:05.500 --> 2:04:07.860
 experience of practice as humans would require.

2:04:07.860 --> 2:04:10.900
 That would be human level extreme generalization.

2:04:10.900 --> 2:04:14.620
 So I don't actually think humans are anywhere

2:04:14.620 --> 2:04:19.580
 near the optimal intelligence bounds

2:04:19.580 --> 2:04:21.300
 if there is such a thing.

2:04:21.300 --> 2:04:23.820
 So I think for humans or in general?

2:04:23.820 --> 2:04:25.140
 In general.

2:04:25.140 --> 2:04:26.780
 I think it's quite likely that there

2:04:26.780 --> 2:04:33.860
 is a hard limit to how intelligent any system can be.

2:04:33.860 --> 2:04:35.980
 But at the same time, I don't think humans are anywhere

2:04:35.980 --> 2:04:39.180
 near that limit.

2:04:39.180 --> 2:04:40.780
 Yeah, last time I think we talked,

2:04:40.780 --> 2:04:43.820
 I think you had this idea that we're only

2:04:43.820 --> 2:04:46.580
 as intelligent as the problems we face.

2:04:46.580 --> 2:04:51.300
 Sort of we are bounded by the problems.

2:04:51.300 --> 2:04:51.940
 In a way, yes.

2:04:51.940 --> 2:04:55.100
 We are bounded by our environments,

2:04:55.100 --> 2:04:58.100
 and we are bounded by the problems we try to solve.

2:04:58.100 --> 2:04:59.220
 Yeah.

2:04:59.220 --> 2:04:59.700
 Yeah.

2:04:59.700 --> 2:05:03.820
 What do you make of Neuralink and outsourcing

2:05:03.820 --> 2:05:07.140
 some of the brain power, like brain computer interfaces?

2:05:07.140 --> 2:05:13.460
 Do you think we can expand or augment our intelligence?

2:05:13.460 --> 2:05:18.340
 I am fairly skeptical of neural interfaces

2:05:18.340 --> 2:05:23.780
 because they are trying to fix one specific bottleneck

2:05:23.780 --> 2:05:26.380
 in human machine cognition, which

2:05:26.380 --> 2:05:29.700
 is the bandwidth bottleneck, input and output

2:05:29.700 --> 2:05:31.820
 of information in the brain.

2:05:31.820 --> 2:05:37.820
 And my perception of the problem is that bandwidth is not

2:05:37.820 --> 2:05:41.140
 at this time a bottleneck at all.

2:05:41.140 --> 2:05:43.580
 Meaning that we already have sensors

2:05:43.580 --> 2:05:48.300
 that enable us to take in far more information than what

2:05:48.300 --> 2:05:50.420
 we can actually process.

2:05:50.420 --> 2:05:53.260
 Well, to push back on that a little bit,

2:05:53.260 --> 2:05:55.420
 to sort of play devil's advocate a little bit,

2:05:55.420 --> 2:05:58.980
 is if you look at the internet, Wikipedia, let's say Wikipedia,

2:05:58.980 --> 2:06:03.300
 I would say that humans, after the advent of Wikipedia,

2:06:03.300 --> 2:06:05.860
 are much more intelligent.

2:06:05.860 --> 2:06:07.820
 Yes, I think that's a good one.

2:06:07.820 --> 2:06:14.180
 But that's also not about, that's about externalizing

2:06:14.180 --> 2:06:18.140
 our intelligence via information processing systems,

2:06:18.140 --> 2:06:19.740
 external information processing systems,

2:06:19.740 --> 2:06:23.780
 which is very different from brain computer interfaces.

2:06:23.780 --> 2:06:27.980
 Right, but the question is whether if we have direct

2:06:27.980 --> 2:06:31.940
 access, if our brain has direct access to Wikipedia without

2:06:31.940 --> 2:06:34.540
 Your brain already has direct access to Wikipedia.

2:06:34.540 --> 2:06:35.900
 It's on your phone.

2:06:35.900 --> 2:06:39.380
 And you have your hands and your eyes and your ears

2:06:39.380 --> 2:06:42.140
 and so on to access that information.

2:06:42.140 --> 2:06:44.340
 And the speed at which you can access it

2:06:44.340 --> 2:06:45.700
 Is bottlenecked by the cognition.

2:06:45.700 --> 2:06:49.620
 I think it's already close, fairly close to optimal,

2:06:49.620 --> 2:06:53.340
 which is why speed reading, for instance, does not work.

2:06:53.340 --> 2:06:55.980
 The faster you read, the less you understand.

2:06:55.980 --> 2:06:58.420
 But maybe it's because it uses the eyes.

2:06:58.420 --> 2:07:00.540
 So maybe.

2:07:00.540 --> 2:07:01.460
 So I don't believe so.

2:07:01.460 --> 2:07:04.620
 I think the brain is very slow.

2:07:04.620 --> 2:07:07.860
 It typically operates, you know, the fastest things

2:07:07.860 --> 2:07:11.420
 that happen in the brain are at the level of 50 milliseconds.

2:07:11.420 --> 2:07:14.580
 Forming a conscious thought can potentially

2:07:14.580 --> 2:07:16.740
 take entire seconds, right?

2:07:16.740 --> 2:07:19.220
 And you can already read pretty fast.

2:07:19.220 --> 2:07:23.460
 So I think the speed at which you can take information in

2:07:23.460 --> 2:07:26.460
 and even the speed at which you can output information

2:07:26.460 --> 2:07:29.900
 can only be very incrementally improved.

2:07:29.900 --> 2:07:31.100
 Maybe there's a question.

2:07:31.100 --> 2:07:34.380
 If you're a very fast typer, if you're a very trained typer,

2:07:34.380 --> 2:07:36.660
 the speed at which you can express your thoughts

2:07:36.660 --> 2:07:40.500
 is already the speed at which you can form your thoughts.

2:07:40.500 --> 2:07:44.540
 Right, so that's kind of an idea that there are

2:07:44.540 --> 2:07:47.020
 fundamental bottlenecks to the human mind.

2:07:47.020 --> 2:07:50.260
 But it's possible that everything we have

2:07:50.260 --> 2:07:53.140
 in the human mind is just to be able to survive

2:07:53.140 --> 2:07:54.420
 in the environment.

2:07:54.420 --> 2:07:58.300
 And there's a lot more to expand.

2:07:58.300 --> 2:08:02.420
 Maybe, you know, you said the speed of the thought.

2:08:02.420 --> 2:08:06.780
 So I think augmenting human intelligence

2:08:06.780 --> 2:08:09.900
 is a very valid and very powerful avenue, right?

2:08:09.900 --> 2:08:12.260
 And that's what computers are about.

2:08:12.260 --> 2:08:15.900
 In fact, that's what all of culture and civilization

2:08:15.900 --> 2:08:16.740
 is about.

2:08:16.740 --> 2:08:20.620
 Our culture is externalized cognition

2:08:20.620 --> 2:08:23.740
 and we rely on culture to think constantly.

2:08:23.740 --> 2:08:26.620
 Yeah, I mean, that's another, yeah.

2:08:26.620 --> 2:08:29.140
 Not just computers, not just phones and the internet.

2:08:29.140 --> 2:08:32.460
 I mean, all of culture, like language, for instance,

2:08:32.460 --> 2:08:34.020
 is a form of externalized cognition.

2:08:34.020 --> 2:08:37.460
 Books are obviously externalized cognition.

2:08:37.460 --> 2:08:38.580
 Yeah, that's a good point.

2:08:38.580 --> 2:08:42.060
 And you can scale that externalized cognition

2:08:42.060 --> 2:08:45.180
 far beyond the capability of the human brain.

2:08:45.180 --> 2:08:48.900
 And you could see civilization itself

2:08:48.900 --> 2:08:54.260
 is it has capabilities that are far beyond any individual brain

2:08:54.260 --> 2:08:55.940
 and will keep scaling it because it's not

2:08:55.940 --> 2:08:59.140
 rebound by individual brains.

2:08:59.140 --> 2:09:01.340
 It's a different kind of system.

2:09:01.340 --> 2:09:06.260
 Yeah, and that system includes nonhuman, nonhumans.

2:09:06.260 --> 2:09:08.700
 First of all, it includes all the other biological systems,

2:09:08.700 --> 2:09:11.660
 which are probably contributing to the overall intelligence

2:09:11.660 --> 2:09:12.900
 of the organism.

2:09:12.900 --> 2:09:14.460
 And then computers are part of it.

2:09:14.460 --> 2:09:16.860
 Nonhuman systems are probably not contributing much,

2:09:16.860 --> 2:09:19.700
 but AIs are definitely contributing to that.

2:09:19.700 --> 2:09:24.260
 Like Google search, for instance, is a big part of it.

2:09:24.260 --> 2:09:29.660
 Yeah, yeah, a huge part, a part that we can't probably

2:09:29.660 --> 2:09:31.060
 introspect.

2:09:31.060 --> 2:09:33.780
 Like how the world has changed in the past 20 years,

2:09:33.780 --> 2:09:35.220
 it's probably very difficult for us

2:09:35.220 --> 2:09:38.620
 to be able to understand until, of course,

2:09:38.620 --> 2:09:41.740
 whoever created the simulation we're in is probably

2:09:41.740 --> 2:09:44.940
 doing metrics, measuring the progress.

2:09:44.940 --> 2:09:48.340
 There was probably a big spike in performance.

2:09:48.340 --> 2:09:51.580
 They're enjoying this.

2:09:51.580 --> 2:09:56.020
 So what are your thoughts on the Turing test

2:09:56.020 --> 2:10:00.340
 and the Lobner Prize, which is one

2:10:00.340 --> 2:10:05.700
 of the most famous attempts at the test of artificial

2:10:05.700 --> 2:10:11.740
 intelligence by doing a natural language open dialogue test

2:10:11.740 --> 2:10:18.860
 that's judged by humans as far as how well the machine did?

2:10:18.860 --> 2:10:21.460
 So I'm not a fan of the Turing test.

2:10:21.460 --> 2:10:25.940
 Itself or any of its variants for two reasons.

2:10:25.940 --> 2:10:34.140
 So first of all, it's really coping out

2:10:34.140 --> 2:10:37.660
 of trying to define and measure intelligence

2:10:37.660 --> 2:10:40.620
 because it's entirely outsourcing that

2:10:40.620 --> 2:10:43.380
 to a panel of human judges.

2:10:43.380 --> 2:10:47.420
 And these human judges, they may not themselves

2:10:47.420 --> 2:10:49.700
 have any proper methodology.

2:10:49.700 --> 2:10:52.660
 They may not themselves have any proper definition

2:10:52.660 --> 2:10:53.620
 of intelligence.

2:10:53.620 --> 2:10:54.780
 They may not be reliable.

2:10:54.780 --> 2:10:57.260
 So the Turing test is already failing

2:10:57.260 --> 2:10:59.620
 one of the core psychometrics principles, which

2:10:59.620 --> 2:11:04.620
 is reliability because you have biased human judges.

2:11:04.620 --> 2:11:07.900
 It's also violating the standardization requirement

2:11:07.900 --> 2:11:10.140
 and the freedom from bias requirement.

2:11:10.140 --> 2:11:13.900
 And so it's really a cope out because you are outsourcing

2:11:13.900 --> 2:11:17.380
 everything that matters, which is precisely describing

2:11:17.380 --> 2:11:22.180
 intelligence and finding a standalone test to measure it.

2:11:22.180 --> 2:11:25.260
 You're outsourcing everything to people.

2:11:25.260 --> 2:11:26.340
 So it's really a cope out.

2:11:26.340 --> 2:11:28.860
 And by the way, we should keep in mind

2:11:28.860 --> 2:11:33.940
 that when Turing proposed the imitation game,

2:11:33.940 --> 2:11:36.780
 it was not meaning for the imitation game

2:11:36.780 --> 2:11:40.700
 to be an actual goal for the field of AI

2:11:40.700 --> 2:11:42.460
 and actual test of intelligence.

2:11:42.460 --> 2:11:48.780
 It was using the imitation game as a thought experiment

2:11:48.780 --> 2:11:53.580
 in a philosophical discussion in his 1950 paper.

2:11:53.580 --> 2:11:58.820
 He was trying to argue that theoretically, it

2:11:58.820 --> 2:12:04.220
 should be possible for something very much like the human mind,

2:12:04.220 --> 2:12:06.100
 indistinguishable from the human mind,

2:12:06.100 --> 2:12:08.060
 to be encoded in a Turing machine.

2:12:08.060 --> 2:12:14.540
 And at the time, that was a very daring idea.

2:12:14.540 --> 2:12:16.580
 It was stretching credulity.

2:12:16.580 --> 2:12:20.140
 But nowadays, I think it's fairly well accepted

2:12:20.140 --> 2:12:22.660
 that the mind is an information processing system

2:12:22.660 --> 2:12:25.420
 and that you could probably encode it into a computer.

2:12:25.420 --> 2:12:29.380
 So another reason why I'm not a fan of this type of test

2:12:29.380 --> 2:12:34.220
 is that the incentives that it creates

2:12:34.220 --> 2:12:39.740
 are incentives that are not conducive to proper scientific

2:12:39.740 --> 2:12:40.780
 research.

2:12:40.780 --> 2:12:45.700
 If your goal is to trick, to convince a panel of human

2:12:45.700 --> 2:12:48.460
 judges that they are talking to a human,

2:12:48.460 --> 2:12:53.420
 then you have an incentive to rely on tricks

2:12:53.420 --> 2:12:56.500
 and prestidigitation.

2:12:56.500 --> 2:12:59.180
 In the same way that, let's say, you're doing physics

2:12:59.180 --> 2:13:01.500
 and you want to solve teleportation.

2:13:01.500 --> 2:13:04.660
 And what if the test that you set out to pass

2:13:04.660 --> 2:13:07.460
 is you need to convince a panel of judges

2:13:07.460 --> 2:13:09.500
 that teleportation took place?

2:13:09.500 --> 2:13:12.580
 And they're just sitting there and watching what you're doing.

2:13:12.580 --> 2:13:17.540
 And that is something that you can achieve with David

2:13:17.540 --> 2:13:22.780
 Copperfield could achieve it in his show at Vegas.

2:13:22.780 --> 2:13:25.260
 And what he's doing is very elaborate.

2:13:25.260 --> 2:13:29.180
 But it's not physics.

2:13:29.180 --> 2:13:31.740
 It's not making any progress in our understanding

2:13:31.740 --> 2:13:32.620
 of the universe.

2:13:32.620 --> 2:13:34.780
 To push back on that is possible.

2:13:34.780 --> 2:13:39.020
 That's the hope with these kinds of subjective evaluations

2:13:39.020 --> 2:13:41.940
 is that it's easier to solve it generally

2:13:41.940 --> 2:13:45.420
 than it is to come up with tricks that convince

2:13:45.420 --> 2:13:46.620
 a large number of judges.

2:13:46.620 --> 2:13:47.340
 That's the hope.

2:13:47.340 --> 2:13:49.300
 In practice, it turns out that it's

2:13:49.300 --> 2:13:51.500
 very easy to deceive people in the same way

2:13:51.500 --> 2:13:54.380
 that you can do magic in Vegas.

2:13:54.380 --> 2:13:57.300
 You can actually very easily convince people

2:13:57.300 --> 2:13:59.500
 that they're talking to a human when they're actually

2:13:59.500 --> 2:14:00.740
 talking to an algorithm.

2:14:00.740 --> 2:14:01.740
 I just disagree.

2:14:01.740 --> 2:14:02.660
 I disagree with that.

2:14:02.660 --> 2:14:03.620
 I think it's easy.

2:14:03.620 --> 2:14:05.100
 I would push.

2:14:05.100 --> 2:14:07.340
 No, it's not easy.

2:14:07.340 --> 2:14:08.300
 It's doable.

2:14:08.300 --> 2:14:12.260
 It's very easy because we are biased.

2:14:12.260 --> 2:14:13.860
 We have theory of mind.

2:14:13.860 --> 2:14:21.020
 We are constantly projecting emotions, intentions, agentness.

2:14:21.020 --> 2:14:24.260
 Agentness is one of our core innate priors.

2:14:24.260 --> 2:14:26.820
 We are projecting these things on everything around us.

2:14:26.820 --> 2:14:31.260
 Like if you paint a smiley on a rock,

2:14:31.260 --> 2:14:33.420
 the rock becomes happy in our eyes.

2:14:33.420 --> 2:14:36.540
 And because we have this extreme bias that

2:14:36.540 --> 2:14:39.740
 permits everything we see around us,

2:14:39.740 --> 2:14:41.780
 it's actually pretty easy to trick people.

2:14:41.780 --> 2:14:44.300
 I just disagree with that.

2:14:44.300 --> 2:14:45.820
 I so totally disagree with that.

2:14:45.820 --> 2:14:50.500
 You brilliantly put as a huge, the anthropomorphization

2:14:50.500 --> 2:14:53.140
 that we naturally do, the agentness of that word.

2:14:53.140 --> 2:14:53.980
 Is that a real word?

2:14:53.980 --> 2:14:55.500
 No, it's not a real word.

2:14:55.500 --> 2:14:56.020
 I like it.

2:14:56.020 --> 2:14:57.780
 But it's a useful word.

2:14:57.780 --> 2:14:58.620
 It's a useful word.

2:14:58.620 --> 2:14:59.660
 Let's make it real.

2:14:59.660 --> 2:15:01.020
 It's a huge help.

2:15:01.020 --> 2:15:04.900
 But I still think it's really difficult to convince.

2:15:04.900 --> 2:15:07.940
 If you do like the Alexa Prize formulation,

2:15:07.940 --> 2:15:10.420
 where you talk for an hour, there's

2:15:10.420 --> 2:15:12.460
 formulations of the test you can create,

2:15:12.460 --> 2:15:13.780
 where it's very difficult.

2:15:13.780 --> 2:15:18.100
 So I like the Alexa Prize better because it's more pragmatic.

2:15:18.100 --> 2:15:19.540
 It's more practical.

2:15:19.540 --> 2:15:22.100
 It's actually incentivizing developers

2:15:22.100 --> 2:15:27.860
 to create something that's useful as a human machine

2:15:27.860 --> 2:15:29.300
 interface.

2:15:29.300 --> 2:15:31.780
 So that's slightly better than just the imitation.

2:15:31.780 --> 2:15:34.100
 So I like it.

2:15:34.100 --> 2:15:36.980
 Your idea is like a test which hopefully

2:15:36.980 --> 2:15:39.620
 help us in creating intelligent systems as a result.

2:15:39.620 --> 2:15:41.700
 Like if you create a system that passes it,

2:15:41.700 --> 2:15:44.740
 it'll be useful for creating further intelligent systems.

2:15:44.740 --> 2:15:46.100
 Yes, at least.

2:15:46.100 --> 2:15:47.620
 Yeah.

2:15:47.620 --> 2:15:51.740
 Just to kind of comment, I'm a little bit surprised

2:15:51.740 --> 2:15:55.660
 how little inspiration people draw from the Turing test

2:15:55.660 --> 2:15:57.180
 today.

2:15:57.180 --> 2:15:59.420
 The media and the popular press might write about it

2:15:59.420 --> 2:16:00.900
 every once in a while.

2:16:00.900 --> 2:16:03.500
 The philosophers might talk about it.

2:16:03.500 --> 2:16:07.020
 But most engineers are not really inspired by it.

2:16:07.020 --> 2:16:11.340
 And I know you don't like the Turing test,

2:16:11.340 --> 2:16:15.060
 but we'll have this argument another time.

2:16:15.060 --> 2:16:18.620
 There's something inspiring about it, I think.

2:16:18.620 --> 2:16:21.740
 As a philosophical device in a physical discussion,

2:16:21.740 --> 2:16:23.780
 I think there is something very interesting about it.

2:16:23.780 --> 2:16:26.220
 I don't think it is in practical terms.

2:16:26.220 --> 2:16:29.060
 I don't think it's conducive to progress.

2:16:29.060 --> 2:16:32.540
 And one of the reasons why is that I

2:16:32.540 --> 2:16:35.300
 think being very human like, being

2:16:35.300 --> 2:16:37.540
 indistinguishable from a human is actually

2:16:37.540 --> 2:16:40.460
 the very last step in the creation of machine

2:16:40.460 --> 2:16:41.020
 intelligence.

2:16:41.020 --> 2:16:46.820
 That the first ARs that will show strong generalization

2:16:46.820 --> 2:16:52.500
 that will actually implement human like broad cognitive

2:16:52.500 --> 2:16:54.980
 abilities, they will not actually behave or look

2:16:54.980 --> 2:16:58.500
 anything like humans.

2:16:58.500 --> 2:17:01.700
 Human likeness is the very last step in that process.

2:17:01.700 --> 2:17:03.780
 And so a good test is a test that

2:17:03.780 --> 2:17:07.060
 points you towards the first step on the ladder,

2:17:07.060 --> 2:17:08.900
 not towards the top of the ladder.

2:17:08.900 --> 2:17:11.980
 So to push back on that, I usually

2:17:11.980 --> 2:17:13.460
 agree with you on most things.

2:17:13.460 --> 2:17:15.060
 I remember you, I think at some point,

2:17:15.060 --> 2:17:17.100
 tweeting something about the Turing test

2:17:17.100 --> 2:17:19.020
 not being being counterproductive

2:17:19.020 --> 2:17:20.340
 or something like that.

2:17:20.340 --> 2:17:23.220
 And I think a lot of very smart people agree with that.

2:17:23.220 --> 2:17:31.460
 I, a computation speaking, not very smart person,

2:17:31.460 --> 2:17:32.300
 disagree with that.

2:17:32.300 --> 2:17:33.820
 Because I think there's some magic

2:17:33.820 --> 2:17:36.900
 to the interactivity with other humans.

2:17:36.900 --> 2:17:39.620
 So to play devil's advocate on your statement,

2:17:39.620 --> 2:17:42.780
 it's possible that in order to demonstrate

2:17:42.780 --> 2:17:45.540
 the generalization abilities of a system,

2:17:45.540 --> 2:17:49.940
 you have to show your ability, in conversation,

2:17:49.940 --> 2:17:55.380
 show your ability to adjust, adapt to the conversation

2:17:55.380 --> 2:17:58.380
 through not just like as a standalone system,

2:17:58.380 --> 2:18:01.380
 but through the process of like the interaction,

2:18:01.380 --> 2:18:05.700
 the game theoretic, where you really

2:18:05.700 --> 2:18:09.180
 are changing the environment by your actions.

2:18:09.180 --> 2:18:11.660
 So in the ARC challenge, for example,

2:18:11.660 --> 2:18:12.820
 you're an observer.

2:18:12.820 --> 2:18:17.460
 You can't scare the test into changing.

2:18:17.460 --> 2:18:19.380
 You can't talk to the test.

2:18:19.380 --> 2:18:21.260
 You can't play with it.

2:18:21.260 --> 2:18:24.300
 So there's some aspect of that interactivity

2:18:24.300 --> 2:18:26.140
 that becomes highly subjective, but it

2:18:26.140 --> 2:18:29.620
 feels like it could be conducive to generalizability.

2:18:29.620 --> 2:18:31.060
 I think you make a great point.

2:18:31.060 --> 2:18:33.580
 The interactivity is a very good setting

2:18:33.580 --> 2:18:36.060
 to force a system to show adaptation,

2:18:36.060 --> 2:18:39.300
 to show generalization.

2:18:39.300 --> 2:18:42.620
 That said, at the same time, it's

2:18:42.620 --> 2:18:44.860
 not something very scalable, because you

2:18:44.860 --> 2:18:46.100
 rely on human judges.

2:18:46.100 --> 2:18:48.700
 It's not something reliable, because the human judges may

2:18:48.700 --> 2:18:49.420
 not, may not.

2:18:49.420 --> 2:18:50.940
 So you don't like human judges.

2:18:50.940 --> 2:18:51.860
 Basically, yes.

2:18:51.860 --> 2:18:52.540
 And I think so.

2:18:52.540 --> 2:18:56.140
 I love the idea of interactivity.

2:18:56.140 --> 2:18:59.620
 I initially wanted an ARC test that

2:18:59.620 --> 2:19:02.820
 had some amount of interactivity where your score on a task

2:19:02.820 --> 2:19:05.380
 would not be 1 or 0, if you can solve it or not,

2:19:05.380 --> 2:19:11.580
 but would be the number of attempts

2:19:11.580 --> 2:19:14.740
 that you can make before you hit the right solution, which

2:19:14.740 --> 2:19:16.900
 means that now you can start applying

2:19:16.900 --> 2:19:19.860
 the scientific method as you solve ARC tasks,

2:19:19.860 --> 2:19:23.780
 that you can start formulating hypotheses and probing

2:19:23.780 --> 2:19:27.300
 the system to see whether the observation will

2:19:27.300 --> 2:19:28.660
 match the hypothesis or not.

2:19:28.660 --> 2:19:30.700
 It would be amazing if you could also,

2:19:30.700 --> 2:19:35.500
 even higher level than that, measure the quality of your attempts,

2:19:35.500 --> 2:19:36.780
 which, of course, is impossible.

2:19:36.780 --> 2:19:38.540
 But again, that gets subjective.

2:19:38.540 --> 2:19:41.620
 How good was your thinking?

2:19:41.620 --> 2:19:43.900
 How efficient was?

2:19:43.900 --> 2:19:48.380
 So one thing that's interesting about this notion of scoring you

2:19:48.380 --> 2:19:50.500
 as how many attempts you need is that you

2:19:50.500 --> 2:19:55.220
 can start producing tasks that are way more ambiguous, right?

2:19:55.220 --> 2:19:56.500
 Right.

2:19:56.500 --> 2:19:59.700
 Because with the different attempts,

2:19:59.700 --> 2:20:03.300
 you can actually probe that ambiguity, right?

2:20:03.300 --> 2:20:04.140
 Right.

2:20:04.140 --> 2:20:08.220
 So that's, in a sense, which is how good can

2:20:08.220 --> 2:20:15.700
 you adapt to the uncertainty and reduce the uncertainty?

2:20:15.700 --> 2:20:18.260
 Yes, it's half fast.

2:20:18.260 --> 2:20:21.180
 It's the efficiency with which you reduce uncertainty

2:20:21.180 --> 2:20:22.940
 in program space, exactly.

2:20:22.940 --> 2:20:24.940
 Very difficult to come up with that kind of test, though.

2:20:24.940 --> 2:20:28.340
 Yeah, so I would love to be able to create something like this.

2:20:28.340 --> 2:20:33.140
 In practice, it would be very, very difficult, but yes.

2:20:33.140 --> 2:20:36.140
 I mean, what you're doing, what you've done with the ARC challenge

2:20:36.140 --> 2:20:37.620
 is brilliant.

2:20:37.620 --> 2:20:40.940
 I'm also not surprised that it's not more popular,

2:20:40.940 --> 2:20:42.140
 but I think it's picking up.

2:20:42.140 --> 2:20:42.860
 It does its niche.

2:20:42.860 --> 2:20:44.100
 It does its niche, yeah.

2:20:44.100 --> 2:20:44.900
 Yeah.

2:20:44.900 --> 2:20:47.100
 What are your thoughts about another test?

2:20:47.100 --> 2:20:48.940
 I talked with Marcus Hutter.

2:20:48.940 --> 2:20:51.660
 He has the Hutter Prize for compression of human knowledge.

2:20:51.660 --> 2:20:55.620
 And the idea is really sort of quantify and reduce

2:20:55.620 --> 2:20:58.260
 the test of intelligence purely to just the ability

2:20:58.260 --> 2:20:59.580
 to compress.

2:20:59.580 --> 2:21:04.660
 What's your thoughts about this intelligence as compression?

2:21:04.660 --> 2:21:07.980
 I mean, it's a very fun test because it's

2:21:07.980 --> 2:21:12.220
 such a simple idea, like you're given Wikipedia,

2:21:12.220 --> 2:21:15.500
 basic English Wikipedia, and you must compress it.

2:21:15.500 --> 2:21:21.140
 And so it stems from the idea that cognition is compression,

2:21:21.140 --> 2:21:24.020
 that the brain is basically a compression algorithm.

2:21:24.020 --> 2:21:25.620
 This is a very old idea.

2:21:25.620 --> 2:21:30.540
 It's a very, I think, striking and beautiful idea.

2:21:30.540 --> 2:21:32.740
 I used to believe it.

2:21:32.740 --> 2:21:36.140
 I eventually had to realize that it was very much

2:21:36.140 --> 2:21:36.900
 a flawed idea.

2:21:36.900 --> 2:21:41.420
 So I no longer believe that cognition is compression.

2:21:41.420 --> 2:21:44.620
 But I can tell you what's the difference.

2:21:44.620 --> 2:21:48.820
 So it's very easy to believe that cognition and compression

2:21:48.820 --> 2:21:51.660
 are the same thing.

2:21:51.660 --> 2:21:53.220
 So Jeff Hawkins, for instance, says

2:21:53.220 --> 2:21:54.780
 that cognition is prediction.

2:21:54.780 --> 2:21:57.740
 And of course, prediction is basically the same thing

2:21:57.740 --> 2:21:58.700
 as compression.

2:21:58.700 --> 2:22:03.580
 It's just including the temporal axis.

2:22:03.580 --> 2:22:05.060
 And it's very easy to believe this

2:22:05.060 --> 2:22:06.900
 because compression is something that we

2:22:06.900 --> 2:22:09.020
 do all the time very naturally.

2:22:09.020 --> 2:22:12.020
 We are constantly compressing information.

2:22:12.020 --> 2:22:15.660
 We are constantly trying.

2:22:15.660 --> 2:22:17.940
 We have this bias towards simplicity.

2:22:17.940 --> 2:22:21.060
 We are constantly trying to organize things in our mind

2:22:21.060 --> 2:22:24.460
 and around us to be more regular.

2:22:24.460 --> 2:22:26.860
 So it's a beautiful idea.

2:22:26.860 --> 2:22:28.620
 It's very easy to believe.

2:22:28.620 --> 2:22:31.580
 There is a big difference between what

2:22:31.580 --> 2:22:33.980
 we do with our brains and compression.

2:22:33.980 --> 2:22:38.220
 So compression is actually kind of a tool

2:22:38.220 --> 2:22:42.060
 in the human cognitive toolkit that is used in many ways.

2:22:42.060 --> 2:22:44.540
 But it's just a tool.

2:22:44.540 --> 2:22:45.940
 It is a tool for cognition.

2:22:45.940 --> 2:22:47.620
 It is not cognition itself.

2:22:47.620 --> 2:22:50.020
 And the big fundamental difference

2:22:50.020 --> 2:22:55.340
 is that cognition is about being able to operate

2:22:55.340 --> 2:23:00.740
 in future situations that include fundamental uncertainty

2:23:00.740 --> 2:23:02.140
 and novelty.

2:23:02.140 --> 2:23:06.860
 So for instance, consider a child at age 10.

2:23:06.860 --> 2:23:10.100
 And so they have 10 years of life experience.

2:23:10.100 --> 2:23:14.260
 They've gotten pain, pleasure, rewards, and punishment

2:23:14.260 --> 2:23:16.500
 in a period of time.

2:23:16.500 --> 2:23:21.980
 If you were to generate the shortest behavioral program

2:23:21.980 --> 2:23:26.740
 that would have basically run that child over these 10 years

2:23:26.740 --> 2:23:32.220
 in an optimal way, the shortest optimal behavioral program

2:23:32.220 --> 2:23:34.820
 given the experience of that child so far,

2:23:34.820 --> 2:23:37.540
 well, that program, that compressed program,

2:23:37.540 --> 2:23:39.940
 this is what you would get if the mind of the child

2:23:39.940 --> 2:23:42.740
 was a compression algorithm essentially,

2:23:42.740 --> 2:23:48.100
 would be utterly unable, inappropriate,

2:23:48.100 --> 2:23:54.380
 to process the next 70 years in the life of that child.

2:23:54.380 --> 2:23:59.020
 So in the models we build of the world,

2:23:59.020 --> 2:24:03.220
 we are not trying to make them actually optimally compressed.

2:24:03.220 --> 2:24:06.660
 We are using compression as a tool

2:24:06.660 --> 2:24:10.060
 to promote simplicity and efficiency in our models.

2:24:10.060 --> 2:24:12.060
 But they are not perfectly compressed

2:24:12.060 --> 2:24:15.300
 because they need to include things

2:24:15.300 --> 2:24:18.540
 that are seemingly useless today, that have seemingly

2:24:18.540 --> 2:24:20.140
 been useless so far.

2:24:20.140 --> 2:24:24.140
 But that may turn out to be useful in the future

2:24:24.140 --> 2:24:25.900
 because you just don't know the future.

2:24:25.900 --> 2:24:28.740
 And that's the fundamental principle

2:24:28.740 --> 2:24:31.260
 that cognition, that intelligence arises from

2:24:31.260 --> 2:24:33.780
 is that you need to be able to run

2:24:33.780 --> 2:24:36.660
 appropriate behavioral programs except you have absolutely

2:24:36.660 --> 2:24:40.940
 no idea what sort of context, environment, situation

2:24:40.940 --> 2:24:42.260
 they are going to be running in.

2:24:42.260 --> 2:24:45.020
 And you have to deal with that uncertainty,

2:24:45.020 --> 2:24:46.580
 with that future anomaly.

2:24:46.580 --> 2:24:52.500
 So an analogy that you can make is with investing,

2:24:52.500 --> 2:24:54.460
 for instance.

2:24:54.460 --> 2:24:59.540
 If I look at the past 20 years of stock market data,

2:24:59.540 --> 2:25:01.860
 and I use a compression algorithm

2:25:01.860 --> 2:25:04.420
 to figure out the best trading strategy,

2:25:04.420 --> 2:25:06.660
 it's going to be you buy Apple stock, then

2:25:06.660 --> 2:25:10.420
 maybe the past few years you buy Tesla stock or something.

2:25:10.420 --> 2:25:13.300
 But is that strategy still going to be

2:25:13.300 --> 2:25:14.660
 true for the next 20 years?

2:25:14.660 --> 2:25:17.980
 Well, actually, probably not, which

2:25:17.980 --> 2:25:21.060
 is why if you're a smart investor,

2:25:21.060 --> 2:25:26.340
 you're not just going to be following the strategy that

2:25:26.340 --> 2:25:28.980
 corresponds to compression of the past.

2:25:28.980 --> 2:25:31.660
 You're going to be following, you're

2:25:31.660 --> 2:25:34.860
 going to have a balanced portfolio, right?

2:25:34.860 --> 2:25:38.180
 Because you just don't know what's going to happen.

2:25:38.180 --> 2:25:40.460
 I mean, I guess in that same sense,

2:25:40.460 --> 2:25:42.540
 the compression is analogous to what

2:25:42.540 --> 2:25:45.900
 you talked about, which is local or robust generalization

2:25:45.900 --> 2:25:47.820
 versus extreme generalization.

2:25:47.820 --> 2:25:52.420
 It's much closer to that side of being able to generalize

2:25:52.420 --> 2:25:53.420
 in the local sense.

2:25:53.420 --> 2:25:59.980
 That's why as humans, when we are children, in our education,

2:25:59.980 --> 2:26:04.180
 so a lot of it is driven by play, driven by curiosity.

2:26:04.180 --> 2:26:07.900
 We are not efficiently compressing things.

2:26:07.900 --> 2:26:09.620
 We're actually exploring.

2:26:09.620 --> 2:26:16.620
 We are retaining all kinds of things

2:26:16.620 --> 2:26:19.660
 from our environment that seem to be completely useless.

2:26:19.660 --> 2:26:24.380
 Because they might turn out to be eventually useful, right?

2:26:24.380 --> 2:26:26.940
 And that's what cognition is really about.

2:26:26.940 --> 2:26:29.300
 And what makes it antagonistic to compression

2:26:29.300 --> 2:26:33.980
 is that it is about hedging for future uncertainty.

2:26:33.980 --> 2:26:35.860
 And that's antagonistic to compression.

2:26:35.860 --> 2:26:36.580
 Yes.

2:26:36.580 --> 2:26:38.500
 Officially hedging.

2:26:38.500 --> 2:26:41.660
 Cognition leverages compression as a tool

2:26:41.660 --> 2:26:47.420
 to promote efficiency and simplicity in our models.

2:26:47.420 --> 2:26:52.260
 It's like Einstein said, make it simpler, but not,

2:26:52.260 --> 2:26:54.940
 however that quote goes, but not too simple.

2:26:54.940 --> 2:26:57.700
 So compression simplifies things,

2:26:57.700 --> 2:27:00.100
 but you don't want to make it too simple.

2:27:00.100 --> 2:27:00.740
 Yes.

2:27:00.740 --> 2:27:03.100
 So a good model of the world is going

2:27:03.100 --> 2:27:06.100
 to include all kinds of things that are completely useless,

2:27:06.100 --> 2:27:08.500
 actually, just in case.

2:27:08.500 --> 2:27:10.020
 Because you need diversity in the same way

2:27:10.020 --> 2:27:11.140
 that in your portfolio.

2:27:11.140 --> 2:27:13.340
 You need all kinds of stocks that may not

2:27:13.340 --> 2:27:15.580
 have performed well so far, but you need diversity.

2:27:15.580 --> 2:27:16.980
 And the reason you need diversity

2:27:16.980 --> 2:27:19.660
 is because fundamentally you don't know what you're doing.

2:27:19.660 --> 2:27:22.020
 And the same is true of the human mind,

2:27:22.020 --> 2:27:26.860
 is that it needs to behave appropriately in the future.

2:27:26.860 --> 2:27:29.860
 And it has no idea what the future is going to be like.

2:27:29.860 --> 2:27:31.460
 But it's not going to be like the past.

2:27:31.460 --> 2:27:33.620
 So compressing the past is not appropriate,

2:27:33.620 --> 2:27:40.500
 because the past is not, it's not predictive of the future.

2:27:40.500 --> 2:27:44.740
 Yeah, history repeats itself, but not perfectly.

2:27:44.740 --> 2:27:48.980
 I don't think I asked you last time the most inappropriately

2:27:48.980 --> 2:27:51.180
 absurd question.

2:27:51.180 --> 2:27:54.420
 We've talked a lot about intelligence,

2:27:54.420 --> 2:28:00.860
 but the bigger question from intelligence is of meaning.

2:28:00.860 --> 2:28:02.980
 Intelligence systems are kind of goal oriented.

2:28:02.980 --> 2:28:05.380
 They're always optimizing for a goal.

2:28:05.380 --> 2:28:07.620
 If you look at the Hutter Prize, actually,

2:28:07.620 --> 2:28:10.860
 I mean, there's always a clean formulation of a goal.

2:28:10.860 --> 2:28:14.220
 But the natural question for us humans,

2:28:14.220 --> 2:28:16.020
 since we don't know our objective function,

2:28:16.020 --> 2:28:18.460
 is what is the meaning of it all?

2:28:18.460 --> 2:28:22.980
 So the absurd question is, what, Francois,

2:28:22.980 --> 2:28:25.660
 do you think is the meaning of life?

2:28:25.660 --> 2:28:26.820
 What's the meaning of life?

2:28:26.820 --> 2:28:28.180
 Yeah, that's a big question.

2:28:28.180 --> 2:28:33.220
 And I think I can give you my answer, at least one

2:28:33.220 --> 2:28:34.540
 of my answers.

2:28:34.540 --> 2:28:42.220
 And so one thing that's very important in understanding who

2:28:42.220 --> 2:28:48.380
 we are is that everything that makes up ourselves,

2:28:48.380 --> 2:28:53.740
 that makes up who we are, even your most personal thoughts,

2:28:53.740 --> 2:28:55.700
 is not actually your own.

2:28:55.700 --> 2:29:00.060
 Even your most personal thoughts are expressed in words

2:29:00.060 --> 2:29:04.940
 that you did not invent and are built on concepts and images

2:29:04.940 --> 2:29:06.900
 that you did not invent.

2:29:06.900 --> 2:29:10.940
 We are very much cultural beings.

2:29:10.940 --> 2:29:12.860
 We are made of culture.

2:29:12.860 --> 2:29:16.660
 What makes us different from animals, for instance?

2:29:16.660 --> 2:29:22.860
 So everything about ourselves is an echo of the past.

2:29:22.860 --> 2:29:29.900
 Is an echo of the past, an echo of people who lived before us.

2:29:29.900 --> 2:29:31.420
 That's who we are.

2:29:31.420 --> 2:29:35.300
 And in the same way, if we manage

2:29:35.300 --> 2:29:41.780
 to contribute something to the collective edifice of culture,

2:29:41.780 --> 2:29:44.580
 a new idea, maybe a beautiful piece of music,

2:29:44.580 --> 2:29:51.260
 a work of art, a grand theory, a new world, maybe,

2:29:51.260 --> 2:29:54.380
 that something is going to become

2:29:54.380 --> 2:30:00.300
 a part of the minds of future humans, essentially, forever.

2:30:00.300 --> 2:30:03.980
 So everything we do creates ripples

2:30:03.980 --> 2:30:06.020
 that propagate into the future.

2:30:06.020 --> 2:30:11.900
 And in a way, this is our path to immortality,

2:30:11.900 --> 2:30:17.580
 is that as we contribute things to culture,

2:30:17.580 --> 2:30:21.420
 culture in turn becomes future humans.

2:30:21.420 --> 2:30:27.660
 And we keep influencing people thousands of years from now.

2:30:27.660 --> 2:30:30.740
 So our actions today create ripples.

2:30:30.740 --> 2:30:35.140
 And these ripples, I think, basically

2:30:35.140 --> 2:30:37.620
 sum up the meaning of life.

2:30:37.620 --> 2:30:42.540
 In the same way that we are the sum

2:30:42.540 --> 2:30:45.500
 of the interactions between many different ripples that

2:30:45.500 --> 2:30:48.100
 came from our past, we are ourselves

2:30:48.100 --> 2:30:50.700
 creating ripples that will propagate into the future.

2:30:50.700 --> 2:30:53.460
 And that's why we should be, this

2:30:53.460 --> 2:30:56.060
 seems like perhaps an eighth thing to say,

2:30:56.060 --> 2:31:02.060
 but we should be kind to others during our time on Earth

2:31:02.060 --> 2:31:05.660
 because every act of kindness creates ripples.

2:31:05.660 --> 2:31:09.380
 And in reverse, every act of violence also creates ripples.

2:31:09.380 --> 2:31:13.260
 And you want to carefully choose which kind of ripples

2:31:13.260 --> 2:31:16.460
 you want to create, and you want to propagate into the future.

2:31:16.460 --> 2:31:19.020
 And in your case, first of all, beautifully put,

2:31:19.020 --> 2:31:21.380
 but in your case, creating ripples

2:31:21.380 --> 2:31:27.780
 into the future human and future AGI systems.

2:31:27.780 --> 2:31:28.500
 Yes.

2:31:28.500 --> 2:31:29.500
 It's fascinating.

2:31:29.500 --> 2:31:30.420
 Our successors.

2:31:30.420 --> 2:31:34.500
 I don't think there's a better way to end it,

2:31:34.500 --> 2:31:37.180
 Francois, as always, for a second time.

2:31:37.180 --> 2:31:39.340
 And I'm sure many times in the future,

2:31:39.340 --> 2:31:40.820
 it's been a huge honor.

2:31:40.820 --> 2:31:43.380
 You're one of the most brilliant people

2:31:43.380 --> 2:31:47.500
 in the machine learning, computer science world.

2:31:47.500 --> 2:31:48.700
 Again, it's a huge honor.

2:31:48.700 --> 2:31:49.460
 Thanks for talking to me.

2:31:49.460 --> 2:31:50.540
 It's been a pleasure.

2:31:50.540 --> 2:31:51.980
 Thanks a lot for having me.

2:31:51.980 --> 2:31:53.900
 We appreciate it.

2:31:53.900 --> 2:31:56.220
 Thanks for listening to this conversation with Francois

2:31:56.220 --> 2:32:00.340
 Chollet, and thank you to our sponsors, Babbel, Masterclass,

2:32:00.340 --> 2:32:01.660
 and Cash App.

2:32:01.660 --> 2:32:03.900
 Click the sponsor links in the description

2:32:03.900 --> 2:32:06.820
 to get a discount and to support this podcast.

2:32:06.820 --> 2:32:09.060
 If you enjoy this thing, subscribe on YouTube,

2:32:09.060 --> 2:32:11.340
 review it with five stars on Apple Podcast,

2:32:11.340 --> 2:32:14.060
 follow on Spotify, support on Patreon,

2:32:14.060 --> 2:32:17.780
 or connect with me on Twitter at Lex Friedman.

2:32:17.780 --> 2:32:19.420
 And now let me leave you with some words

2:32:19.420 --> 2:32:24.380
 from René Descartes in 1668, an excerpt of which Francois

2:32:24.380 --> 2:32:27.780
 includes and is on the measure of intelligence paper.

2:32:27.780 --> 2:32:30.300
 If there were machines which bore a resemblance

2:32:30.300 --> 2:32:34.420
 to our bodies and imitated our actions as closely as possible

2:32:34.420 --> 2:32:36.980
 for all practical purposes, we should still

2:32:36.980 --> 2:32:40.020
 have two very certain means of recognizing

2:32:40.020 --> 2:32:42.220
 that they were not real men.

2:32:42.220 --> 2:32:45.300
 The first is that they could never use words or put together

2:32:45.300 --> 2:32:49.820
 signs, as we do in order to declare our thoughts to others.

2:32:49.820 --> 2:32:53.420
 For we can certainly conceive of a machine so constructed

2:32:53.420 --> 2:32:55.540
 that it utters words and even utters

2:32:55.540 --> 2:32:57.940
 words that correspond to bodily actions causing

2:32:57.940 --> 2:32:59.580
 a change in its organs.

2:32:59.580 --> 2:33:03.380
 But it is not conceivable that such a machine should produce

2:33:03.380 --> 2:33:05.420
 different arrangements of words so as

2:33:05.420 --> 2:33:08.620
 to give an appropriately meaningful answer to whatever

2:33:08.620 --> 2:33:12.780
 is said in its presence as the dullest of men can do.

2:33:12.780 --> 2:33:15.460
 Here, Descartes is anticipating the Turing test,

2:33:15.460 --> 2:33:18.780
 and the argument still continues to this day.

2:33:18.780 --> 2:33:22.140
 Secondly, he continues, even though some machines might

2:33:22.140 --> 2:33:26.580
 do some things as well as we do them, or perhaps even better,

2:33:26.580 --> 2:33:29.100
 they would inevitably fail in others,

2:33:29.100 --> 2:33:32.420
 which would reveal that they are acting not from understanding

2:33:32.420 --> 2:33:36.780
 but only from the disposition of their organs.

2:33:36.780 --> 2:33:39.860
 This is an incredible quote.

2:33:39.860 --> 2:33:43.220
 Whereas reason is a universal instrument

2:33:43.220 --> 2:33:46.580
 which can be used in all kinds of situations,

2:33:46.580 --> 2:33:49.060
 these organs need some particular action.

2:33:49.060 --> 2:33:51.220
 Hence, it is for all practical purposes

2:33:51.220 --> 2:33:54.300
 impossible for a machine to have enough different organs

2:33:54.300 --> 2:33:57.780
 to make it act in all the contingencies of life

2:33:57.780 --> 2:34:01.340
 and the way in which our reason makes us act.

2:34:01.340 --> 2:34:05.060
 That's the debate between mimicry and memorization

2:34:05.060 --> 2:34:07.220
 versus understanding.

2:34:07.220 --> 2:34:32.460
 So thank you for listening and hope to see you next time.

