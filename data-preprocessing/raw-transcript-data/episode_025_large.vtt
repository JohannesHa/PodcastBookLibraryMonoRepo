WEBVTT

00:00.000 --> 00:02.360
 The following is a conversation with Jeff Hawkins.

00:02.360 --> 00:04.120
 He's the founder of the Redwood Center

00:04.120 --> 00:08.980
 for Theoretical Neuroscience in 2002, and NuMenta in 2005.

00:08.980 --> 00:11.920
 In his 2004 book, titled On Intelligence,

00:11.920 --> 00:13.840
 and in the research before and after,

00:13.840 --> 00:16.200
 he and his team have worked to reverse engineer

00:16.200 --> 00:19.160
 the neural cortex, and propose artificial intelligence

00:19.160 --> 00:21.360
 architectures, approaches, and ideas

00:21.360 --> 00:23.640
 that are inspired by the human brain.

00:23.640 --> 00:25.960
 These ideas include Hierarchical Tupperware Memory,

00:25.960 --> 00:28.920
 HTM, from 2004, and new work,

00:28.920 --> 00:30.720
 the Thousand Brains Theory of Intelligence

00:30.720 --> 00:33.760
 from 2017, 18, and 19.

00:33.760 --> 00:36.120
 Jeff's ideas have been an inspiration

00:36.120 --> 00:38.200
 to many who have looked for progress

00:38.200 --> 00:40.480
 beyond the current machine learning approaches,

00:40.480 --> 00:42.720
 but they have also received criticism

00:42.720 --> 00:44.680
 for lacking a body of empirical evidence

00:44.680 --> 00:46.240
 supporting the models.

00:46.240 --> 00:48.440
 This is always a challenge when seeking more

00:48.440 --> 00:51.440
 than small incremental steps forward in AI.

00:51.440 --> 00:54.120
 Jeff is a brilliant mind, and many of the ideas

00:54.120 --> 00:56.500
 he has developed and aggregated from neuroscience

00:56.500 --> 00:59.120
 are worth understanding and thinking about.

00:59.120 --> 01:00.920
 There are limits to deep learning,

01:00.920 --> 01:02.920
 as it is currently defined.

01:02.920 --> 01:05.760
 Forward progress in AI is shrouded in mystery.

01:05.760 --> 01:07.760
 My hope is that conversations like this

01:07.760 --> 01:11.440
 can help provide an inspiring spark for new ideas.

01:11.440 --> 01:14.020
 This is the Artificial Intelligence Podcast.

01:14.020 --> 01:16.720
 If you enjoy it, subscribe on YouTube, iTunes,

01:16.720 --> 01:18.640
 or simply connect with me on Twitter

01:18.640 --> 01:21.520
 at Lex Friedman, spelled F R I D.

01:21.520 --> 01:26.520
 And now, here's my conversation with Jeff Hawkins.

01:26.780 --> 01:29.860
 Are you more interested in understanding the human brain

01:29.860 --> 01:32.000
 or in creating artificial systems

01:32.000 --> 01:34.640
 that have many of the same qualities

01:34.640 --> 01:38.560
 but don't necessarily require that you actually understand

01:38.560 --> 01:41.480
 the underpinning workings of our mind?

01:41.480 --> 01:44.000
 So there's a clear answer to that question.

01:44.000 --> 01:46.760
 My primary interest is understanding the human brain.

01:46.760 --> 01:47.700
 No question about it.

01:47.700 --> 01:52.700
 But I also firmly believe that we will not be able

01:53.280 --> 01:55.040
 to create fully intelligent machines

01:55.040 --> 01:57.280
 until we understand how the human brain works.

01:57.280 --> 02:00.120
 So I don't see those as separate problems.

02:00.120 --> 02:01.720
 I think there's limits to what can be done

02:01.720 --> 02:03.540
 with machine intelligence if you don't understand

02:03.540 --> 02:05.680
 the principles by which the brain works.

02:05.680 --> 02:07.900
 And so I actually believe that studying the brain

02:07.900 --> 02:11.960
 is actually the fastest way to get to machine intelligence.

02:11.960 --> 02:14.640
 And within that, let me ask the impossible question,

02:14.640 --> 02:17.160
 how do you, not define, but at least think

02:17.160 --> 02:19.440
 about what it means to be intelligent?

02:19.440 --> 02:22.280
 So I didn't try to answer that question first.

02:22.280 --> 02:24.520
 We said, let's just talk about how the brain works

02:24.520 --> 02:26.720
 and let's figure out how certain parts of the brain,

02:26.720 --> 02:29.920
 mostly the neocortex, but some other parts too.

02:29.920 --> 02:32.340
 The parts of the brain most associated with intelligence.

02:32.340 --> 02:35.840
 And let's discover the principles by how they work.

02:35.840 --> 02:39.360
 Because intelligence isn't just like some mechanism

02:39.360 --> 02:40.680
 and it's not just some capabilities.

02:40.680 --> 02:42.520
 It's like, okay, we don't even know

02:42.520 --> 02:44.040
 where to begin on this stuff.

02:44.040 --> 02:49.040
 And so now that we've made a lot of progress on this,

02:49.320 --> 02:50.480
 after we've made a lot of progress

02:50.480 --> 02:53.200
 on how the neocortex works, and we can talk about that,

02:53.200 --> 02:55.840
 I now have a very good idea what's gonna be required

02:55.840 --> 02:57.200
 to make intelligent machines.

02:57.200 --> 02:59.600
 I can tell you today, some of the things

02:59.600 --> 03:02.140
 are gonna be necessary, I believe,

03:02.140 --> 03:03.480
 to create intelligent machines.

03:03.480 --> 03:04.600
 Well, so we'll get there.

03:04.600 --> 03:07.440
 We'll get to the neocortex and some of the theories

03:07.440 --> 03:09.200
 of how the whole thing works.

03:09.200 --> 03:11.760
 And you're saying, as we understand more and more

03:11.760 --> 03:14.760
 about the neocortex, about our own human mind,

03:14.760 --> 03:17.680
 we'll be able to start to more specifically define

03:17.680 --> 03:18.680
 what it means to be intelligent.

03:18.680 --> 03:21.840
 It's not useful to really talk about that until.

03:21.840 --> 03:23.560
 I don't know if it's not useful.

03:23.560 --> 03:26.160
 Look, there's a long history of AI, as you know.

03:26.160 --> 03:28.900
 And there's been different approaches taken to it.

03:28.900 --> 03:31.300
 And who knows, maybe they're all useful.

03:32.240 --> 03:37.240
 So the good old fashioned AI, the expert systems,

03:37.280 --> 03:38.920
 the current convolutional neural networks,

03:38.920 --> 03:40.380
 they all have their utility.

03:40.380 --> 03:43.780
 They all have a value in the world.

03:43.780 --> 03:45.220
 But I would think almost everyone agree

03:45.220 --> 03:46.620
 that none of them are really intelligent

03:46.620 --> 03:49.860
 in a sort of a deep way that humans are.

03:49.860 --> 03:53.620
 And so it's just the question of how do you get

03:53.620 --> 03:56.420
 from where those systems were or are today

03:56.420 --> 03:59.240
 to where a lot of people think we're gonna go.

03:59.240 --> 04:02.340
 And there's a big, big gap there, a huge gap.

04:02.340 --> 04:06.220
 And I think the quickest way of bridging that gap

04:06.220 --> 04:08.820
 is to figure out how the brain does that.

04:08.820 --> 04:10.100
 And then we can sit back and look and say,

04:10.100 --> 04:12.980
 oh, which of these principles that the brain works on

04:12.980 --> 04:15.140
 are necessary and which ones are not?

04:15.140 --> 04:16.620
 Clearly, we don't have to build this in,

04:16.620 --> 04:18.460
 and intelligent machines aren't gonna be built

04:18.460 --> 04:22.720
 out of organic living cells.

04:22.720 --> 04:24.700
 But there's a lot of stuff that goes on the brain

04:24.700 --> 04:25.900
 that's gonna be necessary.

04:25.900 --> 04:30.260
 So let me ask maybe, before we get into the fun details,

04:30.260 --> 04:33.060
 let me ask maybe a depressing or a difficult question.

04:33.060 --> 04:36.220
 Do you think it's possible that we will never

04:36.220 --> 04:38.060
 be able to understand how our brain works,

04:38.060 --> 04:41.820
 that maybe there's aspects to the human mind,

04:41.820 --> 04:46.140
 like we ourselves cannot introspectively get to the core,

04:46.140 --> 04:48.100
 that there's a wall you eventually hit?

04:48.100 --> 04:50.220
 Yeah, I don't believe that's the case.

04:52.020 --> 04:53.240
 I have never believed that's the case.

04:53.240 --> 04:56.620
 There's not been a single thing humans have ever put

04:56.620 --> 04:58.620
 their minds to that we've said, oh, we reached the wall,

04:58.620 --> 04:59.700
 we can't go any further.

04:59.700 --> 05:01.660
 It's just, people keep saying that.

05:01.660 --> 05:03.380
 People used to believe that about life.

05:03.380 --> 05:05.180
 Alain Vital, right, there's like,

05:05.180 --> 05:06.380
 what's the difference between living matter

05:06.380 --> 05:07.980
 and nonliving matter, something special

05:07.980 --> 05:09.100
 that we never understand.

05:09.100 --> 05:10.660
 We no longer think that.

05:10.660 --> 05:14.220
 So there's no historical evidence that suggests this

05:14.220 --> 05:16.340
 is the case, and I just never even consider

05:16.340 --> 05:17.620
 that's a possibility.

05:17.620 --> 05:21.860
 I would also say, today, we understand so much

05:21.860 --> 05:22.820
 about the neocortex.

05:22.820 --> 05:25.480
 We've made tremendous progress in the last few years

05:25.480 --> 05:30.000
 that I no longer think of it as an open question.

05:30.000 --> 05:32.100
 The answers are very clear to me.

05:32.100 --> 05:34.800
 The pieces we don't know are clear to me,

05:34.800 --> 05:36.740
 but the framework is all there, and it's like,

05:36.740 --> 05:38.620
 oh, okay, we're gonna be able to do this.

05:38.620 --> 05:41.100
 This is not a problem anymore, just takes time and effort,

05:41.100 --> 05:44.060
 but there's no mystery, a big mystery anymore.

05:44.060 --> 05:47.800
 So then let's get into it for people like myself

05:47.800 --> 05:52.800
 who are not very well versed in the human brain,

05:52.940 --> 05:53.840
 except my own.

05:54.780 --> 05:57.300
 Can you describe to me, at the highest level,

05:57.300 --> 05:59.140
 what are the different parts of the human brain,

05:59.140 --> 06:02.060
 and then zooming in on the neocortex,

06:02.060 --> 06:04.120
 the parts of the neocortex, and so on,

06:04.120 --> 06:05.500
 a quick overview.

06:05.500 --> 06:06.620
 Yeah, sure.

06:06.620 --> 06:09.940
 The human brain, we can divide it roughly into two parts.

06:10.780 --> 06:14.220
 There's the old parts, lots of pieces,

06:14.220 --> 06:15.700
 and then there's the new part.

06:15.700 --> 06:18.020
 The new part is the neocortex.

06:18.020 --> 06:20.420
 It's new because it didn't exist before mammals.

06:20.420 --> 06:22.180
 The only mammals have a neocortex,

06:22.180 --> 06:24.780
 and in humans, in primates, it's very large.

06:24.780 --> 06:26.900
 In the human brain, the neocortex occupies

06:26.900 --> 06:30.660
 about 70 to 75% of the volume of the brain.

06:30.660 --> 06:32.100
 It's huge.

06:32.100 --> 06:34.860
 And the old parts of the brain are,

06:34.860 --> 06:36.020
 there's lots of pieces there.

06:36.020 --> 06:38.740
 There's the spinal cord, and there's the brain stem,

06:38.740 --> 06:40.240
 and the cerebellum, and the different parts

06:40.240 --> 06:42.020
 of the basal ganglia, and so on.

06:42.020 --> 06:42.960
 In the old parts of the brain,

06:42.960 --> 06:44.800
 you have the autonomic regulation,

06:44.800 --> 06:46.280
 like breathing and heart rate.

06:46.280 --> 06:49.460
 You have basic behaviors, so like walking and running

06:49.460 --> 06:51.380
 are controlled by the old parts of the brain.

06:51.380 --> 06:53.060
 All the emotional centers of the brain

06:53.060 --> 06:53.940
 are in the old part of the brain,

06:53.940 --> 06:55.380
 so when you feel anger or hungry, lust,

06:55.380 --> 06:56.500
 or things like that, those are all

06:56.500 --> 06:57.940
 in the old parts of the brain.

06:57.940 --> 07:02.180
 And we associate with the neocortex

07:02.180 --> 07:04.060
 all the things we think about as sort of

07:04.060 --> 07:08.100
 high level perception and cognitive functions,

07:08.100 --> 07:12.240
 anything from seeing and hearing and touching things

07:12.240 --> 07:15.140
 to language to mathematics and engineering

07:15.140 --> 07:16.940
 and science and so on.

07:16.940 --> 07:19.760
 Those are all associated with the neocortex,

07:19.760 --> 07:21.760
 and they're certainly correlated.

07:21.760 --> 07:23.980
 Our abilities in those regards are correlated

07:23.980 --> 07:25.820
 with the relative size of our neocortex

07:25.820 --> 07:27.940
 compared to other mammals.

07:27.940 --> 07:30.520
 So that's like the rough division,

07:30.520 --> 07:32.740
 and you obviously can't understand

07:32.740 --> 07:35.160
 the neocortex completely isolated,

07:35.160 --> 07:37.020
 but you can understand a lot of it

07:37.020 --> 07:40.340
 with just a few interfaces to the old parts of the brain,

07:40.340 --> 07:44.980
 and so it gives you a system to study.

07:44.980 --> 07:48.020
 The other remarkable thing about the neocortex,

07:48.020 --> 07:49.880
 compared to the old parts of the brain,

07:49.880 --> 07:52.900
 is the neocortex is extremely uniform.

07:52.900 --> 07:55.860
 It's not visibly or anatomically,

07:57.060 --> 07:59.460
 it's very, I always like to say

07:59.460 --> 08:01.300
 it's like the size of a dinner napkin,

08:01.300 --> 08:03.740
 about two and a half millimeters thick,

08:03.740 --> 08:05.980
 and it looks remarkably the same everywhere.

08:05.980 --> 08:07.900
 Everywhere you look in that two and a half millimeters

08:07.900 --> 08:10.060
 is this detailed architecture,

08:10.060 --> 08:11.580
 and it looks remarkably the same everywhere,

08:11.580 --> 08:12.620
 and that's across species.

08:12.620 --> 08:15.380
 A mouse versus a cat and a dog and a human.

08:15.380 --> 08:17.060
 Where if you look at the old parts of the brain,

08:17.060 --> 08:19.620
 there's lots of little pieces do specific things.

08:19.620 --> 08:22.060
 So it's like the old parts of our brain evolved,

08:22.060 --> 08:23.660
 like this is the part that controls heart rate,

08:23.660 --> 08:24.860
 and this is the part that controls this,

08:24.860 --> 08:25.780
 and this is this kind of thing,

08:25.780 --> 08:27.180
 and that's this kind of thing,

08:27.180 --> 08:30.100
 and these evolved for eons a long, long time,

08:30.100 --> 08:31.580
 and they have their specific functions,

08:31.580 --> 08:33.220
 and all of a sudden mammals come along,

08:33.220 --> 08:35.180
 and they got this thing called the neocortex,

08:35.180 --> 08:38.140
 and it got large by just replicating the same thing

08:38.140 --> 08:39.420
 over and over and over again.

08:39.420 --> 08:42.660
 This is like, wow, this is incredible.

08:42.660 --> 08:45.380
 So all the evidence we have,

08:46.260 --> 08:50.020
 and this is an idea that was first articulated

08:50.020 --> 08:52.020
 in a very cogent and beautiful argument

08:52.020 --> 08:55.660
 by a guy named Vernon Malcastle in 1978, I think it was,

08:56.820 --> 09:01.580
 that the neocortex all works on the same principle.

09:01.580 --> 09:05.260
 So language, hearing, touch, vision, engineering,

09:05.260 --> 09:06.980
 all these things are basically underlying,

09:06.980 --> 09:10.340
 are all built on the same computational substrate.

09:10.340 --> 09:11.820
 They're really all the same problem.

09:11.820 --> 09:14.860
 So the low level of the building blocks all look similar.

09:14.860 --> 09:16.300
 Yeah, and they're not even that low level.

09:16.300 --> 09:17.900
 We're not talking about like neurons.

09:17.900 --> 09:19.940
 We're talking about this very complex circuit

09:19.940 --> 09:21.420
 that exists throughout the neocortex.

09:21.420 --> 09:23.500
 It's remarkably similar.

09:23.500 --> 09:26.580
 It's like, yes, you see variations of it here and there,

09:26.580 --> 09:29.620
 more of the cell, less and less, and so on.

09:29.620 --> 09:32.700
 But what Malcastle argued was, he says,

09:32.700 --> 09:35.580
 you know, if you take a section of neocortex,

09:35.580 --> 09:38.580
 why is one a visual area and one is a auditory area?

09:38.580 --> 09:41.180
 Or why is, and his answer was,

09:41.180 --> 09:43.180
 it's because one is connected to eyes

09:43.180 --> 09:45.380
 and one is connected to ears.

09:45.380 --> 09:47.820
 Literally, you mean just it's most closest

09:47.820 --> 09:49.020
 in terms of number of connections

09:49.020 --> 09:50.900
 to the sensor. Literally, literally,

09:50.900 --> 09:53.780
 if you took the optic nerve and attached it

09:53.780 --> 09:55.300
 to a different part of the neocortex,

09:55.300 --> 09:57.940
 that part would become a visual region.

09:57.940 --> 10:00.380
 This actually, this experiment was actually done

10:00.380 --> 10:04.980
 by Merkankasur in developing, I think it was lemurs,

10:04.980 --> 10:06.700
 I can't remember what it was, some animal.

10:06.700 --> 10:08.540
 And there's a lot of evidence to this.

10:08.540 --> 10:09.940
 You know, if you take a blind person,

10:09.940 --> 10:12.180
 a person who's born blind at birth,

10:12.180 --> 10:15.420
 they're born with a visual neocortex.

10:15.420 --> 10:18.260
 It doesn't, may not get any input from the eyes

10:18.260 --> 10:21.260
 because of some congenital defect or something.

10:21.260 --> 10:24.700
 And that region becomes, does something else.

10:24.700 --> 10:27.020
 It picks up another task.

10:27.020 --> 10:32.020
 So, and it's, so it's this very complex thing.

10:32.300 --> 10:33.740
 It's not like, oh, they're all built on neurons.

10:33.740 --> 10:36.460
 No, they're all built in this very complex circuit

10:36.460 --> 10:40.300
 and somehow that circuit underlies everything.

10:40.300 --> 10:43.580
 And so this is the, it's called

10:43.580 --> 10:45.900
 the common cortical algorithm, if you will.

10:45.900 --> 10:47.980
 Some scientists just find it hard to believe

10:47.980 --> 10:50.060
 and they just, I can't believe that's true,

10:50.060 --> 10:52.100
 but the evidence is overwhelming in this case.

10:52.100 --> 10:54.340
 And so a large part of what it means

10:54.340 --> 10:56.420
 to figure out how the brain creates intelligence

10:56.420 --> 10:59.860
 and what is intelligence in the brain

10:59.860 --> 11:02.020
 is to understand what that circuit does.

11:02.020 --> 11:05.020
 If you can figure out what that circuit does,

11:05.020 --> 11:06.940
 as amazing as it is, then you can,

11:06.940 --> 11:08.620
 then you understand what all these

11:08.620 --> 11:10.500
 other cognitive functions are.

11:10.500 --> 11:13.300
 So if you were to sort of put neocortex

11:13.300 --> 11:15.140
 outside of your book on intelligence,

11:15.140 --> 11:18.020
 you look, if you wrote a giant tome, a textbook

11:18.020 --> 11:21.980
 on the neocortex, and you look maybe

11:21.980 --> 11:23.740
 a couple of centuries from now,

11:23.740 --> 11:26.500
 how much of what we know now would still be accurate

11:26.500 --> 11:27.660
 two centuries from now?

11:27.660 --> 11:30.820
 So how close are we in terms of understanding?

11:30.820 --> 11:32.980
 I have to speak from my own particular experience here.

11:32.980 --> 11:35.860
 So I run a small research lab here.

11:35.860 --> 11:38.020
 It's like any other research lab.

11:38.020 --> 11:39.420
 I'm sort of the principal investigator.

11:39.420 --> 11:40.260
 There's actually two of us

11:40.260 --> 11:42.540
 and there's a bunch of other people.

11:42.540 --> 11:43.820
 And this is what we do.

11:43.820 --> 11:46.060
 We study the neocortex and we publish our results

11:46.060 --> 11:46.900
 and so on.

11:46.900 --> 11:48.460
 So about three years ago,

11:49.820 --> 11:52.460
 we had a real breakthrough in this field.

11:52.460 --> 11:53.300
 Just tremendous breakthrough.

11:53.300 --> 11:56.500
 We've now published, I think, three papers on it.

11:56.500 --> 12:00.180
 And so I have a pretty good understanding

12:00.180 --> 12:02.300
 of all the pieces and what we're missing.

12:02.300 --> 12:06.260
 I would say that almost all the empirical data

12:06.260 --> 12:08.540
 we've collected about the brain, which is enormous.

12:08.540 --> 12:10.340
 If you don't know the neuroscience literature,

12:10.340 --> 12:13.980
 it's just incredibly big.

12:13.980 --> 12:16.860
 And it's, for the most part, all correct.

12:16.860 --> 12:21.660
 It's facts and experimental results and measurements

12:21.660 --> 12:22.980
 and all kinds of stuff.

12:22.980 --> 12:25.860
 But none of that has been really assimilated

12:25.860 --> 12:27.900
 into a theoretical framework.

12:27.900 --> 12:32.300
 It's data without, in the language of Thomas Kuhn,

12:32.300 --> 12:35.340
 the historian, would be a sort of a pre paradigm science.

12:35.340 --> 12:38.180
 Lots of data, but no way to fit it together.

12:38.180 --> 12:39.540
 I think almost all of that's correct.

12:39.540 --> 12:42.180
 There's just gonna be some mistakes in there.

12:42.180 --> 12:43.300
 And for the most part,

12:43.300 --> 12:45.940
 there aren't really good cogent theories about it,

12:45.940 --> 12:47.300
 how to put it together.

12:47.300 --> 12:50.060
 It's not like we have two or three competing good theories,

12:50.060 --> 12:51.540
 which ones are right and which ones are wrong.

12:51.540 --> 12:54.780
 It's like, nah, people are just scratching their heads.

12:54.780 --> 12:55.620
 Some people have given up

12:55.620 --> 12:57.620
 on trying to figure out what the whole thing does.

12:57.620 --> 13:01.020
 In fact, there's very, very few labs that we do

13:01.020 --> 13:03.340
 that focus really on theory

13:03.340 --> 13:06.780
 and all this unassimilated data and trying to explain it.

13:06.780 --> 13:08.940
 So it's not like we've got it wrong.

13:08.940 --> 13:11.100
 It's just that we haven't got it at all.

13:11.100 --> 13:15.020
 So it's really, I would say, pretty early days

13:15.020 --> 13:19.060
 in terms of understanding the fundamental theory's forces

13:19.060 --> 13:20.220
 of the way our mind works.

13:20.220 --> 13:21.500
 I don't think so.

13:21.500 --> 13:23.740
 I would have said that's true five years ago.

13:25.340 --> 13:26.980
 So as I said,

13:26.980 --> 13:29.300
 we had some really big breakthroughs on this recently

13:29.300 --> 13:30.780
 and we started publishing papers on this.

13:30.780 --> 13:34.180
 So we'll get to that.

13:34.180 --> 13:35.940
 But so I don't think it's,

13:35.940 --> 13:38.260
 I'm an optimist and from where I sit today,

13:38.260 --> 13:39.420
 most people would disagree with this,

13:39.420 --> 13:41.620
 but from where I sit today, from what I know,

13:43.260 --> 13:44.940
 it's not super early days anymore.

13:44.940 --> 13:46.860
 We are, the way these things go

13:46.860 --> 13:48.180
 is it's not a linear path, right?

13:48.180 --> 13:49.820
 You don't just start accumulating

13:49.820 --> 13:50.820
 and get better and better and better.

13:50.820 --> 13:52.900
 No, all this stuff you've collected,

13:52.900 --> 13:53.780
 none of it makes sense.

13:53.780 --> 13:55.580
 All these different things are just sort of around.

13:55.580 --> 13:57.100
 And then you're gonna have some breaking points

13:57.100 --> 13:59.420
 where all of a sudden, oh my God, now we got it right.

13:59.420 --> 14:01.100
 That's how it goes in science.

14:01.100 --> 14:04.460
 And I personally feel like we passed that little thing

14:04.460 --> 14:06.300
 about a couple of years ago,

14:06.300 --> 14:07.580
 all that big thing a couple of years ago.

14:07.580 --> 14:09.620
 So we can talk about that.

14:09.620 --> 14:11.020
 Time will tell if I'm right,

14:11.020 --> 14:12.660
 but I feel very confident about it.

14:12.660 --> 14:15.220
 That's why I'm willing to say it on tape like this.

14:15.220 --> 14:18.060
 At least very optimistic.

14:18.060 --> 14:20.220
 So let's, before those few years ago,

14:20.220 --> 14:23.260
 let's take a step back to HTM,

14:23.260 --> 14:26.020
 the hierarchical temporal memory theory,

14:26.020 --> 14:27.580
 which you first proposed on intelligence

14:27.580 --> 14:29.340
 and went through a few different generations.

14:29.340 --> 14:31.300
 Can you describe what it is,

14:31.300 --> 14:33.740
 how it evolved through the three generations

14:33.740 --> 14:35.460
 since you first put it on paper?

14:35.460 --> 14:39.340
 Yeah, so one of the things that neuroscientists

14:39.340 --> 14:42.980
 just sort of missed for many, many years,

14:42.980 --> 14:45.820
 and especially people who were thinking about theory,

14:45.820 --> 14:47.780
 was the nature of time in the brain.

14:49.100 --> 14:51.700
 Brains process information through time.

14:51.700 --> 14:52.900
 The information coming into the brain

14:52.900 --> 14:54.180
 is constantly changing.

14:55.220 --> 14:57.620
 The patterns from my speech right now,

14:57.620 --> 15:00.140
 if you were listening to it at normal speed,

15:00.140 --> 15:01.500
 would be changing on your ears

15:01.500 --> 15:04.100
 about every 10 milliseconds or so, you'd have a change.

15:04.100 --> 15:06.740
 This constant flow, when you look at the world,

15:06.740 --> 15:08.220
 your eyes are moving constantly,

15:08.220 --> 15:09.700
 three to five times a second,

15:09.700 --> 15:11.380
 and the input's completely changing.

15:11.380 --> 15:13.500
 If I were to touch something like a coffee cup,

15:13.500 --> 15:15.220
 as I move my fingers, the input changes.

15:15.220 --> 15:19.500
 So this idea that the brain works on time changing patterns

15:19.500 --> 15:22.340
 is almost completely, or was almost completely missing

15:22.340 --> 15:23.620
 from a lot of the basic theories,

15:23.620 --> 15:25.020
 like fears of vision and so on.

15:25.020 --> 15:26.860
 It's like, oh no, we're gonna put this image

15:26.860 --> 15:29.580
 in front of you and flash it and say, what is it?

15:29.580 --> 15:32.180
 Convolutional neural networks work that way today, right?

15:32.180 --> 15:34.220
 Classify this picture.

15:34.220 --> 15:35.980
 But that's not what vision is like.

15:35.980 --> 15:38.740
 Vision is this sort of crazy time based pattern

15:38.740 --> 15:40.060
 that's going all over the place,

15:40.060 --> 15:41.820
 and so is touch and so is hearing.

15:41.820 --> 15:43.780
 So the first part of hierarchical temporal memory

15:43.780 --> 15:45.060
 was the temporal part.

15:45.060 --> 15:48.260
 It's to say, you won't understand the brain,

15:48.260 --> 15:50.020
 nor will you understand intelligent machines

15:50.020 --> 15:52.460
 unless you're dealing with time based patterns.

15:52.460 --> 15:55.460
 The second thing was, the memory component of it was,

15:55.460 --> 16:00.300
 is to say that we aren't just processing input,

16:00.300 --> 16:02.820
 we learn a model of the world.

16:02.820 --> 16:05.500
 And the memory stands for that model.

16:05.500 --> 16:07.340
 The point of the brain, the part of the neocortex,

16:07.340 --> 16:08.500
 it learns a model of the world.

16:08.500 --> 16:11.580
 We have to store things, our experiences,

16:11.580 --> 16:14.220
 in a form that leads to a model of the world.

16:14.220 --> 16:15.700
 So we can move around the world,

16:15.700 --> 16:17.380
 we can pick things up and do things and navigate

16:17.380 --> 16:18.220
 and know how it's going on.

16:18.220 --> 16:19.980
 So that's what the memory referred to.

16:19.980 --> 16:22.100
 And many people just, they were thinking about

16:22.100 --> 16:25.140
 like certain processes without memory at all.

16:25.140 --> 16:26.740
 They're just like processing things.

16:26.740 --> 16:29.020
 And then finally, the hierarchical component

16:29.020 --> 16:32.260
 was a reflection to that the neocortex,

16:32.260 --> 16:34.420
 although it's this uniform sheet of cells,

16:35.820 --> 16:37.580
 different parts of it project to other parts,

16:37.580 --> 16:39.340
 which project to other parts.

16:39.340 --> 16:43.060
 And there is a sort of rough hierarchy in terms of that.

16:43.060 --> 16:45.980
 So the hierarchical temporal memory is just saying,

16:45.980 --> 16:47.700
 look, we should be thinking about the brain

16:47.700 --> 16:52.020
 as time based, model memory based,

16:52.020 --> 16:54.780
 and hierarchical processing.

16:54.780 --> 16:58.180
 And that was a placeholder for a bunch of components

16:58.180 --> 17:00.860
 that we would then plug into that.

17:00.860 --> 17:02.620
 We still believe all those things I just said,

17:02.620 --> 17:06.980
 but we now know so much more that I'm stopping to use

17:06.980 --> 17:08.180
 the word hierarchical temporal memory yet

17:08.180 --> 17:11.340
 because it's insufficient to capture the stuff we know.

17:11.340 --> 17:13.660
 So again, it's not incorrect, but it's,

17:13.660 --> 17:15.820
 I now know more and I would rather describe it

17:15.820 --> 17:16.820
 more accurately.

17:16.820 --> 17:20.340
 Yeah, so you're basically, we could think of HTM

17:20.340 --> 17:24.780
 as emphasizing that there's three aspects of intelligence

17:24.780 --> 17:25.900
 that are important to think about

17:25.900 --> 17:28.900
 whatever the eventual theory it converges to.

17:28.900 --> 17:32.460
 So in terms of time, how do you think of nature of time

17:32.460 --> 17:33.860
 across different time scales?

17:33.860 --> 17:36.820
 So you mentioned things changing,

17:36.820 --> 17:39.140
 sensory inputs changing every 10, 20 minutes.

17:39.140 --> 17:42.100
 What about every few minutes, every few months and years?

17:42.100 --> 17:44.820
 Well, if you think about a neuroscience problem,

17:44.820 --> 17:49.620
 the brain problem, neurons themselves can stay active

17:49.620 --> 17:52.780
 for certain periods of time, parts of the brain

17:52.780 --> 17:54.260
 where they stay active for minutes.

17:54.260 --> 17:59.460
 You could hold a certain perception or an activity

17:59.460 --> 18:01.580
 for a certain period of time,

18:01.580 --> 18:04.820
 but most of them don't last that long.

18:04.820 --> 18:07.180
 And so if you think about your thoughts

18:07.180 --> 18:09.180
 are the activity of neurons,

18:09.180 --> 18:10.580
 if you're gonna wanna involve something

18:10.580 --> 18:11.980
 that happened a long time ago,

18:11.980 --> 18:14.420
 even just this morning, for example,

18:14.420 --> 18:16.420
 the neurons haven't been active throughout that time.

18:16.420 --> 18:17.860
 So you have to store that.

18:17.860 --> 18:20.860
 So if I ask you, what did you have for breakfast today?

18:20.860 --> 18:23.660
 That is memory, that is you've built into your model

18:23.660 --> 18:24.860
 the world now, you remember that.

18:24.860 --> 18:27.780
 And that memory is in the synapses,

18:27.780 --> 18:29.980
 is basically in the formation of synapses.

18:29.980 --> 18:34.700
 And so you're sliding into what,

18:34.700 --> 18:36.660
 you know, it's the different timescales.

18:36.660 --> 18:39.060
 There's timescales of which we are like understanding

18:39.060 --> 18:41.260
 my language and moving about and seeing things rapidly

18:41.260 --> 18:42.540
 and over time, that's the timescales

18:42.540 --> 18:44.220
 of activities of neurons.

18:44.220 --> 18:46.140
 But if you wanna get in longer timescales,

18:46.140 --> 18:47.100
 then it's more memory.

18:47.100 --> 18:49.460
 And we have to invoke those memories to say,

18:49.460 --> 18:51.740
 oh yes, well now I can remember what I had for breakfast

18:51.740 --> 18:54.180
 because I stored that someplace.

18:54.180 --> 18:58.140
 I may forget it tomorrow, but I'd store it for now.

18:58.140 --> 19:01.620
 So does memory also need to have,

19:02.820 --> 19:06.180
 so the hierarchical aspect of reality

19:06.180 --> 19:08.780
 is not just about concepts, it's also about time?

19:08.780 --> 19:10.260
 Do you think of it that way?

19:10.260 --> 19:12.820
 Yeah, time is infused in everything.

19:12.820 --> 19:15.540
 It's like you really can't separate it out.

19:15.540 --> 19:18.700
 If I ask you, what is your, you know,

19:18.700 --> 19:21.340
 how's the brain learn a model of this coffee cup here?

19:21.340 --> 19:23.220
 I have a coffee cup and I'm at the coffee cup.

19:23.220 --> 19:25.980
 I say, well, time is not an inherent property

19:25.980 --> 19:28.540
 of the model I have of this cup,

19:28.540 --> 19:31.460
 whether it's a visual model or a tactile model.

19:31.460 --> 19:32.580
 I can sense it through time,

19:32.580 --> 19:34.900
 but the model itself doesn't really have much time.

19:34.900 --> 19:36.420
 If I asked you, if I said,

19:36.420 --> 19:38.980
 well, what is the model of my cell phone?

19:38.980 --> 19:40.740
 My brain has learned a model of the cell phone.

19:40.740 --> 19:43.380
 So if you have a smartphone like this,

19:43.380 --> 19:45.700
 and I said, well, this has time aspects to it.

19:45.700 --> 19:48.040
 I have expectations when I turn it on,

19:48.040 --> 19:50.460
 what's gonna happen, what or how long it's gonna take

19:50.460 --> 19:52.860
 to do certain things, if I bring up an app,

19:52.860 --> 19:54.540
 what sequences, and so I have,

19:54.540 --> 19:57.260
 and it's like melodies in the world, you know?

19:57.260 --> 19:58.540
 Melody has a sense of time.

19:58.540 --> 20:01.220
 So many things in the world move and act,

20:01.220 --> 20:03.740
 and there's a sense of time related to them.

20:03.740 --> 20:08.260
 Some don't, but most things do actually.

20:08.260 --> 20:12.100
 So it's sort of infused throughout the models of the world.

20:12.100 --> 20:13.700
 You build a model of the world,

20:13.700 --> 20:16.420
 you're learning the structure of the objects in the world,

20:16.420 --> 20:18.980
 and you're also learning how those things change

20:18.980 --> 20:20.780
 through time.

20:20.780 --> 20:23.900
 Okay, so it really is just a fourth dimension

20:23.900 --> 20:26.760
 that's infused deeply, and you have to make sure

20:26.760 --> 20:30.080
 that your models of intelligence incorporate it.

20:30.940 --> 20:34.840
 So, like you mentioned, the state of neuroscience

20:34.840 --> 20:37.800
 is deeply empirical, a lot of data collection.

20:37.800 --> 20:41.420
 It's, you know, that's where it is.

20:41.420 --> 20:43.100
 You mentioned Thomas Kuhn, right?

20:43.100 --> 20:44.580
 Yeah.

20:44.580 --> 20:48.020
 And then you're proposing a theory of intelligence,

20:48.020 --> 20:50.460
 and which is really the next step,

20:50.460 --> 20:52.900
 the really important step to take,

20:52.900 --> 20:57.900
 but why is HTM, or what we'll talk about soon,

21:00.840 --> 21:03.700
 the right theory?

21:03.700 --> 21:07.700
 So is it more in the, is it backed by intuition?

21:07.700 --> 21:09.920
 Is it backed by evidence?

21:09.920 --> 21:11.980
 Is it backed by a mixture of both?

21:11.980 --> 21:15.580
 Is it kind of closer to where string theory is in physics,

21:15.580 --> 21:18.460
 where there's mathematical components

21:18.460 --> 21:21.060
 which show that, you know what,

21:21.060 --> 21:24.740
 it seems that this, it fits together too well

21:24.740 --> 21:28.100
 for it not to be true, which is where string theory is.

21:28.100 --> 21:29.500
 Is that where you're kind of seeing?

21:29.500 --> 21:30.740
 It's a mixture of all those things,

21:30.740 --> 21:32.780
 although definitely where we are right now

21:32.780 --> 21:34.620
 is definitely much more on the empirical side

21:34.620 --> 21:36.180
 than, let's say, string theory.

21:37.060 --> 21:39.280
 The way this goes about, we're theorists, right?

21:39.280 --> 21:41.580
 So we look at all this data, and we're trying to come up

21:41.580 --> 21:44.340
 with some sort of model that explains it, basically,

21:44.340 --> 21:46.860
 and there's, unlike string theory,

21:46.860 --> 21:50.220
 there's vast more amounts of empirical data here

21:50.220 --> 21:53.340
 that I think than most physicists deal with.

21:54.660 --> 21:57.540
 And so our challenge is to sort through that

21:57.540 --> 22:02.020
 and figure out what kind of constructs would explain this.

22:02.020 --> 22:04.940
 And when we have an idea,

22:04.940 --> 22:06.400
 you come up with a theory of some sort,

22:06.400 --> 22:08.740
 you have lots of ways of testing it.

22:08.740 --> 22:13.740
 First of all, there are 100 years of assimilated,

22:13.740 --> 22:16.620
 assimilated, unassimilated empirical data from neuroscience.

22:16.620 --> 22:18.140
 So we go back and read papers,

22:18.140 --> 22:20.680
 and we say, oh, did someone find this already?

22:20.680 --> 22:23.280
 We can predict X, Y, and Z,

22:23.280 --> 22:25.220
 and maybe no one's even talked about it

22:25.220 --> 22:28.180
 since 1972 or something, but we go back and find that,

22:28.180 --> 22:31.140
 and we say, oh, either it can support the theory

22:31.140 --> 22:33.420
 or it can invalidate the theory.

22:33.420 --> 22:34.880
 And we say, okay, we have to start over again.

22:34.880 --> 22:38.140
 Oh, no, it's supportive, let's keep going with that one.

22:38.140 --> 22:42.260
 So the way I kind of view it, when we do our work,

22:42.260 --> 22:45.460
 we look at all this empirical data,

22:45.460 --> 22:47.700
 and what I call it is a set of constraints.

22:47.700 --> 22:48.700
 We're not interested in something

22:48.700 --> 22:49.900
 that's biologically inspired.

22:49.900 --> 22:52.140
 We're trying to figure out how the actual brain works.

22:52.140 --> 22:53.660
 So every piece of empirical data

22:53.660 --> 22:55.500
 is a constraint on a theory.

22:55.500 --> 22:57.020
 In theory, if you have the correct theory,

22:57.020 --> 22:59.420
 it needs to explain every pin, right?

22:59.420 --> 23:03.140
 So we have this huge number of constraints on the problem,

23:03.140 --> 23:05.960
 which initially makes it very, very difficult.

23:05.960 --> 23:07.220
 If you don't have many constraints,

23:07.220 --> 23:08.500
 you can make up stuff all the day.

23:08.500 --> 23:10.200
 You can say, oh, here's an answer on how you can do this,

23:10.200 --> 23:11.360
 you can do that, you can do this.

23:11.360 --> 23:13.760
 But if you consider all biology as a set of constraints,

23:13.760 --> 23:15.580
 all neuroscience as a set of constraints,

23:15.580 --> 23:17.240
 and even if you're working in one little part

23:17.240 --> 23:18.380
 of the neocortex, for example,

23:18.380 --> 23:20.620
 there are hundreds and hundreds of constraints.

23:20.620 --> 23:22.460
 These are empirical constraints

23:22.460 --> 23:24.840
 that it's very, very difficult initially

23:24.840 --> 23:27.260
 to come up with a theoretical framework for that.

23:27.260 --> 23:30.100
 But when you do, and it solves all those constraints

23:30.100 --> 23:32.980
 at once, you have a high confidence

23:32.980 --> 23:35.660
 that you got something close to correct.

23:35.660 --> 23:39.160
 It's just mathematically almost impossible not to be.

23:39.160 --> 23:43.900
 So that's the curse and the advantage of what we have.

23:43.900 --> 23:45.260
 The curse is we have to solve,

23:45.260 --> 23:48.960
 we have to meet all these constraints, which is really hard.

23:48.960 --> 23:50.900
 But when you do meet them,

23:50.900 --> 23:53.220
 then you have a great confidence

23:53.220 --> 23:54.940
 that you've discovered something.

23:54.940 --> 23:58.040
 In addition, then we work with scientific labs.

23:58.040 --> 24:00.000
 So we'll say, oh, there's something we can't find,

24:00.000 --> 24:01.260
 we can predict something,

24:01.260 --> 24:04.180
 but we can't find it anywhere in the literature.

24:04.180 --> 24:06.900
 So we will then, we have people we've collaborated with,

24:06.900 --> 24:09.220
 we'll say, sometimes they'll say, you know what?

24:09.220 --> 24:11.740
 I have some collected data, which I didn't publish,

24:11.740 --> 24:13.020
 but we can go back and look at it

24:13.020 --> 24:14.780
 and see if we can find that,

24:14.780 --> 24:17.020
 which is much easier than designing a new experiment.

24:17.020 --> 24:20.340
 You know, neuroscience experiments take a long time, years.

24:20.340 --> 24:23.300
 So, although some people are doing that now too.

24:23.300 --> 24:26.840
 So, but between all of these things,

24:27.740 --> 24:29.100
 I think it's a reasonable,

24:30.020 --> 24:31.620
 actually a very, very good approach.

24:31.620 --> 24:35.020
 We are blessed with the fact that we can test our theories

24:35.020 --> 24:37.100
 out the yin yang here because there's so much

24:37.100 --> 24:39.640
 unassimilar data and we can also falsify our theories

24:39.640 --> 24:41.460
 very easily, which we do often.

24:41.460 --> 24:44.380
 So it's kind of reminiscent to whenever that was

24:44.380 --> 24:47.300
 with Copernicus, you know, when you figure out

24:47.300 --> 24:51.140
 that the sun's at the center of the solar system

24:51.140 --> 24:54.900
 as opposed to earth, the pieces just fall into place.

24:54.900 --> 24:59.580
 Yeah, I think that's the general nature of aha moments

24:59.580 --> 25:02.020
 is, and it's Copernicus, it could be,

25:02.020 --> 25:05.220
 you could say the same thing about Darwin,

25:05.220 --> 25:07.580
 you could say the same thing about, you know,

25:07.580 --> 25:09.660
 about the double helix,

25:09.660 --> 25:12.780
 that people have been working on a problem for so long

25:12.780 --> 25:14.580
 and have all this data and they can't make sense of it,

25:14.580 --> 25:15.820
 they can't make sense of it.

25:15.820 --> 25:17.420
 But when the answer comes to you

25:17.420 --> 25:19.380
 and everything falls into place,

25:19.380 --> 25:21.660
 it's like, oh my gosh, that's it.

25:21.660 --> 25:23.080
 That's got to be right.

25:23.080 --> 25:28.080
 I asked both Jim Watson and Francis Crick about this.

25:29.140 --> 25:31.700
 I asked them, you know, when you were working on

25:31.700 --> 25:34.460
 trying to discover the structure of the double helix,

25:35.760 --> 25:39.620
 and when you came up with the sort of the structure

25:39.620 --> 25:44.020
 that ended up being correct, but it was sort of a guess,

25:44.020 --> 25:45.700
 you know, it wasn't really verified yet.

25:45.700 --> 25:48.460
 I said, did you know that it was right?

25:48.460 --> 25:50.220
 And they both said, absolutely.

25:50.220 --> 25:51.860
 So we absolutely knew it was right.

25:51.860 --> 25:54.740
 And it doesn't matter if other people didn't believe it

25:54.740 --> 25:55.660
 or not, we knew it was right.

25:55.660 --> 25:56.700
 They'd get around to thinking it

25:56.700 --> 25:59.060
 and agree with it eventually anyway.

25:59.060 --> 26:01.300
 And that's the kind of thing you hear a lot with scientists

26:01.300 --> 26:04.220
 who really are studying a difficult problem.

26:04.220 --> 26:07.140
 And I feel that way too about our work.

26:07.140 --> 26:10.700
 Have you talked to Crick or Watson about the problem

26:10.700 --> 26:15.700
 you're trying to solve, the, of finding the DNA of the brain?

26:15.940 --> 26:19.960
 Yeah, in fact, Francis Crick was very interested in this

26:19.960 --> 26:21.540
 in the latter part of his life.

26:21.540 --> 26:23.780
 And in fact, I got interested in brains

26:23.780 --> 26:26.900
 by reading an essay he wrote in 1979

26:26.900 --> 26:28.800
 called Thinking About the Brain.

26:28.800 --> 26:32.620
 And that was when I decided I'm gonna leave my profession

26:32.620 --> 26:35.580
 of computers and engineering and become a neuroscientist.

26:35.580 --> 26:37.660
 Just reading that one essay from Francis Crick.

26:37.660 --> 26:39.840
 I got to meet him later in life.

26:41.640 --> 26:44.660
 I spoke at the Salk Institute and he was in the audience.

26:44.660 --> 26:46.660
 And then I had a tea with him afterwards.

26:48.820 --> 26:50.620
 He was interested in a different problem.

26:50.620 --> 26:52.380
 He was focused on consciousness.

26:53.380 --> 26:54.260
 The easy problem, right?

26:54.260 --> 26:58.640
 Well, I think it's the red herring.

26:58.640 --> 27:01.300
 And so we weren't really overlapping a lot there.

27:02.260 --> 27:04.580
 Jim Watson, who's still alive,

27:05.380 --> 27:07.420
 is also interested in this problem.

27:07.420 --> 27:09.020
 And he was, when he was director

27:09.020 --> 27:11.140
 of the Cold Spring Harbor Laboratories,

27:12.420 --> 27:15.140
 he was really sort of behind moving in the direction

27:15.140 --> 27:16.580
 of neuroscience there.

27:16.580 --> 27:19.300
 And so he had a personal interest in this field.

27:20.220 --> 27:22.280
 And I have met with him numerous times.

27:23.620 --> 27:27.680
 And in fact, the last time was a little bit over a year ago,

27:27.680 --> 27:30.340
 I gave a talk at Cold Spring Harbor Labs

27:30.340 --> 27:34.620
 about the progress we were making in our work.

27:34.620 --> 27:39.620
 And it was a lot of fun because he said,

27:39.860 --> 27:41.100
 well, you wouldn't be coming here

27:41.100 --> 27:42.380
 unless you had something important to say.

27:42.380 --> 27:44.740
 So I'm gonna go attend your talk.

27:44.740 --> 27:46.620
 So he sat in the very front row.

27:46.620 --> 27:50.140
 Next to him was the director of the lab, Bruce Stillman.

27:50.140 --> 27:52.540
 So these guys are in the front row of this auditorium.

27:52.540 --> 27:54.620
 Nobody else in the auditorium wants to sit in the front row

27:54.620 --> 27:56.980
 because there's Jim Watson and there's the director.

27:56.980 --> 28:01.980
 And I gave a talk and then I had dinner with him afterwards.

28:03.700 --> 28:07.060
 But there's a great picture of my colleague Subitai Amantak

28:07.060 --> 28:09.860
 where I'm up there sort of like screaming the basics

28:09.860 --> 28:11.700
 of this new framework we have.

28:11.700 --> 28:13.780
 And Jim Watson's on the edge of his chair.

28:13.780 --> 28:15.180
 He's literally on the edge of his chair,

28:15.180 --> 28:17.820
 like intently staring up at the screen.

28:17.820 --> 28:21.740
 And when he discovered the structure of DNA,

28:21.740 --> 28:23.800
 the first public talk he gave

28:23.800 --> 28:25.940
 was at Cold Spring Harbor Labs.

28:25.940 --> 28:27.460
 And there's a picture, there's a famous picture

28:27.460 --> 28:29.340
 of Jim Watson standing at the whiteboard

28:29.340 --> 28:31.540
 with an overrated thing pointing at something,

28:31.540 --> 28:33.180
 pointing at the double helix with his pointer.

28:33.180 --> 28:34.980
 And it actually looks a lot like the picture of me.

28:34.980 --> 28:36.100
 So there was a sort of funny,

28:36.100 --> 28:37.460
 there's Arian talking about the brain

28:37.460 --> 28:39.300
 and there's Jim Watson staring intently at it.

28:39.300 --> 28:41.620
 And of course there with, whatever, 60 years earlier,

28:41.620 --> 28:44.260
 he was standing pointing at the double helix.

28:44.260 --> 28:47.260
 That's one of the great discoveries in all of,

28:47.260 --> 28:49.740
 whatever, biology, science, all science and DNA.

28:49.740 --> 28:54.540
 So it's funny that there's echoes of that in your presentation.

28:54.540 --> 28:58.360
 Do you think, in terms of evolutionary timeline and history,

28:58.360 --> 29:01.960
 the development of the neocortex was a big leap?

29:01.960 --> 29:06.060
 Or is it just a small step?

29:07.020 --> 29:09.780
 So like, if we ran the whole thing over again,

29:09.780 --> 29:12.660
 from the birth of life on Earth,

29:12.660 --> 29:15.260
 how likely would we develop the mechanism of the neocortex?

29:15.260 --> 29:17.220
 Okay, well those are two separate questions.

29:17.220 --> 29:18.660
 One is, was it a big leap?

29:18.660 --> 29:21.380
 And one was how likely it is, okay?

29:21.380 --> 29:22.880
 They're not necessarily related.

29:22.880 --> 29:23.720
 Maybe correlated.

29:23.720 --> 29:25.100
 Maybe correlated, maybe not.

29:25.100 --> 29:26.100
 And we don't really have enough data

29:26.100 --> 29:28.100
 to make a judgment about that.

29:28.100 --> 29:29.980
 I would say definitely it was a big leap.

29:29.980 --> 29:30.980
 And I can tell you why.

29:30.980 --> 29:34.060
 I don't think it was just another incremental step.

29:34.060 --> 29:35.900
 I don't get that at the moment.

29:35.900 --> 29:38.420
 I don't really have any idea how likely it is.

29:38.420 --> 29:39.860
 If we look at evolution,

29:39.860 --> 29:42.540
 we have one data point, which is Earth, right?

29:42.540 --> 29:45.220
 Life formed on Earth billions of years ago,

29:45.220 --> 29:48.100
 whether it was introduced here or it created it here,

29:48.100 --> 29:49.560
 or someone introduced it, we don't really know,

29:49.560 --> 29:51.220
 but it was here early.

29:51.220 --> 29:55.140
 It took a long, long time to get to multicellular life.

29:55.140 --> 29:57.200
 And then for multicellular life,

29:58.940 --> 30:02.300
 it took a long, long time to get the neocortex.

30:02.300 --> 30:05.460
 And we've only had the neocortex for a few 100,000 years.

30:05.460 --> 30:08.000
 So that's like nothing, okay?

30:08.000 --> 30:09.600
 So is it likely?

30:09.600 --> 30:10.740
 Well, it certainly isn't something

30:10.740 --> 30:13.560
 that happened right away on Earth.

30:13.560 --> 30:15.200
 And there were multiple steps to get there.

30:15.200 --> 30:17.220
 So I would say it's probably not gonna be something

30:17.220 --> 30:18.260
 that would happen instantaneously

30:18.260 --> 30:20.620
 on other planets that might have life.

30:20.620 --> 30:23.160
 It might take several billion years on average.

30:23.160 --> 30:24.380
 Is it likely?

30:24.380 --> 30:25.740
 I don't know, but you'd have to survive

30:25.740 --> 30:27.900
 for several billion years to find out.

30:27.900 --> 30:29.340
 Probably.

30:29.340 --> 30:30.260
 Is it a big leap?

30:30.260 --> 30:35.260
 Yeah, I think it is a qualitative difference

30:35.500 --> 30:37.860
 in all other evolutionary steps.

30:37.860 --> 30:39.820
 I can try to describe that if you'd like.

30:39.820 --> 30:41.980
 Sure, in which way?

30:41.980 --> 30:43.940
 Yeah, I can tell you how.

30:43.940 --> 30:47.740
 Pretty much, let's start with a little preface.

30:47.740 --> 30:50.500
 Many of the things that humans are able to do

30:50.500 --> 30:55.500
 do not have obvious survival advantages precedent.

30:58.620 --> 31:00.260
 We could create music, is that,

31:00.260 --> 31:02.700
 is there a really survival advantage to that?

31:02.700 --> 31:03.900
 Maybe, maybe not.

31:03.900 --> 31:04.900
 What about mathematics?

31:04.900 --> 31:07.020
 Is there a real survival advantage to mathematics?

31:07.020 --> 31:09.340
 Well, you could stretch it.

31:09.340 --> 31:11.540
 You can try to figure these things out, right?

31:13.140 --> 31:14.800
 But most of evolutionary history,

31:14.800 --> 31:18.700
 everything had immediate survival advantages to it.

31:18.700 --> 31:22.020
 So, I'll tell you a story, which I like,

31:22.020 --> 31:26.500
 may or may not be true, but the story goes as follows.

31:29.140 --> 31:30.860
 Organisms have been evolving for,

31:30.860 --> 31:33.740
 since the beginning of life here on Earth,

31:33.740 --> 31:35.700
 and adding this sort of complexity onto that,

31:35.700 --> 31:36.860
 and this sort of complexity onto that,

31:36.860 --> 31:39.700
 and the brain itself is evolved this way.

31:39.700 --> 31:42.420
 In fact, there's old parts, and older parts,

31:42.420 --> 31:43.740
 and older, older parts of the brain

31:43.740 --> 31:45.500
 that kind of just keeps calming on new things,

31:45.500 --> 31:47.260
 and we keep adding capabilities.

31:47.260 --> 31:48.700
 When we got to the neocortex,

31:48.700 --> 31:52.500
 initially it had a very clear survival advantage

31:52.500 --> 31:54.380
 in that it produced better vision,

31:54.380 --> 31:55.700
 and better hearing, and better touch,

31:55.700 --> 31:57.780
 and maybe, and so on.

31:57.780 --> 32:01.140
 But what I think happens is that evolution discovered,

32:01.140 --> 32:05.100
 it took a mechanism, and this is in our recent theories,

32:05.100 --> 32:08.140
 but it took a mechanism evolved a long time ago

32:08.140 --> 32:10.380
 for navigating in the world, for knowing where you are.

32:10.380 --> 32:13.360
 These are the so called grid cells and place cells

32:13.360 --> 32:15.160
 of an old part of the brain.

32:15.160 --> 32:20.160
 And it took that mechanism for building maps of the world,

32:20.900 --> 32:22.580
 and knowing where you are on those maps,

32:22.580 --> 32:24.140
 and how to navigate those maps,

32:24.140 --> 32:27.060
 and turns it into a sort of a slimmed down,

32:27.060 --> 32:28.380
 idealized version of it.

32:29.540 --> 32:31.600
 And that idealized version could now apply

32:31.600 --> 32:32.820
 to building maps of other things.

32:32.820 --> 32:35.100
 Maps of coffee cups, and maps of phones,

32:35.100 --> 32:36.460
 maps of mathematics.

32:36.460 --> 32:37.300
 Concepts almost.

32:37.300 --> 32:40.260
 Concepts, yes, and not just almost, exactly.

32:40.260 --> 32:44.140
 And so, and it just started replicating this stuff, right?

32:44.140 --> 32:45.220
 You just think more, and more, and more.

32:45.220 --> 32:48.780
 So we went from being sort of dedicated purpose

32:48.780 --> 32:51.460
 neural hardware to solve certain problems

32:51.460 --> 32:53.200
 that are important to survival,

32:53.200 --> 32:55.820
 to a general purpose neural hardware

32:55.820 --> 32:58.100
 that could be applied to all problems.

32:58.100 --> 33:01.700
 And now it's escaped the orbit of survival.

33:02.600 --> 33:04.460
 We are now able to apply it to things

33:04.460 --> 33:06.760
 which we find enjoyment,

33:08.700 --> 33:13.700
 but aren't really clearly survival characteristics.

33:13.700 --> 33:16.740
 And that it seems to only have happened in humans,

33:16.740 --> 33:18.260
 to the large extent.

33:19.260 --> 33:20.980
 And so that's what's going on,

33:20.980 --> 33:22.940
 where we sort of have,

33:22.940 --> 33:26.360
 we've sort of escaped the gravity of evolutionary pressure,

33:26.360 --> 33:28.620
 in some sense, in the neocortex.

33:28.620 --> 33:31.540
 And it now does things which are not,

33:31.540 --> 33:32.780
 that are really interesting,

33:32.780 --> 33:34.340
 discovering models of the universe,

33:34.340 --> 33:36.100
 which may not really help us.

33:36.100 --> 33:37.100
 Does it matter?

33:37.100 --> 33:38.600
 How does it help us surviving,

33:38.600 --> 33:40.240
 knowing that there might be multiverses,

33:40.240 --> 33:42.940
 or that there might be the age of the universe,

33:42.940 --> 33:46.140
 or how do various stellar things occur?

33:46.140 --> 33:47.820
 It doesn't really help us survive at all.

33:47.820 --> 33:50.460
 But we enjoy it, and that's what happened.

33:50.460 --> 33:53.300
 Or at least not in the obvious way, perhaps.

33:53.300 --> 33:54.880
 It is required,

33:56.200 --> 33:58.540
 if you look at the entire universe in an evolutionary way,

33:58.540 --> 34:00.900
 it's required for us to do interplanetary travel,

34:00.900 --> 34:03.140
 and therefore survive past our own sun.

34:03.140 --> 34:04.500
 But you know, let's not get too.

34:04.500 --> 34:07.220
 Yeah, but evolution works at one time frame,

34:07.220 --> 34:11.340
 it's survival, if you think of survival of the phenotype,

34:11.340 --> 34:13.180
 survival of the individual.

34:13.180 --> 34:16.360
 What you're talking about there is spans well beyond that.

34:16.360 --> 34:18.740
 So there's no genetic,

34:18.740 --> 34:23.420
 I'm not transferring any genetic traits to my children

34:23.420 --> 34:26.540
 that are gonna help them survive better on Mars.

34:26.540 --> 34:28.260
 Totally different mechanism, that's right.

34:28.260 --> 34:31.340
 So let's get into the new, as you've mentioned,

34:31.340 --> 34:34.860
 this idea of the, I don't know if you have a nice name,

34:34.860 --> 34:35.700
 thousand.

34:35.700 --> 34:37.340
 We call it the thousand brain theory of intelligence.

34:37.340 --> 34:38.180
 I like it.

34:38.180 --> 34:43.180
 Can you talk about this idea of a spatial view of concepts

34:43.620 --> 34:44.460
 and so on?

34:44.460 --> 34:46.500
 Yeah, so can I just describe sort of the,

34:46.500 --> 34:49.300
 there's an underlying core discovery,

34:49.300 --> 34:51.140
 which then everything comes from that.

34:51.140 --> 34:54.580
 That's a very simple, this is really what happened.

34:55.660 --> 34:58.580
 We were deep into problems about understanding

34:58.580 --> 35:00.540
 how we build models of stuff in the world

35:00.540 --> 35:03.020
 and how we make predictions about things.

35:03.020 --> 35:07.220
 And I was holding a coffee cup just like this in my hand.

35:07.220 --> 35:10.540
 And my finger was touching the side, my index finger.

35:10.540 --> 35:12.700
 And then I moved it to the top

35:12.700 --> 35:15.460
 and I was gonna feel the rim at the top of the cup.

35:15.460 --> 35:18.280
 And I asked myself a very simple question.

35:18.280 --> 35:20.100
 I said, well, first of all, I say,

35:20.100 --> 35:22.260
 I know that my brain predicts what it's gonna feel

35:22.260 --> 35:23.300
 before it touches it.

35:23.300 --> 35:26.040
 You can just think about it and imagine it.

35:26.040 --> 35:27.660
 And so we know that the brain's making predictions

35:27.660 --> 35:28.500
 all the time.

35:28.500 --> 35:31.540
 So the question is, what does it take to predict that?

35:31.540 --> 35:33.620
 And there's a very interesting answer.

35:33.620 --> 35:35.400
 First of all, it says the brain has to know

35:35.400 --> 35:36.500
 it's touching a coffee cup.

35:36.500 --> 35:38.020
 It has to have a model of a coffee cup.

35:38.020 --> 35:41.020
 It needs to know where the finger currently is

35:41.020 --> 35:43.260
 on the cup relative to the cup.

35:43.260 --> 35:44.420
 Because when I make a movement,

35:44.420 --> 35:46.340
 it needs to know where it's going to be on the cup

35:46.340 --> 35:50.380
 after the movement is completed relative to the cup.

35:50.380 --> 35:51.900
 And then it can make a prediction

35:51.900 --> 35:53.340
 about what it's gonna sense.

35:53.340 --> 35:54.960
 So this told me that the neocortex,

35:54.960 --> 35:56.380
 which is making this prediction,

35:56.380 --> 35:59.420
 needs to know that it's sensing it's touching a cup.

35:59.420 --> 36:01.420
 And it needs to know the location of my finger

36:01.420 --> 36:04.380
 relative to that cup in a reference frame of the cup.

36:04.380 --> 36:06.300
 It doesn't matter where the cup is relative to my body.

36:06.300 --> 36:08.260
 It doesn't matter its orientation.

36:08.260 --> 36:09.160
 None of that matters.

36:09.160 --> 36:10.940
 It's where my finger is relative to the cup,

36:10.940 --> 36:13.540
 which tells me then that the neocortex

36:13.540 --> 36:17.340
 has a reference frame that's anchored to the cup.

36:17.340 --> 36:19.280
 Because otherwise I wouldn't be able to say the location

36:19.280 --> 36:21.500
 and I wouldn't be able to predict my new location.

36:21.500 --> 36:24.120
 And then we quickly, very instantly can say,

36:24.120 --> 36:26.240
 well, every part of my skin could touch this cup.

36:26.240 --> 36:28.100
 And therefore every part of my skin is making predictions

36:28.100 --> 36:30.940
 and every part of my skin must have a reference frame

36:30.940 --> 36:33.520
 that it's using to make predictions.

36:33.520 --> 36:38.520
 So the big idea is that throughout the neocortex,

36:39.500 --> 36:44.500
 there are, everything is being stored

36:44.940 --> 36:46.740
 and referenced in reference frames.

36:46.740 --> 36:48.820
 You can think of them like XYZ reference frames,

36:48.820 --> 36:50.380
 but they're not like that.

36:50.380 --> 36:52.060
 We know a lot about the neural mechanisms for this,

36:52.060 --> 36:54.860
 but the brain thinks in reference frames.

36:54.860 --> 36:56.700
 And as an engineer, if you're an engineer,

36:56.700 --> 36:57.740
 this is not surprising.

36:57.740 --> 37:00.340
 You'd say, if I wanted to build a CAD model

37:00.340 --> 37:02.120
 of the coffee cup, well, I would bring it up

37:02.120 --> 37:04.100
 and some CAD software, and I would assign

37:04.100 --> 37:05.460
 some reference frame and say this features

37:05.460 --> 37:06.980
 at this locations and so on.

37:06.980 --> 37:09.700
 But the fact that this, the idea that this is occurring

37:09.700 --> 37:14.360
 throughout the neocortex everywhere, it was a novel idea.

37:14.360 --> 37:19.080
 And then a zillion things fell into place after that,

37:19.080 --> 37:19.940
 a zillion.

37:19.940 --> 37:21.860
 So now we think about the neocortex

37:21.860 --> 37:23.420
 as processing information quite differently

37:23.420 --> 37:24.260
 than we used to do it.

37:24.260 --> 37:25.540
 We used to think about the neocortex

37:25.540 --> 37:28.700
 as processing sensory data and extracting features

37:28.700 --> 37:30.860
 from that sensory data and then extracting features

37:30.860 --> 37:33.580
 from the features, very much like a deep learning network

37:33.580 --> 37:34.900
 does today.

37:34.900 --> 37:36.620
 But that's not how the brain works at all.

37:36.620 --> 37:39.300
 The brain works by assigning everything,

37:39.300 --> 37:41.860
 every input, everything to reference frames.

37:41.860 --> 37:44.380
 And there are thousands, hundreds of thousands

37:44.380 --> 37:46.420
 of them active at once in your neocortex.

37:47.660 --> 37:49.580
 It's a surprising thing to think about,

37:49.580 --> 37:51.060
 but once you sort of internalize this,

37:51.060 --> 37:54.380
 you understand that it explains almost every,

37:54.380 --> 37:57.780
 almost all the mysteries we've had about this structure.

37:57.780 --> 38:00.200
 So one of the consequences of that

38:00.200 --> 38:02.620
 is that every small part of the neocortex,

38:02.620 --> 38:06.340
 say a millimeter square, and there's 150,000 of those.

38:06.340 --> 38:08.620
 So it's about 150,000 square millimeters.

38:08.620 --> 38:11.380
 If you take every little square millimeter of the cortex,

38:11.380 --> 38:13.260
 it's got some input coming into it

38:13.260 --> 38:14.940
 and it's gonna have reference frames

38:14.940 --> 38:16.800
 where it's assigned that input to.

38:16.800 --> 38:19.320
 And each square millimeter can learn

38:19.320 --> 38:20.980
 complete models of objects.

38:20.980 --> 38:22.020
 So what do I mean by that?

38:22.020 --> 38:23.300
 If I'm touching the coffee cup,

38:23.300 --> 38:25.580
 well, if I just touch it in one place,

38:25.580 --> 38:27.180
 I can't learn what this coffee cup is

38:27.180 --> 38:28.980
 because I'm just feeling one part.

38:28.980 --> 38:31.060
 But if I move it around the cup

38:31.060 --> 38:32.540
 and touch it at different areas,

38:32.540 --> 38:34.060
 I can build up a complete model of the cup

38:34.060 --> 38:36.700
 because I'm now filling in that three dimensional map,

38:36.700 --> 38:37.540
 which is the coffee cup.

38:37.540 --> 38:38.660
 I can say, oh, what am I feeling

38:38.660 --> 38:39.900
 at all these different locations?

38:39.900 --> 38:43.020
 That's the basic idea, it's more complicated than that.

38:43.020 --> 38:46.220
 But so through time, and we talked about time earlier,

38:46.220 --> 38:48.180
 through time, even a single column,

38:48.180 --> 38:50.300
 which is only looking at, or a single part of the cortex,

38:50.300 --> 38:52.720
 which is only looking at a small part of the world,

38:52.720 --> 38:55.060
 can build up a complete model of an object.

38:55.060 --> 38:57.100
 And so if you think about the part of the brain,

38:57.100 --> 38:59.100
 which is getting input from all my fingers,

38:59.100 --> 39:01.700
 so they're spread across the top of your head here.

39:01.700 --> 39:04.040
 This is the somatosensory cortex.

39:04.040 --> 39:05.180
 There's columns associated

39:05.180 --> 39:07.380
 with all the different areas of my skin.

39:07.380 --> 39:10.100
 And what we believe is happening

39:10.100 --> 39:12.900
 is that all of them are building models of this cup,

39:12.900 --> 39:15.340
 every one of them, or things.

39:15.340 --> 39:16.620
 They're not all building,

39:16.620 --> 39:18.180
 not every column or every part of the cortex

39:18.180 --> 39:19.500
 builds models of everything,

39:19.500 --> 39:21.700
 but they're all building models of something.

39:21.700 --> 39:26.700
 And so you have, so when I touch this cup with my hand,

39:26.700 --> 39:28.980
 there are multiple models of the cup being invoked.

39:28.980 --> 39:30.460
 If I look at it with my eyes,

39:30.460 --> 39:32.540
 there are, again, many models of the cup being invoked,

39:32.540 --> 39:34.300
 because each part of the visual system,

39:34.300 --> 39:35.820
 the brain doesn't process an image.

39:35.820 --> 39:38.740
 That's a misleading idea.

39:38.740 --> 39:40.460
 It's just like your fingers touching the cup,

39:40.460 --> 39:41.300
 so different parts of my retina

39:41.300 --> 39:42.980
 are looking at different parts of the cup.

39:42.980 --> 39:45.540
 And thousands and thousands of models of the cup

39:45.540 --> 39:47.380
 are being invoked at once.

39:47.380 --> 39:48.900
 And they're all voting with each other,

39:48.900 --> 39:50.140
 trying to figure out what's going on.

39:50.140 --> 39:51.740
 So that's why we call it the thousand brains theory

39:51.740 --> 39:54.700
 of intelligence, because there isn't one model of a cup.

39:54.700 --> 39:56.300
 There are thousands of models of this cup.

39:56.300 --> 39:57.940
 There are thousands of models of your cellphone

39:57.940 --> 40:00.860
 and about cameras and microphones and so on.

40:00.860 --> 40:02.860
 It's a distributed modeling system,

40:02.860 --> 40:03.700
 which is very different

40:03.700 --> 40:04.860
 than the way people have thought about it.

40:04.860 --> 40:07.340
 And so that's a really compelling and interesting idea.

40:07.340 --> 40:08.700
 I have two first questions.

40:08.700 --> 40:12.060
 So one, on the ensemble part of everything coming together,

40:12.060 --> 40:13.620
 you have these thousand brains.

40:14.860 --> 40:17.900
 How do you know which one has done the best job

40:17.900 --> 40:18.740
 of forming the...

40:18.740 --> 40:19.580
 Great question.

40:19.580 --> 40:20.420
 Let me try to explain it.

40:20.420 --> 40:23.500
 There's a problem that's known in neuroscience

40:23.500 --> 40:25.220
 called the sensor fusion problem.

40:25.220 --> 40:26.060
 Yes.

40:26.060 --> 40:27.740
 And so the idea is there's something like,

40:27.740 --> 40:29.140
 oh, the image comes from the eye.

40:29.140 --> 40:30.620
 There's a picture on the retina

40:30.620 --> 40:32.380
 and then it gets projected to the neocortex.

40:32.380 --> 40:35.100
 Oh, by now it's all spread out all over the place

40:35.100 --> 40:37.100
 and it's kind of squirrely and distorted

40:37.100 --> 40:39.020
 and pieces are all over the...

40:39.020 --> 40:40.900
 It doesn't look like a picture anymore.

40:40.900 --> 40:43.660
 When does it all come back together again?

40:43.660 --> 40:45.380
 Or you might say, well, yes,

40:45.380 --> 40:48.620
 but I also have sounds or touches associated with the cup.

40:48.620 --> 40:50.660
 So I'm seeing the cup and touching the cup.

40:50.660 --> 40:52.620
 How do they get combined together again?

40:52.620 --> 40:54.260
 So it's called the sensor fusion problem.

40:54.260 --> 40:55.860
 As if all these disparate parts

40:55.860 --> 40:59.020
 have to be brought together into one model someplace.

40:59.020 --> 41:01.140
 That's the wrong idea.

41:01.140 --> 41:03.500
 The right idea is that you've got all these guys voting.

41:03.500 --> 41:05.420
 There's auditory models of the cup.

41:05.420 --> 41:06.620
 There's visual models of the cup.

41:06.620 --> 41:08.260
 There's tactile models of the cup.

41:09.860 --> 41:10.700
 In the vision system,

41:10.700 --> 41:12.580
 there might be ones that are more focused on black and white

41:12.580 --> 41:13.620
 and ones focusing on color.

41:13.620 --> 41:14.460
 It doesn't really matter.

41:14.460 --> 41:17.020
 There's just thousands and thousands of models of this cup.

41:17.020 --> 41:17.900
 And they vote.

41:17.900 --> 41:20.620
 They don't actually come together in one spot.

41:20.620 --> 41:21.900
 Just literally think of it this way.

41:21.900 --> 41:24.100
 Imagine you have these columns

41:24.100 --> 41:26.660
 that are like about the size of a little piece of spaghetti.

41:26.660 --> 41:28.500
 Like a two and a half millimeters tall

41:28.500 --> 41:30.020
 and about a millimeter in wide.

41:30.020 --> 41:33.300
 They're not physical, but you could think of them that way.

41:33.300 --> 41:35.300
 And each one's trying to guess what this thing is

41:35.300 --> 41:36.140
 or touching.

41:36.140 --> 41:38.060
 Now, they can do a pretty good job

41:38.060 --> 41:40.060
 if they're allowed to move over time.

41:40.060 --> 41:41.620
 So I can reach my hand into a black box

41:41.620 --> 41:43.540
 and move my finger around an object.

41:43.540 --> 41:45.540
 And if I touch enough spaces, I go, okay,

41:45.540 --> 41:46.980
 now I know what it is.

41:46.980 --> 41:48.300
 But often we don't do that.

41:48.300 --> 41:49.940
 Often I can just reach and grab something with my hand

41:49.940 --> 41:51.020
 all at once and I get it.

41:51.020 --> 41:53.740
 Or if I had to look through the world through a straw,

41:53.740 --> 41:55.860
 so I'm only invoking one little column,

41:55.860 --> 41:56.700
 I can only see part of something

41:56.700 --> 41:58.140
 because I have to move the straw around.

41:58.140 --> 42:00.460
 But if I open my eyes, I see the whole thing at once.

42:00.460 --> 42:01.460
 So what we think is going on

42:01.460 --> 42:03.180
 is all these little pieces of spaghetti,

42:03.180 --> 42:05.300
 if you will, all these little columns in the cortex,

42:05.300 --> 42:08.580
 are all trying to guess what it is that they're sensing.

42:08.580 --> 42:10.740
 They'll do a better guess if they have time

42:10.740 --> 42:11.700
 and can move over time.

42:11.700 --> 42:13.620
 So if I move my eyes, I move my fingers.

42:13.620 --> 42:16.580
 But if they don't, they have a poor guess.

42:16.580 --> 42:20.060
 It's a probabilistic guess of what they might be touching.

42:20.060 --> 42:22.940
 Now, imagine they can post their probability

42:22.940 --> 42:24.580
 at the top of a little piece of spaghetti.

42:24.580 --> 42:25.420
 Each one of them says,

42:25.420 --> 42:27.420
 I think, and it's not really a probability distribution.

42:27.420 --> 42:29.460
 It's more like a set of possibilities.

42:29.460 --> 42:31.980
 In the brain, it doesn't work as a probability distribution.

42:31.980 --> 42:34.020
 It works as more like what we call a union.

42:34.020 --> 42:35.860
 So you could say, and one column says,

42:35.860 --> 42:37.540
 I think it could be a coffee cup,

42:37.540 --> 42:39.940
 a soda can, or a water bottle.

42:39.940 --> 42:40.900
 And another column says,

42:40.900 --> 42:42.300
 I think it could be a coffee cup

42:42.300 --> 42:46.460
 or a telephone or a camera or whatever, right?

42:46.460 --> 42:49.940
 And all these guys are saying what they think it might be.

42:49.940 --> 42:51.620
 And there's these long range connections

42:51.620 --> 42:53.460
 in certain layers in the cortex.

42:53.460 --> 42:56.660
 So there's in some layers in some cells types

42:56.660 --> 43:00.060
 in each column, send the projections across the brain.

43:00.060 --> 43:01.740
 And that's the voting occurs.

43:01.740 --> 43:04.100
 And so there's a simple associative memory mechanism.

43:04.100 --> 43:06.140
 We've described this in a recent paper

43:06.140 --> 43:09.500
 and we've modeled this that says,

43:09.500 --> 43:11.940
 they can all quickly settle on the only

43:11.940 --> 43:14.900
 or the one best answer for all of them.

43:14.900 --> 43:16.420
 If there is a single best answer,

43:16.420 --> 43:18.940
 they all vote and say, yep, it's gotta be the coffee cup.

43:18.940 --> 43:21.060
 And at that point, they all know it's a coffee cup.

43:21.060 --> 43:23.380
 And at that point, everyone acts as if it's a coffee cup.

43:23.380 --> 43:24.220
 They're like, yep, we know it's a coffee,

43:24.220 --> 43:26.380
 even though I've only seen one little piece of this world,

43:26.380 --> 43:27.700
 I know it's a coffee cup I'm touching

43:27.700 --> 43:28.980
 or I'm seeing or whatever.

43:28.980 --> 43:30.900
 And so you can think of all these columns

43:30.900 --> 43:33.020
 are looking at different parts in different places,

43:33.020 --> 43:35.220
 different sensory input, different locations,

43:35.220 --> 43:36.180
 they're all different.

43:36.180 --> 43:40.460
 But this layer that's doing the voting, it solidifies.

43:40.460 --> 43:42.260
 It's just like it crystallizes and says,

43:42.260 --> 43:44.140
 oh, we all know what we're doing.

43:44.140 --> 43:46.460
 And so you don't bring these models together in one model,

43:46.460 --> 43:49.140
 you just vote and there's a crystallization of the vote.

43:49.140 --> 43:51.780
 Great, that's at least a compelling way

43:51.780 --> 43:56.780
 to think about the way you form a model of the world.

43:58.180 --> 44:00.420
 Now, you talk about a coffee cup.

44:00.420 --> 44:03.220
 Do you see this, as far as I understand,

44:03.220 --> 44:04.660
 you are proposing this as well,

44:04.660 --> 44:06.900
 that this extends to much more than coffee cups?

44:06.900 --> 44:07.740
 Yeah.

44:07.740 --> 44:09.540
 It does.

44:09.540 --> 44:10.780
 Or at least the physical world,

44:10.780 --> 44:14.100
 it expands to the world of concepts.

44:14.100 --> 44:15.020
 Yeah, it does.

44:15.020 --> 44:18.220
 And well, first, the primary thing is evidence for that

44:18.220 --> 44:20.700
 is that the regions of the neocortex

44:20.700 --> 44:22.340
 that are associated with language

44:22.340 --> 44:23.860
 or high level thought or mathematics

44:23.860 --> 44:24.700
 or things like that,

44:24.700 --> 44:26.180
 they look like the regions of the neocortex

44:26.180 --> 44:28.300
 that process vision, hearing, and touch.

44:28.300 --> 44:29.700
 They don't look any different.

44:29.700 --> 44:31.580
 Or they look only marginally different.

44:32.820 --> 44:36.420
 And so one would say, well, if Vernon Mountcastle,

44:36.420 --> 44:38.860
 who proposed that all the parts of the neocortex

44:38.860 --> 44:41.060
 do the same thing, if he's right,

44:41.060 --> 44:42.820
 then the parts that are doing language

44:42.820 --> 44:44.540
 or mathematics or physics

44:44.540 --> 44:45.700
 are working on the same principle.

44:45.700 --> 44:48.500
 They must be working on the principle of reference frames.

44:48.500 --> 44:50.100
 So that's a little odd thought.

44:51.820 --> 44:53.940
 But of course, we had no prior idea

44:53.940 --> 44:55.020
 how these things happen.

44:55.020 --> 44:56.420
 So let's go with that.

44:57.340 --> 44:59.900
 And we, in our recent paper,

44:59.900 --> 45:01.620
 we talked a little bit about that.

45:01.620 --> 45:02.820
 I've been working on it more since.

45:02.820 --> 45:05.380
 I have better ideas about it now.

45:05.380 --> 45:06.980
 I'm sitting here very confident

45:06.980 --> 45:08.020
 that that's what's happening.

45:08.020 --> 45:09.260
 And I can give you some examples

45:09.260 --> 45:11.220
 that help you think about that.

45:11.220 --> 45:12.500
 It's not we understand it completely,

45:12.500 --> 45:14.300
 but I understand it better than I've described it

45:14.300 --> 45:15.660
 in any paper so far.

45:15.660 --> 45:17.700
 So, but we did put that idea out there.

45:17.700 --> 45:18.940
 It says, okay, this is,

45:18.940 --> 45:22.620
 it's a good place to start, you know?

45:22.620 --> 45:24.900
 And the evidence would suggest it's how it's happening.

45:24.900 --> 45:26.660
 And then we can start tackling that problem

45:26.660 --> 45:27.500
 one piece at a time.

45:27.500 --> 45:29.060
 Like, what does it mean to do high level thought?

45:29.060 --> 45:30.020
 What does it mean to do language?

45:30.020 --> 45:34.220
 How would that fit into a reference frame framework?

45:34.220 --> 45:35.980
 Yeah, so there's a,

45:35.980 --> 45:37.580
 I don't know if you could tell me if there's a connection,

45:37.580 --> 45:40.180
 but there's an app called Anki

45:40.180 --> 45:42.420
 that helps you remember different concepts.

45:42.420 --> 45:45.100
 And they talk about like a memory palace

45:45.100 --> 45:47.780
 that helps you remember completely random concepts

45:47.780 --> 45:51.380
 by trying to put them in a physical space in your mind

45:51.380 --> 45:52.220
 and putting them next to each other.

45:52.220 --> 45:53.580
 It's called the method of loci.

45:53.580 --> 45:54.700
 Loci, yeah.

45:54.700 --> 45:57.580
 For some reason, that seems to work really well.

45:57.580 --> 45:59.420
 Now, that's a very narrow kind of application

45:59.420 --> 46:00.580
 of just remembering some facts.

46:00.580 --> 46:03.260
 But that's a very, very telling one.

46:03.260 --> 46:04.100
 Yes, exactly.

46:04.100 --> 46:06.740
 So this seems like you're describing a mechanism

46:06.740 --> 46:09.620
 why this seems to work.

46:09.620 --> 46:11.820
 So basically the way what we think is going on

46:11.820 --> 46:15.060
 is all things you know, all concepts, all ideas,

46:15.060 --> 46:20.060
 words, everything you know are stored in reference frames.

46:20.460 --> 46:24.300
 And so if you want to remember something,

46:24.300 --> 46:26.860
 you have to basically navigate through a reference frame

46:26.860 --> 46:28.620
 the same way a rat navigates through a maze

46:28.620 --> 46:31.420
 and the same way my finger navigates to this coffee cup.

46:31.420 --> 46:33.500
 You are moving through some space.

46:33.500 --> 46:35.900
 And so if you have a random list of things

46:35.900 --> 46:37.460
 you were asked to remember,

46:37.460 --> 46:39.300
 by assigning them to a reference frame,

46:39.300 --> 46:42.100
 you've already know very well to see your house, right?

46:42.100 --> 46:43.580
 And the idea of the method of loci is you can say,

46:43.580 --> 46:45.820
 okay, in my lobby, I'm going to put this thing.

46:45.820 --> 46:47.660
 And then the bedroom, I put this one.

46:47.660 --> 46:48.940
 I go down the hall, I put this thing.

46:48.940 --> 46:50.820
 And then you want to recall those facts

46:50.820 --> 46:51.660
 or recall those things.

46:51.660 --> 46:54.100
 You just walk mentally, you walk through your house.

46:54.100 --> 46:56.540
 You're mentally moving through a reference frame

46:56.540 --> 46:57.660
 that you already had.

46:57.660 --> 46:59.260
 And that tells you,

46:59.260 --> 47:00.580
 there's two things that are really important about that.

47:00.580 --> 47:02.740
 It tells us the brain prefers to store things

47:02.740 --> 47:03.940
 in reference frames.

47:03.940 --> 47:06.820
 And that the method of recalling things

47:06.820 --> 47:08.220
 or thinking, if you will,

47:08.220 --> 47:11.500
 is to move mentally through those reference frames.

47:11.500 --> 47:13.540
 You could move physically through some reference frames,

47:13.540 --> 47:15.220
 like I could physically move through the reference frame

47:15.220 --> 47:16.300
 of this coffee cup.

47:16.300 --> 47:17.900
 I can also mentally move through the reference frame

47:17.900 --> 47:19.980
 of the coffee cup, imagining me touching it.

47:19.980 --> 47:22.420
 But I can also mentally move my house.

47:22.420 --> 47:24.660
 And so now we can ask yourself,

47:24.660 --> 47:26.740
 or are all concepts stored this way?

47:26.740 --> 47:31.380
 There was some recent research using human subjects

47:31.380 --> 47:33.540
 in fMRI, and I'm going to apologize for not knowing

47:33.540 --> 47:35.540
 the name of the scientists who did this.

47:36.660 --> 47:41.060
 But what they did is they put humans in this fMRI machine,

47:41.060 --> 47:42.780
 which is one of these imaging machines.

47:42.780 --> 47:46.460
 And they gave the humans tasks to think about birds.

47:46.460 --> 47:47.780
 So they had different types of birds,

47:47.780 --> 47:49.660
 and birds that look big and small,

47:49.660 --> 47:52.220
 and long necks and long legs, things like that.

47:52.220 --> 47:54.140
 And what they could tell from the fMRI

47:55.260 --> 47:56.700
 was a very clever experiment.

47:57.580 --> 48:00.780
 You get to tell when humans were thinking about the birds,

48:00.780 --> 48:03.580
 that the birds, the knowledge of birds

48:03.580 --> 48:05.500
 was arranged in a reference frame,

48:05.500 --> 48:07.100
 similar to the ones that are used

48:07.100 --> 48:08.980
 when you navigate in a room.

48:08.980 --> 48:10.340
 That these are called grid cells,

48:10.340 --> 48:12.820
 and there are grid cell like patterns of activity

48:12.820 --> 48:15.380
 in the neocortex when they do this.

48:15.380 --> 48:18.980
 So it's a very clever experiment.

48:18.980 --> 48:20.180
 And what it basically says,

48:20.180 --> 48:22.140
 that even when you're thinking about something abstract,

48:22.140 --> 48:24.700
 and you're not really thinking about it as a reference frame,

48:24.700 --> 48:26.980
 it tells us the brain is actually using a reference frame.

48:26.980 --> 48:28.780
 And it's using the same neural mechanisms.

48:28.780 --> 48:30.780
 These grid cells are the basic same neural mechanism

48:30.780 --> 48:32.860
 that we propose that grid cells,

48:32.860 --> 48:34.980
 which exist in the old part of the brain,

48:34.980 --> 48:37.340
 the entorhinal cortex, that that mechanism

48:37.340 --> 48:40.060
 is now similar mechanism is used throughout the neocortex.

48:40.060 --> 48:43.180
 It's the same nature to preserve this interesting way

48:43.180 --> 48:44.580
 of creating reference frames.

48:44.580 --> 48:46.940
 And so now they have empirical evidence

48:46.940 --> 48:49.500
 that when you think about concepts like birds,

48:49.500 --> 48:51.220
 that you're using reference frames

48:51.220 --> 48:53.180
 that are built on grid cells.

48:53.180 --> 48:55.180
 So that's similar to the method of loci,

48:55.180 --> 48:56.820
 but in this case, the birds are related.

48:56.820 --> 48:58.620
 So they create their own reference frame,

48:58.620 --> 49:01.100
 which is consistent with bird space.

49:01.100 --> 49:03.540
 And when you think about something, you go through that.

49:03.540 --> 49:04.820
 You can make the same example,

49:04.820 --> 49:06.620
 let's take mathematics.

49:06.620 --> 49:09.260
 Let's say you wanna prove a conjecture.

49:09.260 --> 49:10.100
 What is a conjecture?

49:10.100 --> 49:13.300
 A conjecture is a statement you believe to be true,

49:13.300 --> 49:15.140
 but you haven't proven it.

49:15.140 --> 49:16.540
 And so it might be an equation.

49:16.540 --> 49:19.140
 I wanna show that this is equal to that.

49:19.140 --> 49:21.180
 And you have some places you start with.

49:21.180 --> 49:22.340
 You say, well, I know this is true,

49:22.340 --> 49:23.420
 and I know this is true.

49:23.420 --> 49:25.900
 And I think that maybe to get to the final proof,

49:25.900 --> 49:28.700
 I need to go through some intermediate results.

49:28.700 --> 49:33.140
 What I believe is happening is literally these equations

49:33.140 --> 49:36.380
 or these points are assigned to a reference frame,

49:36.380 --> 49:37.980
 a mathematical reference frame.

49:37.980 --> 49:39.820
 And when you do mathematical operations,

49:39.820 --> 49:41.660
 a simple one might be multiply or divide,

49:41.660 --> 49:44.060
 but you might be a little plus transform or something else.

49:44.060 --> 49:47.500
 That is like a movement in the reference frame of the math.

49:47.500 --> 49:50.260
 And so you're literally trying to discover a path

49:50.260 --> 49:52.660
 from one location to another location

49:52.660 --> 49:56.140
 in a space of mathematics.

49:56.140 --> 49:58.220
 And if you can get to these intermediate results,

49:58.220 --> 50:00.420
 then you know your map is pretty good,

50:00.420 --> 50:02.940
 and you know you're using the right operations.

50:02.940 --> 50:05.940
 Much of what we think about is solving hard problems

50:05.940 --> 50:08.820
 is designing the correct reference frame for that problem,

50:08.820 --> 50:11.100
 figuring out how to organize the information

50:11.100 --> 50:14.300
 and what behaviors I wanna use in that space

50:14.300 --> 50:15.180
 to get me there.

50:16.220 --> 50:19.260
 Yeah, so if you dig in an idea of this reference frame,

50:19.260 --> 50:21.700
 whether it's the math, you start a set of axioms

50:21.700 --> 50:25.140
 to try to get to proving the conjecture.

50:25.140 --> 50:28.140
 Can you try to describe, maybe take a step back,

50:28.140 --> 50:30.660
 how you think of the reference frame in that context?

50:30.660 --> 50:35.660
 Is it the reference frame that the axioms are happy in?

50:36.140 --> 50:38.780
 Is it the reference frame that might contain everything?

50:38.780 --> 50:41.780
 Is it a changing thing as you?

50:41.780 --> 50:43.140
 You have many, many reference frames.

50:43.140 --> 50:44.580
 I mean, in fact, the way the theory,

50:44.580 --> 50:46.140
 the thousand brain theory of intelligence says

50:46.140 --> 50:47.380
 that every single thing in the world

50:47.380 --> 50:48.300
 has its own reference frame.

50:48.300 --> 50:50.860
 So every word has its own reference frames.

50:50.860 --> 50:52.940
 And we can talk about this.

50:52.940 --> 50:54.460
 The mathematics work out,

50:54.460 --> 50:55.940
 this is no problem for neurons to do this.

50:55.940 --> 50:58.740
 But how many reference frames does a coffee cup have?

50:58.740 --> 51:00.140
 Well, it's on a table.

51:00.140 --> 51:03.700
 Let's say you ask how many reference frames

51:03.700 --> 51:06.020
 could a column in my finger

51:06.020 --> 51:07.420
 that's touching the coffee cup have?

51:07.420 --> 51:09.060
 Because there are many, many copy,

51:09.060 --> 51:10.500
 there are many, many models of the coffee cup.

51:10.500 --> 51:13.020
 So the coffee, there is no one model of a coffee cup.

51:13.020 --> 51:14.220
 There are many models of a coffee cup.

51:14.220 --> 51:15.220
 And you could say, well,

51:15.220 --> 51:17.260
 how many different things can my finger learn?

51:17.260 --> 51:19.540
 Is this the question you want to ask?

51:19.540 --> 51:21.780
 Imagine I say every concept, every idea,

51:21.780 --> 51:23.860
 everything you've ever know about that you can say,

51:23.860 --> 51:27.260
 I know that thing has a reference frame

51:27.260 --> 51:28.180
 associated with it.

51:28.180 --> 51:30.180
 And what we do when we build composite objects,

51:30.180 --> 51:32.460
 we assign reference frames

51:32.460 --> 51:33.940
 to point another reference frame.

51:33.940 --> 51:37.060
 So my coffee cup has multiple components to it.

51:37.060 --> 51:40.660
 It's got a limb, it's got a cylinder, it's got a handle.

51:40.660 --> 51:42.820
 And those things have their own reference frames

51:42.820 --> 51:45.060
 and they're assigned to a master reference frame,

51:45.060 --> 51:46.380
 which is called this cup.

51:46.380 --> 51:48.180
 And now I have this Numenta logo on it.

51:48.180 --> 51:50.420
 Well, that's something that exists elsewhere in the world.

51:50.420 --> 51:51.260
 It's its own thing.

51:51.260 --> 51:52.300
 So it has its own reference frame.

51:52.300 --> 51:53.140
 So we now have to say,

51:53.140 --> 51:56.740
 how can I assign the Numenta logo reference frame

51:56.740 --> 51:59.180
 onto the cylinder or onto the coffee cup?

51:59.180 --> 52:01.500
 So it's all, we talked about this in the paper

52:01.500 --> 52:05.740
 that came out in December of this last year.

52:06.860 --> 52:08.780
 The idea of how you can assign reference frames

52:08.780 --> 52:10.540
 to reference frames, how neurons could do this.

52:10.540 --> 52:12.620
 So, well, my question is,

52:12.620 --> 52:14.740
 even though you mentioned reference frames a lot,

52:14.740 --> 52:16.940
 I almost feel it's really useful to dig into

52:16.940 --> 52:20.140
 how you think of what a reference frame is.

52:20.140 --> 52:22.020
 I mean, it was already helpful for me to understand

52:22.020 --> 52:23.700
 that you think of reference frames

52:23.700 --> 52:26.340
 as something there is a lot of.

52:26.340 --> 52:28.780
 Okay, so let's just say that we're gonna have

52:28.780 --> 52:31.060
 some neurons in the brain, not many, actually,

52:31.060 --> 52:32.740
 10,000, 20,000 are gonna create

52:32.740 --> 52:34.300
 a whole bunch of reference frames.

52:34.300 --> 52:35.540
 What does it mean?

52:35.540 --> 52:37.300
 What is a reference frame?

52:37.300 --> 52:40.060
 First of all, these reference frames are different

52:40.060 --> 52:42.220
 than the ones you might be used to.

52:42.220 --> 52:43.420
 We know lots of reference frames.

52:43.420 --> 52:46.060
 For example, we know the Cartesian coordinates, X, Y, Z,

52:46.060 --> 52:47.580
 that's a type of reference frame.

52:47.580 --> 52:50.260
 We know longitude and latitude,

52:50.260 --> 52:52.780
 that's a different type of reference frame.

52:52.780 --> 52:54.540
 If I look at a printed map,

52:54.540 --> 52:58.460
 you might have columns A through M,

52:58.460 --> 53:00.060
 and rows one through 20,

53:00.060 --> 53:01.420
 that's a different type of reference frame.

53:01.420 --> 53:04.660
 It's kind of a Cartesian coordinate reference frame.

53:04.660 --> 53:06.580
 The interesting thing about the reference frames

53:06.580 --> 53:08.580
 in the brain, and we know this because these

53:08.580 --> 53:10.820
 have been established through neuroscience

53:10.820 --> 53:12.260
 studying the entorhinal cortex.

53:12.260 --> 53:13.580
 So I'm not speculating here, okay?

53:13.580 --> 53:16.780
 This is known neuroscience in an old part of the brain.

53:16.780 --> 53:18.860
 The way these cells create reference frames,

53:18.860 --> 53:20.700
 they have no origin.

53:20.700 --> 53:24.340
 So what it's more like, you have a point,

53:24.340 --> 53:27.620
 a point in some space, and you,

53:27.620 --> 53:29.060
 given a particular movement,

53:29.060 --> 53:31.460
 you can then tell what the next point should be.

53:32.340 --> 53:34.100
 And you can then tell what the next point would be,

53:34.100 --> 53:35.460
 and so on.

53:35.460 --> 53:38.700
 You can use this to calculate

53:38.700 --> 53:40.340
 how to get from one point to another.

53:40.340 --> 53:43.180
 So how do I get from my house to my home,

53:43.180 --> 53:44.940
 or how do I get my finger from the side of my cup

53:44.940 --> 53:46.740
 to the top of the cup?

53:46.740 --> 53:50.540
 How do I get from the axioms to the conjecture?

53:50.540 --> 53:54.420
 So it's a different type of reference frame,

53:54.420 --> 53:57.380
 and I can, if you want, I can describe in more detail,

53:57.380 --> 53:59.060
 I can paint a picture of how you might want

53:59.060 --> 53:59.900
 to think about that.

53:59.900 --> 54:00.980
 It's really helpful to think it's something

54:00.980 --> 54:03.740
 you can move through, but is there,

54:03.740 --> 54:08.700
 is it helpful to think of it as spatial in some sense,

54:08.700 --> 54:09.540
 or is there something that's more?

54:09.540 --> 54:11.140
 No, it's definitely spatial.

54:11.140 --> 54:13.820
 It's spatial in a mathematical sense.

54:13.820 --> 54:14.820
 How many dimensions?

54:14.820 --> 54:16.260
 Can it be a crazy number of dimensions?

54:16.260 --> 54:17.460
 Well, that's an interesting question.

54:17.460 --> 54:20.260
 In the old part of the brain, the entorhinal cortex,

54:20.260 --> 54:22.940
 they studied rats, and initially it looks like,

54:22.940 --> 54:24.220
 oh, this is just two dimensional.

54:24.220 --> 54:27.260
 It's like the rat is in some box in the maze or whatever,

54:27.260 --> 54:28.820
 and they know where the rat is using

54:28.820 --> 54:30.300
 these two dimensional reference frames

54:30.300 --> 54:32.380
 to know where it is in the maze.

54:32.380 --> 54:35.540
 We said, well, okay, but what about bats?

54:35.540 --> 54:38.740
 That's a mammal, and they fly in three dimensional space.

54:38.740 --> 54:39.580
 How do they do that?

54:39.580 --> 54:41.700
 They seem to know where they are, right?

54:41.700 --> 54:44.300
 So this is a current area of active research,

54:44.300 --> 54:46.380
 and it seems like somehow the neurons

54:46.380 --> 54:50.300
 in the entorhinal cortex can learn three dimensional space.

54:50.300 --> 54:52.700
 We just, two members of our team,

54:52.700 --> 54:55.940
 along with Elif Fett from MIT,

54:55.940 --> 54:59.580
 just released a paper this literally last week.

54:59.580 --> 55:03.620
 It's on bioRxiv, where they show that you can,

55:03.620 --> 55:05.460
 if you, the way these things work,

55:05.460 --> 55:06.700
 and I won't get, unless you want to,

55:06.700 --> 55:08.100
 I won't get into the detail,

55:08.100 --> 55:12.540
 but grid cells can represent any n dimensional space.

55:12.540 --> 55:15.340
 It's not inherently limited.

55:15.340 --> 55:16.620
 You can think of it this way.

55:16.620 --> 55:18.620
 If you had two dimensional, the way it works

55:18.620 --> 55:20.780
 is you had a bunch of two dimensional slices.

55:20.780 --> 55:21.940
 That's the way these things work.

55:21.940 --> 55:24.260
 There's a whole bunch of two dimensional models,

55:24.260 --> 55:26.140
 and you can just, you can slice up

55:26.140 --> 55:29.300
 any n dimensional space with two dimensional projections.

55:29.300 --> 55:31.660
 So, and you could have one dimensional models.

55:31.660 --> 55:34.420
 So there's nothing inherent about the mathematics

55:34.420 --> 55:35.780
 about the way the neurons do this,

55:35.780 --> 55:39.460
 which constrain the dimensionality of the space,

55:39.460 --> 55:41.460
 which I think was important.

55:41.460 --> 55:44.060
 So obviously I have a three dimensional map of this cup.

55:44.060 --> 55:46.340
 Maybe it's even more than that, I don't know.

55:46.340 --> 55:48.340
 But it's clearly a three dimensional map of the cup.

55:48.340 --> 55:50.900
 I don't just have a projection of the cup.

55:50.900 --> 55:52.020
 But when I think about birds,

55:52.020 --> 55:53.180
 or when I think about mathematics,

55:53.180 --> 55:55.260
 perhaps it's more than three dimensions.

55:55.260 --> 55:56.260
 Who knows?

55:56.260 --> 56:00.100
 So in terms of each individual column

56:00.100 --> 56:04.020
 building up more and more information over time,

56:04.020 --> 56:06.380
 do you think that mechanism is well understood?

56:06.380 --> 56:09.860
 In your mind, you've proposed a lot of architectures there.

56:09.860 --> 56:11.820
 Is that a key piece, or is it,

56:11.820 --> 56:16.220
 is the big piece, the thousand brain theory of intelligence,

56:16.220 --> 56:17.500
 the ensemble of it all?

56:17.500 --> 56:18.460
 Well, I think they're both big.

56:18.460 --> 56:20.940
 I mean, clearly the concept, as a theorist,

56:20.940 --> 56:23.060
 the concept is most exciting, right?

56:23.060 --> 56:23.900
 The high level concept.

56:23.900 --> 56:24.740
 The high level concept.

56:24.740 --> 56:26.140
 This is a totally new way of thinking

56:26.140 --> 56:27.220
 about how the neocortex works.

56:27.220 --> 56:28.660
 So that is appealing.

56:28.660 --> 56:30.700
 It has all these ramifications.

56:30.700 --> 56:33.780
 And with that, as a framework for how the brain works,

56:33.780 --> 56:34.980
 you can make all kinds of predictions

56:34.980 --> 56:36.220
 and solve all kinds of problems.

56:36.220 --> 56:37.260
 Now we're trying to work through

56:37.260 --> 56:38.460
 many of these details right now.

56:38.460 --> 56:40.540
 Okay, how do the neurons actually do this?

56:40.540 --> 56:42.500
 Well, it turns out, if you think about grid cells

56:42.500 --> 56:44.740
 and place cells in the old parts of the brain,

56:44.740 --> 56:45.980
 there's a lot that's known about them,

56:45.980 --> 56:47.020
 but there's still some mysteries.

56:47.020 --> 56:49.060
 There's a lot of debate about exactly the details,

56:49.060 --> 56:50.740
 how these work and what are the signs.

56:50.740 --> 56:52.860
 And we have that still, that same level of detail,

56:52.860 --> 56:54.140
 that same level of concern.

56:54.140 --> 56:56.820
 What we spend here most of our time doing

56:56.820 --> 57:00.060
 is trying to make a very good list

57:00.060 --> 57:02.660
 of the things we don't understand yet.

57:02.660 --> 57:04.020
 That's the key part here.

57:04.020 --> 57:05.260
 What are the constraints?

57:05.260 --> 57:07.020
 It's not like, oh, this thing seems to work, we're done.

57:07.020 --> 57:08.820
 No, it's like, okay, it kind of works,

57:08.820 --> 57:10.700
 but these are other things we know it has to do

57:10.700 --> 57:12.860
 and it's not doing those yet.

57:12.860 --> 57:15.060
 I would say we're well on the way here.

57:15.060 --> 57:17.100
 We're not done yet.

57:17.100 --> 57:20.020
 There's a lot of trickiness to this system,

57:20.020 --> 57:23.180
 but the basic principles about how different layers

57:23.180 --> 57:26.500
 in the neocortex are doing much of this, we understand.

57:27.340 --> 57:28.620
 But there's some fundamental parts

57:28.620 --> 57:30.020
 that we don't understand as well.

57:30.020 --> 57:34.100
 So what would you say is one of the harder open problems

57:34.100 --> 57:37.220
 or one of the ones that have been bothering you,

57:37.220 --> 57:38.460
 keeping you up at night the most?

57:38.460 --> 57:40.620
 Oh, well, right now, this is a detailed thing

57:40.620 --> 57:42.980
 that wouldn't apply to most people, okay?

57:42.980 --> 57:43.820
 Sure.

57:43.820 --> 57:44.660
 But you want me to answer that question?

57:44.660 --> 57:46.180
 Yeah, please.

57:46.180 --> 57:48.380
 We've talked about as if, oh,

57:48.380 --> 57:50.660
 to predict what you're going to sense on this coffee cup,

57:50.660 --> 57:52.300
 I need to know where my finger is gonna be

57:52.300 --> 57:53.580
 on the coffee cup.

57:53.580 --> 57:55.380
 That is true, but it's insufficient.

57:56.340 --> 57:58.460
 Think about my finger touches the edge of the coffee cup.

57:58.460 --> 58:01.660
 My finger can touch it at different orientations.

58:01.660 --> 58:06.340
 I can rotate my finger around here and that doesn't change.

58:06.340 --> 58:08.780
 I can make that prediction and somehow,

58:08.780 --> 58:10.100
 so it's not just the location.

58:10.100 --> 58:13.300
 There's an orientation component of this as well.

58:13.300 --> 58:15.140
 This is known in the old parts of the brain too.

58:15.140 --> 58:16.620
 There's things called head direction cells,

58:16.620 --> 58:18.020
 which way the rat is facing.

58:18.020 --> 58:20.460
 It's the same kind of basic idea.

58:20.460 --> 58:23.620
 So if my finger were a rat, you know, in three dimensions,

58:23.620 --> 58:25.740
 I have a three dimensional orientation

58:25.740 --> 58:27.220
 and I have a three dimensional location.

58:27.220 --> 58:28.620
 If I was a rat, I would have a,

58:28.620 --> 58:30.620
 you might think of it as a two dimensional location,

58:30.620 --> 58:31.460
 a two dimensional orientation,

58:31.460 --> 58:32.540
 a one dimensional orientation,

58:32.540 --> 58:35.100
 like just which way is it facing?

58:35.100 --> 58:38.260
 So how the two components work together,

58:38.260 --> 58:41.500
 how it is that I combine orientation,

58:41.500 --> 58:43.100
 the orientation of my sensor,

58:43.940 --> 58:48.940
 as well as the location is a tricky problem.

58:49.660 --> 58:52.740
 And I think I've made progress on it.

58:52.740 --> 58:55.140
 So at a bigger version of that,

58:55.140 --> 58:58.460
 so perspective is super interesting, but super specific.

58:58.460 --> 59:00.060
 Yeah, I warned you.

59:00.060 --> 59:01.260
 No, no, no, that's really good,

59:01.260 --> 59:03.740
 but there's a more general version of that.

59:03.740 --> 59:06.940
 Do you think context matters,

59:06.940 --> 59:10.700
 the fact that we're in a building in North America,

59:10.700 --> 59:15.700
 that we, in the day and age where we have mugs?

59:15.940 --> 59:19.180
 I mean, there's all this extra information

59:19.180 --> 59:22.060
 that you bring to the table about everything else

59:22.060 --> 59:24.700
 in the room that's outside of just the coffee cup.

59:24.700 --> 59:27.340
 How does it get connected, do you think?

59:27.340 --> 59:30.300
 Yeah, and that is another really interesting question.

59:30.300 --> 59:32.140
 I'm gonna throw that under the rubric

59:32.140 --> 59:35.100
 or the name of attentional problems.

59:35.100 --> 59:36.180
 First of all, we have this model,

59:36.180 --> 59:38.020
 I have many, many models.

59:38.020 --> 59:40.140
 And also the question, does it matter?

59:40.140 --> 59:42.620
 Well, it matters for certain things, of course it does.

59:42.620 --> 59:44.980
 Maybe what we think of that as a coffee cup

59:44.980 --> 59:45.900
 in another part of the world

59:45.900 --> 59:47.660
 is viewed as something completely different.

59:47.660 --> 59:50.420
 Or maybe our logo, which is very benign

59:50.420 --> 59:51.340
 in this part of the world,

59:51.340 --> 59:52.540
 it means something very different

59:52.540 --> 59:53.780
 in another part of the world.

59:53.780 --> 59:56.500
 So those things do matter.

59:57.380 --> 1:00:00.380
 I think the way to think about it is the following,

1:00:00.380 --> 1:00:01.740
 one way to think about it,

1:00:01.740 --> 1:00:04.740
 is we have all these models of the world, okay?

1:00:04.740 --> 1:00:06.140
 And we model everything.

1:00:06.140 --> 1:00:08.860
 And as I said earlier, I kind of snuck it in there,

1:00:08.860 --> 1:00:12.500
 our models are actually, we build composite structure.

1:00:12.500 --> 1:00:15.260
 So every object is composed of other objects,

1:00:15.260 --> 1:00:16.420
 which are composed of other objects,

1:00:16.420 --> 1:00:18.700
 and they become members of other objects.

1:00:18.700 --> 1:00:20.700
 So this room has chairs and a table and a room

1:00:20.700 --> 1:00:21.620
 and walls and so on.

1:00:21.620 --> 1:00:24.300
 Now we can just arrange these things in a certain way

1:00:24.300 --> 1:00:26.580
 and go, oh, that's the nomenclature conference room.

1:00:26.580 --> 1:00:31.260
 So, and what we do is when we go around the world

1:00:31.260 --> 1:00:33.620
 and we experience the world,

1:00:33.620 --> 1:00:35.740
 by walking into a room, for example,

1:00:35.740 --> 1:00:36.780
 the first thing I do is I can say,

1:00:36.780 --> 1:00:38.660
 oh, I'm in this room, do I recognize the room?

1:00:38.660 --> 1:00:41.900
 Then I can say, oh, look, there's a table here.

1:00:41.900 --> 1:00:43.460
 And by attending to the table,

1:00:43.460 --> 1:00:45.620
 I'm then assigning this table in the context of the room.

1:00:45.620 --> 1:00:48.100
 Then I can say, oh, on the table, there's a coffee cup.

1:00:48.100 --> 1:00:49.740
 Oh, and on the table, there's a logo.

1:00:49.740 --> 1:00:51.260
 And in the logo, there's the word Nementa.

1:00:51.260 --> 1:00:53.420
 Oh, and look in the logo, there's the letter E.

1:00:53.420 --> 1:00:55.740
 Oh, and look, it has an unusual serif.

1:00:55.740 --> 1:00:59.660
 And it doesn't actually, but I pretended to serif.

1:00:59.660 --> 1:01:03.860
 So the point is your attention is kind of drilling

1:01:03.860 --> 1:01:06.500
 deep in and out of these nested structures.

1:01:07.460 --> 1:01:09.340
 And I can pop back up and I can pop back down.

1:01:09.340 --> 1:01:10.900
 I can pop back up and I can pop back down.

1:01:10.900 --> 1:01:13.220
 So when I attend to the coffee cup,

1:01:13.220 --> 1:01:15.660
 I haven't lost the context of everything else,

1:01:15.660 --> 1:01:18.900
 but it's sort of, there's this sort of nested structure.

1:01:18.900 --> 1:01:22.100
 So the attention filters the reference frame information

1:01:22.980 --> 1:01:24.420
 for that particular period of time?

1:01:24.420 --> 1:01:26.620
 Yes, it basically, moment to moment,

1:01:26.620 --> 1:01:28.420
 you attend the sub components,

1:01:28.420 --> 1:01:29.740
 and then you can attend the sub components

1:01:29.740 --> 1:01:30.580
 to sub components.

1:01:30.580 --> 1:01:31.420
 And you can move up and down.

1:01:31.420 --> 1:01:32.340
 You can move up and down.

1:01:32.340 --> 1:01:33.180
 We do that all the time.

1:01:33.180 --> 1:01:35.580
 You're not even, now that I'm aware of it,

1:01:35.580 --> 1:01:36.700
 I'm very conscious of it.

1:01:36.700 --> 1:01:39.980
 But until, but most people don't even think about this.

1:01:39.980 --> 1:01:41.700
 You just walk in a room and you don't say,

1:01:41.700 --> 1:01:43.500
 oh, I looked at the chair and I looked at the board

1:01:43.500 --> 1:01:44.620
 and looked at that word on the board

1:01:44.620 --> 1:01:47.100
 and I looked over here, what's going on, right?

1:01:47.100 --> 1:01:50.020
 So what percent of your day are you deeply aware of this?

1:01:50.020 --> 1:01:52.860
 And what part can you actually relax and just be Jeff?

1:01:52.860 --> 1:01:54.460
 Me personally, like my personal day?

1:01:54.460 --> 1:01:55.540
 Yeah.

1:01:55.540 --> 1:01:58.340
 Unfortunately, I'm afflicted with too much of the former.

1:02:01.340 --> 1:02:02.820
 Well, unfortunately or unfortunately.

1:02:02.820 --> 1:02:03.660
 Yeah.

1:02:03.660 --> 1:02:04.580
 You don't think it's useful?

1:02:04.580 --> 1:02:06.820
 Oh, it is useful, totally useful.

1:02:06.820 --> 1:02:09.180
 I think about this stuff almost all the time.

1:02:09.180 --> 1:02:12.540
 And one of my primary ways of thinking

1:02:12.540 --> 1:02:13.860
 is when I'm in sleep at night,

1:02:13.860 --> 1:02:15.860
 I always wake up in the middle of the night.

1:02:15.860 --> 1:02:17.860
 And then I stay awake for at least an hour

1:02:17.860 --> 1:02:20.700
 with my eyes shut in sort of a half sleep state

1:02:20.700 --> 1:02:21.660
 thinking about these things.

1:02:21.660 --> 1:02:23.700
 I come up with answers to problems very often

1:02:23.700 --> 1:02:25.660
 in that sort of half sleeping state.

1:02:25.660 --> 1:02:27.460
 I think about it on my bike ride, I think about it on walks.

1:02:27.460 --> 1:02:28.780
 I'm just constantly thinking about this.

1:02:28.780 --> 1:02:32.420
 I have to almost schedule time

1:02:32.420 --> 1:02:34.100
 to not think about this stuff

1:02:34.100 --> 1:02:37.820
 because it's very, it's mentally taxing.

1:02:37.820 --> 1:02:39.780
 Are you, when you're thinking about this stuff,

1:02:39.780 --> 1:02:41.220
 are you thinking introspectively,

1:02:41.220 --> 1:02:43.700
 like almost taking a step outside of yourself

1:02:43.700 --> 1:02:45.660
 and trying to figure out what is your mind doing right now?

1:02:45.660 --> 1:02:48.020
 I do that all the time, but that's not all I do.

1:02:49.060 --> 1:02:50.780
 I'm constantly observing myself.

1:02:50.780 --> 1:02:53.060
 So as soon as I started thinking about grid cells,

1:02:53.060 --> 1:02:55.260
 for example, and getting into that,

1:02:55.260 --> 1:02:56.780
 I started saying, oh, well, grid cells

1:02:56.780 --> 1:02:58.380
 can have my place of sense in the world.

1:02:58.380 --> 1:02:59.660
 That's where you know where you are.

1:02:59.660 --> 1:03:01.380
 And it's interesting, we always have a sense

1:03:01.380 --> 1:03:03.020
 of where we are unless we're lost.

1:03:03.020 --> 1:03:04.740
 And so I started at night when I got up

1:03:04.740 --> 1:03:06.980
 to go to the bathroom, I would start trying to do it

1:03:06.980 --> 1:03:08.500
 completely with my eyes closed all the time.

1:03:08.500 --> 1:03:10.060
 And I would test my sense of grid cells.

1:03:10.060 --> 1:03:13.700
 I would walk five feet and say, okay, I think I'm here.

1:03:13.700 --> 1:03:14.540
 Am I really there?

1:03:14.540 --> 1:03:15.460
 What's my error?

1:03:15.460 --> 1:03:16.780
 And then I would calculate my error again

1:03:16.780 --> 1:03:17.940
 and see how the errors could accumulate.

1:03:17.940 --> 1:03:19.460
 So even something as simple as getting up

1:03:19.460 --> 1:03:20.420
 in the middle of the night to go to the bathroom,

1:03:20.420 --> 1:03:22.620
 I'm testing these theories out.

1:03:22.620 --> 1:03:23.460
 It's kind of fun.

1:03:23.460 --> 1:03:25.580
 I mean, the coffee cup is an example of that too.

1:03:25.580 --> 1:03:30.380
 So I find that these sort of everyday introspections

1:03:30.380 --> 1:03:31.940
 are actually quite helpful.

1:03:32.820 --> 1:03:34.860
 It doesn't mean you can ignore the science.

1:03:34.860 --> 1:03:37.060
 I mean, I spend hours every day

1:03:37.060 --> 1:03:39.140
 reading ridiculously complex papers.

1:03:40.180 --> 1:03:41.740
 That's not nearly as much fun,

1:03:41.740 --> 1:03:44.580
 but you have to sort of build up those constraints

1:03:44.580 --> 1:03:46.860
 and the knowledge about the field and who's doing what

1:03:46.860 --> 1:03:48.860
 and what exactly they think is happening here.

1:03:48.860 --> 1:03:50.060
 And then you can sit back and say,

1:03:50.060 --> 1:03:52.500
 okay, let's try to piece this all together.

1:03:53.380 --> 1:03:56.020
 Let's come up with some, I'm very,

1:03:56.020 --> 1:03:58.460
 in this group here, people, they know they do,

1:03:58.460 --> 1:03:59.300
 I do this all the time.

1:03:59.300 --> 1:04:01.220
 I come in with these introspective ideas and say,

1:04:01.220 --> 1:04:02.380
 well, have you ever thought about this?

1:04:02.380 --> 1:04:04.700
 Now watch, well, let's all do this together.

1:04:04.700 --> 1:04:05.940
 And it's helpful.

1:04:05.940 --> 1:04:09.580
 It's not, as long as you don't,

1:04:09.580 --> 1:04:12.340
 all you did was that, then you're just making up stuff.

1:04:12.340 --> 1:04:14.780
 But if you're constraining it by the reality

1:04:14.780 --> 1:04:17.820
 of the neuroscience, then it's really helpful.

1:04:17.820 --> 1:04:20.180
 So let's talk a little bit about deep learning

1:04:20.180 --> 1:04:25.180
 and the successes in the applied space of neural networks,

1:04:26.860 --> 1:04:29.020
 ideas of training model on data

1:04:29.020 --> 1:04:31.420
 and these simple computational units,

1:04:31.420 --> 1:04:36.580
 artificial neurons that with backpropagation,

1:04:36.580 --> 1:04:40.460
 statistical ways of being able to generalize

1:04:40.460 --> 1:04:42.780
 from the training set onto data

1:04:42.780 --> 1:04:44.300
 that's similar to that training set.

1:04:44.300 --> 1:04:47.420
 So where do you think are the limitations

1:04:47.420 --> 1:04:48.460
 of those approaches?

1:04:48.460 --> 1:04:50.380
 What do you think are its strengths

1:04:50.380 --> 1:04:52.180
 relative to your major efforts

1:04:52.180 --> 1:04:56.020
 of constructing a theory of human intelligence?

1:04:56.020 --> 1:04:57.820
 Well, I'm not an expert in this field.

1:04:57.820 --> 1:04:59.140
 I'm somewhat knowledgeable.

1:04:59.140 --> 1:04:59.980
 So, but I'm not.

1:04:59.980 --> 1:05:01.620
 Some of it is in just your intuition.

1:05:01.620 --> 1:05:02.460
 What are your?

1:05:02.460 --> 1:05:03.860
 Well, I have a little bit more than intuition,

1:05:03.860 --> 1:05:05.420
 but I just want to say like,

1:05:05.420 --> 1:05:07.660
 you know, one of the things that you asked me,

1:05:07.660 --> 1:05:09.220
 do I spend all my time thinking about neuroscience?

1:05:09.220 --> 1:05:10.060
 I do.

1:05:10.060 --> 1:05:11.340
 That's to the exclusion of thinking about things

1:05:11.340 --> 1:05:13.660
 like convolutional neural networks.

1:05:13.660 --> 1:05:15.260
 But I try to stay current.

1:05:15.260 --> 1:05:17.860
 So look, I think it's great, the progress they've made.

1:05:17.860 --> 1:05:18.780
 It's fantastic.

1:05:18.780 --> 1:05:19.860
 And as I mentioned earlier,

1:05:19.860 --> 1:05:21.860
 it's very highly useful for many things.

1:05:22.940 --> 1:05:26.140
 The models that we have today are actually derived

1:05:26.140 --> 1:05:28.220
 from a lot of neuroscience principles.

1:05:28.220 --> 1:05:30.020
 There are distributed processing systems

1:05:30.020 --> 1:05:31.260
 and distributed memory systems,

1:05:31.260 --> 1:05:33.260
 and that's how the brain works.

1:05:33.260 --> 1:05:35.900
 They use things that we might call them neurons,

1:05:35.900 --> 1:05:37.020
 but they're really not neurons at all.

1:05:37.020 --> 1:05:39.220
 So we can just, they're not really neurons.

1:05:39.220 --> 1:05:41.220
 So they're distributed processing systems.

1:05:41.220 --> 1:05:44.700
 And that nature of hierarchy,

1:05:44.700 --> 1:05:47.140
 that came also from neuroscience.

1:05:47.140 --> 1:05:48.220
 And so there's a lot of things,

1:05:48.220 --> 1:05:49.780
 the learning rules, basically,

1:05:49.780 --> 1:05:51.140
 not back prop, but other, you know,

1:05:51.140 --> 1:05:52.540
 sort of heavy on top of that.

1:05:52.540 --> 1:05:55.020
 I'd be curious to say they're not neurons at all.

1:05:55.020 --> 1:05:56.180
 Can you describe in which way?

1:05:56.180 --> 1:05:57.700
 I mean, some of it is obvious,

1:05:57.700 --> 1:06:00.380
 but I'd be curious if you have specific ways

1:06:00.380 --> 1:06:02.820
 in which you think are the biggest differences.

1:06:02.820 --> 1:06:04.940
 Yeah, we had a paper in 2016 called

1:06:04.940 --> 1:06:06.940
 Why Neurons Have Thousands of Synapses.

1:06:06.940 --> 1:06:09.460
 And if you read that paper,

1:06:09.460 --> 1:06:11.420
 you'll know what I'm talking about here.

1:06:11.420 --> 1:06:14.460
 A real neuron in the brain is a complex thing.

1:06:14.460 --> 1:06:17.180
 And let's just start with the synapses on it,

1:06:17.180 --> 1:06:19.020
 which is a connection between neurons.

1:06:19.020 --> 1:06:20.700
 Real neurons can have everywhere

1:06:20.700 --> 1:06:23.500
 from five to 30,000 synapses on them.

1:06:25.460 --> 1:06:27.220
 The ones near the cell body,

1:06:27.220 --> 1:06:30.420
 the ones that are close to the soma of the cell body,

1:06:30.420 --> 1:06:32.100
 those are like the ones that people model

1:06:32.100 --> 1:06:33.740
 in artificial neurons.

1:06:33.740 --> 1:06:35.060
 There is a few hundred of those.

1:06:35.060 --> 1:06:37.100
 Maybe they can affect the cell.

1:06:37.100 --> 1:06:39.700
 They can make the cell become active.

1:06:39.700 --> 1:06:43.540
 95% of the synapses can't do that.

1:06:43.540 --> 1:06:44.580
 They're too far away.

1:06:44.580 --> 1:06:45.980
 So if you activate one of those synapses,

1:06:45.980 --> 1:06:47.860
 it just doesn't affect the cell body enough

1:06:47.860 --> 1:06:48.860
 to make any difference.

1:06:48.860 --> 1:06:50.100
 Any one of them individually.

1:06:50.100 --> 1:06:50.940
 Any one of them individually,

1:06:50.940 --> 1:06:52.540
 or even if you do a mass of them.

1:06:54.060 --> 1:06:57.420
 What real neurons do is the following.

1:06:57.420 --> 1:07:02.420
 If you activate or you get 10 to 20 of them

1:07:03.500 --> 1:07:04.460
 active at the same time,

1:07:04.460 --> 1:07:06.660
 meaning they're all receiving an input at the same time,

1:07:06.660 --> 1:07:09.100
 and those 10 to 20 synapses or 40 synapses

1:07:09.100 --> 1:07:11.340
 within a very short distance on the dendrite,

1:07:11.340 --> 1:07:13.300
 like 40 microns, a very small area.

1:07:13.300 --> 1:07:14.580
 So if you activate a bunch of these

1:07:14.580 --> 1:07:17.580
 right next to each other at some distant place,

1:07:17.580 --> 1:07:19.300
 what happens is it creates

1:07:19.300 --> 1:07:21.300
 what's called the dendritic spike.

1:07:21.300 --> 1:07:24.540
 And the dendritic spike travels through the dendrites

1:07:24.540 --> 1:07:26.900
 and can reach the soma or the cell body.

1:07:27.820 --> 1:07:31.260
 Now, when it gets there, it changes the voltage,

1:07:31.260 --> 1:07:33.580
 which is sort of like gonna make the cell fire,

1:07:33.580 --> 1:07:36.060
 but never enough to make the cell fire.

1:07:36.060 --> 1:07:38.500
 It's sort of what we call, it says we depolarize the cell,

1:07:38.500 --> 1:07:39.580
 you raise the voltage a little bit,

1:07:39.580 --> 1:07:41.620
 but not enough to do anything.

1:07:41.620 --> 1:07:42.580
 It's like, well, what good is that?

1:07:42.580 --> 1:07:44.460
 And then it goes back down again.

1:07:44.460 --> 1:07:47.780
 So we propose a theory,

1:07:47.780 --> 1:07:50.500
 which I'm very confident in basics are,

1:07:50.500 --> 1:07:52.780
 is that what's happening there is

1:07:52.780 --> 1:07:55.860
 those 95% of the synapses are recognizing

1:07:55.860 --> 1:07:58.460
 dozens to hundreds of unique patterns.

1:07:58.460 --> 1:08:02.060
 They can write about 10, 20 synapses at a time,

1:08:02.060 --> 1:08:04.460
 and they're acting like predictions.

1:08:04.460 --> 1:08:07.620
 So the neuron actually is a predictive engine on its own.

1:08:07.620 --> 1:08:09.700
 It can fire when it gets enough,

1:08:09.700 --> 1:08:10.900
 what they call proximal input

1:08:10.900 --> 1:08:11.980
 from those ones near the cell fire,

1:08:11.980 --> 1:08:15.460
 but it can get ready to fire from dozens to hundreds

1:08:15.460 --> 1:08:18.100
 of patterns that it recognizes from the other guys.

1:08:18.100 --> 1:08:21.260
 And the advantage of this to the neuron

1:08:21.260 --> 1:08:23.500
 is that when it actually does produce a spike

1:08:23.500 --> 1:08:24.780
 in action potential,

1:08:24.780 --> 1:08:27.700
 it does so slightly sooner than it would have otherwise.

1:08:27.700 --> 1:08:29.740
 And so what could is slightly sooner?

1:08:29.740 --> 1:08:31.820
 Well, the slightly sooner part is it,

1:08:31.820 --> 1:08:34.940
 all the excitatory neurons in the brain

1:08:34.940 --> 1:08:36.660
 are surrounded by these inhibitory neurons,

1:08:36.660 --> 1:08:38.980
 and they're very fast, the inhibitory neurons,

1:08:38.980 --> 1:08:40.420
 these basket cells.

1:08:40.420 --> 1:08:42.580
 And if I get my spike out

1:08:42.580 --> 1:08:44.220
 a little bit sooner than someone else,

1:08:44.220 --> 1:08:47.020
 I inhibit all my neighbors around me, right?

1:08:47.020 --> 1:08:49.740
 And what you end up with is a different representation.

1:08:49.740 --> 1:08:52.060
 You end up with a reputation that matches your prediction.

1:08:52.060 --> 1:08:53.780
 It's a sparser representation,

1:08:53.780 --> 1:08:55.740
 meaning fewer neurons are active,

1:08:55.740 --> 1:08:57.860
 but it's much more specific.

1:08:57.860 --> 1:09:00.300
 And so we showed how networks of these neurons

1:09:00.300 --> 1:09:04.180
 can do very sophisticated temporal prediction, basically.

1:09:04.180 --> 1:09:07.020
 So this, summarize this,

1:09:07.020 --> 1:09:10.980
 real neurons in the brain are time based prediction engines,

1:09:10.980 --> 1:09:14.660
 and there's no concept of this at all

1:09:14.660 --> 1:09:18.100
 in artificial, what we call point neurons.

1:09:18.100 --> 1:09:20.060
 I don't think you can build a brain without them.

1:09:20.060 --> 1:09:21.340
 I don't think you can build intelligence without them,

1:09:21.340 --> 1:09:26.020
 because it's where a large part of the time comes from.

1:09:26.020 --> 1:09:29.060
 These are predictive models, and the time is,

1:09:29.060 --> 1:09:32.220
 there's a prior and a prediction and an action,

1:09:32.220 --> 1:09:34.940
 and it's inherent through every neuron in the neocortex.

1:09:34.940 --> 1:09:37.740
 So I would say that point neurons sort of model

1:09:37.740 --> 1:09:40.620
 a piece of that, and not very well at that either.

1:09:40.620 --> 1:09:45.620
 But like for example, synapses are very unreliable,

1:09:46.060 --> 1:09:49.900
 and you cannot assign any precision to them.

1:09:49.900 --> 1:09:52.460
 So even one digit of precision is not possible.

1:09:52.460 --> 1:09:55.540
 So the way real neurons work is they don't add these,

1:09:55.540 --> 1:09:57.420
 they don't change these weights accurately

1:09:57.420 --> 1:09:59.340
 like artificial neural networks do.

1:09:59.340 --> 1:10:01.020
 They basically form new synapses,

1:10:01.020 --> 1:10:03.780
 and so what you're trying to always do is

1:10:03.780 --> 1:10:06.540
 detect the presence of some 10 to 20

1:10:06.540 --> 1:10:08.780
 active synapses at the same time,

1:10:08.780 --> 1:10:11.300
 as opposed, and they're almost binary.

1:10:11.300 --> 1:10:12.820
 It's like, because you can't really represent

1:10:12.820 --> 1:10:14.620
 anything much finer than that.

1:10:14.620 --> 1:10:16.220
 So these are the kind of,

1:10:16.220 --> 1:10:18.060
 and I think that's actually another essential component,

1:10:18.060 --> 1:10:20.940
 because the brain works on sparse patterns,

1:10:20.940 --> 1:10:24.180
 and all that mechanism is based on sparse patterns,

1:10:24.180 --> 1:10:26.620
 and I don't actually think you could build real brains

1:10:26.620 --> 1:10:29.100
 or machine intelligence without

1:10:29.100 --> 1:10:30.900
 incorporating some of those ideas.

1:10:30.900 --> 1:10:32.660
 It's hard to even think about the complexity

1:10:32.660 --> 1:10:34.420
 that emerges from the fact that

1:10:34.420 --> 1:10:37.140
 the timing of the firing matters in the brain,

1:10:37.140 --> 1:10:40.980
 the fact that you form new synapses,

1:10:40.980 --> 1:10:44.020
 and I mean, everything you just mentioned

1:10:44.020 --> 1:10:44.940
 in the past couple minutes.

1:10:44.940 --> 1:10:46.540
 Trust me, if you spend time on it,

1:10:46.540 --> 1:10:47.940
 you can get your mind around it.

1:10:47.940 --> 1:10:49.860
 It's not like, it's no longer a mystery to me.

1:10:49.860 --> 1:10:53.820
 No, but sorry, as a function, in a mathematical way,

1:10:53.820 --> 1:10:56.940
 can you start getting an intuition about

1:10:56.940 --> 1:10:58.540
 what gets it excited, what not,

1:10:58.540 --> 1:10:59.380
 and what kind of representation?

1:10:59.380 --> 1:11:01.060
 Yeah, it's not as easy as,

1:11:02.580 --> 1:11:04.660
 there's many other types of neural networks

1:11:04.660 --> 1:11:07.580
 that are more amenable to pure analysis,

1:11:09.220 --> 1:11:10.780
 especially very simple networks.

1:11:10.780 --> 1:11:12.580
 Oh, I have four neurons, and they're doing this.

1:11:12.580 --> 1:11:14.500
 Can we describe to them mathematically

1:11:14.500 --> 1:11:16.300
 what they're doing type of thing?

1:11:16.300 --> 1:11:19.340
 Even the complexity of convolutional neural networks today,

1:11:19.340 --> 1:11:20.300
 it's sort of a mystery.

1:11:20.300 --> 1:11:22.500
 They can't really describe the whole system.

1:11:22.500 --> 1:11:24.780
 And so it's different.

1:11:24.780 --> 1:11:29.780
 My colleague Subitai Ahmad, he did a nice paper on this.

1:11:31.500 --> 1:11:32.740
 You can get all this stuff on our website

1:11:32.740 --> 1:11:34.100
 if you're interested,

1:11:34.100 --> 1:11:36.180
 talking about sort of the mathematical properties

1:11:36.180 --> 1:11:37.660
 of sparse representations.

1:11:37.660 --> 1:11:40.620
 And so what we can do is we can show mathematically,

1:11:40.620 --> 1:11:44.940
 for example, why 10 to 20 synapses to recognize a pattern

1:11:44.940 --> 1:11:47.740
 is the correct number, is the right number you'd wanna use.

1:11:47.740 --> 1:11:49.980
 And by the way, that matches biology.

1:11:49.980 --> 1:11:53.900
 We can show mathematically some of these concepts

1:11:53.900 --> 1:11:58.620
 about the show why the brain is so robust

1:11:58.620 --> 1:12:01.020
 to noise and error and fallout and so on.

1:12:01.020 --> 1:12:02.260
 We can show that mathematically

1:12:02.260 --> 1:12:05.020
 as well as empirically in simulations.

1:12:05.020 --> 1:12:07.860
 But the system can't be analyzed completely.

1:12:07.860 --> 1:12:11.980
 Any complex system can't, and so that's out of the realm.

1:12:11.980 --> 1:12:16.980
 But there is mathematical benefits and intuitions

1:12:17.660 --> 1:12:19.460
 that can be derived from mathematics.

1:12:19.460 --> 1:12:20.860
 And we try to do that as well.

1:12:20.860 --> 1:12:23.300
 Most of our papers have a section about that.

1:12:23.300 --> 1:12:25.900
 So I think it's refreshing and useful for me

1:12:25.900 --> 1:12:29.060
 to be talking to you about deep neural networks,

1:12:29.060 --> 1:12:30.900
 because your intuition basically says

1:12:30.900 --> 1:12:34.540
 that we can't achieve anything like intelligence

1:12:34.540 --> 1:12:35.940
 with artificial neural networks.

1:12:35.940 --> 1:12:36.940
 Well, not in the current form.

1:12:36.940 --> 1:12:37.780
 Not in the current form.

1:12:37.780 --> 1:12:40.180
 I'm sure we can do it in the ultimate form, sure.

1:12:40.180 --> 1:12:41.260
 So let me dig into it

1:12:41.260 --> 1:12:43.300
 and see what your thoughts are there a little bit.

1:12:43.300 --> 1:12:45.980
 So I'm not sure if you read this little blog post

1:12:45.980 --> 1:12:49.460
 called Bitter Lesson by Rich Sutton recently.

1:12:49.460 --> 1:12:51.660
 He's a reinforcement learning pioneer.

1:12:51.660 --> 1:12:53.260
 I'm not sure if you're familiar with him.

1:12:53.260 --> 1:12:56.780
 His basic idea is that all the stuff we've done in AI

1:12:56.780 --> 1:13:00.660
 in the past 70 years, he's one of the old school guys.

1:13:02.980 --> 1:13:06.860
 The biggest lesson learned is that all the tricky things

1:13:06.860 --> 1:13:10.420
 we've done, they benefit in the short term,

1:13:10.420 --> 1:13:12.100
 but in the long term, what wins out

1:13:12.100 --> 1:13:16.700
 is a simple general method that just relies on Moore's law,

1:13:16.700 --> 1:13:19.820
 on computation getting faster and faster.

1:13:19.820 --> 1:13:21.260
 This is what he's saying.

1:13:21.260 --> 1:13:23.220
 This is what has worked up to now.

1:13:23.220 --> 1:13:25.380
 This is what has worked up to now.

1:13:25.380 --> 1:13:29.060
 If you're trying to build a system,

1:13:29.060 --> 1:13:30.060
 if we're talking about,

1:13:30.060 --> 1:13:31.420
 he's not concerned about intelligence.

1:13:31.420 --> 1:13:34.420
 He's concerned about a system that works

1:13:34.420 --> 1:13:36.500
 in terms of making predictions

1:13:36.500 --> 1:13:38.780
 on applied narrow AI problems, right?

1:13:38.780 --> 1:13:40.620
 That's what this discussion is about.

1:13:40.620 --> 1:13:44.220
 That you just try to go as general as possible

1:13:44.220 --> 1:13:48.500
 and wait years or decades for the computation

1:13:48.500 --> 1:13:50.220
 to make it actually.

1:13:50.220 --> 1:13:51.700
 Is he saying that as a criticism

1:13:51.700 --> 1:13:53.260
 or is he saying this is a prescription

1:13:53.260 --> 1:13:54.340
 of what we ought to be doing?

1:13:54.340 --> 1:13:55.860
 Well, it's very difficult.

1:13:55.860 --> 1:13:57.980
 He's saying this is what has worked

1:13:57.980 --> 1:14:00.340
 and yes, a prescription, but it's a difficult prescription

1:14:00.340 --> 1:14:02.380
 because it says all the fun things

1:14:02.380 --> 1:14:05.820
 you guys are trying to do, we are trying to do.

1:14:05.820 --> 1:14:07.340
 He's part of the community.

1:14:07.340 --> 1:14:10.780
 He's saying it's only going to be short term gains.

1:14:10.780 --> 1:14:13.780
 So this all leads up to a question, I guess,

1:14:13.780 --> 1:14:15.580
 on artificial neural networks

1:14:15.580 --> 1:14:19.060
 and maybe our own biological neural networks

1:14:19.060 --> 1:14:23.780
 is do you think if we just scale things up significantly,

1:14:23.780 --> 1:14:27.180
 so take these dumb artificial neurons,

1:14:27.180 --> 1:14:29.020
 the point neurons, I like that term.

1:14:30.420 --> 1:14:33.260
 If we just have a lot more of them,

1:14:33.260 --> 1:14:34.540
 do you think some of the elements

1:14:34.540 --> 1:14:38.060
 that we see in the brain may start emerging?

1:14:38.060 --> 1:14:39.540
 No, I don't think so.

1:14:39.540 --> 1:14:43.420
 We can do bigger problems of the same type.

1:14:43.420 --> 1:14:45.260
 I mean, it's been pointed out by many people

1:14:45.260 --> 1:14:46.860
 that today's convolutional neural networks

1:14:46.860 --> 1:14:47.860
 aren't really much different

1:14:47.860 --> 1:14:50.580
 than the ones we had quite a while ago.

1:14:50.580 --> 1:14:51.820
 They're bigger and train more

1:14:51.820 --> 1:14:53.900
 and we have more labeled data and so on.

1:14:56.300 --> 1:14:58.580
 But I don't think you can get to the kind of things

1:14:58.580 --> 1:15:01.380
 I know the brain can do and that we think about

1:15:01.380 --> 1:15:03.700
 as intelligence by just scaling it up.

1:15:03.700 --> 1:15:06.580
 So that may be, it's a good description

1:15:06.580 --> 1:15:07.660
 of what's happened in the past,

1:15:07.660 --> 1:15:09.940
 what's happened recently with the reemergence

1:15:09.940 --> 1:15:12.500
 of artificial neural networks.

1:15:12.500 --> 1:15:14.380
 It may be a good prescription

1:15:14.380 --> 1:15:16.540
 for what's gonna happen in the short term.

1:15:17.580 --> 1:15:19.180
 But I don't think that's the path.

1:15:19.180 --> 1:15:20.860
 I've said that earlier.

1:15:20.860 --> 1:15:21.700
 There's an alternate path.

1:15:21.700 --> 1:15:22.900
 I should mention to you, by the way,

1:15:22.900 --> 1:15:25.900
 that we've made sufficient progress

1:15:25.900 --> 1:15:28.900
 on the whole cortical theory in the last few years

1:15:28.900 --> 1:15:33.900
 that last year we decided to start actively pursuing

1:15:35.660 --> 1:15:39.140
 how do we get these ideas embedded into machine learning?

1:15:40.100 --> 1:15:41.860
 Well, that's, again, being led by my colleague,

1:15:41.860 --> 1:15:45.140
 Subed Tariman, and he's more of a machine learning guy.

1:15:45.140 --> 1:15:46.740
 I'm more of a neuroscience guy.

1:15:46.740 --> 1:15:51.180
 So this is now, I wouldn't say our focus,

1:15:51.180 --> 1:15:54.140
 but it is now an equal focus here

1:15:54.140 --> 1:15:58.220
 because we need to proselytize what we've learned

1:15:58.220 --> 1:16:00.220
 and we need to show how it's beneficial

1:16:01.460 --> 1:16:03.740
 to the machine learning layer.

1:16:03.740 --> 1:16:05.580
 So we're putting, we have a plan in place right now.

1:16:05.580 --> 1:16:07.700
 In fact, we just did our first paper on this.

1:16:07.700 --> 1:16:09.700
 I can tell you about that.

1:16:09.700 --> 1:16:11.380
 But one of the reasons I wanna talk to you

1:16:11.380 --> 1:16:14.100
 is because I'm trying to get more people

1:16:14.100 --> 1:16:15.980
 in the machine learning community to say,

1:16:15.980 --> 1:16:17.140
 I need to learn about this stuff.

1:16:17.140 --> 1:16:19.380
 And maybe we should just think about this a bit more

1:16:19.380 --> 1:16:20.860
 about what we've learned about the brain

1:16:20.860 --> 1:16:23.860
 and what are those team at Nimenta, what have they done?

1:16:23.860 --> 1:16:25.220
 Is that useful for us?

1:16:25.220 --> 1:16:28.500
 Yeah, so is there elements of all the cortical theory

1:16:28.500 --> 1:16:29.820
 that things we've been talking about

1:16:29.820 --> 1:16:31.900
 that may be useful in the short term?

1:16:31.900 --> 1:16:33.420
 Yes, in the short term, yes.

1:16:33.420 --> 1:16:34.780
 This is the, sorry to interrupt,

1:16:34.780 --> 1:16:37.740
 but the open question is,

1:16:37.740 --> 1:16:39.260
 it certainly feels from my perspective

1:16:39.260 --> 1:16:41.060
 that in the long term,

1:16:41.060 --> 1:16:42.820
 some of the ideas we've been talking about

1:16:42.820 --> 1:16:44.260
 will be extremely useful.

1:16:44.260 --> 1:16:46.020
 The question is whether in the short term.

1:16:46.020 --> 1:16:48.340
 Well, this is always what I would call

1:16:48.340 --> 1:16:50.620
 the entrepreneur's dilemma.

1:16:50.620 --> 1:16:53.060
 So you have this long term vision,

1:16:53.060 --> 1:16:55.300
 oh, we're gonna all be driving electric cars

1:16:55.300 --> 1:16:56.780
 or we're all gonna have computers

1:16:56.780 --> 1:16:59.020
 or we're all gonna, whatever.

1:16:59.020 --> 1:17:01.860
 And you're at some point in time and you say,

1:17:01.860 --> 1:17:02.980
 I can see that long term vision,

1:17:02.980 --> 1:17:03.820
 I'm sure it's gonna happen.

1:17:03.820 --> 1:17:05.780
 How do I get there without killing myself?

1:17:05.780 --> 1:17:07.380
 Without going out of business, right?

1:17:07.380 --> 1:17:08.740
 That's the challenge.

1:17:08.740 --> 1:17:09.580
 That's the dilemma.

1:17:09.580 --> 1:17:11.100
 That's the really difficult thing to do.

1:17:11.100 --> 1:17:13.100
 So we're facing that right now.

1:17:13.100 --> 1:17:14.660
 So ideally what you'd wanna do

1:17:14.660 --> 1:17:16.100
 is find some steps along the way

1:17:16.100 --> 1:17:17.420
 that you can get there incrementally.

1:17:17.420 --> 1:17:19.180
 You don't have to like throw it all out

1:17:19.180 --> 1:17:20.460
 and start over again.

1:17:20.460 --> 1:17:22.340
 The first thing that we've done

1:17:22.340 --> 1:17:25.380
 is we focus on the sparse representations.

1:17:25.380 --> 1:17:28.420
 So just in case you don't know what that means

1:17:28.420 --> 1:17:31.220
 or some of the listeners don't know what that means,

1:17:31.220 --> 1:17:34.100
 in the brain, if I have like 10,000 neurons,

1:17:34.100 --> 1:17:36.980
 what you would see is maybe 2% of them active at a time.

1:17:36.980 --> 1:17:39.540
 You don't see 50%, you don't see 30%,

1:17:39.540 --> 1:17:41.220
 you might see 2%.

1:17:41.220 --> 1:17:42.660
 And it's always like that.

1:17:42.660 --> 1:17:44.380
 For any set of sensory inputs?

1:17:44.380 --> 1:17:45.340
 It doesn't matter if anything,

1:17:45.340 --> 1:17:47.380
 doesn't matter any part of the brain.

1:17:47.380 --> 1:17:51.100
 But which neurons differs?

1:17:51.100 --> 1:17:52.620
 Which neurons are active?

1:17:52.620 --> 1:17:55.380
 Yeah, so let's say I take 10,000 neurons

1:17:55.380 --> 1:17:56.300
 that are representing something.

1:17:56.300 --> 1:17:57.940
 They're sitting there in a little block together.

1:17:57.940 --> 1:18:00.060
 It's a teeny little block of neurons, 10,000 neurons.

1:18:00.060 --> 1:18:01.620
 And they're representing a location,

1:18:01.620 --> 1:18:02.500
 they're representing a cup,

1:18:02.500 --> 1:18:04.060
 they're representing the input from my sensors.

1:18:04.060 --> 1:18:05.380
 I don't know, it doesn't matter.

1:18:05.380 --> 1:18:07.020
 It's representing something.

1:18:07.020 --> 1:18:09.140
 The way the representations occur,

1:18:09.140 --> 1:18:10.620
 it's always a sparse representation.

1:18:10.620 --> 1:18:11.860
 Meaning it's a population code.

1:18:11.860 --> 1:18:14.980
 So which 200 cells are active tells me what's going on.

1:18:14.980 --> 1:18:18.060
 It's not, individual cells aren't that important at all.

1:18:18.060 --> 1:18:20.260
 It's the population code that matters.

1:18:20.260 --> 1:18:23.140
 And when you have sparse population codes,

1:18:23.140 --> 1:18:26.300
 then all kinds of beautiful properties come out of them.

1:18:26.300 --> 1:18:28.100
 So the brain uses sparse population codes.

1:18:28.100 --> 1:18:30.780
 We've written and described these benefits

1:18:30.780 --> 1:18:32.420
 in some of our papers.

1:18:32.420 --> 1:18:37.420
 So they give this tremendous robustness to the systems.

1:18:37.660 --> 1:18:39.180
 Brains are incredibly robust.

1:18:39.180 --> 1:18:41.140
 Neurons are dying all the time and spasming

1:18:41.140 --> 1:18:43.940
 and synapses are falling apart all the time.

1:18:43.940 --> 1:18:45.340
 And it keeps working.

1:18:45.340 --> 1:18:50.340
 So what Sibutai and Louise, one of our other engineers here

1:18:51.220 --> 1:18:55.740
 have done, have shown they're introducing sparseness

1:18:55.740 --> 1:18:56.860
 into convolutional neural networks.

1:18:56.860 --> 1:18:58.140
 Now other people are thinking along these lines,

1:18:58.140 --> 1:19:00.980
 but we're going about it in a more principled way, I think.

1:19:00.980 --> 1:19:04.100
 And we're showing that if you enforce sparseness

1:19:04.100 --> 1:19:06.340
 throughout these convolutional neural networks

1:19:07.340 --> 1:19:09.660
 in both the act, which sort of,

1:19:09.660 --> 1:19:12.780
 which neurons are active and the connections between them,

1:19:12.780 --> 1:19:15.660
 that you get some very desirable properties.

1:19:15.660 --> 1:19:18.860
 So one of the current hot topics in deep learning right now

1:19:18.860 --> 1:19:20.900
 are these adversarial examples.

1:19:20.900 --> 1:19:23.500
 So, you know, you give me any deep learning network

1:19:23.500 --> 1:19:26.060
 and I can give you a picture that looks perfect

1:19:26.060 --> 1:19:27.100
 and you're going to call it, you know,

1:19:27.100 --> 1:19:30.300
 you're going to say the monkey is, you know, an airplane.

1:19:30.300 --> 1:19:32.540
 So that's a problem.

1:19:32.540 --> 1:19:34.140
 And DARPA just announced some big thing.

1:19:34.140 --> 1:19:36.580
 They're trying to, you know, have some contest for this.

1:19:36.580 --> 1:19:40.180
 But if you enforce sparse representations here,

1:19:40.180 --> 1:19:41.500
 many of these problems go away.

1:19:41.500 --> 1:19:44.940
 They're much more robust and they're not easy to fool.

1:19:44.940 --> 1:19:48.340
 So we've already shown some of those results,

1:19:48.340 --> 1:19:51.140
 just literally in January or February,

1:19:51.140 --> 1:19:52.780
 just like last month we did that.

1:19:53.740 --> 1:19:57.340
 And you can, I think it's on bioRxiv right now,

1:19:57.340 --> 1:19:59.540
 or on iRxiv, you can read about it.

1:19:59.540 --> 1:20:03.100
 But, so that's like a baby step, okay?

1:20:03.100 --> 1:20:04.340
 That's taking something from the brain.

1:20:04.340 --> 1:20:05.620
 We know about sparseness.

1:20:05.620 --> 1:20:06.500
 We know why it's important.

1:20:06.500 --> 1:20:08.060
 We know what it gives the brain.

1:20:08.060 --> 1:20:09.500
 So let's try to enforce that onto this.

1:20:09.500 --> 1:20:12.420
 What's your intuition why sparsity leads to robustness?

1:20:12.420 --> 1:20:15.060
 Because it feels like it would be less robust.

1:20:15.060 --> 1:20:17.260
 Why would you feel the rest robust to you?

1:20:17.260 --> 1:20:22.260
 So it just feels like if the fewer neurons are involved,

1:20:24.380 --> 1:20:26.660
 the more fragile the representation.

1:20:26.660 --> 1:20:28.260
 But I didn't say there was lots of few neurons.

1:20:28.260 --> 1:20:29.860
 I said, let's say 200.

1:20:29.860 --> 1:20:31.020
 That's a lot.

1:20:31.020 --> 1:20:32.620
 There's still a lot, it's just.

1:20:32.620 --> 1:20:35.260
 So here's an intuition for it.

1:20:35.260 --> 1:20:39.860
 This is a bit technical, so for engineers,

1:20:39.860 --> 1:20:41.260
 machine learning people, this will be easy,

1:20:41.260 --> 1:20:42.980
 but all the listeners, maybe not.

1:20:44.300 --> 1:20:45.740
 If you're trying to classify something,

1:20:45.740 --> 1:20:48.380
 you're trying to divide some very high dimensional space

1:20:48.380 --> 1:20:50.380
 into different pieces, A and B.

1:20:50.380 --> 1:20:52.820
 And you're trying to create some point where you say,

1:20:52.820 --> 1:20:54.780
 all these points in this high dimensional space are A,

1:20:54.780 --> 1:20:57.580
 and all these points in this high dimensional space are B.

1:20:57.580 --> 1:21:01.980
 And if you have points that are close to that line,

1:21:01.980 --> 1:21:02.900
 it's not very robust.

1:21:02.900 --> 1:21:04.940
 It works for all the points you know about,

1:21:04.940 --> 1:21:07.100
 but it's not very robust,

1:21:07.100 --> 1:21:08.260
 because you can just move a little bit

1:21:08.260 --> 1:21:10.300
 and you've crossed over the line.

1:21:10.300 --> 1:21:12.700
 When you have sparse representations,

1:21:12.700 --> 1:21:16.060
 imagine I pick, I'm gonna pick 200 cells active

1:21:16.060 --> 1:21:19.260
 out of 10,000, okay?

1:21:19.260 --> 1:21:20.340
 So I have 200 cells active.

1:21:20.340 --> 1:21:22.220
 Now let's say I pick randomly another,

1:21:22.220 --> 1:21:24.420
 a different representation, 200.

1:21:24.420 --> 1:21:26.740
 The overlap between those is gonna be very small,

1:21:26.740 --> 1:21:28.060
 just a few.

1:21:28.060 --> 1:21:32.740
 I can pick millions of samples randomly of 200 neurons,

1:21:32.740 --> 1:21:36.980
 and not one of them will overlap more than just a few.

1:21:36.980 --> 1:21:39.140
 So one way to think about it is,

1:21:39.140 --> 1:21:41.460
 if I wanna fool one of these representations

1:21:41.460 --> 1:21:43.460
 to look like one of those other representations,

1:21:43.460 --> 1:21:45.660
 I can't move just one cell, or two cells,

1:21:45.660 --> 1:21:46.780
 or three cells, or four cells.

1:21:46.780 --> 1:21:49.140
 I have to move 100 cells.

1:21:49.140 --> 1:21:52.700
 And that makes them robust.

1:21:52.700 --> 1:21:56.180
 In terms of further, so you mentioned sparsity.

1:21:56.180 --> 1:21:57.260
 What would be the next thing?

1:21:57.260 --> 1:21:58.100
 Yeah.

1:21:58.100 --> 1:22:00.460
 Okay, so we have, we picked one.

1:22:00.460 --> 1:22:02.380
 We don't know if it's gonna work well yet.

1:22:02.380 --> 1:22:04.540
 So again, we're trying to come up with incremental ways

1:22:04.540 --> 1:22:07.860
 to moving from brain theory to add pieces

1:22:07.860 --> 1:22:10.140
 to machine learning, current machine learning world,

1:22:10.140 --> 1:22:12.260
 and one step at a time.

1:22:12.260 --> 1:22:13.740
 So the next thing we're gonna try to do

1:22:13.740 --> 1:22:15.820
 is sort of incorporate some of the ideas

1:22:15.820 --> 1:22:19.100
 of the thousand brains theory,

1:22:19.100 --> 1:22:22.580
 that you have many, many models that are voting.

1:22:22.580 --> 1:22:23.700
 Now that idea is not new.

1:22:23.700 --> 1:22:25.300
 There's a mixture of models that's been around

1:22:25.300 --> 1:22:26.280
 for a long time.

1:22:27.160 --> 1:22:29.740
 But the way the brain does it is a little different.

1:22:29.740 --> 1:22:33.620
 And the way it votes is different.

1:22:33.620 --> 1:22:36.220
 And the kind of way it represents uncertainty

1:22:36.220 --> 1:22:37.180
 is different.

1:22:37.180 --> 1:22:39.980
 So we're just starting this work,

1:22:39.980 --> 1:22:42.280
 but we're gonna try to see if we can sort of incorporate

1:22:42.280 --> 1:22:43.760
 some of the principles of voting,

1:22:43.760 --> 1:22:45.940
 or principles of the thousand brain theory.

1:22:45.940 --> 1:22:49.420
 Like lots of simple models that talk to each other

1:22:49.420 --> 1:22:53.040
 in a certain way.

1:22:53.940 --> 1:22:57.700
 And can we build more machines, systems that learn faster

1:22:57.700 --> 1:23:02.700
 and also, well mostly are multimodal

1:23:03.220 --> 1:23:07.500
 and robust to multimodal type of issues.

1:23:07.500 --> 1:23:09.580
 So one of the challenges there

1:23:09.580 --> 1:23:13.100
 is the machine learning computer vision community

1:23:13.100 --> 1:23:15.600
 has certain sets of benchmarks,

1:23:15.600 --> 1:23:18.180
 sets of tests based on which they compete.

1:23:18.180 --> 1:23:22.060
 And I would argue, especially from your perspective,

1:23:22.060 --> 1:23:24.660
 that those benchmarks aren't that useful

1:23:24.660 --> 1:23:28.860
 for testing the aspects that the brain is good at,

1:23:28.860 --> 1:23:29.940
 or intelligence.

1:23:29.940 --> 1:23:31.300
 They're not really testing intelligence.

1:23:31.300 --> 1:23:32.980
 They're very fine.

1:23:32.980 --> 1:23:34.780
 And it's been extremely useful

1:23:34.780 --> 1:23:37.420
 for developing specific mathematical models,

1:23:37.420 --> 1:23:40.420
 but it's not useful in the long term

1:23:40.420 --> 1:23:41.680
 for creating intelligence.

1:23:41.680 --> 1:23:44.660
 So you think you also have a role in proposing

1:23:44.660 --> 1:23:47.020
 better tests?

1:23:47.020 --> 1:23:48.460
 Yeah, this is a very,

1:23:48.460 --> 1:23:50.460
 you've identified a very serious problem.

1:23:51.440 --> 1:23:53.340
 First of all, the tests that they have

1:23:53.340 --> 1:23:54.580
 are the tests that they want.

1:23:54.580 --> 1:23:55.860
 Not the tests of the other things

1:23:55.860 --> 1:23:57.620
 that we're trying to do, right?

1:23:58.740 --> 1:24:01.700
 You know, what are the, so on.

1:24:01.700 --> 1:24:04.220
 The second thing is sometimes these,

1:24:04.220 --> 1:24:06.620
 to be competitive in these tests,

1:24:06.620 --> 1:24:09.940
 you have to have huge data sets and huge computing power.

1:24:10.820 --> 1:24:13.420
 And so, you know, and we don't have that here.

1:24:13.420 --> 1:24:15.500
 We don't have it as well as other big teams

1:24:15.500 --> 1:24:17.600
 that big companies do.

1:24:18.700 --> 1:24:20.900
 So there's numerous issues there.

1:24:20.900 --> 1:24:22.420
 You know, we come out, you know,

1:24:22.420 --> 1:24:24.260
 where our approach to this is all based on,

1:24:24.260 --> 1:24:26.100
 in some sense, you might argue, elegance.

1:24:26.100 --> 1:24:27.780
 We're coming at it from like a theoretical base

1:24:27.780 --> 1:24:29.980
 that we think, oh my God, this is so clearly elegant.

1:24:29.980 --> 1:24:30.820
 This is how brains work.

1:24:30.820 --> 1:24:31.860
 This is what intelligence is.

1:24:31.860 --> 1:24:33.940
 But the machine learning world has gotten in this phase

1:24:33.940 --> 1:24:35.500
 where they think it doesn't matter.

1:24:35.500 --> 1:24:36.600
 Doesn't matter what you think,

1:24:36.600 --> 1:24:39.440
 as long as you do, you know, 0.1% better on this benchmark,

1:24:39.440 --> 1:24:40.780
 that's what, that's all that matters.

1:24:40.780 --> 1:24:42.740
 And that's a problem.

1:24:43.860 --> 1:24:46.060
 You know, we have to figure out how to get around that.

1:24:46.060 --> 1:24:47.300
 That's a challenge for us.

1:24:47.300 --> 1:24:50.500
 That's one of the challenges that we have to deal with.

1:24:50.500 --> 1:24:52.820
 So I agree, you've identified a big issue.

1:24:52.820 --> 1:24:55.900
 It's difficult for those reasons.

1:24:55.900 --> 1:24:59.580
 But you know, part of the reasons I'm talking to you here

1:24:59.580 --> 1:25:01.620
 today is I hope I'm gonna get some machine learning people

1:25:01.620 --> 1:25:03.260
 to say, I'm gonna read those papers.

1:25:03.260 --> 1:25:04.500
 Those might be some interesting ideas.

1:25:04.500 --> 1:25:08.460
 I'm tired of doing this 0.1% improvement stuff, you know?

1:25:08.460 --> 1:25:10.340
 Well, that's why I'm here as well,

1:25:10.340 --> 1:25:13.020
 because I think machine learning now as a community

1:25:13.020 --> 1:25:18.020
 is at a place where the next step needs to be orthogonal

1:25:18.500 --> 1:25:21.300
 to what has received success in the past.

1:25:21.300 --> 1:25:23.100
 Well, you see other leaders saying this,

1:25:23.100 --> 1:25:25.500
 machine learning leaders, you know,

1:25:25.500 --> 1:25:27.940
 Jeff Hinton with his capsules idea.

1:25:27.940 --> 1:25:29.300
 Many people have gotten up to say, you know,

1:25:29.300 --> 1:25:32.100
 we're gonna hit road map, maybe we should look at the brain,

1:25:32.100 --> 1:25:33.460
 you know, things like that.

1:25:33.460 --> 1:25:38.100
 So hopefully that thinking will occur organically.

1:25:38.100 --> 1:25:40.740
 And then we're in a nice position for people to come

1:25:40.740 --> 1:25:41.740
 and look at our work and say,

1:25:41.740 --> 1:25:43.180
 well, what can we learn from these guys?

1:25:43.180 --> 1:25:47.500
 Yeah, MIT is launching a billion dollar computing college

1:25:47.500 --> 1:25:49.220
 that's centered around this idea, so.

1:25:49.220 --> 1:25:50.980
 Is it on this idea of what?

1:25:50.980 --> 1:25:52.700
 Well, the idea that, you know,

1:25:52.700 --> 1:25:54.980
 the humanities, psychology, and neuroscience

1:25:54.980 --> 1:25:58.860
 have to work all together to get to build the S.

1:25:58.860 --> 1:26:00.340
 Yeah, I mean, Stanford just did

1:26:00.340 --> 1:26:02.500
 this Human Centered AI Center.

1:26:02.500 --> 1:26:04.420
 I'm a little disappointed in these initiatives

1:26:04.420 --> 1:26:08.340
 because, you know, they're focusing

1:26:08.340 --> 1:26:09.940
 on sort of the human side of it,

1:26:09.940 --> 1:26:12.140
 and it could very easily slip into

1:26:12.140 --> 1:26:16.060
 how humans interact with intelligent machines,

1:26:16.060 --> 1:26:17.620
 which is nothing wrong with that,

1:26:17.620 --> 1:26:19.420
 but that's not, that is orthogonal

1:26:19.420 --> 1:26:20.380
 to what we're trying to do.

1:26:20.380 --> 1:26:21.340
 We're trying to say, like,

1:26:21.340 --> 1:26:22.860
 what is the essence of intelligence?

1:26:22.860 --> 1:26:23.700
 I don't care.

1:26:23.700 --> 1:26:25.500
 In fact, I wanna build intelligent machines

1:26:25.500 --> 1:26:28.620
 that aren't emotional, that don't smile at you,

1:26:28.620 --> 1:26:31.820
 that, you know, that aren't trying to tuck you in at night.

1:26:31.820 --> 1:26:34.020
 Yeah, there is that pattern that you,

1:26:34.020 --> 1:26:36.500
 when you talk about understanding humans

1:26:36.500 --> 1:26:38.380
 is important for understanding intelligence,

1:26:38.380 --> 1:26:41.140
 that you start slipping into topics of ethics

1:26:41.140 --> 1:26:43.700
 or, yeah, like you said,

1:26:43.700 --> 1:26:45.700
 the interactive elements as opposed to,

1:26:45.700 --> 1:26:47.380
 no, no, no, we have to zoom in on the brain,

1:26:47.380 --> 1:26:51.460
 study what the human brain, the baby, the...

1:26:51.460 --> 1:26:52.900
 Let's study what a brain does.

1:26:52.900 --> 1:26:53.740
 Does.

1:26:53.740 --> 1:26:54.780
 And then we can decide which parts of that

1:26:54.780 --> 1:26:57.740
 we wanna recreate in some system,

1:26:57.740 --> 1:26:59.900
 but until you have that theory about what the brain does,

1:26:59.900 --> 1:27:01.300
 what's the point, you know, it's just,

1:27:01.300 --> 1:27:02.740
 you're gonna be wasting time, I think.

1:27:02.740 --> 1:27:04.060
 Right, just to break it down

1:27:04.060 --> 1:27:05.620
 on the artificial neural network side,

1:27:05.620 --> 1:27:06.740
 maybe you could speak to this

1:27:06.740 --> 1:27:09.180
 on the biological neural network side,

1:27:09.180 --> 1:27:11.940
 the process of learning versus the process of inference.

1:27:13.300 --> 1:27:15.620
 Maybe you can explain to me,

1:27:15.620 --> 1:27:18.460
 is there a difference between,

1:27:18.460 --> 1:27:19.860
 you know, in artificial neural networks,

1:27:19.860 --> 1:27:21.500
 there's a difference between the learning stage

1:27:21.500 --> 1:27:22.940
 and the inference stage.

1:27:22.940 --> 1:27:24.980
 Do you see the brain as something different?

1:27:24.980 --> 1:27:29.020
 One of the big distinctions that people often say,

1:27:29.020 --> 1:27:30.660
 I don't know how correct it is,

1:27:30.660 --> 1:27:32.940
 is artificial neural networks need a lot of data.

1:27:32.940 --> 1:27:34.820
 They're very inefficient learning.

1:27:34.820 --> 1:27:37.340
 Do you see that as a correct distinction

1:27:37.340 --> 1:27:40.300
 from the biology of the human brain,

1:27:40.300 --> 1:27:41.980
 that the human brain is very efficient,

1:27:41.980 --> 1:27:44.220
 or is that just something we deceive ourselves?

1:27:44.220 --> 1:27:45.420
 No, it is efficient, obviously.

1:27:45.420 --> 1:27:47.580
 We can learn new things almost instantly.

1:27:47.580 --> 1:27:50.020
 And so what elements do you think are useful?

1:27:50.020 --> 1:27:50.860
 Yeah, I can talk about that.

1:27:50.860 --> 1:27:52.300
 You brought up two issues there.

1:27:52.300 --> 1:27:54.820
 So remember I talked early about the constraints

1:27:54.820 --> 1:27:57.260
 we always feel, well, one of those constraints

1:27:57.260 --> 1:28:00.940
 is the fact that brains are continually learning.

1:28:00.940 --> 1:28:03.780
 That's not something we said, oh, we can add that later.

1:28:03.780 --> 1:28:05.780
 That's something that was upfront,

1:28:05.780 --> 1:28:07.740
 had to be there from the start,

1:28:08.900 --> 1:28:11.260
 made our problems harder.

1:28:11.260 --> 1:28:14.420
 But we showed, going back to the 2016 paper

1:28:14.420 --> 1:28:16.780
 on sequence memory, we showed how that happens,

1:28:16.780 --> 1:28:19.940
 how the brains infer and learn at the same time.

1:28:19.940 --> 1:28:21.740
 And our models do that.

1:28:21.740 --> 1:28:24.060
 And they're not two separate phases,

1:28:24.060 --> 1:28:26.340
 or two separate sets of time.

1:28:26.340 --> 1:28:29.780
 I think that's a big, big problem in AI,

1:28:29.780 --> 1:28:32.540
 at least for many applications, not for all.

1:28:33.420 --> 1:28:34.380
 So I can talk about that.

1:28:34.380 --> 1:28:37.180
 There are some, it gets detailed,

1:28:37.180 --> 1:28:39.660
 there are some parts of the neocortex in the brain

1:28:39.660 --> 1:28:41.740
 where actually what's going on,

1:28:41.740 --> 1:28:46.740
 there's these cycles of activity in the brain.

1:28:46.860 --> 1:28:49.260
 And there's very strong evidence

1:28:49.260 --> 1:28:51.260
 that you're doing more of inference

1:28:51.260 --> 1:28:52.300
 on one part of the phase,

1:28:52.300 --> 1:28:54.100
 and more of learning on the other part of the phase.

1:28:54.100 --> 1:28:55.500
 So the brain can actually sort of separate

1:28:55.500 --> 1:28:56.660
 different populations of cells

1:28:56.660 --> 1:28:58.340
 or going back and forth like this.

1:28:58.340 --> 1:29:01.540
 But in general, I would say that's an important problem.

1:29:01.540 --> 1:29:05.620
 We have all of our networks that we've come up with do both.

1:29:05.620 --> 1:29:08.220
 And they're continuous learning networks.

1:29:08.220 --> 1:29:10.980
 And you mentioned benchmarks earlier.

1:29:10.980 --> 1:29:12.500
 Well, there are no benchmarks about that.

1:29:12.500 --> 1:29:17.180
 So we have to, we get in our little soapbox,

1:29:17.180 --> 1:29:19.220
 and hey, by the way, this is important,

1:29:19.220 --> 1:29:20.580
 and here's a mechanism for doing that.

1:29:20.580 --> 1:29:23.900
 But until you can prove it to someone

1:29:23.900 --> 1:29:26.700
 in some commercial system or something, it's a little harder.

1:29:26.700 --> 1:29:28.980
 So yeah, one of the things I had to linger on that

1:29:28.980 --> 1:29:33.780
 is in some ways to learn the concept of a coffee cup,

1:29:33.780 --> 1:29:35.900
 you only need this one coffee cup

1:29:35.900 --> 1:29:37.980
 and maybe some time alone in a room with it.

1:29:37.980 --> 1:29:39.940
 Well, the first thing is,

1:29:39.940 --> 1:29:41.820
 imagine I reach my hand into a black box

1:29:41.820 --> 1:29:43.700
 and I'm reaching, I'm trying to touch something.

1:29:43.700 --> 1:29:46.220
 I don't know upfront if it's something I already know

1:29:46.220 --> 1:29:47.860
 or if it's a new thing.

1:29:47.860 --> 1:29:50.460
 And I have to, I'm doing both at the same time.

1:29:50.460 --> 1:29:53.260
 I don't say, oh, let's see if it's a new thing.

1:29:53.260 --> 1:29:54.740
 Oh, let's see if it's an old thing.

1:29:54.740 --> 1:29:55.580
 I don't do that.

1:29:55.580 --> 1:29:59.420
 As I go, my brain says, oh, it's new or it's not new.

1:29:59.420 --> 1:30:02.300
 And if it's new, I start learning what it is.

1:30:02.300 --> 1:30:04.820
 And by the way, it starts learning from the get go,

1:30:04.820 --> 1:30:06.020
 even if it's gonna recognize it.

1:30:06.020 --> 1:30:08.900
 So they're not separate problems.

1:30:08.900 --> 1:30:10.060
 And so that's the thing there.

1:30:10.060 --> 1:30:12.540
 The other thing you mentioned was the fast learning.

1:30:13.540 --> 1:30:15.580
 So I was just talking about continuous learning,

1:30:15.580 --> 1:30:16.660
 but there's also fast learning.

1:30:16.660 --> 1:30:18.780
 Literally, I can show you this coffee cup

1:30:18.780 --> 1:30:20.060
 and I say, here's a new coffee cup.

1:30:20.060 --> 1:30:21.340
 It's got the logo on it.

1:30:21.340 --> 1:30:23.860
 Take a look at it, done, you're done.

1:30:23.860 --> 1:30:25.380
 You can predict what it's gonna look like,

1:30:25.380 --> 1:30:27.460
 you know, in different positions.

1:30:27.460 --> 1:30:29.540
 So I can talk about that too.

1:30:29.540 --> 1:30:34.220
 In the brain, the way learning occurs,

1:30:34.220 --> 1:30:35.700
 I mentioned this earlier, but I'll mention it again.

1:30:35.700 --> 1:30:36.820
 The way learning occurs,

1:30:36.820 --> 1:30:39.180
 imagine I am a section of a dendrite of a neuron,

1:30:40.140 --> 1:30:43.740
 and I'm gonna learn something new.

1:30:43.740 --> 1:30:44.580
 Doesn't matter what it is.

1:30:44.580 --> 1:30:46.180
 I'm just gonna learn something new.

1:30:46.180 --> 1:30:48.900
 I need to recognize a new pattern.

1:30:48.900 --> 1:30:52.540
 So what I'm gonna do is I'm gonna form new synapses.

1:30:52.540 --> 1:30:55.140
 New synapses, we're gonna rewire the brain

1:30:55.140 --> 1:30:57.900
 onto that section of the dendrite.

1:30:57.900 --> 1:31:01.020
 Once I've done that, everything else that neuron has learned

1:31:01.020 --> 1:31:02.580
 is not affected by it.

1:31:02.580 --> 1:31:04.340
 That's because it's isolated

1:31:04.340 --> 1:31:06.380
 to that small section of the dendrite.

1:31:06.380 --> 1:31:09.580
 They're not all being added together, like a point neuron.

1:31:09.580 --> 1:31:11.740
 So if I learn something new on this segment here,

1:31:11.740 --> 1:31:13.180
 it doesn't change any of the learning

1:31:13.180 --> 1:31:14.860
 that occur anywhere else in that neuron.

1:31:14.860 --> 1:31:18.420
 So I can add something without affecting previous learning.

1:31:18.420 --> 1:31:19.780
 And I can do it quickly.

1:31:20.940 --> 1:31:22.300
 Now let's talk, we can talk about the quickness,

1:31:22.300 --> 1:31:24.020
 how it's done in real neurons.

1:31:24.020 --> 1:31:26.740
 You might say, well, doesn't it take time to form synapses?

1:31:26.740 --> 1:31:30.900
 Yes, it can take maybe an hour to form a new synapse.

1:31:30.900 --> 1:31:32.500
 We can form memories quicker than that,

1:31:32.500 --> 1:31:35.860
 and I can explain that how it happens too, if you want.

1:31:35.860 --> 1:31:38.300
 But it's getting a bit neurosciencey.

1:31:39.460 --> 1:31:41.380
 That's great, but is there an understanding

1:31:41.380 --> 1:31:43.100
 of these mechanisms at every level?

1:31:43.100 --> 1:31:43.940
 Yeah.

1:31:43.940 --> 1:31:46.420
 So from the short term memories and the forming.

1:31:48.620 --> 1:31:51.580
 So this idea of synaptogenesis, the growth of new synapses,

1:31:51.580 --> 1:31:54.100
 that's well described, it's well understood.

1:31:54.100 --> 1:31:55.820
 And that's an essential part of learning.

1:31:55.820 --> 1:31:56.780
 That is learning.

1:31:56.780 --> 1:31:58.180
 That is learning.

1:31:58.180 --> 1:31:59.020
 Okay.

1:32:01.980 --> 1:32:03.860
 Going back many, many years,

1:32:03.860 --> 1:32:06.340
 people, you know, it was, what's his name,

1:32:06.340 --> 1:32:09.580
 the psychologist who proposed, Hebb, Donald Hebb.

1:32:09.580 --> 1:32:12.020
 He proposed that learning was the modification

1:32:12.020 --> 1:32:15.460
 of the strength of a connection between two neurons.

1:32:15.460 --> 1:32:18.180
 People interpreted that as the modification

1:32:18.180 --> 1:32:19.660
 of the strength of a synapse.

1:32:19.660 --> 1:32:20.980
 He didn't say that.

1:32:20.980 --> 1:32:22.340
 He just said there's a modification

1:32:22.340 --> 1:32:24.540
 between the effect of one neuron and another.

1:32:24.540 --> 1:32:26.500
 So synaptogenesis is totally consistent

1:32:26.500 --> 1:32:28.180
 with what Donald Hebb said.

1:32:28.180 --> 1:32:29.860
 But anyway, there's these mechanisms,

1:32:29.860 --> 1:32:30.860
 the growth of new synapses.

1:32:30.860 --> 1:32:32.260
 You can go online, you can watch a video

1:32:32.260 --> 1:32:33.900
 of a synapse growing in real time.

1:32:33.900 --> 1:32:37.140
 It's literally, you can see this little thing going boop.

1:32:37.140 --> 1:32:38.420
 It's pretty impressive.

1:32:38.420 --> 1:32:39.740
 So those mechanisms are known.

1:32:39.740 --> 1:32:42.340
 Now there's another thing that we've speculated

1:32:42.340 --> 1:32:43.540
 and we've written about,

1:32:43.540 --> 1:32:45.780
 which is consistent with known neuroscience,

1:32:45.780 --> 1:32:48.340
 but it's less proven.

1:32:48.340 --> 1:32:50.580
 And this is the idea, how do I form a memory

1:32:50.580 --> 1:32:51.620
 really, really quickly?

1:32:51.620 --> 1:32:52.820
 Like instantaneous.

1:32:52.820 --> 1:32:54.580
 If it takes an hour to grow a synapse,

1:32:54.580 --> 1:32:56.820
 like that's not instantaneous.

1:32:56.820 --> 1:33:01.700
 So there are types of synapses called silent synapses.

1:33:01.700 --> 1:33:04.060
 They look like a synapse, but they don't do anything.

1:33:04.060 --> 1:33:04.900
 They're just sitting there.

1:33:04.900 --> 1:33:07.900
 It's like if an action potential comes in,

1:33:07.900 --> 1:33:10.140
 it doesn't release any neurotransmitter.

1:33:10.140 --> 1:33:12.500
 Some parts of the brain have more of these than others.

1:33:12.500 --> 1:33:14.020
 For example, the hippocampus has a lot of them,

1:33:14.020 --> 1:33:17.020
 which is where we associate most short term memory with.

1:33:18.540 --> 1:33:22.100
 So what we speculated, again, in that 2016 paper,

1:33:22.100 --> 1:33:26.420
 we proposed that the way we form very quick memories,

1:33:26.420 --> 1:33:28.940
 very short term memories, or quick memories,

1:33:28.940 --> 1:33:33.860
 is that we convert silent synapses into active synapses.

1:33:33.860 --> 1:33:36.060
 It's like saying a synapse has a zero weight

1:33:36.060 --> 1:33:36.900
 and a one weight,

1:33:37.860 --> 1:33:41.460
 but the longterm memory has to be formed by synaptogenesis.

1:33:41.460 --> 1:33:43.300
 So you can remember something really quickly

1:33:43.300 --> 1:33:46.220
 by just flipping a bunch of these guys from silent to active.

1:33:46.220 --> 1:33:49.140
 It's not from 0.1 to 0.15.

1:33:49.140 --> 1:33:50.700
 It's like, it doesn't do anything

1:33:50.700 --> 1:33:52.260
 till it releases transmitter.

1:33:52.260 --> 1:33:53.500
 And if I do that over a bunch of these,

1:33:53.500 --> 1:33:55.820
 I've got a very quick short term memory.

1:33:56.860 --> 1:33:58.500
 So I guess the lesson behind this

1:33:58.500 --> 1:34:01.860
 is that most neural networks today are fully connected.

1:34:01.860 --> 1:34:03.380
 Every neuron connects every other neuron

1:34:03.380 --> 1:34:04.580
 from layer to layer.

1:34:04.580 --> 1:34:06.060
 That's not correct in the brain.

1:34:06.060 --> 1:34:06.980
 We don't want that.

1:34:06.980 --> 1:34:08.340
 We actually don't want that.

1:34:08.340 --> 1:34:09.260
 It's bad.

1:34:09.260 --> 1:34:10.700
 You want a very sparse connectivity

1:34:10.700 --> 1:34:14.500
 so that any neuron connects to some subset of the neurons

1:34:14.500 --> 1:34:15.340
 in the other layer.

1:34:15.340 --> 1:34:18.980
 And it does so on a dendrite by dendrite segment basis.

1:34:18.980 --> 1:34:21.580
 So it's a very some parcelated out type of thing.

1:34:21.580 --> 1:34:25.380
 And that then learning is not adjusting all these weights,

1:34:25.380 --> 1:34:26.340
 but learning is just saying,

1:34:26.340 --> 1:34:30.180
 okay, connect to these 10 cells here right now.

1:34:30.180 --> 1:34:32.980
 In that process, you know, with artificial neural networks,

1:34:32.980 --> 1:34:36.060
 it's a very simple process of backpropagation

1:34:36.060 --> 1:34:37.180
 that adjusts the weights.

1:34:37.180 --> 1:34:40.100
 The process of synaptogenesis.

1:34:40.100 --> 1:34:40.940
 Synaptogenesis.

1:34:40.940 --> 1:34:42.300
 Synaptogenesis.

1:34:42.300 --> 1:34:43.140
 It's even easier.

1:34:43.140 --> 1:34:43.980
 It's even easier.

1:34:43.980 --> 1:34:44.820
 It's even easier.

1:34:44.820 --> 1:34:47.260
 Backpropagation requires something

1:34:47.260 --> 1:34:48.700
 that really can't happen in brains.

1:34:48.700 --> 1:34:51.220
 This backpropagation of this error signal,

1:34:51.220 --> 1:34:52.060
 that really can't happen.

1:34:52.060 --> 1:34:53.500
 People are trying to make it happen in brains,

1:34:53.500 --> 1:34:54.740
 but it doesn't happen in brains.

1:34:54.740 --> 1:34:56.780
 This is pure Hebbian learning.

1:34:56.780 --> 1:34:58.660
 Well, synaptogenesis is pure Hebbian learning.

1:34:58.660 --> 1:35:00.140
 It's basically saying,

1:35:00.140 --> 1:35:01.540
 there's a population of cells over here

1:35:01.540 --> 1:35:03.020
 that are active right now.

1:35:03.020 --> 1:35:04.340
 And there's a population of cells over here

1:35:04.340 --> 1:35:05.380
 active right now.

1:35:05.380 --> 1:35:07.980
 How do I form connections between those active cells?

1:35:07.980 --> 1:35:11.100
 And it's literally saying this guy became active.

1:35:11.100 --> 1:35:13.260
 These 100 neurons here became active

1:35:13.260 --> 1:35:15.080
 before this neuron became active.

1:35:15.080 --> 1:35:17.140
 So form connections to those ones.

1:35:17.140 --> 1:35:17.960
 That's it.

1:35:17.960 --> 1:35:19.940
 There's no propagation of error, nothing.

1:35:19.940 --> 1:35:20.980
 All the networks we do,

1:35:20.980 --> 1:35:25.700
 all the models we have work on almost completely on

1:35:25.700 --> 1:35:26.540
 Hebbian learning,

1:35:26.540 --> 1:35:30.260
 but on dendritic segments

1:35:30.260 --> 1:35:33.060
 and multiple synapses at the same time.

1:35:33.060 --> 1:35:34.540
 So now let's sort of turn the question

1:35:34.540 --> 1:35:35.820
 that you already answered,

1:35:35.820 --> 1:35:37.760
 and maybe you can answer it again.

1:35:38.820 --> 1:35:41.260
 If you look at the history of artificial intelligence,

1:35:41.260 --> 1:35:43.540
 where do you think we stand?

1:35:43.540 --> 1:35:45.780
 How far are we from solving intelligence?

1:35:45.780 --> 1:35:47.700
 You said you were very optimistic.

1:35:47.700 --> 1:35:48.900
 Can you elaborate on that?

1:35:48.900 --> 1:35:53.500
 Yeah, it's always the crazy question to ask

1:35:53.500 --> 1:35:55.100
 because no one can predict the future.

1:35:55.100 --> 1:35:55.940
 Absolutely.

1:35:55.940 --> 1:35:58.180
 So I'll tell you a story.

1:35:58.180 --> 1:36:01.400
 I used to run a different neuroscience institute

1:36:01.400 --> 1:36:02.620
 called the Redwood Neuroscience Institute,

1:36:02.620 --> 1:36:04.740
 and we would hold these symposiums

1:36:04.740 --> 1:36:06.340
 and we'd get like 35 scientists

1:36:06.340 --> 1:36:08.060
 from around the world to come together.

1:36:08.060 --> 1:36:10.380
 And I used to ask them all the same question.

1:36:10.380 --> 1:36:11.740
 I would say, well, how long do you think it'll be

1:36:11.740 --> 1:36:14.540
 before we understand how the neocortex works?

1:36:14.540 --> 1:36:15.540
 And everyone went around the room

1:36:15.540 --> 1:36:16.560
 and they had introduced the name

1:36:16.560 --> 1:36:18.240
 and they have to answer that question.

1:36:18.240 --> 1:36:22.940
 So I got, the typical answer was 50 to 100 years.

1:36:22.940 --> 1:36:24.780
 Some people would say 500 years.

1:36:24.780 --> 1:36:25.860
 Some people said never.

1:36:25.860 --> 1:36:27.820
 I said, why are you a neuroscientist?

1:36:27.820 --> 1:36:30.500
 It's never gonna, it's a good pay.

1:36:32.780 --> 1:36:34.380
 It's interesting.

1:36:34.380 --> 1:36:36.300
 So, you know, but it doesn't work like that.

1:36:36.300 --> 1:36:38.740
 As I mentioned earlier, these are not,

1:36:38.740 --> 1:36:39.620
 these are step functions.

1:36:39.620 --> 1:36:41.780
 Things happen and then bingo, they happen.

1:36:41.780 --> 1:36:43.620
 You can't predict that.

1:36:43.620 --> 1:36:45.620
 I feel I've already passed a step function.

1:36:45.620 --> 1:36:49.100
 So if I can do my job correctly over the next five years,

1:36:50.740 --> 1:36:53.540
 then, meaning I can proselytize these ideas.

1:36:53.540 --> 1:36:56.140
 I can convince other people they're right.

1:36:56.140 --> 1:36:58.740
 We can show that other people,

1:36:58.740 --> 1:37:00.260
 machine learning people should pay attention

1:37:00.260 --> 1:37:01.420
 to these ideas.

1:37:01.420 --> 1:37:04.580
 Then we're definitely in an under 20 year timeframe.

1:37:04.580 --> 1:37:07.780
 If I can do those things, if I'm not successful in that,

1:37:07.780 --> 1:37:09.780
 and this is the last time anyone talks to me

1:37:09.780 --> 1:37:12.180
 and no one reads our papers and you know,

1:37:12.180 --> 1:37:13.980
 and I'm wrong or something like that,

1:37:13.980 --> 1:37:15.940
 then I don't know.

1:37:15.940 --> 1:37:17.820
 But it's not 50 years.

1:37:21.820 --> 1:37:22.940
 Think about electric cars.

1:37:22.940 --> 1:37:24.940
 How quickly are they gonna populate the world?

1:37:24.940 --> 1:37:27.040
 It probably takes about a 20 year span.

1:37:27.900 --> 1:37:28.820
 It'll be something like that.

1:37:28.820 --> 1:37:31.740
 But I think if I can do what I said, we're starting it.

1:37:31.740 --> 1:37:34.220
 And of course there could be other,

1:37:34.220 --> 1:37:35.400
 you said step functions.

1:37:35.400 --> 1:37:40.100
 It could be everybody gives up on your ideas for 20 years

1:37:40.100 --> 1:37:42.180
 and then all of a sudden somebody picks it up again.

1:37:42.180 --> 1:37:43.620
 Wait, that guy was onto something.

1:37:43.620 --> 1:37:46.660
 Yeah, so that would be a failure on my part, right?

1:37:47.540 --> 1:37:49.820
 Think about Charles Babbage.

1:37:49.820 --> 1:37:52.220
 Charles Babbage, he's the guy who invented the computer

1:37:52.220 --> 1:37:55.820
 back in the 18 something, 1800s.

1:37:55.820 --> 1:37:59.460
 And everyone forgot about it until 100 years later.

1:37:59.460 --> 1:38:00.900
 And say, hey, this guy figured this stuff out

1:38:00.900 --> 1:38:02.380
 a long time ago.

1:38:02.380 --> 1:38:03.940
 But he was ahead of his time.

1:38:03.940 --> 1:38:06.460
 I don't think, as I said,

1:38:06.460 --> 1:38:09.780
 I recognize this is part of any entrepreneur's challenge.

1:38:09.780 --> 1:38:11.500
 I use entrepreneur broadly in this case.

1:38:11.500 --> 1:38:12.980
 I'm not meaning like I'm building a business

1:38:12.980 --> 1:38:13.820
 or trying to sell something.

1:38:13.820 --> 1:38:15.900
 I mean, I'm trying to sell ideas.

1:38:15.900 --> 1:38:19.380
 And this is the challenge as to how you get people

1:38:19.380 --> 1:38:21.540
 to pay attention to you, how do you get them

1:38:21.540 --> 1:38:24.700
 to give you positive or negative feedback,

1:38:24.700 --> 1:38:25.960
 how do you get the people to act differently

1:38:25.960 --> 1:38:27.220
 based on your ideas.

1:38:27.220 --> 1:38:30.180
 So we'll see how well we do on that.

1:38:30.180 --> 1:38:32.300
 So you know that there's a lot of hype

1:38:32.300 --> 1:38:34.640
 behind artificial intelligence currently.

1:38:34.640 --> 1:38:39.540
 Do you, as you look to spread the ideas

1:38:39.540 --> 1:38:43.300
 that are of neocortical theory, the things you're working on,

1:38:43.300 --> 1:38:45.100
 do you think there's some possibility

1:38:45.100 --> 1:38:47.300
 we'll hit an AI winter once again?

1:38:47.300 --> 1:38:48.940
 Yeah, it's certainly a possibility.

1:38:48.940 --> 1:38:49.780
 No question about it.

1:38:49.780 --> 1:38:50.600
 Is that something you worry about?

1:38:50.600 --> 1:38:52.740
 Yeah, well, I guess, do I worry about it?

1:38:54.340 --> 1:38:57.540
 I haven't decided yet if that's good or bad for my mission.

1:38:57.540 --> 1:38:59.660
 That's true, that's very true.

1:38:59.660 --> 1:39:02.940
 Because it's almost like you need the winter

1:39:02.940 --> 1:39:04.300
 to refresh the palette.

1:39:04.300 --> 1:39:07.860
 Yeah, it's like, I want, here's what you wanna have it is.

1:39:07.860 --> 1:39:12.180
 You want, like to the extent that everyone is so thrilled

1:39:12.180 --> 1:39:15.460
 about the current state of machine learning and AI

1:39:15.460 --> 1:39:18.100
 and they don't imagine they need anything else,

1:39:18.100 --> 1:39:19.740
 it makes my job harder.

1:39:19.740 --> 1:39:22.580
 If everything crashed completely

1:39:22.580 --> 1:39:24.260
 and every student left the field

1:39:24.260 --> 1:39:26.200
 and there was no money for anybody to do anything

1:39:26.200 --> 1:39:27.460
 and it became an embarrassment

1:39:27.460 --> 1:39:29.020
 to talk about machine intelligence and AI,

1:39:29.020 --> 1:39:30.740
 that wouldn't be good for us either.

1:39:30.740 --> 1:39:33.400
 You want sort of the soft landing approach, right?

1:39:33.400 --> 1:39:36.620
 You want enough people, the senior people in AI

1:39:36.620 --> 1:39:37.860
 and machine learning to say, you know,

1:39:37.860 --> 1:39:38.900
 we need other approaches.

1:39:38.900 --> 1:39:40.460
 We really need other approaches.

1:39:40.460 --> 1:39:42.020
 Damn, we need other approaches.

1:39:42.020 --> 1:39:43.100
 Maybe we should look to the brain.

1:39:43.100 --> 1:39:44.220
 Okay, let's look to the brain.

1:39:44.220 --> 1:39:45.380
 Who's got some brain ideas?

1:39:45.380 --> 1:39:47.900
 Okay, let's start a little project on the side here

1:39:47.900 --> 1:39:49.700
 trying to do brain idea related stuff.

1:39:49.700 --> 1:39:51.820
 That's the ideal outcome we would want.

1:39:51.820 --> 1:39:53.980
 So I don't want a total winter

1:39:53.980 --> 1:39:57.680
 and yet I don't want it to be sunny all the time either.

1:39:57.680 --> 1:40:00.300
 So what do you think it takes to build a system

1:40:00.300 --> 1:40:03.020
 with human level intelligence

1:40:03.020 --> 1:40:06.820
 where once demonstrated you would be very impressed?

1:40:06.820 --> 1:40:08.700
 So does it have to have a body?

1:40:08.700 --> 1:40:12.780
 Does it have to have the C word we used before,

1:40:12.780 --> 1:40:17.780
 consciousness as an entirety in a holistic sense?

1:40:19.140 --> 1:40:20.500
 First of all, I don't think the goal

1:40:20.500 --> 1:40:23.740
 is to create a machine that is human level intelligence.

1:40:23.740 --> 1:40:24.980
 I think it's a false goal.

1:40:24.980 --> 1:40:27.380
 Back to Turing, I think it was a false statement.

1:40:27.380 --> 1:40:29.060
 We want to understand what intelligence is

1:40:29.060 --> 1:40:30.780
 and then we can build intelligent machines

1:40:30.780 --> 1:40:33.380
 of all different scales, all different capabilities.

1:40:34.260 --> 1:40:35.300
 A dog is intelligent.

1:40:35.300 --> 1:40:38.460
 I don't need, that'd be pretty good to have a dog.

1:40:38.460 --> 1:40:39.580
 But what about something that doesn't look

1:40:39.580 --> 1:40:41.660
 like an animal at all, in different spaces?

1:40:41.660 --> 1:40:44.300
 So my thinking about this is that

1:40:44.300 --> 1:40:46.060
 we want to define what intelligence is,

1:40:46.060 --> 1:40:48.840
 agree upon what makes an intelligent system.

1:40:48.840 --> 1:40:51.100
 We can then say, okay, we're now gonna build systems

1:40:51.100 --> 1:40:54.340
 that work on those principles or some subset of them

1:40:54.340 --> 1:40:57.380
 and we can apply them to all different types of problems.

1:40:57.380 --> 1:41:00.860
 And the kind, the idea, it's not computing.

1:41:00.860 --> 1:41:05.340
 We don't ask, if I take a little one chip computer,

1:41:05.340 --> 1:41:06.660
 I don't say, well, that's not a computer

1:41:06.660 --> 1:41:09.660
 because it's not as powerful as this big server over here.

1:41:09.660 --> 1:41:11.260
 No, no, because we know that what the principles

1:41:11.260 --> 1:41:12.940
 of computing are and I can apply those principles

1:41:12.940 --> 1:41:14.860
 to a small problem or into a big problem.

1:41:14.860 --> 1:41:16.520
 And same, intelligence needs to get there.

1:41:16.520 --> 1:41:17.620
 We have to say, these are the principles.

1:41:17.620 --> 1:41:19.020
 I can make a small one, a big one.

1:41:19.020 --> 1:41:19.940
 I can make them distributed.

1:41:19.940 --> 1:41:21.620
 I can put them on different sensors.

1:41:21.620 --> 1:41:23.220
 They don't have to be human like at all.

1:41:23.220 --> 1:41:24.740
 Now, you did bring up a very interesting question

1:41:24.740 --> 1:41:25.620
 about embodiment.

1:41:25.620 --> 1:41:27.500
 Does it have to have a body?

1:41:27.500 --> 1:41:30.660
 It has to have some concept of movement.

1:41:30.660 --> 1:41:33.260
 It has to be able to move through these reference frames

1:41:33.260 --> 1:41:34.460
 I talked about earlier.

1:41:34.460 --> 1:41:35.820
 Whether it's physically moving,

1:41:35.820 --> 1:41:37.420
 like I need, if I'm gonna have an AI

1:41:37.420 --> 1:41:38.780
 that understands coffee cups,

1:41:38.780 --> 1:41:40.500
 it's gonna have to pick up the coffee cup

1:41:40.500 --> 1:41:43.180
 and touch it and look at it with its eyes and hands

1:41:43.180 --> 1:41:45.380
 or something equivalent to that.

1:41:45.380 --> 1:41:48.100
 If I have a mathematical AI,

1:41:48.100 --> 1:41:51.340
 maybe it needs to move through mathematical spaces.

1:41:51.340 --> 1:41:55.240
 I could have a virtual AI that lives in the internet

1:41:55.240 --> 1:41:58.980
 and its movements are traversing links

1:41:58.980 --> 1:42:00.260
 and digging into files,

1:42:00.260 --> 1:42:03.100
 but it's got a location that it's traveling

1:42:03.100 --> 1:42:04.940
 through some space.

1:42:04.940 --> 1:42:09.060
 You can't have an AI that just take some flash thing input.

1:42:09.060 --> 1:42:10.620
 We call it flash inference.

1:42:10.620 --> 1:42:12.860
 Here's a pattern, done.

1:42:12.860 --> 1:42:15.740
 No, it's movement pattern, movement pattern,

1:42:15.740 --> 1:42:19.020
 movement pattern, attention, digging, building structure,

1:42:19.020 --> 1:42:20.420
 figuring out the model of the world.

1:42:20.420 --> 1:42:22.740
 So some sort of embodiment,

1:42:22.740 --> 1:42:25.780
 whether it's physical or not, has to be part of it.

1:42:25.780 --> 1:42:28.020
 So self awareness and the way to be able to answer

1:42:28.020 --> 1:42:28.860
 where am I?

1:42:28.860 --> 1:42:29.680
 Well, you're bringing up self,

1:42:29.680 --> 1:42:31.460
 that's a different topic, self awareness.

1:42:31.460 --> 1:42:33.700
 No, the very narrow definition of self,

1:42:33.700 --> 1:42:37.740
 meaning knowing a sense of self enough to know

1:42:37.740 --> 1:42:39.980
 where am I in the space where it's actually.

1:42:39.980 --> 1:42:43.500
 Yeah, basically the system needs to know its location

1:42:43.500 --> 1:42:46.020
 or each component of the system needs to know

1:42:46.020 --> 1:42:48.620
 where it is in the world at that point in time.

1:42:48.620 --> 1:42:51.660
 So self awareness and consciousness.

1:42:51.660 --> 1:42:55.620
 Do you think one, from the perspective of neuroscience

1:42:55.620 --> 1:42:58.180
 and neurocortex, these are interesting topics,

1:42:58.180 --> 1:42:59.780
 solvable topics.

1:42:59.780 --> 1:43:02.180
 Do you have any ideas of why the heck it is

1:43:02.180 --> 1:43:04.420
 that we have a subjective experience at all?

1:43:04.420 --> 1:43:05.260
 Yeah, I have a lot of thoughts on that.

1:43:05.260 --> 1:43:08.460
 And is it useful or is it just a side effect of us?

1:43:08.460 --> 1:43:10.140
 It's interesting to think about.

1:43:10.140 --> 1:43:13.460
 I don't think it's useful as a means to figure out

1:43:13.460 --> 1:43:15.160
 how to build intelligent machines.

1:43:16.360 --> 1:43:20.180
 It's something that systems do

1:43:20.180 --> 1:43:22.780
 and we can talk about what it is that are like,

1:43:22.780 --> 1:43:23.980
 well, if I build a system like this,

1:43:23.980 --> 1:43:25.300
 then it would be self aware.

1:43:25.300 --> 1:43:28.340
 Or if I build it like this, it wouldn't be self aware.

1:43:28.340 --> 1:43:30.040
 So that's a choice I can have.

1:43:30.040 --> 1:43:32.300
 It's not like, oh my God, it's self aware.

1:43:32.300 --> 1:43:35.800
 I can't turn, I heard an interview recently

1:43:35.800 --> 1:43:37.120
 with this philosopher from Yale,

1:43:37.120 --> 1:43:39.020
 I can't remember his name, I apologize for that.

1:43:39.020 --> 1:43:39.860
 But he was talking about,

1:43:39.860 --> 1:43:41.420
 well, if these computers are self aware,

1:43:41.420 --> 1:43:42.900
 then it would be a crime to unplug them.

1:43:42.900 --> 1:43:45.060
 And I'm like, oh, come on, that's not,

1:43:45.060 --> 1:43:47.260
 I unplug myself every night, I go to sleep.

1:43:47.260 --> 1:43:48.260
 Is that a crime?

1:43:48.260 --> 1:43:51.340
 I plug myself in again in the morning and there I am.

1:43:51.340 --> 1:43:56.020
 So people get kind of bent out of shape about this.

1:43:56.020 --> 1:43:59.500
 I have very definite, very detailed understanding

1:43:59.500 --> 1:44:02.260
 or opinions about what it means to be conscious

1:44:02.260 --> 1:44:04.580
 and what it means to be self aware.

1:44:04.580 --> 1:44:06.780
 I don't think it's that interesting a problem.

1:44:06.780 --> 1:44:08.740
 You've talked to Christoph Koch.

1:44:08.740 --> 1:44:10.900
 He thinks that's the only problem.

1:44:10.900 --> 1:44:12.380
 I didn't actually listen to your interview with him,

1:44:12.380 --> 1:44:15.820
 but I know him and I know that's the thing he cares about.

1:44:15.820 --> 1:44:18.260
 He also thinks intelligence and consciousness are disjoint.

1:44:18.260 --> 1:44:21.020
 So I mean, it's not, you don't have to have one or the other.

1:44:21.020 --> 1:44:21.860
 So he is.

1:44:21.860 --> 1:44:22.740
 I disagree with that.

1:44:22.740 --> 1:44:24.600
 I just totally disagree with that.

1:44:24.600 --> 1:44:26.300
 So where's your thoughts and consciousness,

1:44:26.300 --> 1:44:27.660
 where does it emerge from?

1:44:27.660 --> 1:44:28.500
 Because it is.

1:44:28.500 --> 1:44:30.860
 So then we have to break it down to the two parts, okay?

1:44:30.860 --> 1:44:32.140
 Because consciousness isn't one thing.

1:44:32.140 --> 1:44:33.660
 That's part of the problem with that term

1:44:33.660 --> 1:44:35.460
 is it means different things to different people

1:44:35.460 --> 1:44:37.600
 and there's different components of it.

1:44:37.600 --> 1:44:40.820
 There is a concept of self awareness, okay?

1:44:40.820 --> 1:44:43.100
 That can be very easily explained.

1:44:43.100 --> 1:44:46.060
 You have a model of your own body.

1:44:46.060 --> 1:44:48.140
 The neocortex models things in the world

1:44:48.140 --> 1:44:50.500
 and it also models your own body.

1:44:50.500 --> 1:44:53.340
 And then it has a memory.

1:44:53.340 --> 1:44:55.860
 It can remember what you've done, okay?

1:44:55.860 --> 1:44:57.540
 So it can remember what you did this morning,

1:44:57.540 --> 1:44:59.640
 can remember what you had for breakfast and so on.

1:44:59.640 --> 1:45:01.820
 And so I can say to you, okay, Lex,

1:45:03.080 --> 1:45:06.900
 were you conscious this morning when you had your bagel?

1:45:06.900 --> 1:45:08.820
 And you'd say, yes, I was conscious.

1:45:08.820 --> 1:45:10.300
 Now what if I could take your brain

1:45:10.300 --> 1:45:12.020
 and revert all the synapses back

1:45:12.020 --> 1:45:14.180
 to the state they were this morning?

1:45:14.180 --> 1:45:15.900
 And then I said to you, Lex,

1:45:15.900 --> 1:45:17.220
 were you conscious when you ate the bagel?

1:45:17.220 --> 1:45:18.540
 And you said, no, I wasn't conscious.

1:45:18.540 --> 1:45:19.740
 I said, here's a video of eating the bagel.

1:45:19.740 --> 1:45:21.460
 And you said, I wasn't there.

1:45:22.420 --> 1:45:23.380
 That's not possible

1:45:23.380 --> 1:45:25.660
 because I must've been unconscious at that time.

1:45:25.660 --> 1:45:27.460
 So we can just make this one to one correlation

1:45:27.460 --> 1:45:31.000
 between memory of your body's trajectory through the world

1:45:31.000 --> 1:45:32.100
 over some period of time,

1:45:32.100 --> 1:45:34.260
 a memory and the ability to recall that memory

1:45:34.260 --> 1:45:35.900
 is what you would call conscious.

1:45:35.900 --> 1:45:38.940
 I was conscious of that, it's a self awareness.

1:45:38.940 --> 1:45:41.340
 And any system that can recall,

1:45:41.340 --> 1:45:43.540
 memorize what it's done recently

1:45:43.540 --> 1:45:46.340
 and bring that back and invoke it again

1:45:46.340 --> 1:45:48.220
 would say, yeah, I'm aware.

1:45:48.220 --> 1:45:49.380
 I remember what I did.

1:45:49.380 --> 1:45:50.420
 All right, I got it.

1:45:51.340 --> 1:45:52.420
 That's an easy one.

1:45:52.420 --> 1:45:54.780
 Although some people think that's a hard one.

1:45:54.780 --> 1:45:57.380
 The more challenging part of consciousness

1:45:57.380 --> 1:45:59.060
 is this one that's sometimes used

1:45:59.060 --> 1:46:01.300
 going by the word of qualia,

1:46:01.300 --> 1:46:04.860
 which is, why does an object seem red?

1:46:04.860 --> 1:46:06.860
 Or what is pain?

1:46:06.860 --> 1:46:08.740
 And why does pain feel like something?

1:46:08.740 --> 1:46:10.380
 Why do I feel redness?

1:46:10.380 --> 1:46:12.660
 Or why do I feel painness?

1:46:12.660 --> 1:46:13.500
 And then I could say, well,

1:46:13.500 --> 1:46:15.620
 why does sight seems different than hearing?

1:46:15.620 --> 1:46:16.460
 It's the same problem.

1:46:16.460 --> 1:46:18.580
 It's really, these are all just neurons.

1:46:18.580 --> 1:46:20.300
 And so how is it that,

1:46:20.300 --> 1:46:24.140
 why does looking at you feel different than hearing you?

1:46:24.140 --> 1:46:26.080
 It feels different, but there's just neurons in my head.

1:46:26.080 --> 1:46:27.820
 They're all doing the same thing.

1:46:27.820 --> 1:46:29.820
 So that's an interesting question.

1:46:29.820 --> 1:46:31.540
 The best treatise I've read about this

1:46:31.540 --> 1:46:33.580
 is by a guy named Oregon.

1:46:33.580 --> 1:46:34.740
 He wrote a book called,

1:46:35.740 --> 1:46:37.480
 Why Red Doesn't Sound Like a Bell.

1:46:37.480 --> 1:46:42.040
 It's a little, it's not a trade book, easy to read,

1:46:42.040 --> 1:46:46.040
 but it, and it's an interesting question.

1:46:46.040 --> 1:46:47.880
 Take something like color.

1:46:47.880 --> 1:46:49.360
 Color really doesn't exist in the world.

1:46:49.360 --> 1:46:51.160
 It's not a property of the world.

1:46:51.160 --> 1:46:54.240
 Property of the world that exists is light frequency.

1:46:54.240 --> 1:46:55.640
 And that gets turned into,

1:46:55.640 --> 1:46:57.960
 we have certain cells in the retina

1:46:57.960 --> 1:46:59.320
 that respond to different frequencies

1:46:59.320 --> 1:47:00.240
 different than others.

1:47:00.240 --> 1:47:01.440
 And so when they enter the brain,

1:47:01.440 --> 1:47:02.440
 you just have a bunch of axons

1:47:02.440 --> 1:47:04.500
 that are firing at different rates.

1:47:04.500 --> 1:47:06.680
 And from that, we perceive color.

1:47:06.680 --> 1:47:07.960
 But there is no color in the brain.

1:47:07.960 --> 1:47:10.840
 I mean, there's no color coming in on those synapses.

1:47:10.840 --> 1:47:14.380
 It's just a correlation between some axons

1:47:14.380 --> 1:47:16.400
 and some property of frequency.

1:47:17.360 --> 1:47:18.880
 And that isn't even color itself.

1:47:18.880 --> 1:47:20.140
 Frequency doesn't have a color.

1:47:20.140 --> 1:47:22.940
 It's just what it is.

1:47:22.940 --> 1:47:24.120
 So then the question is,

1:47:24.120 --> 1:47:26.820
 well, why does it even appear to have a color at all?

1:47:27.880 --> 1:47:29.080
 Just as you're describing it,

1:47:29.080 --> 1:47:31.000
 there seems to be a connection to those ideas

1:47:31.000 --> 1:47:32.560
 of reference frames.

1:47:32.560 --> 1:47:37.040
 I mean, it just feels like consciousness

1:47:37.040 --> 1:47:38.400
 having the subject,

1:47:38.400 --> 1:47:42.600
 assigning the feeling of red to the actual color

1:47:42.600 --> 1:47:47.600
 or to the wavelength is useful for intelligence.

1:47:47.920 --> 1:47:49.600
 Yeah, I think that's a good way of putting it.

1:47:49.600 --> 1:47:51.600
 It's useful as a predictive mechanism

1:47:51.600 --> 1:47:53.840
 or useful as a generalization idea.

1:47:53.840 --> 1:47:55.660
 It's a way of grouping things together to say,

1:47:55.660 --> 1:47:57.560
 it's useful to have a model like this.

1:47:57.560 --> 1:48:02.560
 So think about the well known syndrome

1:48:02.640 --> 1:48:04.800
 that people who've lost a limb experience

1:48:04.800 --> 1:48:06.960
 called phantom limbs.

1:48:06.960 --> 1:48:11.960
 And what they claim is they can have their arm is removed,

1:48:12.120 --> 1:48:13.280
 but they feel their arm.

1:48:13.280 --> 1:48:15.960
 That not only feel it, they know it's there.

1:48:15.960 --> 1:48:17.740
 It's there, I know it's there.

1:48:17.740 --> 1:48:19.000
 They'll swear to you that it's there.

1:48:19.000 --> 1:48:20.360
 And then they can feel pain in their arm

1:48:20.360 --> 1:48:21.840
 and they'll feel pain in their finger.

1:48:21.840 --> 1:48:25.280
 And if they move their non existent arm behind their back,

1:48:25.280 --> 1:48:27.320
 then they feel the pain behind their back.

1:48:27.320 --> 1:48:30.120
 So this whole idea that your arm exists

1:48:30.120 --> 1:48:31.360
 is a model of your brain.

1:48:31.360 --> 1:48:33.480
 It may or may not really exist.

1:48:34.360 --> 1:48:38.520
 And just like, but it's useful to have a model of something

1:48:38.520 --> 1:48:40.360
 that sort of correlates to things in the world.

1:48:40.360 --> 1:48:41.960
 So you can make predictions about what would happen

1:48:41.960 --> 1:48:43.520
 when those things occur.

1:48:43.520 --> 1:48:44.640
 It's a little bit of a fuzzy,

1:48:44.640 --> 1:48:46.480
 but I think you're getting quite towards the answer there.

1:48:46.480 --> 1:48:51.280
 It's useful for the model to express things certain ways

1:48:51.280 --> 1:48:53.640
 that we can then map them into these reference frames

1:48:53.640 --> 1:48:55.780
 and make predictions about them.

1:48:55.780 --> 1:48:57.680
 I need to spend more time on this topic.

1:48:57.680 --> 1:48:58.880
 It doesn't bother me.

1:48:58.880 --> 1:49:00.360
 Do you really need to spend more time?

1:49:00.360 --> 1:49:01.840
 Yeah, I know.

1:49:01.840 --> 1:49:04.720
 It does feel special that we have subjective experience,

1:49:04.720 --> 1:49:07.320
 but I'm yet to know why.

1:49:07.320 --> 1:49:09.040
 I'm just personally curious.

1:49:09.040 --> 1:49:11.400
 It's not necessary for the work we're doing here.

1:49:11.400 --> 1:49:13.080
 I don't think I need to solve that problem

1:49:13.080 --> 1:49:15.560
 to build intelligent machines at all, not at all.

1:49:15.560 --> 1:49:17.800
 But there is sort of the silly notion

1:49:17.800 --> 1:49:19.480
 that you described briefly

1:49:20.440 --> 1:49:23.280
 that doesn't seem so silly to us humans is,

1:49:23.280 --> 1:49:27.080
 if you're successful building intelligent machines,

1:49:27.080 --> 1:49:30.240
 it feels wrong to then turn them off.

1:49:30.240 --> 1:49:33.240
 Because if you're able to build a lot of them,

1:49:33.240 --> 1:49:38.240
 it feels wrong to then be able to turn off the...

1:49:38.760 --> 1:49:39.600
 Well, why?

1:49:39.600 --> 1:49:41.840
 Let's break that down a bit.

1:49:41.840 --> 1:49:43.920
 As humans, why do we fear death?

1:49:43.920 --> 1:49:45.760
 There's two reasons we fear death.

1:49:47.060 --> 1:49:47.900
 Well, first of all, I'll say,

1:49:47.900 --> 1:49:48.960
 when you're dead, it doesn't matter at all.

1:49:48.960 --> 1:49:49.800
 Who cares?

1:49:49.800 --> 1:49:50.640
 You're dead.

1:49:50.640 --> 1:49:51.840
 So why do we fear death?

1:49:51.840 --> 1:49:53.480
 We fear death for two reasons.

1:49:53.480 --> 1:49:57.760
 One is because we are programmed genetically to fear death.

1:49:57.760 --> 1:50:01.840
 That's a survival and pop beginning of the genes thing.

1:50:02.940 --> 1:50:05.120
 And we also are programmed to feel sad

1:50:05.120 --> 1:50:06.880
 when people we know die.

1:50:06.880 --> 1:50:08.560
 We don't feel sad for someone we don't know dies.

1:50:08.560 --> 1:50:09.600
 There's people dying right now,

1:50:09.600 --> 1:50:10.420
 they're only just gonna say,

1:50:10.420 --> 1:50:11.260
 I don't feel bad about them,

1:50:11.260 --> 1:50:12.080
 because I don't know them.

1:50:12.080 --> 1:50:13.420
 But if I knew them, I'd feel really bad.

1:50:13.420 --> 1:50:16.840
 So again, these are old brain,

1:50:16.840 --> 1:50:19.880
 genetically embedded things that we fear death.

1:50:19.880 --> 1:50:24.280
 It's outside of those uncomfortable feelings.

1:50:24.280 --> 1:50:25.840
 There's nothing else to worry about.

1:50:25.840 --> 1:50:27.360
 Well, wait, hold on a second.

1:50:27.360 --> 1:50:30.440
 Do you know the denial of death by Becker?

1:50:30.440 --> 1:50:31.360
 No.

1:50:31.360 --> 1:50:34.440
 There's a thought that death is,

1:50:36.760 --> 1:50:41.280
 our whole conception of our world model

1:50:41.280 --> 1:50:43.800
 kind of assumes immortality.

1:50:43.800 --> 1:50:47.040
 And then death is this terror that underlies it all.

1:50:47.040 --> 1:50:47.880
 So like...

1:50:47.880 --> 1:50:50.400
 Some people's world model, not mine.

1:50:50.400 --> 1:50:52.760
 But, okay, so what Becker would say

1:50:52.760 --> 1:50:54.520
 is that you're just living in an illusion.

1:50:54.520 --> 1:50:56.200
 You've constructed an illusion for yourself

1:50:56.200 --> 1:50:59.000
 because it's such a terrible terror,

1:50:59.000 --> 1:51:00.160
 the fact that this...

1:51:00.160 --> 1:51:01.160
 What's the illusion?

1:51:01.160 --> 1:51:02.640
 The illusion that death doesn't matter.

1:51:02.640 --> 1:51:04.800
 You're still not coming to grips with...

1:51:04.800 --> 1:51:05.620
 The illusion of what?

1:51:05.620 --> 1:51:07.120
 That death is...

1:51:07.120 --> 1:51:08.700
 Going to happen.

1:51:08.700 --> 1:51:10.440
 Oh, like it's not gonna happen?

1:51:10.440 --> 1:51:11.880
 You're actually operating.

1:51:11.880 --> 1:51:14.280
 You haven't, even though you said you've accepted it,

1:51:14.280 --> 1:51:16.120
 you haven't really accepted the notion that you're gonna die

1:51:16.120 --> 1:51:16.960
 is what you say.

1:51:16.960 --> 1:51:21.440
 So it sounds like you disagree with that notion.

1:51:21.440 --> 1:51:22.400
 Yeah, yeah, totally.

1:51:22.400 --> 1:51:27.400
 I literally, every night I go to bed, it's like dying.

1:51:28.040 --> 1:51:28.880
 Like little deaths.

1:51:28.880 --> 1:51:29.720
 It's little deaths.

1:51:29.720 --> 1:51:32.960
 And if I didn't wake up, it wouldn't matter to me.

1:51:32.960 --> 1:51:35.160
 Only if I knew that was gonna happen would it be bothersome.

1:51:35.160 --> 1:51:37.600
 If I didn't know it was gonna happen, how would I know?

1:51:37.600 --> 1:51:39.520
 Then I would worry about my wife.

1:51:39.520 --> 1:51:43.040
 So imagine I was a loner and I lived in Alaska

1:51:43.040 --> 1:51:45.420
 and I lived out there and there was no animals.

1:51:45.420 --> 1:51:46.480
 Nobody knew I existed.

1:51:46.480 --> 1:51:48.720
 I was just eating these roots all the time.

1:51:48.720 --> 1:51:51.120
 And nobody knew I was there.

1:51:51.120 --> 1:51:53.320
 And one day I didn't wake up.

1:51:54.680 --> 1:51:57.040
 What pain in the world would there exist?

1:51:57.040 --> 1:51:59.800
 Well, so most people that think about this problem

1:51:59.800 --> 1:52:01.960
 would say that you're just deeply enlightened

1:52:01.960 --> 1:52:04.120
 or are completely delusional.

1:52:04.120 --> 1:52:05.920
 One of the two.

1:52:05.920 --> 1:52:10.720
 But I would say that's a very enlightened way

1:52:10.720 --> 1:52:11.720
 to see the world.

1:52:13.120 --> 1:52:14.760
 That's the rational one as well.

1:52:14.760 --> 1:52:15.760
 It's rational, that's right.

1:52:15.760 --> 1:52:17.840
 But the fact is we don't,

1:52:19.040 --> 1:52:22.360
 I mean, we really don't have an understanding

1:52:22.360 --> 1:52:24.920
 of why the heck it is we're born and why we die

1:52:24.920 --> 1:52:25.960
 and what happens after we die.

1:52:25.960 --> 1:52:27.880
 Well, maybe there isn't a reason, maybe there is.

1:52:27.880 --> 1:52:30.120
 So I'm interested in those big problems too, right?

1:52:30.120 --> 1:52:32.560
 You interviewed Max Tegmark,

1:52:32.560 --> 1:52:33.600
 and there's people like that, right?

1:52:33.600 --> 1:52:35.240
 I'm interested in those big problems as well.

1:52:35.240 --> 1:52:38.240
 And in fact, when I was young,

1:52:38.240 --> 1:52:41.200
 I made a list of the biggest problems I could think of.

1:52:41.200 --> 1:52:43.360
 First, why does anything exist?

1:52:43.360 --> 1:52:46.600
 Second, why do we have the laws of physics that we have?

1:52:46.600 --> 1:52:49.200
 Third, is life inevitable?

1:52:49.200 --> 1:52:50.120
 And why is it here?

1:52:50.120 --> 1:52:52.240
 Fourth, is intelligence inevitable?

1:52:52.240 --> 1:52:53.080
 And why is it here?

1:52:53.080 --> 1:52:55.000
 I stopped there because I figured

1:52:55.000 --> 1:52:57.840
 if you can make a truly intelligent system,

1:52:57.840 --> 1:52:59.240
 that will be the quickest way

1:52:59.240 --> 1:53:01.040
 to answer the first three questions.

1:53:01.040 --> 1:53:04.440
 I'm serious.

1:53:04.440 --> 1:53:07.960
 And so I said, my mission, you asked me earlier,

1:53:07.960 --> 1:53:09.440
 my first mission is to understand the brain,

1:53:09.440 --> 1:53:10.760
 but I felt that is the shortest way

1:53:10.760 --> 1:53:12.160
 to get to true machine intelligence.

1:53:12.160 --> 1:53:13.680
 And I wanna get to true machine intelligence

1:53:13.680 --> 1:53:15.920
 because even if it doesn't occur in my lifetime,

1:53:15.920 --> 1:53:17.480
 other people will benefit from it

1:53:17.480 --> 1:53:19.200
 because I think it'll occur in my lifetime,

1:53:19.200 --> 1:53:21.040
 but 20 years, you never know.

1:53:23.640 --> 1:53:26.080
 But that will be the quickest way for us to,

1:53:26.080 --> 1:53:27.800
 we can make super mathematicians,

1:53:27.800 --> 1:53:29.520
 we can make super space explorers,

1:53:29.520 --> 1:53:34.240
 we can make super physicist brains that do these things

1:53:34.240 --> 1:53:37.440
 and that can run experiments that we can't run.

1:53:37.440 --> 1:53:40.360
 We don't have the abilities to manipulate things and so on,

1:53:40.360 --> 1:53:42.800
 but we can build intelligent machines that do all those things

1:53:42.800 --> 1:53:46.560
 with the ultimate goal of finding out the answers

1:53:46.560 --> 1:53:47.680
 to the other questions.

1:53:48.800 --> 1:53:51.480
 Let me ask you another depressing and difficult question,

1:53:51.480 --> 1:53:56.480
 which is once we achieve that goal of creating,

1:53:57.880 --> 1:54:01.200
 no, of understanding intelligence,

1:54:01.200 --> 1:54:02.960
 do you think we would be happier,

1:54:02.960 --> 1:54:04.760
 more fulfilled as a species?

1:54:04.760 --> 1:54:05.720
 The understanding intelligence

1:54:05.720 --> 1:54:07.920
 or understanding the answers to the big questions?

1:54:07.920 --> 1:54:08.960
 Understanding intelligence.

1:54:08.960 --> 1:54:11.800
 Oh, totally, totally.

1:54:11.800 --> 1:54:13.960
 It would be far more fun place to live.

1:54:13.960 --> 1:54:14.800
 You think so?

1:54:14.800 --> 1:54:15.680
 Oh yeah, why not?

1:54:15.680 --> 1:54:19.720
 I mean, just put aside this terminator nonsense

1:54:19.720 --> 1:54:24.320
 and just think about, you can think about,

1:54:24.320 --> 1:54:26.840
 we can talk about the risks of AI if you want.

1:54:26.840 --> 1:54:28.240
 I'd love to, so let's talk about.

1:54:28.240 --> 1:54:30.640
 But I think the world would be far better knowing things.

1:54:30.640 --> 1:54:32.080
 We're always better than know things.

1:54:32.080 --> 1:54:35.080
 Do you think it's better, is it a better place to live in

1:54:35.080 --> 1:54:37.440
 that I know that our planet is one of many

1:54:37.440 --> 1:54:39.240
 in the solar system and the solar system's one of many

1:54:39.240 --> 1:54:40.080
 in the galaxy?

1:54:40.080 --> 1:54:43.400
 I think it's a more, I dread, I sometimes think like,

1:54:43.400 --> 1:54:45.360
 God, what would it be like to live 300 years ago?

1:54:45.360 --> 1:54:47.440
 I'd be looking up at the sky, I can't understand anything.

1:54:47.440 --> 1:54:49.240
 Oh my God, I'd be like going to bed every night going,

1:54:49.240 --> 1:54:50.160
 what's going on here?

1:54:50.160 --> 1:54:52.480
 Well, I mean, in some sense I agree with you,

1:54:52.480 --> 1:54:54.800
 but I'm not exactly sure.

1:54:54.800 --> 1:54:57.240
 So I'm also a scientist, so I share your views,

1:54:57.240 --> 1:55:00.960
 but I'm not, we're like rolling down the hill together.

1:55:02.640 --> 1:55:03.480
 What's down the hill?

1:55:03.480 --> 1:55:05.280
 I feel like we're climbing a hill.

1:55:05.280 --> 1:55:06.120
 Whatever.

1:55:06.120 --> 1:55:07.640
 We're getting closer to enlightenment

1:55:07.640 --> 1:55:10.200
 and you're going down the hill.

1:55:10.200 --> 1:55:12.240
 We're climbing, we're getting pulled up a hill

1:55:12.240 --> 1:55:13.840
 by our curiosity.

1:55:13.840 --> 1:55:16.120
 Our curiosity is, we're pulling ourselves up the hill

1:55:16.120 --> 1:55:16.960
 by our curiosity.

1:55:16.960 --> 1:55:19.160
 Yeah, Sisyphus was doing the same thing with the rock.

1:55:19.160 --> 1:55:20.880
 Yeah, yeah, yeah, yeah.

1:55:20.880 --> 1:55:24.280
 But okay, our happiness aside, do you have concerns

1:55:24.280 --> 1:55:29.040
 about, you talk about Sam Harris, Elon Musk,

1:55:29.040 --> 1:55:31.880
 of existential threats of intelligent systems?

1:55:31.880 --> 1:55:33.800
 No, I'm not worried about existential threats at all.

1:55:33.800 --> 1:55:36.400
 There are some things we really do need to worry about.

1:55:36.400 --> 1:55:38.440
 Even today's AI, we have things we have to worry about.

1:55:38.440 --> 1:55:39.560
 We have to worry about privacy

1:55:39.560 --> 1:55:42.800
 and about how it impacts false beliefs in the world.

1:55:42.800 --> 1:55:47.000
 And we have real problems and things to worry about

1:55:47.000 --> 1:55:48.280
 with today's AI.

1:55:48.280 --> 1:55:51.480
 And that will continue as we create more intelligent systems.

1:55:51.480 --> 1:55:53.080
 There's no question, the whole issue

1:55:53.080 --> 1:55:57.080
 about making intelligent armaments and weapons

1:55:57.080 --> 1:55:59.920
 is something that really we have to think about carefully.

1:55:59.920 --> 1:56:01.880
 I don't think of those as existential threats.

1:56:01.880 --> 1:56:04.320
 I think those are the kind of threats we always face

1:56:04.320 --> 1:56:05.840
 and we'll have to face them here

1:56:05.840 --> 1:56:08.560
 and we'll have to deal with them.

1:56:10.400 --> 1:56:12.040
 We could talk about what people think

1:56:12.040 --> 1:56:13.880
 are the existential threats,

1:56:13.880 --> 1:56:16.200
 but when I hear people talking about them,

1:56:16.200 --> 1:56:17.760
 they all sound hollow to me.

1:56:17.760 --> 1:56:20.000
 They're based on ideas, they're based on people

1:56:20.000 --> 1:56:22.160
 who really have no idea what intelligence is.

1:56:22.160 --> 1:56:24.920
 And if they knew what intelligence was,

1:56:24.920 --> 1:56:26.640
 they wouldn't say those things.

1:56:26.640 --> 1:56:28.600
 So those are not experts in the field.

1:56:28.600 --> 1:56:32.040
 Yeah, so there's two, right?

1:56:32.040 --> 1:56:33.720
 So one is like super intelligence.

1:56:33.720 --> 1:56:37.720
 So a system that becomes far, far superior

1:56:37.720 --> 1:56:42.720
 in reasoning ability than us humans.

1:56:43.160 --> 1:56:44.800
 How is that an existential threat?

1:56:46.200 --> 1:56:49.120
 Then, so there's a lot of ways in which it could be.

1:56:49.120 --> 1:56:54.040
 One way is us humans are actually irrational, inefficient

1:56:54.040 --> 1:56:59.040
 and get in the way of, not happiness,

1:57:00.520 --> 1:57:02.120
 but whatever the objective function is

1:57:02.120 --> 1:57:04.320
 of maximizing that objective function.

1:57:04.320 --> 1:57:05.160
 Super intelligent.

1:57:05.160 --> 1:57:06.720
 The paperclip problem and things like that.

1:57:06.720 --> 1:57:09.440
 So the paperclip problem but with the super intelligent.

1:57:09.440 --> 1:57:10.480
 Yeah, yeah, yeah, yeah.

1:57:10.480 --> 1:57:14.180
 So we already face this threat in some sense.

1:57:15.680 --> 1:57:17.320
 They're called bacteria.

1:57:17.320 --> 1:57:18.960
 These are organisms in the world

1:57:18.960 --> 1:57:21.400
 that would like to turn everything into bacteria.

1:57:21.400 --> 1:57:23.040
 And they're constantly morphing,

1:57:23.040 --> 1:57:26.360
 they're constantly changing to evade our protections.

1:57:26.360 --> 1:57:30.680
 And in the past, they have killed huge swaths

1:57:30.680 --> 1:57:33.400
 of populations of humans on this planet.

1:57:33.400 --> 1:57:34.560
 So if you wanna worry about something

1:57:34.560 --> 1:57:38.360
 that's gonna multiply endlessly, we have it.

1:57:38.360 --> 1:57:40.600
 And I'm far more worried in that regard.

1:57:40.600 --> 1:57:43.280
 I'm far more worried that some scientists in the laboratory

1:57:43.280 --> 1:57:45.440
 will create a super virus or a super bacteria

1:57:45.440 --> 1:57:47.120
 that we cannot control.

1:57:47.120 --> 1:57:49.640
 That is a more of an existential threat.

1:57:49.640 --> 1:57:52.160
 Putting an intelligence thing on top of it

1:57:52.160 --> 1:57:54.240
 actually seems to make it less existential to me.

1:57:54.240 --> 1:57:56.640
 It's like, it limits its power.

1:57:56.640 --> 1:57:57.720
 It limits where it can go.

1:57:57.720 --> 1:57:59.820
 It limits the number of things it can do in many ways.

1:57:59.820 --> 1:58:03.080
 A bacteria is something you can't even see.

1:58:03.080 --> 1:58:04.240
 So that's only one of those problems.

1:58:04.240 --> 1:58:05.080
 Yes, exactly.

1:58:05.080 --> 1:58:09.600
 So the other one, just in your intuition about intelligence,

1:58:09.600 --> 1:58:12.480
 when you think about intelligence of us humans,

1:58:12.480 --> 1:58:14.880
 do you think of that as something,

1:58:14.880 --> 1:58:16.960
 if you look at intelligence on a spectrum

1:58:16.960 --> 1:58:18.900
 from zero to us humans,

1:58:18.900 --> 1:58:22.080
 do you think you can scale that to something far,

1:58:22.080 --> 1:58:24.760
 far superior to all the mechanisms we've been talking about?

1:58:24.760 --> 1:58:28.360
 I wanna make another point here, Lex, before I get there.

1:58:28.360 --> 1:58:30.920
 Intelligence is the neocortex.

1:58:30.920 --> 1:58:32.400
 It is not the entire brain.

1:58:34.080 --> 1:58:36.200
 The goal is not to make a human.

1:58:36.200 --> 1:58:38.400
 The goal is not to make an emotional system.

1:58:38.400 --> 1:58:39.560
 The goal is not to make a system

1:58:39.560 --> 1:58:41.440
 that wants to have sex and reproduce.

1:58:41.440 --> 1:58:42.880
 Why would I build that?

1:58:42.880 --> 1:58:44.560
 If I wanna have a system that wants to reproduce

1:58:44.560 --> 1:58:47.260
 and have sex, make bacteria, make computer viruses.

1:58:47.260 --> 1:58:49.800
 Those are bad things, don't do that.

1:58:49.800 --> 1:58:52.360
 Those are really bad, don't do those things.

1:58:52.360 --> 1:58:53.560
 Regulate those.

1:58:53.560 --> 1:58:56.120
 But if I just say I want an intelligent system,

1:58:56.120 --> 1:58:58.560
 why does it have to have any of the human like emotions?

1:58:58.560 --> 1:59:00.400
 Why does it even care if it lives?

1:59:00.400 --> 1:59:02.560
 Why does it even care if it has food?

1:59:02.560 --> 1:59:03.840
 It doesn't care about those things.

1:59:03.840 --> 1:59:06.320
 It's just, you know, it's just in a trance

1:59:06.320 --> 1:59:08.280
 thinking about mathematics or it's out there

1:59:08.280 --> 1:59:12.280
 just trying to build the space for it on Mars.

1:59:14.000 --> 1:59:15.320
 That's a choice we make.

1:59:15.320 --> 1:59:17.160
 Don't make human like things,

1:59:17.160 --> 1:59:18.480
 don't make replicating things,

1:59:18.480 --> 1:59:19.840
 don't make things that have emotions,

1:59:19.840 --> 1:59:21.000
 just stick to the neocortex.

1:59:21.000 --> 1:59:23.120
 So that's a view actually that I share

1:59:23.120 --> 1:59:25.400
 but not everybody shares in the sense that

1:59:25.400 --> 1:59:29.840
 you have faith and optimism about us as engineers of systems,

1:59:29.840 --> 1:59:34.840
 humans as builders of systems to not put in stupid, not.

1:59:34.840 --> 1:59:37.640
 So this is why I mentioned the bacteria one.

1:59:37.640 --> 1:59:40.760
 Because you might say, well, some person's gonna do that.

1:59:40.760 --> 1:59:42.920
 Well, some person today could create a bacteria

1:59:42.920 --> 1:59:46.920
 that's resistant to all the known antibacterial agents.

1:59:46.920 --> 1:59:49.200
 So we already have that threat.

1:59:49.200 --> 1:59:51.320
 We already know this is going on.

1:59:51.320 --> 1:59:52.760
 It's not a new threat.

1:59:52.760 --> 1:59:56.280
 So just accept that and then we have to deal with it, right?

1:59:56.280 --> 1:59:59.600
 Yeah, so my point is nothing to do with intelligence.

1:59:59.600 --> 2:00:01.920
 Intelligence is a separate component

2:00:01.920 --> 2:00:03.560
 that you might apply to a system

2:00:03.560 --> 2:00:06.040
 that wants to reproduce and do stupid things.

2:00:06.040 --> 2:00:07.240
 Let's not do that.

2:00:07.240 --> 2:00:08.400
 Yeah, in fact, it is a mystery

2:00:08.400 --> 2:00:10.520
 why people haven't done that yet.

2:00:10.520 --> 2:00:14.320
 My dad is a physicist, believes that the reason,

2:00:14.320 --> 2:00:18.080
 he says, for example, nuclear weapons haven't proliferated

2:00:18.080 --> 2:00:19.040
 amongst evil people.

2:00:19.040 --> 2:00:21.680
 So one belief that I share is that

2:00:21.680 --> 2:00:25.160
 there's not that many evil people in the world

2:00:25.160 --> 2:00:30.160
 that would use, whether it's bacteria or nuclear weapons

2:00:31.280 --> 2:00:35.000
 or maybe the future AI systems to do bad.

2:00:35.000 --> 2:00:36.200
 So the fraction is small.

2:00:36.200 --> 2:00:38.400
 And the second is that it's actually really hard,

2:00:38.400 --> 2:00:42.480
 technically, so the intersection between evil

2:00:42.480 --> 2:00:45.160
 and competent is small in terms of, and that's the.

2:00:45.160 --> 2:00:47.000
 And by the way, to really annihilate humanity,

2:00:47.000 --> 2:00:50.800
 you'd have to have sort of the nuclear winter phenomenon,

2:00:50.800 --> 2:00:54.080
 which is not one person shooting or even 10 bombs.

2:00:54.080 --> 2:00:56.440
 You'd have to have some automated system

2:00:56.440 --> 2:00:58.520
 that detonates a million bombs

2:00:58.520 --> 2:01:00.400
 or whatever many thousands we have.

2:01:00.400 --> 2:01:03.080
 So extreme evil combined with extreme competence.

2:01:03.080 --> 2:01:05.080
 And to start with building some stupid system

2:01:05.080 --> 2:01:08.000
 that would automatically, Dr. Strangelove type of thing,

2:01:08.000 --> 2:01:11.920
 you know, I mean, look, we could have

2:01:11.920 --> 2:01:14.600
 some nuclear bomb go off in some major city in the world.

2:01:14.600 --> 2:01:17.120
 I think that's actually quite likely, even in my lifetime.

2:01:17.120 --> 2:01:18.480
 I don't think that's an unlikely thing.

2:01:18.480 --> 2:01:19.600
 And it'd be a tragedy.

2:01:20.600 --> 2:01:23.160
 But it won't be an existential threat.

2:01:23.160 --> 2:01:26.560
 And it's the same as, you know, the virus of 1917,

2:01:26.560 --> 2:01:28.960
 whatever it was, you know, the influenza.

2:01:30.000 --> 2:01:33.880
 These bad things can happen and the plague and so on.

2:01:33.880 --> 2:01:35.360
 We can't always prevent them.

2:01:35.360 --> 2:01:37.040
 We always try, but we can't.

2:01:37.040 --> 2:01:38.240
 But they're not existential threats

2:01:38.240 --> 2:01:41.200
 until we combine all those crazy things together.

2:01:41.200 --> 2:01:45.440
 So on the spectrum of intelligence from zero to human,

2:01:45.440 --> 2:01:47.960
 do you have a sense of whether it's possible

2:01:47.960 --> 2:01:51.560
 to create several orders of magnitude

2:01:51.560 --> 2:01:54.680
 or at least double that of human intelligence?

2:01:54.680 --> 2:01:55.920
 Talking about neuro context.

2:01:55.920 --> 2:01:59.000
 I think it's the wrong thing to say double the intelligence.

2:01:59.000 --> 2:02:01.600
 Break it down into different components.

2:02:01.600 --> 2:02:03.640
 Can I make something that's a million times fast

2:02:03.640 --> 2:02:04.520
 than a human brain?

2:02:04.520 --> 2:02:06.280
 Yes, I can do that.

2:02:06.280 --> 2:02:09.160
 Could I make something that is,

2:02:09.160 --> 2:02:10.960
 has a lot more storage than the human brain?

2:02:10.960 --> 2:02:11.880
 Yes, I could do that.

2:02:11.880 --> 2:02:13.600
 More common, more copies of common.

2:02:13.600 --> 2:02:14.720
 Can I make something that attaches

2:02:14.720 --> 2:02:16.160
 to different sensors than human brain?

2:02:16.160 --> 2:02:17.280
 Yes, I can do that.

2:02:17.280 --> 2:02:19.280
 Could I make something that's distributed?

2:02:19.280 --> 2:02:21.160
 So these people, yeah, we talked early

2:02:21.160 --> 2:02:23.120
 about the departure of the neocortex voting.

2:02:23.120 --> 2:02:24.240
 They don't have to be co located.

2:02:24.240 --> 2:02:25.680
 Like, you know, they can be all around the place.

2:02:25.680 --> 2:02:26.680
 I could do that too.

2:02:29.080 --> 2:02:32.440
 Those are the levers I have, but is it more intelligent?

2:02:32.440 --> 2:02:33.760
 Well, it depends what I train it on.

2:02:33.760 --> 2:02:34.800
 What is it doing?

2:02:34.800 --> 2:02:35.640
 If it's.

2:02:35.640 --> 2:02:36.720
 Well, so here's the thing.

2:02:36.720 --> 2:02:39.400
 So let's say larger neocortex

2:02:39.400 --> 2:02:44.400
 and or whatever size that allows for higher

2:02:44.720 --> 2:02:47.920
 and higher hierarchies to form,

2:02:47.920 --> 2:02:50.160
 we're talking about reference frames and concepts.

2:02:50.160 --> 2:02:51.920
 Could I have something that's a super physicist

2:02:51.920 --> 2:02:52.920
 or a super mathematician?

2:02:52.920 --> 2:02:53.760
 Yes.

2:02:53.760 --> 2:02:56.680
 And the question is, once you have a super physicist,

2:02:56.680 --> 2:02:58.760
 will they be able to understand something?

2:03:00.440 --> 2:03:02.200
 Do you have a sense that it will be orders of math,

2:03:02.200 --> 2:03:03.040
 like us compared to ants?

2:03:03.040 --> 2:03:04.560
 Could we ever understand it?

2:03:04.560 --> 2:03:06.080
 Yeah.

2:03:06.080 --> 2:03:11.080
 Most people cannot understand general relativity.

2:03:11.920 --> 2:03:13.280
 It's a really hard thing to get.

2:03:13.280 --> 2:03:15.800
 I mean, yeah, you can paint it in a fuzzy picture,

2:03:15.800 --> 2:03:17.560
 stretchy space, you know?

2:03:17.560 --> 2:03:19.920
 But the field equations to do that

2:03:19.920 --> 2:03:23.080
 and the deep intuitions are really, really hard.

2:03:23.080 --> 2:03:25.960
 And I've tried, I'm unable to do it.

2:03:25.960 --> 2:03:28.800
 Like it's easy to get special relativity,

2:03:28.800 --> 2:03:30.960
 but general relativity, man, that's too much.

2:03:32.360 --> 2:03:34.960
 And so we already live with this to some extent.

2:03:34.960 --> 2:03:37.320
 The vast majority of people can't understand actually

2:03:37.320 --> 2:03:40.280
 what the vast majority of other people actually know.

2:03:40.280 --> 2:03:41.960
 We're just, either we don't have the effort to,

2:03:41.960 --> 2:03:43.280
 or we can't, or we don't have time,

2:03:43.280 --> 2:03:45.080
 or just not smart enough, whatever.

2:03:46.920 --> 2:03:48.560
 But we have ways of communicating.

2:03:48.560 --> 2:03:51.600
 Einstein has spoken in a way that I can understand.

2:03:51.600 --> 2:03:54.600
 He's given me analogies that are useful.

2:03:54.600 --> 2:03:56.880
 I can use those analogies from my own work

2:03:56.880 --> 2:03:59.880
 and think about concepts that are similar.

2:04:01.040 --> 2:04:02.200
 It's not stupid.

2:04:02.200 --> 2:04:04.040
 It's not like he's existing some other plane

2:04:04.040 --> 2:04:06.680
 and there's no connection with my plane in the world here.

2:04:06.680 --> 2:04:07.840
 So that will occur.

2:04:07.840 --> 2:04:09.280
 It already has occurred.

2:04:09.280 --> 2:04:10.760
 That's what my point of this story is.

2:04:10.760 --> 2:04:11.720
 It already has occurred.

2:04:11.720 --> 2:04:13.160
 We live it every day.

2:04:14.360 --> 2:04:17.040
 One could argue that when we create machine intelligence

2:04:17.040 --> 2:04:18.720
 that think a million times faster than us

2:04:18.720 --> 2:04:20.920
 that it'll be so far we can't make the connections.

2:04:20.920 --> 2:04:22.560
 But you know, at the moment,

2:04:23.480 --> 2:04:25.640
 everything that seems really, really hard

2:04:25.640 --> 2:04:26.680
 to figure out in the world,

2:04:26.680 --> 2:04:29.000
 when you actually figure it out, it's not that hard.

2:04:29.000 --> 2:04:32.160
 You know, almost everyone can understand the multiverses.

2:04:32.160 --> 2:04:34.040
 Almost everyone can understand quantum physics.

2:04:34.040 --> 2:04:36.120
 Almost everyone can understand these basic things,

2:04:36.120 --> 2:04:39.000
 even though hardly any people could figure those things out.

2:04:39.000 --> 2:04:41.320
 Yeah, but really understand.

2:04:41.320 --> 2:04:42.360
 But you don't need to really.

2:04:42.360 --> 2:04:43.800
 Only a few people really understand.

2:04:43.800 --> 2:04:47.880
 You need to only understand the projections,

2:04:47.880 --> 2:04:50.120
 the sprinkles of the useful insights from that.

2:04:50.120 --> 2:04:51.760
 That was my example of Einstein, right?

2:04:51.760 --> 2:04:53.800
 His general theory of relativity is one thing

2:04:53.800 --> 2:04:56.000
 that very, very, very few people can get.

2:04:56.000 --> 2:04:58.240
 And what if we just said those other few people

2:04:58.240 --> 2:05:00.600
 are also artificial intelligences?

2:05:00.600 --> 2:05:01.440
 How bad is that?

2:05:01.440 --> 2:05:02.720
 In some sense they are, right?

2:05:02.720 --> 2:05:04.280
 Yeah, they say already.

2:05:04.280 --> 2:05:06.280
 I mean, Einstein wasn't a really normal person.

2:05:06.280 --> 2:05:07.560
 He had a lot of weird quirks.

2:05:07.560 --> 2:05:09.440
 And so did the other people who worked with him.

2:05:09.440 --> 2:05:11.360
 So, you know, maybe they already were sort of

2:05:11.360 --> 2:05:14.200
 this astral plane of intelligence that,

2:05:14.200 --> 2:05:15.240
 we live with it already.

2:05:15.240 --> 2:05:17.000
 It's not a problem.

2:05:17.000 --> 2:05:20.160
 It's still useful and, you know.

2:05:20.160 --> 2:05:22.960
 So do you think we are the only intelligent life

2:05:22.960 --> 2:05:24.880
 out there in the universe?

2:05:24.880 --> 2:05:26.640
 I would say that intelligent life

2:05:27.880 --> 2:05:29.760
 has and will exist elsewhere in the universe.

2:05:29.760 --> 2:05:31.480
 I'll say that.

2:05:31.480 --> 2:05:32.600
 There was a question about

2:05:32.600 --> 2:05:34.080
 contemporaneous intelligence life,

2:05:34.080 --> 2:05:35.600
 which is hard to even answer

2:05:35.600 --> 2:05:39.480
 when we think about relativity and the nature of space time.

2:05:39.480 --> 2:05:41.120
 Can't say what exactly is this time

2:05:41.120 --> 2:05:43.160
 someplace else in the world.

2:05:43.160 --> 2:05:44.600
 But I think it's, you know,

2:05:44.600 --> 2:05:48.440
 I do worry a lot about the filter idea,

2:05:48.440 --> 2:05:52.240
 which is that perhaps intelligent species

2:05:52.240 --> 2:05:54.040
 don't last very long.

2:05:54.040 --> 2:05:55.720
 And so we haven't been around very long.

2:05:55.720 --> 2:05:57.200
 And as a technological species,

2:05:57.200 --> 2:05:59.800
 we've been around for almost nothing, you know.

2:05:59.800 --> 2:06:01.600
 What, 200 years, something like that.

2:06:02.720 --> 2:06:04.160
 And we don't have any data,

2:06:04.160 --> 2:06:06.040
 a good data point on whether it's likely

2:06:06.040 --> 2:06:07.400
 that we'll survive or not.

2:06:08.480 --> 2:06:10.960
 So do I think that there have been intelligent life

2:06:10.960 --> 2:06:11.800
 elsewhere in the universe?

2:06:11.800 --> 2:06:13.400
 Almost certainly, of course.

2:06:13.400 --> 2:06:15.280
 In the past, in the future, yes.

2:06:16.440 --> 2:06:17.880
 Does it survive for a long time?

2:06:17.880 --> 2:06:18.840
 I don't know.

2:06:18.840 --> 2:06:21.120
 This is another reason I'm excited about our work,

2:06:21.120 --> 2:06:24.240
 is our work meaning the general world of AI.

2:06:24.240 --> 2:06:27.440
 I think we can build intelligent machines

2:06:28.640 --> 2:06:30.120
 that outlast us.

2:06:32.040 --> 2:06:34.080
 You know, they don't have to be tied to Earth.

2:06:34.080 --> 2:06:35.800
 They don't have to, you know,

2:06:35.800 --> 2:06:38.200
 I'm not saying they're recreating, you know,

2:06:38.200 --> 2:06:40.680
 aliens, I'm just saying,

2:06:40.680 --> 2:06:41.920
 if I asked myself,

2:06:41.920 --> 2:06:44.280
 and this might be a good point to end on here.

2:06:44.280 --> 2:06:45.120
 If I asked myself, you know,

2:06:45.120 --> 2:06:47.240
 what's special about our species?

2:06:47.240 --> 2:06:49.040
 We're not particularly interesting physically.

2:06:49.040 --> 2:06:51.480
 We don't fly, we're not good swimmers,

2:06:51.480 --> 2:06:54.000
 we're not very fast, we're not very strong, you know.

2:06:54.000 --> 2:06:55.480
 It's our brain, that's the only thing.

2:06:55.480 --> 2:06:57.440
 And we are the only species on this planet

2:06:57.440 --> 2:06:58.760
 that's built the model of the world

2:06:58.760 --> 2:07:01.160
 that extends beyond what we can actually sense.

2:07:01.160 --> 2:07:03.000
 We're the only people who know about

2:07:03.000 --> 2:07:05.160
 the far side of the moon, and the other universes,

2:07:05.160 --> 2:07:07.280
 and I mean, other galaxies, and other stars,

2:07:07.280 --> 2:07:09.520
 and about what happens in the atom.

2:07:09.520 --> 2:07:12.440
 There's no, that knowledge doesn't exist anywhere else.

2:07:12.440 --> 2:07:13.800
 It's only in our heads.

2:07:13.800 --> 2:07:15.000
 Cats don't do it, dogs don't do it,

2:07:15.000 --> 2:07:16.360
 monkeys don't do it, it's just on.

2:07:16.360 --> 2:07:18.320
 And that is what we've created that's unique.

2:07:18.320 --> 2:07:20.360
 Not our genes, it's knowledge.

2:07:20.360 --> 2:07:23.160
 And if I asked me, what is the legacy of humanity?

2:07:23.160 --> 2:07:25.120
 What should our legacy be?

2:07:25.120 --> 2:07:25.960
 It should be knowledge.

2:07:25.960 --> 2:07:27.560
 We should preserve our knowledge

2:07:27.560 --> 2:07:30.080
 in a way that it can exist beyond us.

2:07:30.080 --> 2:07:32.040
 And I think the best way of doing that,

2:07:32.040 --> 2:07:33.080
 in fact you have to do it,

2:07:33.080 --> 2:07:34.880
 is it has to go along with intelligent machines

2:07:34.880 --> 2:07:36.400
 that understand that knowledge.

2:07:37.800 --> 2:07:41.920
 It's a very broad idea, but we should be thinking,

2:07:41.920 --> 2:07:43.800
 I call it a state planning for humanity.

2:07:43.800 --> 2:07:46.560
 We should be thinking about what we wanna leave behind

2:07:46.560 --> 2:07:49.320
 when as a species we're no longer here.

2:07:49.320 --> 2:07:51.080
 And that'll happen sometime.

2:07:51.080 --> 2:07:52.480
 Sooner or later it's gonna happen.

2:07:52.480 --> 2:07:56.080
 And understanding intelligence and creating intelligence

2:07:56.080 --> 2:07:58.400
 gives us a better chance to prolong.

2:07:58.400 --> 2:08:01.120
 It does give us a better chance to prolong life, yes.

2:08:01.120 --> 2:08:03.200
 It gives us a chance to live on other planets.

2:08:03.200 --> 2:08:06.080
 But even beyond that, I mean our solar system

2:08:06.080 --> 2:08:08.680
 will disappear one day, just given enough time.

2:08:08.680 --> 2:08:11.760
 So I don't know, I doubt we'll ever be able to travel

2:08:12.880 --> 2:08:15.480
 to other things, but we could tell the stars,

2:08:15.480 --> 2:08:17.800
 but we could send intelligent machines to do that.

2:08:17.800 --> 2:08:22.800
 So you have an optimistic, a hopeful view of our knowledge

2:08:23.160 --> 2:08:26.040
 of the echoes of human civilization

2:08:26.040 --> 2:08:29.240
 living through the intelligent systems we create?

2:08:29.240 --> 2:08:30.080
 Oh, totally.

2:08:30.080 --> 2:08:31.400
 Well, I think the intelligent systems we create

2:08:31.400 --> 2:08:35.000
 are in some sense the vessel for bringing them beyond Earth

2:08:36.200 --> 2:08:38.800
 or making them last beyond humans themselves.

2:08:39.840 --> 2:08:41.280
 How do you feel about that?

2:08:41.280 --> 2:08:43.640
 That they won't be human, quote unquote?

2:08:43.640 --> 2:08:45.080
 Who cares?

2:08:45.080 --> 2:08:46.120
 Human, what is human?

2:08:46.120 --> 2:08:48.640
 Our species are changing all the time.

2:08:48.640 --> 2:08:51.680
 Human today is not the same as human just 50 years ago.

2:08:52.600 --> 2:08:53.440
 What is human?

2:08:53.440 --> 2:08:54.520
 Do we care about our genetics?

2:08:54.520 --> 2:08:56.160
 Why is that important?

2:08:56.160 --> 2:08:58.320
 As I point out, our genetics are no more interesting

2:08:58.320 --> 2:08:59.440
 than a bacterium's genetics.

2:08:59.440 --> 2:09:01.720
 It's no more interesting than a monkey's genetics.

2:09:01.720 --> 2:09:04.560
 What we have, what's unique and what's valuable

2:09:04.560 --> 2:09:07.400
 is our knowledge, what we've learned about the world.

2:09:07.400 --> 2:09:09.640
 And that is the rare thing.

2:09:09.640 --> 2:09:11.480
 That's the thing we wanna preserve.

2:09:11.480 --> 2:09:13.640
 It's, who cares about our genes?

2:09:13.640 --> 2:09:14.480
 That's not.

2:09:14.480 --> 2:09:16.280
 It's the knowledge.

2:09:16.280 --> 2:09:17.120
 It's the knowledge.

2:09:17.120 --> 2:09:19.080
 That's a really good place to end.

2:09:19.080 --> 2:09:20.120
 Thank you so much for talking to me.

2:09:20.120 --> 2:09:45.120
 No, it was fun.

