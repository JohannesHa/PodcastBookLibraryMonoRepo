WEBVTT

00:00.000 --> 00:05.420
 The following is a conversation with Dimitri Dolgov, the CTO of Waymo, which

00:05.420 --> 00:09.260
 is an autonomous driving company that started as Google self driving car

00:09.260 --> 00:13.160
 project in 2009 and became Waymo in 2016.

00:13.940 --> 00:15.700
 Dimitri was there all along.

00:16.180 --> 00:20.140
 Waymo is currently leading in the fully autonomous vehicle space and that they

00:20.220 --> 00:25.460
 actually have an at scale deployment of publicly accessible autonomous vehicles

00:25.460 --> 00:32.060
 driving passengers around with no safety driver, with nobody in the driver's seat.

00:32.560 --> 00:37.400
 This to me is an incredible accomplishment of engineering on one of

00:37.400 --> 00:41.300
 the most difficult and exciting artificial intelligence challenges of

00:41.300 --> 00:42.440
 the 21st century.

00:43.200 --> 00:46.900
 Quick mention of a sponsor, followed by some thoughts related to the episode.

00:47.440 --> 00:51.860
 Thank you to Triolabs, a company that helps businesses apply machine

00:51.860 --> 00:54.260
 learning to solve real world problems.

00:54.260 --> 00:58.500
 Blinkist, an app I use for reading through summaries of books, better

00:58.500 --> 01:02.900
 help, online therapy with a licensed professional, and Cash App, the app

01:02.900 --> 01:04.220
 I use to send money to friends.

01:04.820 --> 01:08.020
 Please check out the sponsors in the description to get a discount

01:08.060 --> 01:09.380
 at the support this podcast.

01:10.060 --> 01:13.980
 As a side note, let me say that autonomous and semi autonomous driving

01:14.160 --> 01:18.420
 was the focus of my work at MIT and as a problem space that I find

01:18.420 --> 01:23.300
 fascinating and full of open questions from both robotics and a human

01:23.300 --> 01:24.500
 psychology perspective.

01:25.220 --> 01:29.420
 There's quite a bit that I could say here about my experiences in academia

01:29.420 --> 01:35.700
 on this topic that revealed to me, let's say the less admirable size of human

01:35.700 --> 01:39.620
 beings, but I choose to focus on the positive, on solutions.

01:40.020 --> 01:44.220
 I'm brilliant engineers like Dimitri and the team at Waymo, who work

01:44.220 --> 01:48.300
 tirelessly to innovate and to build amazing technology that will define

01:48.300 --> 01:48.900
 our future.

01:48.900 --> 01:53.020
 Because of Dimitri and others like him, I'm excited for this future.

01:53.900 --> 01:59.100
 And who knows, perhaps I too will help contribute something of value to it.

01:59.900 --> 02:03.220
 If you enjoy this thing, subscribe on YouTube, review it with five stars

02:03.220 --> 02:07.380
 and up a podcast, follow on Spotify, support on Patreon, or connect with

02:07.380 --> 02:09.340
 me on Twitter at Lex Friedman.

02:10.140 --> 02:13.700
 And now here's my conversation with Dimitri Dolgov.

02:14.940 --> 02:17.500
 When did you first fall in love with MIT?

02:17.500 --> 02:20.860
 When did you first fall in love with robotics or even computer

02:20.860 --> 02:21.740
 science more in general?

02:22.300 --> 02:27.340
 Computer science first at a fairly young age, then robotics happened much later.

02:28.340 --> 02:39.020
 I think my first interesting introduction to computers was in the late 80s when

02:39.020 --> 02:44.580
 we got our first computer, I think it was an IBM, I think IBM AT.

02:44.580 --> 02:48.020
 Those things that had like a turbo button in the front, the radio

02:48.020 --> 02:50.020
 precedent, you know, make, make the thing goes faster.

02:50.500 --> 02:52.100
 Did that already have floppy disks?

02:52.420 --> 02:52.740
 Yeah.

02:52.780 --> 02:52.980
 Yeah.

02:52.980 --> 02:53.140
 Yeah.

02:53.140 --> 02:53.340
 Yeah.

02:53.340 --> 02:57.060
 Like the, the 5.4 inch ones.

02:57.140 --> 02:58.740
 I think there was a bigger inch.

02:58.780 --> 02:59.220
 So good.

02:59.220 --> 03:01.700
 When something then five inches and three inches.

03:02.060 --> 03:03.100
 Yeah, I think that was the five.

03:03.100 --> 03:06.580
 I don't, I maybe that was before that was the giant plates and it didn't get that.

03:07.180 --> 03:09.300
 But it was definitely not the, not the three inch ones.

03:09.300 --> 03:15.660
 Anyway, so that, that, you know, we got that computer, I spent the first few

03:15.660 --> 03:20.660
 months just playing video games as you would expect, I got bored of that.

03:20.900 --> 03:25.740
 So I started messing around and trying to figure out how to, you know, make

03:25.740 --> 03:33.340
 the thing do other stuff, got into exploring programming and a couple of

03:33.340 --> 03:39.580
 years later, it got to a point where, I actually wrote a game, a lot of games

03:39.620 --> 03:43.660
 and a game developer, a Japanese game developer actually offered to buy it

03:43.660 --> 03:45.100
 for me for a few hundred bucks.

03:45.100 --> 03:48.740
 But you know, for, for a kid in Russia, that's a big deal.

03:48.740 --> 03:49.180
 That's a big deal.

03:49.180 --> 03:49.460
 Yeah.

03:49.780 --> 03:50.660
 I did not take the deal.

03:51.140 --> 03:51.500
 Wow.

03:51.700 --> 03:52.260
 Integrity.

03:52.500 --> 03:52.780
 Yeah.

03:53.020 --> 03:58.860
 I, I instead, yes, that was not the most acute financial move that I made in my

03:58.860 --> 04:02.380
 life, you know, looking back at it now, I, I instead put it, well, you know, I had

04:02.380 --> 04:07.180
 a reason I put it online, it was, what'd you call it back in the days?

04:07.180 --> 04:08.420
 It was a freeware thing, right?

04:08.460 --> 04:11.260
 It was not open source, but you could upload the binaries, you would put the

04:11.260 --> 04:14.420
 game online and the idea was that, you know, people like it and then they, you

04:14.420 --> 04:16.500
 know, contribute on the send you a little donations, right?

04:16.540 --> 04:20.020
 So I did my quick math of like, you know, of course, you know, thousands and

04:20.020 --> 04:22.620
 millions of people are going to play my game, send me a couple of bucks a piece,

04:22.620 --> 04:23.700
 you know, should definitely do that.

04:24.500 --> 04:26.580
 As I said, not, not the best.

04:26.580 --> 04:29.060
 You're already playing with business models at that young age.

04:29.300 --> 04:30.500
 Remember what language it was?

04:30.500 --> 04:35.020
 What programming, it was a Pascal, which what Pascal, Pascal, and that

04:35.020 --> 04:37.100
 a graphical component, so it's not text based.

04:37.180 --> 04:37.460
 Yeah.

04:37.460 --> 04:37.660
 Yeah.

04:37.660 --> 04:43.140
 It was, uh, like, uh, I think there are 300, 320 by 200, uh, whatever it was.

04:43.180 --> 04:46.340
 I think that kind of the earlier, that's the resolution, right?

04:46.420 --> 04:49.420
 And I actually think the reason why this company wanted to buy it is not like the

04:49.420 --> 04:51.340
 fancy graphics or the implementation.

04:51.340 --> 04:55.620
 That was maybe the idea, uh, of my actual game, the idea of the game.

04:57.140 --> 04:59.020
 Well, one of the things I, it's so funny.

04:59.020 --> 05:05.540
 I'm used to play this game called golden X and the simplicity of the graphics and

05:05.860 --> 05:10.260
 something about the simplicity of the music, like it's still haunts me.

05:10.740 --> 05:12.060
 I don't know if that's a childhood thing.

05:12.060 --> 05:14.940
 I don't know if that's the same thing for call of duty these days for young kids,

05:15.340 --> 05:20.580
 but I still think that the simple one of the games are simple.

05:21.260 --> 05:28.260
 That simple purity makes for like allows your imagination to take over and

05:28.260 --> 05:30.300
 thereby creating a more magical experience.

05:30.780 --> 05:34.100
 Like now with better and better graphics, it feels like your

05:34.100 --> 05:38.980
 imagination doesn't get to, uh, create worlds, which is kind of interesting.

05:38.980 --> 05:43.020
 Um, it could be just an old man on a porch, like way waving at kids

05:43.020 --> 05:44.780
 these days that have no respect.

05:44.780 --> 05:49.140
 But I still think that graphics almost get in the way of the experience.

05:49.860 --> 05:50.220
 I don't know.

05:50.740 --> 05:51.340
 Flip a bird.

05:51.860 --> 05:57.300
 Yeah, I don't know if the imagination is closed.

05:57.300 --> 06:01.540
 I don't yet, but that that's more about games that op like that's more

06:01.540 --> 06:09.580
 like Tetris world where they optimally masterfully, like create a fun, short

06:09.580 --> 06:14.540
 term dopamine experience versus I'm more referring to like role playing

06:14.540 --> 06:18.860
 games where there's like a story you can live in it for months or years.

06:18.860 --> 06:23.900
 Um, like, uh, there's an elder scroll series, which is probably my favorite

06:23.900 --> 06:26.540
 set of games that was a magical experience.

06:26.540 --> 06:28.100
 And that the graphics are terrible.

06:28.460 --> 06:31.460
 The characters were all randomly generated, but they're, I don't know.

06:31.500 --> 06:33.660
 That's it pulls you in.

06:33.700 --> 06:34.660
 There's a story.

06:34.700 --> 06:40.180
 It's like an interactive version of an elder scrolls Tolkien world.

06:40.580 --> 06:41.660
 And you get to live in it.

06:42.140 --> 06:42.580
 I don't know.

06:43.460 --> 06:44.020
 I miss it.

06:44.580 --> 06:49.540
 It's one of the things that suck about being an adult is there's no, you have

06:49.540 --> 06:54.060
 to live in the real world as opposed to the elder scrolls world, you know, whatever

06:54.060 --> 06:54.940
 brings you joy, right?

06:54.940 --> 06:55.940
 Minecraft, right?

06:55.940 --> 06:56.780
 Minecraft is a great example.

06:56.780 --> 07:01.260
 You create, like it's not the fancy graphics, but it's the creation of your own worlds.

07:01.420 --> 07:02.460
 Yeah, that one is crazy.

07:02.700 --> 07:06.700
 You know, one of the pitches for being a parent that people tell me is that you

07:06.700 --> 07:11.700
 can like use the excuse of parenting to, to go back into the video game world.

07:12.180 --> 07:17.940
 And like, like that's like, you know, father, son, father, daughter time, but

07:17.940 --> 07:19.820
 really you just get to play video games with your kids.

07:19.820 --> 07:27.260
 So anyway, at that time, did you have any ridiculously ambitious dreams of where as

07:27.260 --> 07:29.780
 a creator, you might go as an engineer?

07:29.780 --> 07:33.820
 Did you, what, what did you think of yourself as, as an engineer, as a tinker,

07:33.860 --> 07:36.100
 or did you want to be like an astronaut or something like that?

07:37.220 --> 07:42.220
 You know, I'm tempted to make something up about, you know, robots, uh, engineering

07:42.220 --> 07:45.940
 or, you know, mysteries of the universe, but that's not the actual memory that

07:45.940 --> 07:48.780
 pops into my mind when you, when you asked me about childhood dreams.

07:48.780 --> 07:55.860
 So I'll actually share the, the, the real thing, uh, when I was maybe four or five

07:55.860 --> 08:00.340
 years old, I, you know, as we all do, I thought about, you know, what I wanted

08:00.340 --> 08:07.820
 to do when I grow up and I had this dream of being a traffic control cop.

08:08.660 --> 08:11.380
 Uh, you know, they don't have those today's I think, but you know, back in

08:11.380 --> 08:15.300
 the eighties and in Russia, uh, you probably are familiar with that Lex.

08:15.300 --> 08:19.300
 They had these, uh, you know, police officers that would stand in the middle

08:19.300 --> 08:21.940
 of intersection all day and they would have their like stripe back, black and

08:21.940 --> 08:26.060
 white batons that they would use to control the flow of traffic and, you

08:26.060 --> 08:29.940
 know, for whatever reasons, I was strangely infatuated with this whole

08:29.940 --> 08:31.740
 process and like that, that was my dream.

08:32.220 --> 08:36.580
 Uh, that's what I wanted to do when I grew up and, you know, my parents, uh,

08:37.100 --> 08:41.820
 both physics profs, by the way, I think were, you know, a little concerned, uh,

08:41.860 --> 08:44.220
 with that level of ambition coming from their child.

08:44.220 --> 08:46.420
 Uh, uh, you know, that age.

08:46.740 --> 08:50.020
 Well, that it's an interesting, I don't know if you can relate,

08:50.060 --> 08:51.900
 but I very much love that idea.

08:52.300 --> 08:57.980
 I have a OCD nature that I think lends itself very close to the engineering

08:57.980 --> 09:05.580
 mindset, which is you want to kind of optimize, you know, solve a problem by

09:05.580 --> 09:11.220
 create, creating an automated solution, like a, like a set of rules, that set

09:11.220 --> 09:14.340
 of rules you can follow and then thereby make it ultra efficient.

09:14.820 --> 09:17.300
 I don't know if that's, it was of that nature.

09:17.340 --> 09:18.380
 I certainly have that.

09:18.580 --> 09:22.420
 There's like fact, like SimCity and factory building games, all those

09:22.420 --> 09:26.020
 kinds of things kind of speak to that engineering mindset, or

09:26.020 --> 09:27.220
 did you just like the uniform?

09:27.500 --> 09:28.900
 I think it was more of the latter.

09:28.900 --> 09:33.140
 I think it was the uniform and the, you know, the, the stripe baton that

09:33.140 --> 09:35.780
 made cars go in the right directions.

09:36.820 --> 09:40.980
 But I guess, you know, I, it is, I did end up, uh, I guess, uh,

09:40.980 --> 09:44.860
 you know, working on the transportation industry one way or another uniform.

09:44.860 --> 09:45.900
 No, but that's right.

09:46.900 --> 09:51.500
 Maybe, maybe, maybe it was my, you know, deep inner infatuation with the,

09:51.620 --> 09:55.180
 you know, traffic control batons that led to this career.

09:55.460 --> 09:55.740
 Okay.

09:55.740 --> 10:00.620
 What, uh, when did you, when was the leap from programming to robotics?

10:00.940 --> 10:01.540
 That happened later.

10:01.620 --> 10:05.340
 That was after grad school, uh, after, and I actually, the most self driving

10:05.340 --> 10:10.020
 cars was I think my first real hands on introduction to robotics.

10:10.020 --> 10:14.580
 But I never really had that much hands on experience in school and training.

10:14.580 --> 10:17.020
 I, you know, worked on applied math and physics.

10:17.340 --> 10:22.780
 Then in college, I did more half, uh, abstract computer science.

10:23.540 --> 10:28.380
 And it was after grad school that I really got involved in robotics, which

10:28.380 --> 10:29.900
 was actually self driving cars.

10:29.980 --> 10:32.300
 And, you know, that was a big flip.

10:32.500 --> 10:33.740
 What, uh, what grad school?

10:34.140 --> 10:37.020
 So I went to grad school in Michigan, and then I did a postdoc at Stanford,

10:37.020 --> 10:41.380
 uh, which is, that was the postdoc where I got to play with self driving cars.

10:42.260 --> 10:42.540
 Yeah.

10:42.540 --> 10:43.620
 So we'll return there.

10:43.740 --> 10:46.020
 Let's go back to, uh, to Moscow.

10:46.020 --> 10:50.260
 So, uh, you know, for episode 100, I talked to my dad and also I

10:50.260 --> 10:51.780
 grew up with my dad, I guess.

10:53.860 --> 11:01.020
 Uh, so I had to put up with them for many years and, uh, he, he went to the

11:01.260 --> 11:06.260
 FISTIEG or MIPT, it's weird to say in English, cause I've heard all this

11:06.260 --> 11:09.860
 in Russian, Moscow Institute of Physics and Technology.

11:09.900 --> 11:15.500
 And to me, that was like, I met some super interesting, as a child, I met

11:15.500 --> 11:17.060
 some super interesting characters.

11:17.740 --> 11:21.460
 It felt to me like the greatest university in the world, the most elite

11:21.460 --> 11:26.340
 university in the world, and just the people that I met that came out of there

11:26.340 --> 11:32.300
 were like, not only brilliant, but also special humans.

11:32.300 --> 11:37.900
 It seems like that place really tested the soul, uh, both like in terms

11:37.900 --> 11:40.180
 of technically and like spiritually.

11:40.620 --> 11:43.660
 So that could be just the romanticization of that place.

11:43.660 --> 11:47.660
 I'm not sure, but so maybe you can speak to it, but is it correct to

11:47.660 --> 11:49.980
 say that you spent some time at FISTIEG?

11:50.060 --> 11:50.660
 Yeah, that's right.

11:50.740 --> 11:51.380
 Six years.

11:51.460 --> 11:54.780
 Uh, I got my bachelor's and master's in physics and math there.

11:55.260 --> 11:59.220
 And it's actually interesting cause my, my dad, and actually both my parents,

11:59.220 --> 12:03.740
 uh, went there and I think all the stories that I heard, uh, like, just

12:03.740 --> 12:07.700
 like you, Alex, uh, growing up about the place and, you know, how interesting

12:07.700 --> 12:11.820
 and special and magical it was, I think that was a significant, maybe the

12:11.820 --> 12:16.420
 main reason, uh, I wanted to go there, uh, for college, uh, enough so that

12:16.460 --> 12:21.300
 I actually went back to Russia from the U S I graduated high school in the U S.

12:21.820 --> 12:23.740
 Um, and you went back there.

12:23.780 --> 12:24.300
 I went back there.

12:24.300 --> 12:28.220
 Yeah, that's exactly the reaction most of my peers in college had.

12:28.220 --> 12:32.300
 But, you know, perhaps a little bit stronger that like, you know, point

12:32.300 --> 12:34.780
 me out as this crazy kid, were your parents supportive of that?

12:34.780 --> 12:35.280
 Yeah.

12:35.540 --> 12:35.740
 Yeah.

12:35.740 --> 12:38.780
 My games, your previous question, they, uh, they supported me and, you know,

12:38.780 --> 12:43.380
 letting me kind of pursue my passions and the things that I was interested in.

12:43.380 --> 12:44.140
 That's a bold move.

12:44.140 --> 12:44.340
 Wow.

12:44.380 --> 12:45.140
 What was it like there?

12:45.340 --> 12:49.140
 It was interesting, you know, definitely fairly hardcore on the fundamentals

12:49.220 --> 12:53.260
 of, you know, math and physics and, uh, you know, lots of good memories,

12:53.300 --> 12:54.940
 uh, from, you know, from those times.

12:55.460 --> 12:56.100
 So, okay.

12:56.180 --> 12:57.140
 So Stanford.

12:57.140 --> 12:58.700
 How'd you get into autonomous vehicles?

12:59.180 --> 13:06.060
 I had the great fortune, uh, and great honor to join Stanford's

13:06.060 --> 13:07.340
 DARPA urban challenge team.

13:07.460 --> 13:12.300
 And, uh, 2006 there, this was a third in the sequence of the DARPA challenges.

13:12.300 --> 13:14.940
 There were two grand challenges prior to that.

13:14.940 --> 13:19.220
 And then in 2007, they held the DARPA urban challenge.

13:19.380 --> 13:25.900
 So, you know, I was doing my, my postdoc I had, I joined the team and, uh, worked

13:25.900 --> 13:29.940
 on motion planning, uh, for, you know, that, that competition.

13:30.220 --> 13:30.700
 So, okay.

13:30.700 --> 13:35.740
 So for people who might not know, I know from, from certain autonomous vehicles is

13:35.740 --> 13:39.460
 a funny world in a certain circle of people, everybody knows everything.

13:39.860 --> 13:45.020
 And then the certain circle, uh, nobody knows anything in terms of general public.

13:45.020 --> 13:46.020
 So it's interesting.

13:46.020 --> 13:50.420
 It's, it's a good question of what to talk about, but I do think that the urban

13:50.420 --> 13:56.060
 challenge is worth revisiting. It's a fun little challenge.

13:56.100 --> 14:03.020
 One that, first of all, like sparked so much, so many incredible minds to focus

14:03.020 --> 14:06.740
 on one of the hardest problems of our time in artificial intelligence.

14:06.740 --> 14:10.580
 So that's, that's a success from a perspective of a single little challenge.

14:11.140 --> 14:14.100
 But can you talk about like, what did the challenge involve?

14:14.220 --> 14:18.660
 So were there pedestrians, were there other cars, what was the goal?

14:18.660 --> 14:20.220
 Uh, who was on the team?

14:20.540 --> 14:24.540
 How long did it take any fun, fun sort of specs?

14:25.460 --> 14:26.180
 Sure, sure, sure.

14:26.220 --> 14:29.900
 So the way the challenge was constructed and just a little bit of backgrounding,

14:29.900 --> 14:34.020
 as I mentioned, this was the third, uh, competition in that series.

14:34.220 --> 14:36.940
 The first year we're at the grand challenge called the grand challenge.

14:36.940 --> 14:40.220
 The goal there was to just drive in a completely static environment.

14:40.260 --> 14:45.780
 You know, you had to drive in a desert, uh, that was very successful.

14:45.780 --> 14:49.980
 So then DARPA followed with what they called the urban challenge, where the

14:49.980 --> 14:54.780
 goal was to have, you know, build vehicles that could operate in more dynamic

14:54.780 --> 14:56.780
 environments and, you know, share them with other vehicles.

14:56.780 --> 15:00.420
 There were no pedestrians there, but what DARPA did is they took over

15:00.460 --> 15:02.060
 an abandoned air force base.

15:02.460 --> 15:06.060
 Uh, and it was kind of like a little fake city that they built out there.

15:06.460 --> 15:11.500
 And they had a bunch of, uh, robots, uh, you know, cars, uh, that were

15:11.500 --> 15:13.740
 autonomous, uh, in there all at the same time.

15:13.740 --> 15:19.420
 Uh, mixed in with other vehicles driven by professional, uh, drivers and each

15:19.420 --> 15:24.940
 car, uh, had a mission and so there's a crude map that they received, uh,

15:24.980 --> 15:28.060
 beginning and they had a mission and go here and then there and over here.

15:28.300 --> 15:32.980
 Um, and they kind of all were sharing this environment at the same time.

15:32.980 --> 15:34.300
 They had to interact with each other.

15:34.300 --> 15:35.820
 They had to interact with the human drivers.

15:36.060 --> 15:42.340
 There's this very first, very rudimentary, um, version of, uh,

15:42.340 --> 15:47.300
 self driving car that, you know, could operate, uh, and, uh, in a, in an

15:47.300 --> 15:50.620
 environment, you know, shared with other dynamic actors that, as you said,

15:50.860 --> 15:54.260
 you know, really, you know, many ways, you know, kickstarted this whole industry.

15:55.220 --> 15:55.420
 Okay.

15:55.420 --> 15:58.380
 So who was on the team and how'd you do?

15:58.420 --> 15:58.780
 I forget.

15:59.780 --> 16:01.980
 Uh, I came in second.

16:02.460 --> 16:05.140
 Uh, perhaps that was my contribution to the team.

16:05.140 --> 16:07.660
 I think the Stanford team came in first in the DARPA challenge.

16:07.700 --> 16:10.300
 Uh, but then I joined the team and, you know, you were the one with the

16:10.300 --> 16:13.860
 bug in the code, I mean, do you have sort of memories of some

16:13.860 --> 16:18.900
 particularly challenging things or, you know, one of the cool things,

16:18.900 --> 16:23.900
 it's not, you know, this isn't a product, this isn't the thing that, uh, you know,

16:24.220 --> 16:27.220
 it there's, you have a little bit more freedom to experiment so you can take

16:27.220 --> 16:30.140
 risks and there's, uh, so you can make mistakes.

16:30.460 --> 16:32.540
 Uh, so is there interesting mistakes?

16:33.100 --> 16:36.060
 Is there interesting challenges that stand out to you as some, like, taught

16:36.060 --> 16:42.180
 you, um, a good technical lesson or a good philosophical lesson from that time?

16:42.540 --> 16:42.860
 Yeah.

16:43.020 --> 16:46.260
 Uh, you know, definitely, definitely a very memorable time, not really

16:46.260 --> 16:51.740
 challenged, but like one of the most vivid memories that I have from the time.

16:52.100 --> 16:58.660
 And I think that was actually one of the days that really got me hooked, uh, on

16:58.660 --> 17:05.660
 this whole field was, uh, the first time I got to run my software and I got to

17:05.660 --> 17:11.500
 software on the car and, uh, I was working on a part of our planning algorithm,

17:11.580 --> 17:13.820
 uh, that had to navigate in parking lots.

17:13.860 --> 17:16.580
 So it was something that, you know, called free space emotion planning.

17:16.780 --> 17:20.940
 So the very first version of that, uh, was, you know, we tried on the car, it

17:20.940 --> 17:24.420
 was on Stanford's campus, uh, in the middle of the night and you had this

17:24.420 --> 17:28.380
 little course constructed with cones, uh, in the middle of a parking lot.

17:28.380 --> 17:31.700
 So we're there in like 3 am, you know, by the time we got the code to, you

17:31.700 --> 17:36.300
 know, uh, uh, you know, compile and turn over, uh, and, you know, it drove, I

17:36.300 --> 17:39.980
 could actually did something quite reasonable and, you know, it was of

17:39.980 --> 17:46.620
 course very buggy at the time and had all kinds of problems, but it was pretty

17:46.700 --> 17:48.140
 darn magical.

17:48.180 --> 17:52.300
 I remember going back and, you know, later at night and trying to fall

17:52.300 --> 17:55.220
 asleep and just, you know, being unable to fall asleep for the rest of the

17:55.220 --> 17:57.900
 night, uh, just my mind was blown.

17:57.900 --> 18:02.340
 Just like, and that, that, that's what I've been doing ever since for more

18:02.340 --> 18:06.460
 than a decade, uh, in terms of challenges and, uh, you know, interesting

18:06.460 --> 18:09.780
 memories, like on the day of the competition, uh, it was pretty nerve

18:09.780 --> 18:10.260
 wrecking.

18:10.300 --> 18:13.780
 Uh, I remember standing there with Mike Montemarillo, who was, uh, the

18:13.780 --> 18:15.740
 software lead and wrote most of the code.

18:15.780 --> 18:19.060
 I think I did one little part of the planner, Mike, you know, incredibly

18:19.420 --> 18:22.820
 that, you know, pretty much the rest of it, uh, with, with, you know, a bunch

18:22.820 --> 18:25.660
 of other incredible people, but I remember standing on the day of the

18:25.660 --> 18:29.860
 competition, uh, you know, watching the car, you know, with Mike and cars

18:29.860 --> 18:32.180
 are completely empty, right?

18:32.180 --> 18:35.300
 They're all there lined up in the beginning of the race and then, you

18:35.300 --> 18:38.340
 know, DARPA sends them, you know, on their mission one by one.

18:38.500 --> 18:42.180
 So then leave and Mike, you just, they had these sirens, they all had

18:42.180 --> 18:43.580
 their different silence silence, right?

18:43.580 --> 18:46.100
 Each siren had its own personality, if you will.

18:46.260 --> 18:48.460
 So, you know, off they go and you don't see them.

18:48.460 --> 18:50.740
 You just kind of, and then every once in a while they come a little bit

18:50.740 --> 18:55.060
 closer to where the audience is and you can kind of hear, you know, the

18:55.060 --> 18:57.380
 sound of your car and then, you know, it seems to be moving along.

18:57.380 --> 18:58.420
 So that, you know, gives you hope.

18:58.700 --> 19:01.500
 And then, you know, it goes away and you can't hear it for too long.

19:01.500 --> 19:02.420
 You start getting anxious, right?

19:02.420 --> 19:04.140
 So it's a little bit like, you know, sending your kids to college and like,

19:04.140 --> 19:05.500
 you know, kind of you invested in them.

19:05.700 --> 19:09.060
 You hope you, you, you, you, you, you, you build it properly, but like,

19:09.100 --> 19:11.180
 it's still, uh, anxiety inducing.

19:11.700 --> 19:16.860
 Uh, so that was, uh, an incredibly, uh, fun, uh, few days in terms of, you

19:16.860 --> 19:20.740
 know, bugs, as you mentioned, you know, one that that was my bug that caused

19:20.740 --> 19:24.540
 us the loss of the first place, uh, is still a debate that, you know,

19:24.540 --> 19:27.820
 occasionally have with people on the CMU team, CMU came first, I should

19:27.820 --> 19:32.380
 mention, uh, that you haven't heard of them, but yeah, it's something, you

19:32.380 --> 19:35.340
 know, it's a small school, but it's, it's, it's, you know, really a glitch

19:35.340 --> 19:38.140
 that, you know, they happen to succeed at something robotics related.

19:38.140 --> 19:39.060
 Very scenic though.

19:39.060 --> 19:41.340
 So most people go there for the scenery.

19:41.460 --> 19:43.780
 Um, yeah, it's a beautiful campus.

19:45.340 --> 19:46.580
 I'm like, unlike Stanford.

19:46.780 --> 19:48.420
 So for people, yeah, that's true.

19:48.420 --> 19:51.540
 Unlike Stanford, for people who don't know, CMU is one of the great robotics

19:51.540 --> 19:55.300
 and sort of artificial intelligence universities in the world, CMU, Carnegie

19:55.300 --> 19:57.660
 Mellon university, okay, sorry, go ahead.

19:58.380 --> 19:59.180
 Good, good PSA.

19:59.420 --> 20:06.380
 So in the part that I contributed to, which was navigating parking lots and

20:06.380 --> 20:12.180
 the way that part of the mission work is, uh, you in a parking lot, you

20:12.180 --> 20:15.700
 would get from DARPA an outline of the map.

20:15.700 --> 20:18.540
 You basically get this, you know, giant polygon that defined the

20:18.540 --> 20:21.700
 perimeter of the parking lot, uh, and there would be an entrance and, you

20:21.700 --> 20:25.300
 know, so maybe multiple entrances or access to it, and then you would get a

20:25.300 --> 20:32.180
 goal, uh, within that open space, uh, X, Y, you know, heading where the car had

20:32.180 --> 20:36.380
 to park and had no information about the optical, so obstacles that the car might

20:36.380 --> 20:36.860
 encounter there.

20:36.860 --> 20:40.740
 So it had to navigate a kind of completely free space, uh, from the

20:40.740 --> 20:43.740
 entrance to the parking lot into that parking space.

20:43.740 --> 20:50.100
 And then, uh, once parked there, it had to, uh, exit the parking lot, you know,

20:50.100 --> 20:53.020
 while of course, I'm counting and reasoning about all the obstacles that

20:53.060 --> 20:54.620
 it encounters in real time.

20:54.860 --> 21:00.940
 So, uh, Our interpretation, or at least my interpretation of the rules was that

21:00.940 --> 21:03.220
 you had to reverse out of the parking spot.

21:03.420 --> 21:04.860
 And that's what our cars did.

21:04.900 --> 21:08.540
 Even if there's no obstacle in front, that's not what CMU's car did.

21:08.620 --> 21:10.580
 And it just kind of drove right through.

21:10.620 --> 21:12.260
 So there's still a debate.

21:12.260 --> 21:14.860
 And of course, you know, as you stop and then reverse out and go out the

21:14.860 --> 21:16.460
 different way that costs you some time.

21:16.580 --> 21:20.260
 And so there's still a debate whether, you know, it was my poor implementation

21:20.300 --> 21:26.100
 that cost us extra time or whether it was, you know, CMU, uh, violating an

21:26.100 --> 21:27.380
 important rule of the competition.

21:27.380 --> 21:30.700
 And, you know, I have my own, uh, opinion here in terms of other bugs.

21:30.700 --> 21:34.380
 And like, uh, I, I have to apologize to Mike Montemarila, uh, for sharing this

21:34.380 --> 21:38.180
 on air, but it is actually, uh, one of the more memorable ones.

21:38.180 --> 21:42.940
 Uh, and it's something that's kind of become a bit of, uh, a metaphor and

21:42.940 --> 21:46.100
 a label in the industry, uh, since then, I think, you know, at least in some

21:46.100 --> 21:48.820
 circles, it's called the victory circle or victory lap.

21:49.020 --> 21:52.860
 Um, and, uh, uh, our cars did that.

21:53.060 --> 21:57.540
 So in one of the missions in the urban challenge, in one of the courses, uh,

21:57.580 --> 22:02.020
 there was this big oval, right by the start and finish of the race.

22:02.020 --> 22:05.620
 So the ARPA had a lot of the missions would finish kind of in that same location.

22:05.620 --> 22:08.620
 Uh, and it was pretty cool because you could see the cars come by, you know,

22:08.620 --> 22:11.780
 kind of finished that part leg of the trip, that leg of the mission, and then,

22:11.780 --> 22:14.860
 you know, go on and finish the rest of it.

22:15.220 --> 22:22.260
 Uh, and other vehicles would, you know, come hit their waypoint, uh, and, you

22:22.260 --> 22:24.100
 know, exit the oval and off they would go.

22:24.340 --> 22:28.060
 Our car on the hand, which hit the checkpoint, and then it would do an extra

22:28.060 --> 22:31.620
 lap around the oval and only then, you know, uh, leave and go on its merry way.

22:31.620 --> 22:34.620
 So over the course of the full day, it accumulated, uh, uh,

22:34.620 --> 22:38.100
 some extra time and the problem was that we had a bug where it wouldn't, you know,

22:38.100 --> 22:41.380
 start reasoning about the next waypoint and plan a route to get to that next

22:41.380 --> 22:42.660
 point until it hit a previous one.

22:42.820 --> 22:46.180
 And in that particular case, by the time you hit the, that, that one, it was too

22:46.180 --> 22:49.140
 late for us to consider the next one and kind of make a lane change.

22:49.140 --> 22:50.900
 So at every time we would do like an extra lap.

22:50.940 --> 22:54.980
 So, you know, and that's the Stanford victory lap.

22:55.060 --> 22:56.300
 The victory lap.

22:57.100 --> 22:59.580
 Oh, that's there's, I feel like there's something philosophically

22:59.580 --> 23:03.620
 profound in there somehow, but, uh, I mean, ultimately everybody is

23:03.620 --> 23:05.180
 a winner in that kind of competition.

23:06.140 --> 23:13.100
 And it led to sort of famously to the creation of, um, Google self driving

23:13.100 --> 23:15.180
 car project and now Waymo.

23:15.740 --> 23:19.900
 So can we, uh, give an overview of how is Waymo born?

23:20.340 --> 23:22.860
 How's the Google self driving car project born?

23:23.180 --> 23:24.620
 What's the, what is the mission?

23:24.780 --> 23:25.700
 What is the hope?

23:26.300 --> 23:32.460
 What is it is the engineering kind of, uh, set of milestones that

23:32.460 --> 23:35.700
 it seeks to accomplish, there's a lot of questions in there.

23:35.780 --> 23:40.060
 Uh, yeah, uh, I don't know, kind of the DARPA urban challenge and the DARPA

23:40.060 --> 23:44.380
 and previous DARPA grand challenges, uh, kind of led, I think to a very large

23:44.420 --> 23:48.100
 degree to that next step and then, you know, Larry and Sergey, um, uh, Larry

23:48.100 --> 23:52.180
 Page and Sergey Brin, uh, uh, Google founders course, uh, I saw that

23:52.180 --> 23:54.820
 competition and believed in the technology.

23:54.940 --> 23:59.900
 So, you know, the Google self driving car project was born, you know, at that time.

23:59.900 --> 24:04.300
 And we started in 2009, it was a pretty small group of us, about a dozen people,

24:04.820 --> 24:09.300
 uh, who came together, uh, to, to work on this project at Google.

24:09.620 --> 24:18.140
 At that time we saw an incredible early result in the DARPA urban challenge.

24:18.140 --> 24:23.980
 I think we're all incredibly excited, uh, about where we got to and we believed

24:23.980 --> 24:27.500
 in the future of the technology, but we still had a very, you know,

24:27.500 --> 24:30.940
 very, you know, rudimentary understanding of the problem space.

24:31.660 --> 24:37.620
 So the first goal of this project in 2009 was to really better

24:37.660 --> 24:39.260
 understand what we're up against.

24:39.620 --> 24:44.340
 Uh, and, you know, with that goal in mind, when we started the project, we created a

24:44.340 --> 24:46.860
 few milestones for ourselves, uh, that.

24:48.300 --> 24:49.460
 Maximized learnings.

24:49.700 --> 24:54.300
 Well, the two milestones were, you know, uh, one was to drive a hundred thousand

24:54.300 --> 24:57.940
 miles in autonomous mode, which was at that time, you know, orders of magnitude

24:57.940 --> 25:00.620
 that, uh, more than anybody has ever done.

25:01.100 --> 25:07.060
 And the second milestone was to drive 10 routes, uh, each one was a hundred miles

25:07.060 --> 25:12.700
 long, uh, and there were specifically chosen to become extra spicy and extra

25:12.700 --> 25:18.460
 complicated and sample the full complexity of the, that, that, uh, domain.

25:18.460 --> 25:24.100
 Um, uh, and you had to drive each one from beginning to end with no intervention,

25:24.140 --> 25:24.900
 no human intervention.

25:24.900 --> 25:28.420
 So you would get to the beginning of the course, uh, you would press the button

25:28.460 --> 25:32.900
 that would engage in autonomy and you had to go for a hundred miles, you know,

25:32.900 --> 25:35.140
 beginning to end, uh, with no interventions.

25:35.220 --> 25:40.460
 Um, and it sampled again, the full complexity of driving conditions.

25:40.460 --> 25:42.940
 Some, uh, were on freeways.

25:42.940 --> 25:45.180
 We had one route that went all through all the freeways and all

25:45.180 --> 25:46.820
 the bridges in the Bay area.

25:46.820 --> 25:50.500
 You know, we had, uh, some that went around Lake Tahoe and kind of mountains,

25:50.540 --> 25:51.700
 uh, roads.

25:52.060 --> 25:56.900
 We had some that drove through dense urban, um, environments like in downtown

25:56.900 --> 25:59.180
 Palo Alto and through San Francisco.

25:59.460 --> 26:04.820
 So it was incredibly, uh, interesting, uh, to work on.

26:04.900 --> 26:10.940
 And it, uh, it took us just under two years, uh, about a year and a half,

26:10.940 --> 26:14.180
 a little bit more to finish both of these milestones.

26:14.180 --> 26:20.100
 And in that process, uh, you know, it was an incredible amount of fun,

26:20.100 --> 26:22.740
 probably the most fun I had in my professional career.

26:22.780 --> 26:24.700
 And you're just learning so much.

26:24.700 --> 26:26.820
 You are, you know, the goal here is to learn and prototype.

26:26.820 --> 26:29.180
 You're not yet starting to build a production system, right?

26:29.180 --> 26:33.380
 So you just, you were, you know, this is when you're kind of working 24 seven

26:33.380 --> 26:34.700
 and you're hacking things together.

26:34.740 --> 26:37.580
 And you also don't know how hard this is.

26:37.620 --> 26:38.500
 I mean, that's the point.

26:38.820 --> 26:42.780
 Like, so, I mean, that's an ambitious, if I put myself in that mindset, even

26:42.780 --> 26:45.940
 still, that's a really ambitious set of goals.

26:46.660 --> 26:56.260
 Like just those two picking, picking 10 different, difficult, spicy challenges.

26:56.580 --> 26:58.860
 And then having zero interventions.

26:59.460 --> 27:05.940
 So like not saying gradually we're going to like, you know, over a period of 10

27:05.940 --> 27:09.580
 years, we're going to have a bunch of routes and gradually reduce the number

27:09.580 --> 27:13.980
 of interventions, you know, that literally says like, by as soon as

27:13.980 --> 27:17.580
 possible, we want to have zero and on hard roads.

27:17.980 --> 27:22.620
 So like, to me, if I was facing that, it's unclear that whether that takes

27:23.180 --> 27:25.220
 two years or whether that takes 20 years.

27:26.420 --> 27:27.780
 I mean, it took us under two.

27:27.820 --> 27:32.940
 I guess that that speaks to a really big difference between doing something

27:32.980 --> 27:37.820
 once and having a prototype where you are going after, you know, learning

27:37.820 --> 27:42.780
 about the problem versus how you go about engineering a product that, you

27:42.780 --> 27:47.180
 know, where you look at, you know, you do properly do evaluation, you look

27:47.180 --> 27:49.980
 at metrics, you drive down and you're confident that you can do that.

27:50.380 --> 27:55.820
 And I guess that's the, you know, why it took a dozen people, you know, 16

27:55.820 --> 28:00.780
 months or a little bit more than that back in 2009 and 2010 with the

28:00.780 --> 28:05.420
 technology of, you know, the more than a decade ago that amount of time to

28:05.420 --> 28:10.220
 achieve that milestone of, you know, 10 routes, a hundred miles each and no

28:10.220 --> 28:17.340
 interventions, and, you know, it took us a little bit longer to get to, you

28:17.340 --> 28:19.980
 know, a full driverless product that customers use.

28:20.380 --> 28:21.740
 That's another really important moment.

28:21.740 --> 28:29.580
 Is there some memories of technical lessons or just one, like, what did you

28:29.580 --> 28:32.220
 learn about the problem of driving from that experience?

28:32.220 --> 28:36.540
 I mean, we can, we can now talk about like what you learned from modern day

28:36.540 --> 28:41.340
 Waymo, but I feel like you may have learned some profound things in those

28:41.420 --> 28:47.580
 early days, even more so because it feels like what Waymo is now is to trying

28:47.580 --> 28:51.020
 to, you know, how to do scale, how to make sure you create a product, how to

28:51.020 --> 28:54.140
 make sure it's like safety and all those things, which is all fascinating

28:54.140 --> 28:59.500
 challenges, but like you were facing the more fundamental philosophical

28:59.500 --> 29:02.540
 problem of driving in those early days.

29:02.540 --> 29:07.820
 Like what the hell is driving as an autonomous, or maybe I'm again

29:07.820 --> 29:14.540
 romanticizing it, but is it, is there, is there some valuable lessons you

29:14.540 --> 29:16.620
 picked up over there at those two years?

29:18.060 --> 29:18.620
 A ton.

29:19.020 --> 29:25.500
 The most important one is probably that we believe that it's doable and we've

29:25.500 --> 29:31.500
 gotten far enough into the problem that, you know, we had a, I think only a

29:31.500 --> 29:37.020
 glimpse of the true complexity of the, that the domain, you know, it's a

29:37.020 --> 29:39.260
 little bit like, you know, climbing a mountain where you kind of, you know,

29:39.260 --> 29:42.140
 see the next peak and you think that's kind of the summit, but then you get

29:42.140 --> 29:45.260
 to that and you kind of see that, that this is just the start of the journey.

29:46.140 --> 29:50.620
 But we've tried, we've sampled enough of the problem space and we've made

29:50.620 --> 29:56.540
 enough rapid success, even, you know, with technology of 2009, 2010, that

29:57.020 --> 30:02.220
 it gave us confidence to then, you know, pursue this as a real product.

30:02.940 --> 30:04.060
 So, okay.

30:04.140 --> 30:09.260
 So the next step, you mentioned the milestones that you had in the, in those

30:09.260 --> 30:13.500
 two years, what are the next milestones that then led to the creation of Waymo

30:13.500 --> 30:14.060
 and beyond?

30:14.780 --> 30:18.140
 Yeah, we had a, it was a really interesting journey and, you know, Waymo

30:18.140 --> 30:24.780
 came a little bit later, then, you know, we completed those milestones in 2010.

30:25.020 --> 30:30.300
 That was the pivot when we decided to focus on actually building a product

30:30.300 --> 30:31.420
 using this technology.

30:32.460 --> 30:37.660
 The initial couple of years after that, we were focused on a freeway, you

30:37.660 --> 30:41.180
 know, what you would call a driver assist, maybe, you know, an L3 driver

30:41.180 --> 30:42.780
 assist program.

30:42.780 --> 30:49.500
 Then around 2013, we've learned enough about the space and thought more deeply

30:49.500 --> 30:54.940
 about, you know, the product that we wanted to build, that we pivoted, we

30:54.940 --> 31:01.900
 pivoted towards this vision of building a driver and deploying it fully driverless

31:01.900 --> 31:02.940
 vehicles without a person.

31:02.940 --> 31:05.100
 And that that's the path that we've been on since then.

31:05.100 --> 31:08.540
 And very, it was exactly the right decision for us.

31:08.540 --> 31:13.580
 So there was a moment where you're also considered like, what is the right

31:13.580 --> 31:14.620
 trajectory here?

31:14.780 --> 31:18.140
 What is the right role of automation in the, in the task of driving?

31:18.140 --> 31:23.180
 There's still, it wasn't from the early days, obviously you want to go fully

31:23.180 --> 31:23.740
 autonomous.

31:24.060 --> 31:25.020
 From the early days, it was not.

31:25.100 --> 31:31.740
 I think it was in 20, around 2013, maybe that we've, that became very clear and

31:31.740 --> 31:36.860
 we made that pivot and also became very clear and that it's either the way you

31:36.860 --> 31:41.500
 go building a driver assist system is, you know, fundamentally different from

31:41.500 --> 31:43.500
 how you go building a fully driverless vehicle.

31:43.500 --> 31:48.700
 So, you know, we've pivoted towards the ladder and that's what we've been

31:48.700 --> 31:49.820
 working on ever since.

31:50.620 --> 31:57.900
 And so that was around 2013, then there's sequence of really meaningful for us

31:57.900 --> 32:00.540
 really important defining milestones since then.

32:00.540 --> 32:11.740
 And in 2015, we had our first, actually the world's first fully driverless

32:12.220 --> 32:14.780
 trade on public roads.

32:15.020 --> 32:17.500
 It was in a custom built vehicle that we had.

32:17.500 --> 32:18.380
 I must've seen those.

32:18.380 --> 32:21.100
 We called them the Firefly, that, you know, funny looking marshmallow looking

32:21.100 --> 32:21.340
 thing.

32:22.700 --> 32:30.060
 And we put a passenger, his name was Steve Mann, you know, great friend of

32:30.060 --> 32:34.300
 our project from the early days, the man happens to be blind.

32:34.540 --> 32:35.900
 So we put them in that vehicle.

32:36.060 --> 32:38.140
 The car had no steering wheel, no pedals.

32:38.140 --> 32:39.580
 It was an uncontrolled environment.

32:40.460 --> 32:44.060
 You know, no, you know, lead or chase cars, no police escorts.

32:44.540 --> 32:47.740
 And, you know, we did that trip a few times in Austin, Texas.

32:47.900 --> 32:49.500
 So that was a really big milestone.

32:49.500 --> 32:50.460
 But that was in Austin.

32:50.620 --> 32:50.860
 Yeah.

32:51.180 --> 32:51.500
 Okay.

32:52.860 --> 32:56.620
 And, you know, we only, but at that time we're only, it took a tremendous

32:56.620 --> 32:57.340
 amount of engineering.

32:57.340 --> 33:00.380
 It took a tremendous amount of validation to get to that point.

33:01.020 --> 33:03.580
 But, you know, we only did it a few times.

33:03.820 --> 33:04.540
 We only did that.

33:04.540 --> 33:05.340
 It was a fixed route.

33:05.500 --> 33:08.060
 It was not kind of a controlled environment, but it was a fixed route.

33:08.060 --> 33:09.180
 And we only did a few times.

33:10.220 --> 33:19.820
 Then in 2016, end of 2016, beginning of 2017 is when we founded Waymo, the

33:19.820 --> 33:20.220
 company.

33:20.220 --> 33:25.100
 That's when we kind of, that was the next phase of the project where I

33:25.100 --> 33:30.220
 wanted, we believed in kind of the commercial vision of this technology.

33:30.460 --> 33:33.420
 And it made sense to create an independent entity, you know, within

33:33.420 --> 33:38.300
 that alphabet umbrella to pursue this product at scale.

33:39.420 --> 33:46.540
 Beyond that in 2017, later in 2017 was another really huge step for us.

33:46.540 --> 33:52.460
 Really big milestone where we started, I think it was October of 2017 where

33:52.460 --> 33:59.340
 when we started regular driverless operations on public roads, that first

33:59.340 --> 34:02.780
 day of operations, we drove in one day.

34:02.780 --> 34:05.980
 And that first day, a hundred miles and driverless fashion.

34:05.980 --> 34:08.460
 And then we've now the most, the most important thing about that milestone

34:08.460 --> 34:11.500
 was not that, you know, a hundred miles in one day, but that it was the

34:11.500 --> 34:14.940
 start of kind of regular ongoing driverless operations.

34:14.940 --> 34:18.140
 And when you say driverless, it means no driver.

34:19.100 --> 34:19.740
 That's exactly right.

34:19.740 --> 34:24.780
 So on that first day, we actually hit a mix and in some, we didn't want

34:24.780 --> 34:27.100
 to like, you know, be on YouTube and Twitter that same day.

34:27.100 --> 34:32.460
 So in, in many of the rides we had somebody in the driver's seat, but

34:32.460 --> 34:36.860
 they could not disengage like the car, not disengage, but actually on that

34:36.860 --> 34:42.540
 first day, some of the miles were driven and just completely empty driver's seat.

34:42.540 --> 34:46.780
 And this is the key distinction that I think people don't realize it's, you

34:46.780 --> 34:53.020
 know, that oftentimes when you talk about autonomous vehicles, you're, there's

34:53.020 --> 34:59.420
 often a driver in the seat that's ready to to take over what's called a safety

34:59.420 --> 35:05.420
 driver and then Waymo is really one of the only companies at least that I'm

35:05.420 --> 35:10.860
 aware of, or at least as like boldly and carefully and all, and all of that is

35:10.940 --> 35:12.540
 actually has cases.

35:12.540 --> 35:16.780
 And now we'll talk about more and more where there's literally no driver.

35:17.100 --> 35:21.500
 So that's another, the interesting case of where the driver's not supposed

35:21.500 --> 35:24.700
 to disengage, that's like a nice middle ground, they're still there, but

35:24.700 --> 35:28.380
 they're not supposed to disengage, but really there's the case when there's

35:28.380 --> 35:34.540
 no, okay, there's something magical about there being nobody in the driver's seat.

35:34.540 --> 35:41.260
 Like, just like to me, you mentioned the first time you wrote some code for free

35:41.260 --> 35:46.700
 space navigation of the parking lot, that was like a magical moment to me, just

35:46.700 --> 35:53.900
 sort of as an observer of robots, the first magical moment is seeing an

35:53.900 --> 36:01.660
 autonomous vehicle turn, like make a left turn, like apply sufficient torque to

36:01.660 --> 36:05.740
 the steering wheel to where it, like, there's a lot of rotation and for some

36:05.740 --> 36:09.660
 reason, and there's nobody in the driver's seat, for some reason that

36:10.300 --> 36:16.060
 communicates that here's a being with power that makes a decision.

36:16.060 --> 36:19.660
 There's something about like the steering wheel, cause we perhaps romanticize

36:19.660 --> 36:24.300
 the notion of the steering wheel, it's so essential to our conception, our 20th

36:24.300 --> 36:28.380
 century conception of a car and it turning the steering wheel with nobody

36:28.380 --> 36:34.460
 in driver's seat, that to me, I think maybe to others, it's really powerful.

36:34.460 --> 36:39.100
 Like this thing is in control and then there's this leap of trust that you give.

36:39.100 --> 36:42.620
 Like I'm going to put my life in the hands of this thing that's in control.

36:42.620 --> 36:47.420
 So in that sense, when there's no, but no driver in the driver's seat, that's a

36:47.420 --> 36:49.260
 magical moment for robots.

36:49.820 --> 36:54.700
 So I'm, I've gotten a chance to last year to take a ride in a, in a

36:54.700 --> 36:58.700
 way more vehicle and that, that was the magical moment. There's like nobody in

36:58.700 --> 37:03.180
 the driver's seat. It's, it's like the little details. You would think it

37:03.180 --> 37:07.500
 doesn't matter whether there's a driver or not, but like if there's no driver

37:07.500 --> 37:12.380
 and the steering wheel is turning on its own, I don't know. That's magical.

37:13.260 --> 37:17.260
 It's absolutely magical. I, I have taken many of these rides and like completely

37:17.260 --> 37:22.220
 empty car, no human in the car pulls up, you know, you call it on your cell phone.

37:22.220 --> 37:27.740
 It pulls up, you get in, it takes you on its way. There's nobody in the car, but

37:27.740 --> 37:31.340
 you, right? That's something called, you know, fully driverless, you know, our

37:31.980 --> 37:39.900
 writer only mode of operation. Yeah. It, it is magical. It is, you know,

37:39.900 --> 37:44.780
 transformative. This is what we hear from our writers. It kind of really

37:44.780 --> 37:48.780
 changes your experience. And not like that, that really is what unlocks the

37:48.780 --> 37:53.580
 real potential of this technology. But, you know, coming back to our journey,

37:53.580 --> 37:58.780
 you know, that was 2017 when we started, you know, truly driverless operations.

37:58.780 --> 38:05.740
 Then in 2018, we've launched our public commercial service that we called

38:05.740 --> 38:13.820
 Waymo One in Phoenix. In 2019, we started offering truly driverless writer

38:13.820 --> 38:22.940
 only rides to our early rider population of users. And then, you know, 2020 has

38:22.940 --> 38:26.700
 also been a pretty interesting year. One of the first ones, less about

38:26.700 --> 38:31.180
 technology, but more about the maturing and the growth of Waymo as a company.

38:31.980 --> 38:37.500
 We raised our first round of external financing this year, you know, we were

38:37.500 --> 38:42.060
 part of Alphabet. So obviously we have access to, you know, significant resources

38:42.060 --> 38:45.900
 but as kind of on the journey of Waymo maturing as a company, it made sense

38:45.900 --> 38:50.620
 for us to, you know, partially go externally in this round. So, you know,

38:50.620 --> 38:59.740
 we're raised about $3.2 billion from that round. We've also started putting

38:59.740 --> 39:05.420
 our fifth generation of our driver, our hardware, that is on the new vehicle,

39:05.420 --> 39:10.380
 but it's also a qualitatively different set of self driving hardware.

39:10.380 --> 39:18.620
 That is now on the JLR pace. So that was a very important step for us.

39:19.340 --> 39:25.580
 Hardware specs, fifth generation. I think it'd be fun to maybe, I apologize if

39:25.580 --> 39:31.980
 I'm interrupting, but maybe talk about maybe the generations with a focus on

39:31.980 --> 39:35.660
 what we're talking about on the fifth generation in terms of hardware specs,

39:35.660 --> 39:36.700
 like what's on this car.

39:36.700 --> 39:41.580
 Sure. So we separated out, you know, the actual car that we are driving from

39:41.580 --> 39:45.820
 the self driving hardware we put on it. Right now we have, so this is, as I

39:45.820 --> 39:49.980
 mentioned, the fifth generation, you know, we've gone through, we started,

39:49.980 --> 39:54.860
 you know, building our own hardware, you know, many, many years ago. And

39:56.060 --> 40:01.020
 that, you know, Firefly vehicle also had the hardware suite that was mostly

40:01.020 --> 40:07.020
 designed, engineered, and built in house. Lighters are one of the more important

40:07.580 --> 40:11.820
 components that we design and build from the ground up. So on the fifth

40:11.820 --> 40:18.700
 generation of our drivers of our self driving hardware that we're switching

40:18.700 --> 40:24.220
 to right now, we have, as with previous generations, in terms of sensing,

40:24.220 --> 40:29.580
 we have lighters, cameras, and radars, and we have a pretty beefy computer

40:29.580 --> 40:33.420
 that processes all that information and makes decisions in real time on

40:33.420 --> 40:41.180
 board the car. So in all of the, and it's really a qualitative jump forward

40:41.180 --> 40:45.660
 in terms of the capabilities and the various parameters and the specs of

40:45.660 --> 40:49.260
 the hardware compared to what we had before and compared to what you can

40:49.260 --> 40:51.580
 kind of get off the shelf in the market today.

40:51.580 --> 40:54.700
 Meaning from fifth to fourth or from fifth to first?

40:54.700 --> 40:57.340
 Definitely from first to fifth, but also from the fourth.

40:57.340 --> 40:58.700
 That was the world's dumbest question.

40:58.700 --> 41:07.500
 Definitely from fourth to fifth, as well as the last step is a big step forward.

41:07.500 --> 41:13.900
 So everything's in house. So like LIDAR is built in house and cameras are

41:13.900 --> 41:14.540
 built in house?

41:15.740 --> 41:18.780
 You know, it's different. We work with partners and there's some components

41:19.340 --> 41:26.780
 that we get from our manufacturing and supply chain partners. What exactly

41:26.780 --> 41:34.140
 is in house is a bit different. We do a lot of custom design on all of

41:34.140 --> 41:37.580
 our sensing modalities, lighters, radars, cameras, you know, exactly.

41:37.580 --> 41:43.180
 There's lighters are almost exclusively in house and some of the

41:43.180 --> 41:45.980
 technologies that we have, some of the fundamental technologies there

41:45.980 --> 41:51.420
 are completely unique to Waymo. That is also largely true about radars

41:51.420 --> 41:55.180
 and cameras. It's a little bit more of a mix in terms of what we do

41:55.180 --> 41:57.980
 ourselves versus what we get from partners.

41:57.980 --> 42:01.580
 Is there something super sexy about the computer that you can mention

42:01.580 --> 42:08.300
 that's not top secret? Like for people who enjoy computers for, I

42:08.300 --> 42:12.700
 mean, there's a lot of machine learning involved, but there's a lot

42:12.700 --> 42:17.260
 of just basic compute. You have to probably do a lot of signal

42:17.260 --> 42:20.780
 processing on all the different sensors. You have to integrate everything

42:20.780 --> 42:23.820
 has to be in real time. There's probably some kind of redundancy

42:23.820 --> 42:27.420
 type of situation. Is there something interesting you can say about

42:27.420 --> 42:31.820
 the computer for the people who love hardware? It does have all of

42:31.820 --> 42:34.380
 the characteristics, all the properties that you just mentioned.

42:34.380 --> 42:41.020
 Redundancy, very beefy compute for general processing, as well as

42:41.020 --> 42:45.260
 inference and ML models. It is some of the more sensitive stuff that

42:45.260 --> 42:49.420
 I don't want to get into for IP reasons, but it can be shared a

42:49.420 --> 42:54.860
 little bit in terms of the specs of the sensors that we have on the

42:54.860 --> 42:58.460
 car. We actually shared some videos of what our

43:00.700 --> 43:05.100
 lighters see in the world. We have 29 cameras. We have five lighters.

43:05.100 --> 43:09.260
 We have six radars on these vehicles, and you can get a feel for

43:09.260 --> 43:12.380
 the amount of data that they're producing. That all has to be

43:12.380 --> 43:16.860
 processed in real time to do perception, to do complex

43:16.860 --> 43:19.740
 reasoning. That kind of gives you some idea of how beefy those computers

43:19.740 --> 43:22.540
 are, but I don't want to get into specifics of exactly how we build

43:22.540 --> 43:25.500
 them. Okay, well, let me try some more questions that you can get

43:25.500 --> 43:28.780
 into the specifics of, like GPU wise. Is that something you can get

43:28.780 --> 43:33.020
 into? I know that Google works with GPUs and so on. I mean, for

43:33.020 --> 43:35.980
 machine learning folks, it's kind of interesting. Or is there no...

43:38.780 --> 43:43.340
 How do I ask it? I've been talking to people in the government about

43:43.340 --> 43:46.860
 UFOs and they don't answer any questions. So this is how I feel

43:46.860 --> 43:51.980
 right now asking about GPUs. But is there something interesting that

43:51.980 --> 43:57.500
 you could reveal? Or is it just... Or leave it up to our

43:57.500 --> 44:02.060
 imagination, some of the compute. Is there any, I guess, is there any

44:02.060 --> 44:05.820
 fun trickery? Like I talked to Chris Latner for a second time and he

44:05.820 --> 44:09.580
 was a key person about GPUs, and there's a lot of fun stuff going

44:09.580 --> 44:15.580
 on in Google in terms of hardware that optimizes for machine

44:15.580 --> 44:19.420
 learning. Is there something you can reveal in terms of how much,

44:19.420 --> 44:22.380
 you mentioned customization, how much customization there is for

44:23.100 --> 44:26.220
 hardware for machine learning purposes? I'm going to be like that

44:26.220 --> 44:34.540
 government person who bought UFOs. I guess I will say that it's

44:34.540 --> 44:41.340
 really... Compute is really important. We have very data hungry

44:41.340 --> 44:45.900
 and compute hungry ML models all over our stack. And this is where

44:48.300 --> 44:52.060
 both being part of Alphabet, as well as designing our own sensors

44:52.060 --> 44:55.820
 and the entire hardware suite together, where on one hand you

44:55.820 --> 45:01.740
 get access to really rich raw sensor data that you can pipe

45:01.740 --> 45:07.820
 from your sensors into your compute platform and build like

45:07.820 --> 45:11.020
 build the whole pipe from sensor raw sensor data to the big

45:11.020 --> 45:14.060
 compute as then have the massive compute to process all that

45:14.060 --> 45:17.420
 data. And this is where we're finding that having a lot of

45:17.420 --> 45:21.260
 control of that hardware part of the stack is really

45:21.260 --> 45:24.780
 advantageous. One of the fascinating magical places to me

45:25.340 --> 45:29.980
 again, might not be able to speak to the details, but it is

45:29.980 --> 45:32.940
 the other compute, which is like, we're just talking about a

45:32.940 --> 45:39.340
 single car, but the driving experience is a source of a lot

45:39.340 --> 45:42.060
 of fascinating data. And you have a huge amount of data

45:42.060 --> 45:47.820
 coming in on the car and the infrastructure of storing some

45:47.820 --> 45:52.460
 of that data to then train or to analyze or so on. That's a

45:52.460 --> 45:58.220
 fascinating piece of it that I understand a single car. I

45:58.220 --> 46:00.940
 don't understand how you pull it all together in a nice way.

46:00.940 --> 46:03.100
 Is that something that you could speak to in terms of the

46:03.100 --> 46:08.460
 challenges of seeing the network of cars and then

46:08.460 --> 46:12.620
 bringing the data back and analyzing things that like edge

46:12.620 --> 46:15.340
 cases of driving, be able to learn on them to improve the

46:15.340 --> 46:20.060
 system to see where things went wrong, where things went right

46:20.060 --> 46:22.220
 and analyze all that kind of stuff. Is there something

46:22.220 --> 46:25.340
 interesting there from an engineering perspective?

46:25.340 --> 46:30.780
 Oh, there's an incredible amount of really interesting

46:30.780 --> 46:35.100
 work that's happening there, both in the real time operation

46:35.100 --> 46:38.220
 of the fleet of cars and the information that they exchange

46:38.220 --> 46:42.300
 with each other in real time to make better decisions as well

46:43.340 --> 46:46.700
 as on the kind of the off board component where you have to

46:46.700 --> 46:50.380
 deal with massive amounts of data for training your ML

46:50.380 --> 46:54.620
 models, evaluating the ML models for simulating the entire

46:54.620 --> 46:57.980
 system and for evaluating your entire system. And this is

46:57.980 --> 47:02.300
 where being part of Alphabet has once again been tremendously

47:03.420 --> 47:06.460
 advantageous because we consume an incredible amount of

47:06.460 --> 47:10.140
 compute for ML infrastructure. We build a lot of custom

47:10.140 --> 47:15.740
 frameworks to get good at data mining, finding the

47:16.460 --> 47:19.180
 interesting edge cases for training and for evaluation of

47:19.180 --> 47:23.900
 the system for both training and evaluating some components

47:23.900 --> 47:27.020
 and your sub parts of the system and various ML models,

47:27.020 --> 47:30.300
 as well as the evaluating the entire system and simulation.

47:31.020 --> 47:33.820
 Okay. That first piece that you mentioned that cars

47:33.820 --> 47:36.700
 communicating to each other, essentially, I mean, through

47:36.700 --> 47:40.060
 perhaps through a centralized point, but what that's

47:40.060 --> 47:43.420
 fascinating too, how much does that help you? Like if you

47:43.420 --> 47:46.940
 imagine, you know, right now the number of way more vehicles

47:46.940 --> 47:50.460
 is whatever X. I don't know if you can talk to what that

47:50.460 --> 47:54.380
 number is, but it's not in the hundreds of millions yet. And

47:55.100 --> 47:59.340
 imagine if the whole world is way more vehicles, like that

47:59.340 --> 48:03.660
 changes potentially the power of connectivity. Like the more

48:03.660 --> 48:06.300
 cars you have, I guess, actually, if you look at

48:06.300 --> 48:09.980
 Phoenix, cause there's enough vehicles, there's enough, when

48:09.980 --> 48:13.100
 there's like some level of density, you can start to

48:13.100 --> 48:15.900
 probably do some really interesting stuff with the fact

48:15.900 --> 48:21.420
 that cars can negotiate, can be, can communicate with each

48:21.420 --> 48:24.300
 other and thereby make decisions. Is there something

48:24.300 --> 48:27.820
 interesting there that you can talk to about like, how does

48:27.820 --> 48:31.100
 that help with the driving problem from, as compared to

48:31.100 --> 48:34.220
 just a single car solving the driving problem by itself?

48:35.660 --> 48:40.460
 Yeah, it's a spectrum. I first and say that, you know, it's,

48:40.460 --> 48:44.140
 it helps and it helps in various ways, but it's not required

48:44.140 --> 48:46.700
 right now with the way we build our system, like each cars can

48:46.700 --> 48:49.180
 operate independently. They can operate with no connectivity.

48:49.660 --> 48:53.180
 So I think it is important that, you know, you have a fully

48:53.740 --> 48:59.580
 autonomous, fully capable driver that, you know, computerized

48:59.580 --> 49:03.340
 driver that each car has. Then, you know, they do share

49:03.340 --> 49:06.140
 information and they share information in real time. It

49:06.140 --> 49:11.820
 really, really helps. So the way we do this today is, you know,

49:11.820 --> 49:15.180
 whenever one car encounters something interesting in the

49:15.180 --> 49:17.980
 world, whether it might be an accident or a new construction

49:17.980 --> 49:21.420
 zone, that information immediately gets, you know,

49:21.420 --> 49:23.900
 uploaded over the air and it's propagated to the rest of the

49:23.900 --> 49:26.860
 fleet. So, and that's kind of how we think about maps as

49:27.420 --> 49:32.940
 priors in terms of the knowledge of our drivers, of our fleet of

49:32.940 --> 49:38.140
 drivers that is distributed across the fleet and it's

49:38.140 --> 49:41.740
 updated in real time. So that's one use case. And

49:41.740 --> 49:46.940
 you know, you can imagine as the, you know, the density of

49:46.940 --> 49:50.060
 these vehicles go up, that they can exchange more information

49:50.060 --> 49:53.260
 in terms of what they're planning to do and start

49:53.820 --> 49:56.540
 influencing how they interact with each other, as well as,

49:56.540 --> 49:59.660
 you know, potentially sharing some observations, right, to

49:59.660 --> 50:01.820
 help with, you know, if you have enough density of these

50:01.820 --> 50:04.060
 vehicles where, you know, one car might be seeing something

50:04.060 --> 50:06.780
 that another is relevant to another car that is very

50:06.780 --> 50:08.940
 dynamic. You know, it's not part of kind of your updating

50:08.940 --> 50:11.500
 your static prior of the map of the world, but it's more of a

50:11.500 --> 50:14.220
 dynamic information that could be relevant to the decisions

50:14.220 --> 50:16.380
 that another car is making real time. So you can see them

50:16.380 --> 50:18.860
 exchanging that information and you can build on that. But

50:18.860 --> 50:23.660
 again, I see that as an advantage, but it's not a

50:23.660 --> 50:28.460
 requirement. So what about the human in the loop? So when I

50:28.460 --> 50:34.780
 got a chance to drive with a ride in a Waymo, you know,

50:34.780 --> 50:39.740
 there's customer service. So like there is somebody that's

50:39.740 --> 50:48.300
 able to dynamically like tune in and help you out. What role

50:48.300 --> 50:51.500
 does the human play in that picture? That's a fascinating

50:51.500 --> 50:53.980
 like, you know, the idea of teleoperation, be able to

50:53.980 --> 50:57.180
 remotely control a vehicle. So here, what we're talking

50:57.180 --> 51:03.900
 about is like, like frictionless, like a human being

51:03.900 --> 51:08.460
 able to in a in a frictionless way, sort of help you out. I

51:08.460 --> 51:10.780
 don't know if they're able to actually control the vehicle.

51:10.780 --> 51:14.300
 Is that something you could talk to? Yes. Okay. To be clear,

51:14.300 --> 51:16.300
 we don't do teleporation. I kind of believe in

51:16.300 --> 51:19.100
 teleporation for various reasons. That's not what we

51:19.100 --> 51:22.300
 have in our cars. We do, as you mentioned, have, you know,

51:22.300 --> 51:24.780
 version of, you know, customer support. You know, we call it

51:24.780 --> 51:28.860
 life health. In fact, we find it that it's very important for

51:28.860 --> 51:32.300
 our ride experience, especially if it's your first trip, you've

51:32.300 --> 51:35.020
 never been in a fully driverless ride or only way more

51:35.020 --> 51:37.660
 vehicle you get in, there's nobody there. And so you can

51:37.660 --> 51:40.460
 imagine having all kinds of, you know, questions in your head,

51:40.460 --> 51:43.260
 like how this thing works. So we've put a lot of thought into

51:43.260 --> 51:47.420
 kind of guiding our, our writers or customers through that

51:47.420 --> 51:49.500
 experience, especially for the first time they get some

51:49.500 --> 51:54.380
 information on the phone. If the fully driverless vehicle is

51:54.380 --> 51:58.060
 used to service their trip, when you get into the car, we

51:58.060 --> 52:01.260
 have an in car, you know, screen and audio that kind of guides

52:01.260 --> 52:05.820
 them and explains what to expect. They also have a button

52:05.820 --> 52:09.660
 that they can push that will connect them to, you know, a

52:09.660 --> 52:13.260
 real life human being that they can talk to, right, about this

52:13.260 --> 52:16.460
 whole process. So that's one aspect of it. There is, you

52:16.460 --> 52:20.460
 know, I should mention that there is another function that

52:21.100 --> 52:24.700
 humans provide to our cars, but it's not teleoperation. You can

52:24.700 --> 52:26.620
 think of it a little bit more like, you know, fleet

52:26.620 --> 52:29.980
 assistance, kind of like, you know, traffic control that you

52:29.980 --> 52:34.860
 have, where our cars, again, they're responsible on their own

52:34.860 --> 52:37.740
 for making all of the decisions, all of the driving decisions

52:37.740 --> 52:40.460
 that don't require connectivity. They, you know,

52:40.460 --> 52:44.300
 anything that is safety or latency critical is done, you

52:44.300 --> 52:49.020
 know, purely autonomously by onboard, our onboard system.

52:49.020 --> 52:51.260
 But there are situations where, you know, if connectivity is

52:51.260 --> 52:53.980
 available, when a car encounters a particularly challenging

52:53.980 --> 52:57.100
 situation, you can imagine like a super hairy scene of an

52:57.100 --> 53:00.780
 accident, the cars will do their best, they will recognize that

53:00.780 --> 53:05.660
 it's an off nominal situation, they will do their best to come

53:05.660 --> 53:07.500
 up with the right interpretation, the best course

53:07.500 --> 53:09.980
 of action in that scenario. But if connectivity is available,

53:09.980 --> 53:13.420
 they can ask for confirmation from, you know, human

53:15.660 --> 53:18.940
 assistant to kind of confirm those actions and perhaps

53:19.580 --> 53:22.140
 provide a little bit of kind of contextual information and

53:22.140 --> 53:26.380
 guidance. So October 8th was when you're talking about the

53:26.380 --> 53:33.500
 was Waymo launched the fully self, the public version of

53:33.500 --> 53:38.300
 its fully driverless, that's the right term, I think, service

53:38.300 --> 53:41.580
 in Phoenix. Is that October 8th? That's right. It was the

53:41.580 --> 53:43.820
 introduction of fully driverless, right, our only

53:43.820 --> 53:47.660
 vehicles into our public Waymo One service. Okay, so that's

53:47.660 --> 53:51.420
 that's amazing. So it's like anybody can get into Waymo in

53:51.420 --> 53:57.100
 Phoenix. So we previously had early people in our early

53:57.100 --> 54:01.100
 rider program, taking fully driverless rides in Phoenix.

54:01.100 --> 54:06.220
 And just this a little while ago, we opened on October 8th,

54:06.220 --> 54:09.500
 we opened that mode of operation to the public. So I

54:09.500 --> 54:14.300
 can download the app and go on a ride. There's a lot more

54:14.300 --> 54:17.100
 demand right now for that service. And then we have

54:17.100 --> 54:20.300
 capacity. So we're kind of managing that. But that's

54:20.300 --> 54:22.540
 exactly the way to describe it. Yeah, that's interesting. So

54:22.540 --> 54:28.700
 there's more demand than you can handle. Like what has been

54:28.700 --> 54:34.620
 reception so far? I mean, okay, so this is a product,

54:34.620 --> 54:38.140
 right? That's a whole nother discussion of like how

54:38.140 --> 54:41.420
 compelling of a product it is. Great. But it's also like one

54:41.420 --> 54:43.980
 of the most kind of transformational technologies of

54:43.980 --> 54:48.300
 the 21st century. So it's also like a tourist attraction.

54:48.300 --> 54:52.380
 Like it's fun to, you know, to be a part of it. So it'd be

54:52.380 --> 54:56.540
 interesting to see like, what do people say? What do people,

54:56.540 --> 54:59.500
 what have been the feedback so far? You know, still early

54:59.500 --> 55:04.140
 days, but so far, the feedback has been incredible, incredibly

55:04.140 --> 55:07.180
 positive. They, you know, we asked them for feedback during

55:07.180 --> 55:10.700
 the ride, we asked them for feedback after the ride as part

55:10.700 --> 55:12.780
 of their trip. We asked them some questions, we asked them

55:12.780 --> 55:17.100
 to rate the performance of our driver. Most by far, you know,

55:17.100 --> 55:21.740
 most of our drivers give us five stars in our app, which is

55:21.740 --> 55:24.700
 absolutely great to see. And you know, that's and we're

55:24.700 --> 55:26.620
 they're also giving us feedback on you know, things we can

55:26.620 --> 55:29.180
 improve. And you know, that's that's one of the main reasons

55:29.180 --> 55:31.340
 we're doing this as Phoenix and you know, over the last couple

55:31.340 --> 55:35.420
 of years, and every day today, we are just learning a

55:35.420 --> 55:38.300
 tremendous amount of new stuff from our users. There's there's

55:38.300 --> 55:41.980
 no substitute for actually doing the real thing, actually

55:41.980 --> 55:44.780
 having a fully driverless product out there in the field

55:44.780 --> 55:48.140
 with, you know, users that are actually paying us money to

55:48.140 --> 55:51.740
 get from point A to point B. So this is a legitimate like,

55:51.740 --> 55:56.140
 there's a paid service. That's right. And the idea is you use

55:56.140 --> 55:59.340
 the app to go from point A to point B. And then what what are

55:59.340 --> 56:03.260
 the A's? What are the what's the freedom of the of the starting

56:03.260 --> 56:07.900
 and ending places? It's an area of geography where that

56:07.900 --> 56:12.140
 service is enabled. It's a decent size of geography of

56:12.140 --> 56:15.420
 territory. It's actually larger than the size of San Francisco.

56:16.300 --> 56:20.220
 And you know, within that, you have full freedom of, you know,

56:20.220 --> 56:22.540
 selecting where you want to go. You know, of course, there's

56:22.540 --> 56:27.100
 some and you on your app, you get a map, you tell the car

56:27.100 --> 56:31.340
 where you want to be picked up, where you want the car to pull

56:31.340 --> 56:33.020
 over and pick you up. And then you tell it where you want to

56:33.020 --> 56:34.940
 be dropped off. All right. And of course, there are some

56:34.940 --> 56:37.740
 exclusions, right? You want to be you know, you were in terms

56:37.740 --> 56:40.860
 of where the car is allowed to pull over, right? So that you

56:40.860 --> 56:43.820
 can do. But you know, besides that, it's amazing. It's not

56:43.820 --> 56:45.900
 like a fixed just would be very I guess. I don't know. Maybe

56:45.900 --> 56:47.740
 that's what's the question behind your question. But it's

56:47.740 --> 56:51.420
 not a, you know, preset set of yes, I guess. So within the

56:51.420 --> 56:54.220
 geographic constraints with that within that area anywhere

56:54.220 --> 56:56.780
 else, it can be you can be picked up and dropped off

56:56.780 --> 56:59.740
 anywhere. That's right. And you know, people use them on like

56:59.740 --> 57:02.540
 all kinds of trips. They we have and we have an incredible

57:02.540 --> 57:05.340
 spectrum of riders. We I think the youngest actually have car

57:05.340 --> 57:07.180
 seats them and we have, you know, people taking their kids

57:07.180 --> 57:09.900
 and rides. I think the youngest riders we had on cars are, you

57:09.900 --> 57:12.220
 know, one or two years old, you know, and the full spectrum of

57:12.220 --> 57:17.020
 use cases people you can take them to, you know, schools to,

57:17.020 --> 57:21.500
 you know, go grocery shopping, to restaurants, to bars, you

57:21.500 --> 57:24.220
 know, run errands, you know, go shopping, etc, etc. You can go

57:24.220 --> 57:27.180
 to your office, right? Like the full spectrum of use cases,

57:27.180 --> 57:31.740
 and people are going to use them in their daily lives to get

57:31.740 --> 57:37.020
 around. And we see all kinds of really interesting use cases

57:37.020 --> 57:40.140
 and that that that's providing us incredibly valuable

57:40.140 --> 57:43.740
 experience that we then, you know, use to improve our

57:43.740 --> 57:50.220
 product. So as somebody who's been on done a few long rants

57:50.220 --> 57:53.740
 with Joe Rogan and others about the toxicity of the internet

57:53.740 --> 57:56.860
 and the comments and the negativity in the comments, I'm

57:56.860 --> 58:01.740
 fascinated by feedback. I believe that most people are

58:01.740 --> 58:07.420
 good and kind and intelligent and can provide, like, even in

58:07.420 --> 58:11.100
 disagreement, really fascinating ideas. So on a product

58:11.100 --> 58:14.540
 side, it's fascinating to me, like, how do you get the richest

58:14.540 --> 58:19.500
 possible user feedback, like, to improve? What's, what are the

58:19.500 --> 58:23.980
 channels that you use to measure? Because, like, you're

58:23.980 --> 58:28.540
 no longer, that's one of the magical things about autonomous

58:28.540 --> 58:32.300
 vehicles is it's not like it's frictionless interaction with

58:32.300 --> 58:35.820
 the human. So like, you don't get to, you know, it's just

58:35.820 --> 58:39.100
 giving a ride. So like, how do you get feedback from people

58:39.100 --> 58:39.980
 to in order to improve?

58:40.780 --> 58:44.940
 Yeah, great question, various mechanisms. So as part of the

58:44.940 --> 58:48.220
 normal flow, we ask people for feedback, they as the car is

58:48.220 --> 58:51.260
 driving around, we have on the phone and in the car, and we

58:51.260 --> 58:54.060
 have a touchscreen in the car, you can actually click some

58:54.060 --> 58:57.660
 buttons and provide real time feedback on how the car is

58:57.660 --> 59:00.460
 doing, and how the car is handling a particular situation,

59:00.460 --> 59:02.540
 you know, both positive and negative. So that's one

59:02.540 --> 59:05.900
 channel, we have, as we discussed, customer support or

59:05.900 --> 59:09.020
 life help, where, you know, if a customer wants to, has a

59:09.020 --> 59:13.660
 question, or he has some sort of concern, they can talk to a

59:13.660 --> 59:16.460
 person in real time. So that that is another mechanism that

59:16.460 --> 59:21.340
 gives us feedback. At the end of a trip, you know, we also ask

59:21.340 --> 59:25.100
 them how things went, they give us comments, and you know, star

59:25.100 --> 59:29.340
 rating. And you know, if it's, we also, you know, ask them to

59:30.780 --> 59:33.420
 explain what you know, one, well, and you know, what could

59:33.420 --> 59:39.420
 be improved. And we have our writers providing very rich

59:40.060 --> 59:44.300
 feedback, they're a lot, a large fraction is very passionate,

59:44.300 --> 59:45.980
 very excited about this technology. So we get really

59:45.980 --> 59:49.660
 good feedback. We also run UXR studies, right, you know,

59:49.660 --> 59:53.340
 specific and that are kind of more, you know, go more in

59:53.340 --> 59:56.220
 depth. And we will run both kind of lateral and longitudinal

59:56.220 --> 1:00:01.260
 studies, where we have deeper engagement with our customers,

1:00:01.260 --> 1:00:04.220
 you know, we have our user experience research team,

1:00:04.220 --> 1:00:07.020
 tracking over time, that's things about longitudinal is

1:00:07.020 --> 1:00:09.260
 cool. That's that's exactly right. And you know, that's

1:00:09.260 --> 1:00:12.700
 another really valuable feedback, source of feedback.

1:00:12.700 --> 1:00:15.340
 And we're just covering a tremendous amount, right?

1:00:16.380 --> 1:00:19.420
 People go grocery shopping, and they like want to load, you

1:00:19.420 --> 1:00:22.140
 know, 20 bags of groceries in our cars and like that, that's

1:00:22.140 --> 1:00:26.700
 one workflow that you maybe don't think about, you know,

1:00:26.700 --> 1:00:29.500
 getting just right when you're building the driverless

1:00:29.500 --> 1:00:34.940
 product. I have people like, you know, who bike as part of

1:00:34.940 --> 1:00:37.100
 their trip. So they, you know, bike somewhere, then they get

1:00:37.100 --> 1:00:39.660
 on our cars, they take apart their bike, they load into our

1:00:39.660 --> 1:00:42.140
 vehicle, then go and that's, you know, how they, you know,

1:00:42.140 --> 1:00:45.340
 where we want to pull over and how that, you know, get in and

1:00:45.340 --> 1:00:51.020
 get out process works, provides very useful feedback in terms

1:00:51.020 --> 1:00:55.420
 of what makes a good pickup and drop off location, we get

1:00:55.420 --> 1:01:00.780
 really valuable feedback. And in fact, we had to do some really

1:01:00.780 --> 1:01:05.180
 interesting work with high definition maps, and thinking

1:01:05.180 --> 1:01:08.700
 about walking directions. And if you imagine you're in a store,

1:01:08.700 --> 1:01:11.020
 right in some giant space, and then you know, you want to be

1:01:11.020 --> 1:01:14.380
 picked up somewhere, like if you just drop a pin at a current

1:01:14.380 --> 1:01:16.780
 location, which is maybe in the middle of a shopping mall, like

1:01:16.780 --> 1:01:20.140
 what's the best location for the car to come pick you up? And

1:01:20.140 --> 1:01:22.220
 you can have simple heuristics where you're just going to take

1:01:22.220 --> 1:01:25.500
 your you know, you clean in distance and find the nearest

1:01:25.500 --> 1:01:28.300
 spot where the car can pull over that's closest to you. But

1:01:28.300 --> 1:01:30.220
 oftentimes, that's not the most convenient one. You know, I have

1:01:30.220 --> 1:01:32.860
 many anecdotes where that heuristic breaks in horrible

1:01:32.860 --> 1:01:38.220
 ways. One example that I often mentioned is somebody wanted to

1:01:38.220 --> 1:01:44.300
 be, you know, dropped off in Phoenix. And you know, we got

1:01:44.300 --> 1:01:49.180
 car picked location that was close, the closest to there,

1:01:49.180 --> 1:01:51.820
 you know, where the pin was dropped on the map in terms of,

1:01:51.820 --> 1:01:55.180
 you know, latitude and longitude. But it happened to be

1:01:55.180 --> 1:01:58.460
 on the other side of a parking lot that had this row of

1:01:58.460 --> 1:02:01.500
 cacti. And the poor person had to like walk all around the

1:02:01.500 --> 1:02:04.300
 parking lot to get to where they wanted to be in 110 degree

1:02:04.300 --> 1:02:06.620
 heat. So that, you know, that was about so then, you know, we

1:02:06.620 --> 1:02:10.060
 took all take all of these, all that feedback from our users

1:02:10.060 --> 1:02:14.060
 and incorporate it into our system and improve it. Yeah, I

1:02:14.060 --> 1:02:17.900
 feel like that's like requires AGI to solve the problem of

1:02:17.900 --> 1:02:21.260
 like, when you're, which is a very common case, when you're in

1:02:21.260 --> 1:02:24.700
 a big space of some kind, like apartment building, it doesn't

1:02:24.700 --> 1:02:29.180
 matter, it's some large space. And then you call the, like a

1:02:29.180 --> 1:02:32.780
 Waymo from there, right? Like, whatever, it doesn't matter,

1:02:32.780 --> 1:02:37.580
 ride share vehicle. And like, where's the pin supposed to

1:02:37.580 --> 1:02:41.580
 drop? I feel like that's, you don't think, I think that

1:02:41.580 --> 1:02:45.660
 requires AGI. I'm gonna, in order to solve. Okay, the

1:02:45.660 --> 1:02:49.980
 alternative, which I think the Google search engine is taught

1:02:50.700 --> 1:02:55.420
 is like, there's something really valuable about the

1:02:55.420 --> 1:02:58.620
 perhaps slightly dumb answer, but a really powerful one,

1:02:58.620 --> 1:03:02.780
 which is like, what was done in the past by others? Like, what

1:03:02.780 --> 1:03:06.380
 was the choice made by others? That seems to be like in terms

1:03:06.380 --> 1:03:09.900
 of Google search, when you have like billions of searches, you

1:03:09.900 --> 1:03:13.820
 could, you could see which, like when they recommend what you

1:03:13.820 --> 1:03:17.660
 might possibly mean, they suggest based on not some machine

1:03:17.660 --> 1:03:20.860
 learning thing, which they also do, but like, on what was

1:03:20.860 --> 1:03:23.580
 successful for others in the past and finding a thing that

1:03:23.580 --> 1:03:27.820
 they were happy with. Is that integrated at all? Waymo, like

1:03:27.820 --> 1:03:31.740
 what, what pickups worked for others? It is. I think you're

1:03:31.740 --> 1:03:34.220
 exactly right. So there's a real, it's an interesting

1:03:34.220 --> 1:03:43.580
 problem. Naive solutions have interesting failure modes. So

1:03:43.580 --> 1:03:48.780
 there's definitely lots of things that can be done to

1:03:48.780 --> 1:03:54.940
 improve. And both learning from, you know, what works, but

1:03:54.940 --> 1:03:57.980
 doesn't work in actual heal from getting richer data and

1:03:57.980 --> 1:04:01.500
 getting more information about the environment and richer

1:04:01.500 --> 1:04:04.060
 maps. But you're absolutely right, that there's something

1:04:04.060 --> 1:04:07.580
 like there's some properties of solutions that in terms of the

1:04:07.580 --> 1:04:10.140
 effect that they have on users so much, much, much better than

1:04:10.140 --> 1:04:11.900
 others, right? And predictability and

1:04:11.900 --> 1:04:14.460
 understandability is important. So you can have maybe

1:04:14.460 --> 1:04:17.260
 something that is not quite as optimal, but is very natural

1:04:17.260 --> 1:04:21.580
 and predictable to the user and kind of works the same way all

1:04:21.580 --> 1:04:25.260
 the time. And that matters, that matters a lot for the user

1:04:25.260 --> 1:04:30.300
 experience. And but you know, to get to the basics, the pretty

1:04:30.300 --> 1:04:35.420
 fundamental property is that the car actually arrives where you

1:04:35.420 --> 1:04:37.180
 told it to, right? Like, you can always, you know, change it,

1:04:37.180 --> 1:04:39.100
 see it on the map, and you can move it around if you don't

1:04:39.100 --> 1:04:42.620
 like it. And but like, that property that the car actually

1:04:42.620 --> 1:04:47.740
 shows up reliably is critical, which, you know, where compared

1:04:47.740 --> 1:04:52.780
 to some of the human driven analogs, I think, you know, you

1:04:52.780 --> 1:04:56.460
 can have more predictability. It's actually the fact, if I

1:04:56.460 --> 1:05:00.140
 have a little bit of a detour here, I think the fact that

1:05:00.140 --> 1:05:03.100
 it's, you know, your phone and the cars, two computers talking

1:05:03.100 --> 1:05:06.140
 to each other, can lead to some really interesting things we

1:05:06.140 --> 1:05:09.740
 can do in terms of the user interfaces, both in terms of

1:05:09.740 --> 1:05:13.340
 function, like the car actually shows up exactly where you told

1:05:13.340 --> 1:05:16.140
 it, you want it to be, but also some, you know, really

1:05:16.140 --> 1:05:18.380
 interesting things on the user interface, like as the car is

1:05:18.380 --> 1:05:21.180
 driving, as you call it, and it's on the way to come pick

1:05:21.180 --> 1:05:23.580
 you up. And of course, you get the position of the car and the

1:05:23.580 --> 1:05:26.860
 route on the map. But and they actually follow that route, of

1:05:26.860 --> 1:05:29.580
 course. But it can also share some really interesting

1:05:29.580 --> 1:05:34.140
 information about what it's doing. So, you know, our cars, as

1:05:34.140 --> 1:05:36.940
 they are coming to pick you up, if it's come, if a car is

1:05:36.940 --> 1:05:39.180
 coming up to a stop sign, it will actually show you that

1:05:39.180 --> 1:05:41.340
 like, it's there sitting, because it's at a stop sign or

1:05:41.340 --> 1:05:42.860
 a traffic light will show you that it's got, you know,

1:05:42.860 --> 1:05:44.700
 sitting at a red light. So, you know, they're like little

1:05:44.700 --> 1:05:51.340
 things, right? But I find those little touches really

1:05:51.340 --> 1:05:54.620
 interesting, really magical. And it's just, you know, little

1:05:54.620 --> 1:05:57.180
 things like that, that you can do to kind of delight your

1:05:57.180 --> 1:06:02.940
 users. You know, this makes me think of, there's some products

1:06:02.940 --> 1:06:07.340
 that I just love. Like, there's a there's a company called

1:06:07.340 --> 1:06:13.500
 Rev, Rev.com, where I like for this podcast, for example, I

1:06:13.500 --> 1:06:17.900
 can drag and drop a video. And then they do all the

1:06:17.900 --> 1:06:21.340
 captioning. It's humans doing the captioning, but they

1:06:21.340 --> 1:06:24.780
 connect, they automate everything of connecting you to

1:06:24.780 --> 1:06:27.180
 the humans, and they do the captioning and transcription.

1:06:27.180 --> 1:06:29.980
 It's all effortless. And it like, I remember when I first

1:06:29.980 --> 1:06:35.500
 started using them, I was like, life's good. Like, because it

1:06:35.500 --> 1:06:39.020
 was so painful to figure that out earlier. The same thing

1:06:39.020 --> 1:06:43.260
 with something called iZotope RX, this company I use for

1:06:43.260 --> 1:06:46.380
 cleaning up audio, like the sound cleanup they do. It's

1:06:46.380 --> 1:06:49.580
 like drag and drop, and it just cleans everything up very

1:06:49.580 --> 1:06:52.940
 nicely. Another experience like that I had with Amazon

1:06:52.940 --> 1:06:57.180
 OneClick purchase, first time. I mean, other places do that

1:06:57.180 --> 1:07:00.140
 now, but just the effortlessness of purchasing,

1:07:00.140 --> 1:07:04.380
 making it frictionless. It kind of communicates to me, like,

1:07:04.380 --> 1:07:08.700
 I'm a fan of design. I'm a fan of products that you can just

1:07:08.700 --> 1:07:12.540
 create a really pleasant experience. The simplicity of

1:07:12.540 --> 1:07:16.380
 it, the elegance just makes you fall in love with it. So on

1:07:16.380 --> 1:07:19.820
 the, do you think about this kind of stuff? I mean, it's

1:07:19.820 --> 1:07:22.540
 exactly what we've been talking about. It's like the little

1:07:22.540 --> 1:07:25.500
 details that somehow make you fall in love with the product.

1:07:25.500 --> 1:07:30.860
 Is that, we went from like urban challenge days, where

1:07:30.860 --> 1:07:34.540
 love was not part of the conversation, probably. And to

1:07:34.540 --> 1:07:39.180
 this point where there's a, where there's human beings and

1:07:39.180 --> 1:07:42.780
 you want them to fall in love with the experience. Is that

1:07:42.780 --> 1:07:45.020
 something you're trying to optimize for? Try to think

1:07:45.020 --> 1:07:47.820
 about, like, how do you create an experience that people love?

1:07:48.700 --> 1:07:55.100
 Absolutely. I think that's the vision is removing any friction

1:07:55.100 --> 1:08:02.300
 or complexity from getting our users, our writers to where

1:08:02.300 --> 1:08:06.780
 they want to go. Making that as simple as possible. And then,

1:08:06.780 --> 1:08:10.620
 you know, beyond that, just transportation, making things

1:08:10.620 --> 1:08:13.580
 and goods get to their destination as seamlessly as

1:08:13.580 --> 1:08:17.020
 possible. I talked about a drag and drop experience where I

1:08:17.020 --> 1:08:20.460
 kind of express your intent and then it just magically happens.

1:08:20.460 --> 1:08:23.100
 And for our writers, that's what we're trying to get to is

1:08:23.100 --> 1:08:28.380
 you download an app and you click and car shows up. It's

1:08:28.380 --> 1:08:33.580
 the same car. It's very predictable. It's a safe and

1:08:33.580 --> 1:08:37.500
 high quality experience. And then it gets you in a very

1:08:37.500 --> 1:08:43.900
 reliable, very convenient, frictionless way to where you

1:08:43.900 --> 1:08:47.900
 want to be. And along the journey, I think we also want to

1:08:47.900 --> 1:08:52.940
 do little things to delight our users. Like the ride sharing

1:08:52.940 --> 1:08:56.620
 companies, because they don't control the experience, I

1:08:56.620 --> 1:09:00.300
 think they can't make people fall in love necessarily with

1:09:00.300 --> 1:09:04.140
 the experience. Or maybe they, they haven't put in the effort,

1:09:04.140 --> 1:09:08.060
 but I think if I were to speak to the ride sharing experience

1:09:08.060 --> 1:09:11.340
 I currently have, it's just very, it's just very

1:09:11.340 --> 1:09:16.540
 convenient, but there's a lot of room for like falling in love

1:09:16.540 --> 1:09:20.140
 with it. Like we can speak to sort of car companies, car

1:09:20.140 --> 1:09:22.380
 companies do this. Well, you can fall in love with a car,

1:09:22.380 --> 1:09:26.620
 right? And be like a loyal car person, like whatever. Like I

1:09:26.620 --> 1:09:31.260
 like badass hot rods, I guess, 69 Corvette. And at this point,

1:09:31.260 --> 1:09:35.580
 you know, you can't really, cars are so, owning a car is so

1:09:35.580 --> 1:09:41.020
 20th century, man. But is there something about the Waymo

1:09:41.020 --> 1:09:43.660
 experience where you hope that people will fall in love with

1:09:43.660 --> 1:09:48.780
 it? Is that part of it? Or is it part of, is it just about

1:09:48.780 --> 1:09:52.380
 making a convenient ride, not ride sharing, I don't know what

1:09:52.380 --> 1:09:56.060
 the right term is, but just a convenient A to B autonomous

1:09:56.060 --> 1:10:02.460
 transport or like, do you want them to fall in love with

1:10:02.460 --> 1:10:06.140
 Waymo? To maybe elaborate a little bit. I mean, almost like

1:10:06.140 --> 1:10:11.820
 from a business perspective, I'm curious, like how, do you

1:10:11.820 --> 1:10:15.260
 want to be in the background invisible or do you want to be

1:10:15.260 --> 1:10:20.060
 like a source of joy that's in very much in the foreground? I

1:10:20.060 --> 1:10:24.540
 want to provide the best, most enjoyable transportation

1:10:24.540 --> 1:10:31.260
 solution. And that means building it, building our

1:10:31.260 --> 1:10:34.300
 product and building our service in a way that people do.

1:10:34.300 --> 1:10:41.900
 Kind of use in a very seamless, frictionless way in their

1:10:41.900 --> 1:10:45.580
 day to day lives. And I think that does mean, you know, in

1:10:45.580 --> 1:10:48.300
 some way falling in love in that product, right, just kind of

1:10:48.300 --> 1:10:54.700
 becomes part of your routine. It comes down my mind to safety,

1:10:54.700 --> 1:11:02.060
 predictability of the experience, and privacy aspects

1:11:02.060 --> 1:11:07.100
 of it, right? Our cars, you get the same car, you get very

1:11:07.100 --> 1:11:11.340
 predictable behavior. And you get a lot of different

1:11:11.340 --> 1:11:14.940
 things. And that is important. And if you're going to use it

1:11:14.940 --> 1:11:18.700
 in your daily life, privacy, and when you're in a car, you

1:11:18.700 --> 1:11:21.020
 can do other things. You're spending a bunch, just another

1:11:21.020 --> 1:11:24.380
 space where you're spending a significant part of your life.

1:11:24.380 --> 1:11:27.820
 And so not having to share it with other people who you don't

1:11:27.820 --> 1:11:32.380
 want to share it with, I think is a very nice property. Maybe

1:11:32.380 --> 1:11:34.540
 you want to take a phone call or do something else in the

1:11:34.540 --> 1:11:40.620
 vehicle. And, you know, safety on the quality of the driving,

1:11:40.620 --> 1:11:45.660
 as well as the physical safety of not having to share that

1:11:45.660 --> 1:11:52.300
 ride is important to a lot of people. What about the idea

1:11:52.300 --> 1:11:56.940
 that when there's somebody like a human driving, and they do

1:11:56.940 --> 1:12:01.180
 a rolling stop on a stop sign, like sometimes like, you know,

1:12:01.180 --> 1:12:04.220
 you get an Uber or Lyft or whatever, like human driver,

1:12:04.220 --> 1:12:07.980
 and, you know, they can be a little bit aggressive as

1:12:07.980 --> 1:12:14.540
 drivers. It feels like there's not all aggression is bad. Now

1:12:14.540 --> 1:12:17.500
 that may be a wrong, again, 20th century conception of

1:12:17.500 --> 1:12:21.100
 driving. Maybe it's possible to create a driving experience.

1:12:21.100 --> 1:12:24.940
 Like if you're in the back, busy doing something, maybe

1:12:24.940 --> 1:12:27.740
 aggression is not a good thing. It's a very different kind of

1:12:27.740 --> 1:12:32.540
 experience perhaps. But it feels like in order to navigate

1:12:32.540 --> 1:12:38.540
 this world, you need to, how do I phrase this? You need to kind

1:12:38.540 --> 1:12:42.140
 of bend the rules a little bit, or at least test the rules. I

1:12:42.140 --> 1:12:44.540
 don't know what language politicians use to discuss this,

1:12:44.540 --> 1:12:48.700
 but whatever language they use, you like flirt with the rules.

1:12:48.700 --> 1:12:55.580
 I don't know. But like you sort of have a bit of an aggressive

1:12:55.580 --> 1:13:00.460
 way of driving that asserts your presence in this world,

1:13:00.460 --> 1:13:03.660
 thereby making other vehicles and people respect your

1:13:03.660 --> 1:13:06.780
 presence and thereby allowing you to sort of navigate

1:13:06.780 --> 1:13:10.060
 through intersections in a timely fashion. I don't know if

1:13:10.060 --> 1:13:14.300
 any of that made sense, but like, how does that fit into the

1:13:14.300 --> 1:13:18.620
 experience of driving autonomously? Is that?

1:13:18.620 --> 1:13:20.460
 It's a lot of thoughts. This is you're hitting on a very

1:13:20.460 --> 1:13:27.500
 important point of a number of behavioral components and, you

1:13:27.500 --> 1:13:34.380
 know, parameters that make your driving feel assertive and

1:13:34.380 --> 1:13:37.260
 natural and comfortable and predictable. Our cars will

1:13:37.260 --> 1:13:39.740
 follow rules, right? They will do the safest thing possible in

1:13:39.740 --> 1:13:43.580
 all situations. Let me be clear on that. But if you think of

1:13:43.580 --> 1:13:47.660
 really, really good drivers, just think about

1:13:47.660 --> 1:13:49.740
 professional lemon drivers, right? They will follow the

1:13:49.740 --> 1:13:53.900
 rules. They're very, very smooth, and yet they're very

1:13:53.900 --> 1:13:58.140
 efficient. But they're assertive. They're comfortable

1:13:58.140 --> 1:14:02.140
 for the people in the vehicle. They're predictable for the

1:14:02.140 --> 1:14:03.820
 other people outside the vehicle that they share the

1:14:03.820 --> 1:14:06.540
 environment with. And that's the kind of driver that we want

1:14:06.540 --> 1:14:11.100
 to build. And you think if maybe there's a sport analogy

1:14:11.100 --> 1:14:17.740
 there, right? You can do in very many sports, the true

1:14:17.740 --> 1:14:20.620
 professionals are very efficient in their movements,

1:14:20.620 --> 1:14:25.100
 right? They don't do like, you know, hectic flailing, right?

1:14:25.100 --> 1:14:29.020
 They're, you know, smooth and precise, right? And they get

1:14:29.020 --> 1:14:30.860
 the best results. So that's the kind of driver that we want to

1:14:30.860 --> 1:14:33.100
 build. In terms of, you know, aggressiveness. Yeah, you can

1:14:33.100 --> 1:14:35.740
 like, you know, roll through the stop signs. You can do crazy

1:14:35.740 --> 1:14:38.060
 lane changes. It typically doesn't get you to your

1:14:38.060 --> 1:14:40.700
 destination faster. Typically not the safest or most

1:14:40.700 --> 1:14:45.820
 predictable, very most comfortable thing to do. But

1:14:45.820 --> 1:14:49.660
 there is a way to do both. And that's what we're

1:14:49.660 --> 1:14:53.820
 doing. We're trying to build the driver that is safe,

1:14:53.820 --> 1:14:58.140
 comfortable, smooth, and predictable. Yeah, that's a

1:14:58.140 --> 1:15:00.380
 really interesting distinction. I think in the early days of

1:15:00.380 --> 1:15:03.660
 autonomous vehicles, the vehicles felt cautious as

1:15:03.660 --> 1:15:08.620
 opposed to efficient. And I'm still probably, but when I

1:15:08.620 --> 1:15:13.500
 rode in the Waymo, I mean, there was, it was, it was quite

1:15:13.500 --> 1:15:19.740
 assertive. It moved pretty quickly. Like, yeah, then he's

1:15:19.740 --> 1:15:22.940
 one of the surprising feelings was that it actually, it went

1:15:22.940 --> 1:15:28.300
 fast. And it didn't feel like, awkwardly cautious than

1:15:28.300 --> 1:15:31.900
 autonomous vehicle. Like, like, so I've also programmed

1:15:31.900 --> 1:15:34.860
 autonomous vehicles and everything I've ever built was

1:15:34.860 --> 1:15:39.260
 felt awkwardly, either overly aggressive. Okay, especially

1:15:39.260 --> 1:15:44.860
 when it was my code, or like, awkwardly cautious is the way

1:15:44.860 --> 1:15:53.180
 I would put it. And Waymo's vehicle felt like, assertive

1:15:53.180 --> 1:15:57.180
 and I think efficient is like the right terminology here.

1:15:57.180 --> 1:16:01.340
 It wasn't, and I also like the professional limo driver,

1:16:01.340 --> 1:16:06.060
 because we often think like, you know, an Uber driver or a

1:16:06.060 --> 1:16:09.820
 bus driver or a taxi. This is the funny thing is people

1:16:09.820 --> 1:16:14.940
 think they track taxi drivers are professionals. They, I

1:16:14.940 --> 1:16:18.460
 mean, it's, it's like, that that's like saying, I'm a

1:16:18.460 --> 1:16:20.780
 professional walker, just because I've been walking all

1:16:20.780 --> 1:16:25.580
 my life. I think there's an art to it, right? And if you take

1:16:25.580 --> 1:16:30.700
 it seriously as an art form, then there's a certain way that

1:16:30.700 --> 1:16:33.900
 mastery looks like. It's interesting to think about what

1:16:33.900 --> 1:16:39.180
 does mastery look like in driving? And perhaps what we

1:16:39.180 --> 1:16:43.020
 associate with like aggressiveness is unnecessary,

1:16:43.020 --> 1:16:46.940
 like, it's not part of the experience of driving. It's

1:16:46.940 --> 1:16:54.860
 like, unnecessary fluff, that efficiency, you can be,

1:16:54.860 --> 1:17:00.380
 you can create a good driving experience within the rules.

1:17:00.380 --> 1:17:03.100
 That's, I mean, you're the first person to tell me this.

1:17:03.100 --> 1:17:04.940
 So it's, it's kind of interesting. I need to think

1:17:04.940 --> 1:17:07.900
 about this, but that's exactly what it felt like with Waymo.

1:17:07.900 --> 1:17:10.060
 I kind of had this intuition. Maybe it's the Russian thing.

1:17:10.060 --> 1:17:13.740
 I don't know that you have to break the rules in life to get

1:17:13.740 --> 1:17:19.020
 anywhere, but maybe, maybe it's possible that that's not the

1:17:19.020 --> 1:17:23.500
 case in driving. I have to think about that, but it

1:17:23.500 --> 1:17:25.980
 certainly felt that way on the streets of Phoenix when I was

1:17:25.980 --> 1:17:29.340
 there in Waymo, that, that, that that was a very pleasant

1:17:29.340 --> 1:17:32.460
 experience and it wasn't frustrating in that like, come

1:17:32.460 --> 1:17:35.260
 on, move already kind of feeling. It wasn't, that wasn't

1:17:35.260 --> 1:17:37.900
 there. Yeah. I mean, that's what, that's what we're going

1:17:37.900 --> 1:17:41.420
 after. I don't think you have to pick one. I think truly good

1:17:41.420 --> 1:17:45.020
 driving. It gives you both efficiency, a certainness, but

1:17:45.020 --> 1:17:49.900
 also comfort and predictability and safety. And, you know, it's,

1:17:49.900 --> 1:17:54.940
 that's what fundamental improvements in the core

1:17:54.940 --> 1:17:59.260
 capabilities truly unlock. And you can kind of think of it as,

1:17:59.260 --> 1:18:01.980
 you know, a precision and recall trade off. You have certain

1:18:01.980 --> 1:18:04.460
 capabilities of your model. And then it's very easy when, you

1:18:04.460 --> 1:18:06.540
 know, you have some curve of precision and recall, you can

1:18:06.540 --> 1:18:08.540
 move things around and can choose your operating point and

1:18:08.540 --> 1:18:10.700
 your training of precision versus recall, false positives

1:18:10.700 --> 1:18:14.220
 versus false negatives. Right. But then, and you know, you can

1:18:14.220 --> 1:18:16.940
 tune things on that curve and be kind of more cautious or more

1:18:16.940 --> 1:18:19.340
 aggressive, but then aggressive is bad or, you know, cautious is

1:18:19.340 --> 1:18:22.540
 bad, but true capabilities come from actually moving the whole

1:18:22.540 --> 1:18:28.540
 curve up. And then you are kind of on a very different plane of

1:18:28.540 --> 1:18:31.340
 those trade offs. And that, that's what we're trying to do

1:18:31.340 --> 1:18:34.700
 here is to move the whole curve up. Before I forget, let's talk

1:18:34.700 --> 1:18:39.420
 about trucks a little bit. So I also got a chance to check out

1:18:39.420 --> 1:18:44.300
 some of the Waymo trucks. I'm not sure if we want to go too

1:18:44.300 --> 1:18:47.180
 much into that space, but it's a fascinating one. So maybe we

1:18:47.180 --> 1:18:51.020
 can mention at least briefly, you know, Waymo is also now

1:18:51.020 --> 1:18:56.540
 doing autonomous trucking and how different like

1:18:56.540 --> 1:18:58.780
 philosophically and technically is that whole space of

1:18:58.780 --> 1:19:06.060
 problems. It's one of our two big products and you know,

1:19:06.060 --> 1:19:09.020
 commercial applications of our driver, right? Right. Hailing

1:19:09.020 --> 1:19:12.700
 and deliveries. You know, we have Waymo One and Waymo Via

1:19:12.700 --> 1:19:16.220
 moving people and moving goods. You know, trucking is an

1:19:16.220 --> 1:19:21.580
 example of moving goods. We've been working on trucking since

1:19:21.580 --> 1:19:31.340
 2017. It is a very interesting space. And your question of

1:19:31.340 --> 1:19:35.020
 how different is it? It has this really nice property that

1:19:35.020 --> 1:19:38.780
 the first order challenges, like the science, the hard

1:19:38.780 --> 1:19:42.140
 engineering, whether it's, you know, hardware or, you know,

1:19:42.140 --> 1:19:45.420
 onboard software or off board software, all of the, you know,

1:19:45.420 --> 1:19:48.780
 systems that you build for, you know, training your ML models

1:19:48.780 --> 1:19:51.820
 for, you know, evaluating your time system. Like those

1:19:51.820 --> 1:19:56.460
 fundamentals carry over. Like the true challenges of, you

1:19:56.460 --> 1:20:00.620
 know, driving perception, semantic understanding,

1:20:00.620 --> 1:20:04.860
 prediction, decision making, planning, evaluation, the

1:20:04.860 --> 1:20:08.780
 simulator, ML infrastructure, those carry over. Like the data

1:20:08.780 --> 1:20:12.380
 and the application and kind of the domains might be

1:20:12.380 --> 1:20:16.060
 different, but the most difficult problems, all of that

1:20:16.060 --> 1:20:19.420
 carries over between the domains. So that's very nice.

1:20:19.420 --> 1:20:22.300
 So that's how we approach it. We're kind of build investing

1:20:22.300 --> 1:20:26.220
 in the core, the technical core. And then there's

1:20:26.220 --> 1:20:30.620
 specialization of that core technology to different

1:20:30.620 --> 1:20:34.540
 product lines, to different commercial applications. So on

1:20:34.540 --> 1:20:38.140
 just to tease it apart a little bit on trucks. So starting with

1:20:38.140 --> 1:20:42.140
 the hardware, the configuration of the sensors is different.

1:20:42.140 --> 1:20:46.300
 They're different physically, geometrically, you know, different

1:20:46.300 --> 1:20:50.860
 vehicles. So for example, we have two of our main laser on

1:20:50.860 --> 1:20:54.380
 the trucks on both sides so that we have, you know, not have the

1:20:54.380 --> 1:20:59.100
 blind spots. Whereas on the JLR eye pace, we have, you know, one

1:20:59.100 --> 1:21:02.940
 of it sitting at the very top, but the actual sensors are

1:21:02.940 --> 1:21:06.700
 almost the same. Now we're largely the same. So all of the

1:21:06.700 --> 1:21:11.180
 investment that over the years we've put into building our

1:21:11.180 --> 1:21:13.580
 custom lighters, custom radars, pulling the whole system

1:21:13.580 --> 1:21:16.540
 together, that carries over very nicely. Then, you know, on the

1:21:16.540 --> 1:21:20.780
 perception side, the like the fundamental challenges of

1:21:20.780 --> 1:21:22.940
 seeing, understanding the world, whether it's, you know, object

1:21:22.940 --> 1:21:25.740
 detection, classification, you know, tracking, semantic

1:21:25.740 --> 1:21:28.300
 understanding, all that carries over. You know, yes, there's

1:21:28.300 --> 1:21:31.100
 some specialization when you're driving on freeways, you know,

1:21:31.100 --> 1:21:33.820
 range becomes more important. The domain is a little bit

1:21:33.820 --> 1:21:36.860
 different. But again, the fundamentals carry over very,

1:21:36.860 --> 1:21:41.100
 very nicely. Same, and you guess you get into prediction or

1:21:41.100 --> 1:21:45.260
 decision making, right, the fundamentals of what it takes to

1:21:45.260 --> 1:21:49.580
 predict what other people are going to do to find the long

1:21:49.580 --> 1:21:53.420
 tail to improve your system in that long tail of behavior

1:21:53.420 --> 1:21:56.060
 prediction and response that carries over right and so on and

1:21:56.060 --> 1:22:00.060
 so on. So I mean, that's pretty exciting. By the way, does

1:22:00.060 --> 1:22:05.100
 Waymo via include using the smaller vehicles for

1:22:05.100 --> 1:22:07.580
 transportation of goods? That's an interesting distinction. So

1:22:07.580 --> 1:22:12.220
 I would say there's three interesting modes of operation.

1:22:13.020 --> 1:22:16.860
 So one is moving humans, one is moving goods, and one is like

1:22:16.860 --> 1:22:21.020
 moving nothing, zero occupancy, meaning like you're going to

1:22:21.740 --> 1:22:27.580
 the destination, your empty vehicle. I mean, it's the third

1:22:27.580 --> 1:22:29.820
 is the less of it. If that's the entirety of it, it's the less,

1:22:29.820 --> 1:22:31.660
 you know, exciting from the commercial perspective.

1:22:31.660 --> 1:22:38.140
 Well, I mean, in terms of like, if you think about what's

1:22:38.140 --> 1:22:42.060
 inside a vehicle as it's moving, because it does, you

1:22:42.060 --> 1:22:45.580
 know, some significant fraction of the vehicle's movement has

1:22:45.580 --> 1:22:50.700
 to be empty. I mean, it's kind of fascinating. Maybe just on

1:22:50.700 --> 1:22:57.340
 that small point, is there different control and like

1:22:57.340 --> 1:23:01.500
 policies that are applied for zero occupancy vehicle? So

1:23:01.500 --> 1:23:04.940
 vehicle with nothing in it, or is it just move as if there is

1:23:04.940 --> 1:23:08.780
 a person inside? What was with some subtle differences?

1:23:09.500 --> 1:23:13.100
 As a first order approximation, there are no differences. And

1:23:13.100 --> 1:23:17.740
 if you think about, you know, safety and comfort and quality

1:23:17.740 --> 1:23:26.540
 of driving, only part of it has to do with the people or the

1:23:26.540 --> 1:23:29.340
 goods inside of the vehicle. But you don't want to be, you

1:23:29.340 --> 1:23:31.820
 know, you want to drive smoothly, as we discussed, not

1:23:31.820 --> 1:23:34.780
 for the purely for the benefit of whatever you have inside the

1:23:34.780 --> 1:23:38.540
 car, right? It's also for the benefit of the people outside

1:23:38.540 --> 1:23:41.660
 kind of fitting naturally and predictably into that whole

1:23:41.660 --> 1:23:43.820
 environment, right? So, you know, yes, there are some

1:23:43.820 --> 1:23:47.180
 second order things you can do, you can change your route, and

1:23:47.180 --> 1:23:50.860
 you optimize maybe kind of your fleet, things at the fleet

1:23:50.860 --> 1:23:54.300
 scale. And you would take into account whether some of your

1:23:54.300 --> 1:23:58.780
 you know, some of your cars are actually, you know, serving a

1:23:58.780 --> 1:24:01.180
 useful trip, whether with people or with goods, whereas, you

1:24:01.180 --> 1:24:05.180
 know, other cars are, you know, driving completely empty to that

1:24:05.180 --> 1:24:09.500
 next valuable trip that they're going to provide. But that those

1:24:09.500 --> 1:24:13.260
 are mostly second order effects. Okay, cool. So Phoenix

1:24:14.380 --> 1:24:18.780
 is, is an incredible place. And what you've announced in

1:24:18.780 --> 1:24:23.340
 Phoenix is, it's kind of amazing. But, you know, that's

1:24:23.340 --> 1:24:30.220
 just like one city. How do you take over the world? I mean,

1:24:30.220 --> 1:24:33.420
 I'm asking for a friend. One step at a time.

1:24:35.980 --> 1:24:40.460
 Is that a cartoon pinky in the brain? Yeah. Okay. But, you

1:24:40.460 --> 1:24:44.540
 know, gradually is a true answer. So I think the heart of

1:24:44.540 --> 1:24:48.860
 your question is, can you ask a better question than I asked?

1:24:48.860 --> 1:24:52.940
 You're asking a great question. Answer that one. I'm just

1:24:52.940 --> 1:24:56.300
 gonna, you know, phrase it in the terms that I want to

1:24:56.300 --> 1:25:01.660
 answer. Exactly right. Brilliant. Please. You know,

1:25:01.660 --> 1:25:04.940
 where are we today? And, you know, what happens next? And

1:25:04.940 --> 1:25:08.220
 what does it take to go beyond Phoenix? And what does it

1:25:08.220 --> 1:25:13.660
 take to get this technology to more places and more people

1:25:13.660 --> 1:25:23.100
 around the world, right? So our next big area of focus is

1:25:23.100 --> 1:25:26.700
 exactly that. Larger scale commercialization and just,

1:25:26.700 --> 1:25:35.340
 you know, scaling up. If I think about, you know, the

1:25:35.340 --> 1:25:39.100
 main, and, you know, Phoenix gives us that platform and

1:25:39.100 --> 1:25:44.940
 gives us that foundation of upon which we can build. And

1:25:44.940 --> 1:25:51.580
 it's, there are few really challenging aspects of this

1:25:51.580 --> 1:25:56.460
 whole problem that you have to pull together in order to build

1:25:56.460 --> 1:26:03.900
 the technology in order to deploy it into the field to go

1:26:03.900 --> 1:26:09.820
 from a driverless car to a fleet of cars that are providing a

1:26:09.820 --> 1:26:14.140
 service, and then all the way to commercialization. So, and

1:26:14.140 --> 1:26:15.980
 then, you know, this is what we have in Phoenix. We've taken

1:26:15.980 --> 1:26:21.100
 the technology from a proof point to an actual deployment

1:26:21.100 --> 1:26:25.980
 and have taken our driver from, you know, one car to a fleet

1:26:25.980 --> 1:26:29.980
 that can provide a service. Beyond that, if I think about

1:26:29.980 --> 1:26:35.820
 what it will take to scale up and, you know, deploy in, you

1:26:35.820 --> 1:26:41.740
 know, more places with more customers, I tend to think about

1:26:41.740 --> 1:26:48.380
 three main dimensions, three main axes of scale. One is the

1:26:48.380 --> 1:26:51.660
 core technology, you know, the hardware and software core

1:26:51.660 --> 1:26:56.540
 capabilities of our driver. The second dimension is

1:26:56.540 --> 1:27:01.900
 evaluation and deployment. And the third one is the, you know,

1:27:01.900 --> 1:27:06.060
 product, commercial, and operational excellence. So you

1:27:06.060 --> 1:27:09.660
 can talk a bit about where we are along, you know, each one of

1:27:09.660 --> 1:27:11.900
 those three dimensions about where we are today and, you

1:27:11.900 --> 1:27:16.780
 know, what has, what will happen next. On, you know, the core

1:27:16.780 --> 1:27:19.580
 technology, you know, the hardware and software, you

1:27:19.580 --> 1:27:25.420
 know, together comprise a driver, we, you know, obviously

1:27:25.420 --> 1:27:30.460
 have that foundation that is providing fully driverless

1:27:30.460 --> 1:27:34.780
 trips to our customers as we speak, in fact. And we've

1:27:34.780 --> 1:27:39.500
 learned a tremendous amount from that. So now what we're

1:27:39.500 --> 1:27:44.380
 doing is we are incorporating all those lessons into some

1:27:44.380 --> 1:27:47.180
 pretty fundamental improvements in our core technology, both on

1:27:47.180 --> 1:27:51.660
 the hardware side and on the software side to build a more

1:27:51.660 --> 1:27:54.860
 general, more robust solution that then will enable us to

1:27:54.860 --> 1:28:00.460
 massively scale beyond Phoenix. So on the hardware side, all of

1:28:00.460 --> 1:28:05.180
 those lessons are now incorporated into this fifth

1:28:05.180 --> 1:28:09.500
 generation hardware platform that is, you know, being

1:28:09.500 --> 1:28:13.180
 deployed right now. And that's the platform, the fourth

1:28:13.180 --> 1:28:14.860
 generation, the thing that we have right now driving in

1:28:14.860 --> 1:28:18.700
 Phoenix, it's good enough to operate fully driverlessly,

1:28:18.700 --> 1:28:21.500
 you know, night and day, you know, various speeds and

1:28:21.500 --> 1:28:25.020
 various conditions, but the fifth generation is the platform

1:28:25.020 --> 1:28:30.140
 upon which we want to go to massive scale. We, in turn,

1:28:30.140 --> 1:28:32.620
 we've really made qualitative improvements in terms of the

1:28:32.620 --> 1:28:35.980
 capability of the system, the simplicity of the architecture,

1:28:35.980 --> 1:28:39.900
 the reliability of the redundancy. It is designed to be

1:28:39.900 --> 1:28:42.300
 manufacturable at very large scale and, you know, provides

1:28:42.300 --> 1:28:46.380
 the right unit economics. So that's the next big step for us

1:28:46.380 --> 1:28:49.580
 on the hardware side. That's already there for scale,

1:28:49.580 --> 1:28:53.500
 the version five. That's right. And is that coincidence or

1:28:53.500 --> 1:28:55.580
 should we look into a conspiracy theory that it's the

1:28:55.580 --> 1:28:59.660
 same version as the pixel phone? Is that what's the

1:28:59.660 --> 1:29:04.220
 hardware? They neither confirm nor deny. All right, cool. So,

1:29:04.220 --> 1:29:08.140
 sorry. So that's the, okay, that's that axis. What else?

1:29:08.140 --> 1:29:11.100
 So similarly, you know, hardware is a very discreet

1:29:11.100 --> 1:29:14.940
 jump, but, you know, similar to how we're making that change

1:29:14.940 --> 1:29:16.940
 from the fourth generation hardware to the fifth, we're

1:29:16.940 --> 1:29:19.420
 making similar improvements on the software side to make it

1:29:19.420 --> 1:29:22.300
 more, you know, robust and more general and allow us to kind of

1:29:22.300 --> 1:29:25.740
 quickly scale beyond Phoenix. So that's the first dimension of

1:29:25.740 --> 1:29:27.980
 core technology. The second dimension is evaluation and

1:29:27.980 --> 1:29:34.300
 deployment. How do you measure your system? How do you

1:29:34.300 --> 1:29:37.500
 evaluate it? How do you build a release and deployment process

1:29:37.500 --> 1:29:40.780
 where, you know, with confidence, you can, you know,

1:29:40.780 --> 1:29:45.420
 regularly release new versions of your driver into a fleet?

1:29:45.420 --> 1:29:49.180
 How do you get good at it so that it is not, you know, a

1:29:49.180 --> 1:29:52.540
 huge tax on your researchers and engineers that, you know, so

1:29:52.540 --> 1:29:55.740
 you can, how do you build all these, you know, processes, the

1:29:55.740 --> 1:29:58.620
 frameworks, the simulation, the evaluation, the data science,

1:29:58.620 --> 1:30:01.340
 the validation, so that, you know, people can focus on

1:30:01.340 --> 1:30:04.380
 improving the system and kind of the releases just go out the

1:30:04.380 --> 1:30:07.340
 door and get deployed across the fleet. So we've gotten really

1:30:07.340 --> 1:30:11.820
 good at that in Phoenix. That's been a tremendously difficult

1:30:11.820 --> 1:30:15.180
 problem, but that's what we have in Phoenix right now that gives

1:30:15.180 --> 1:30:17.660
 us that foundation. And now we're working on kind of

1:30:17.660 --> 1:30:20.220
 incorporating all the lessons that we've learned to make it

1:30:20.220 --> 1:30:22.860
 more efficient, to go to new places, you know, and scale up

1:30:22.860 --> 1:30:25.660
 and just kind of, you know, stamp things out. So that's that

1:30:25.660 --> 1:30:28.700
 second dimension of evaluation and deployment. And the third

1:30:28.700 --> 1:30:33.340
 dimension is product, commercial, and operational

1:30:33.340 --> 1:30:38.140
 excellence, right? And again, Phoenix there is providing an

1:30:38.140 --> 1:30:40.940
 incredibly valuable platform. You know, that's why we're doing

1:30:40.940 --> 1:30:43.660
 things end to end in Phoenix. We're learning, as you know, we

1:30:43.660 --> 1:30:47.900
 discussed a little earlier today, tremendous amount of

1:30:47.900 --> 1:30:50.460
 really valuable lessons from our users getting really

1:30:50.460 --> 1:30:54.860
 incredible feedback. And we'll continue to iterate on that and

1:30:54.860 --> 1:30:59.420
 incorporate all those lessons into making our product, you

1:30:59.420 --> 1:31:01.660
 know, even better and more convenient for our users.

1:31:01.660 --> 1:31:06.620
 So you're converting this whole process in Phoenix into

1:31:06.620 --> 1:31:11.260
 something that could be copy and pasted elsewhere. So like,

1:31:11.260 --> 1:31:13.180
 perhaps you didn't think of it that way when you were doing

1:31:13.180 --> 1:31:17.660
 the experimentation in Phoenix, but so how long did you

1:31:17.660 --> 1:31:22.140
 basically, and you can correct me, but you've, I mean, it's

1:31:22.140 --> 1:31:24.700
 still early days, but you've taken the full journey in

1:31:24.700 --> 1:31:29.180
 Phoenix, right? As you were saying of like what it takes to

1:31:29.180 --> 1:31:31.900
 basically automate. I mean, it's not the entirety of Phoenix,

1:31:31.900 --> 1:31:36.300
 right? But I imagine it can encompass the entirety of

1:31:36.300 --> 1:31:41.340
 Phoenix. That's some near term date, but that's not even

1:31:41.340 --> 1:31:43.740
 perhaps important. Like as long as it's a large enough

1:31:43.740 --> 1:31:51.580
 geographic area. So what, how copy pastable is that process

1:31:51.580 --> 1:31:58.300
 currently and how like, you know, like when you copy and

1:31:58.300 --> 1:32:05.260
 paste in Google docs, I think now in, or in word, you can

1:32:05.260 --> 1:32:09.340
 like apply source formatting or apply destination formatting.

1:32:09.340 --> 1:32:14.620
 So how, when you copy and paste the Phoenix into like, say

1:32:14.620 --> 1:32:20.060
 Boston, how do you apply the destination formatting? Like

1:32:20.060 --> 1:32:25.980
 how much of the core of the entire process of bringing an

1:32:25.980 --> 1:32:30.460
 actual public transportation, autonomous transportation

1:32:30.460 --> 1:32:35.340
 service to a city is there in Phoenix that you understand

1:32:35.340 --> 1:32:39.660
 enough to copy and paste into Boston or wherever? So we're

1:32:39.660 --> 1:32:41.980
 not quite there yet. We're not at a point where we're kind of

1:32:41.980 --> 1:32:47.100
 massively copy and pasting all over the place. But Phoenix,

1:32:47.100 --> 1:32:50.940
 what we did in Phoenix, and we very intentionally have chosen

1:32:50.940 --> 1:32:56.620
 Phoenix as our first full deployment area, you know,

1:32:56.620 --> 1:32:59.580
 exactly for that reason to kind of tease the problem apart,

1:32:59.580 --> 1:33:03.180
 look at each dimension and focus on the fundamentals of

1:33:03.180 --> 1:33:06.460
 complexity and de risking those dimensions, and then bringing

1:33:06.460 --> 1:33:09.340
 the entire thing together to get all the way and force

1:33:09.340 --> 1:33:12.460
 ourselves to learn all those hard lessons on technology,

1:33:12.460 --> 1:33:15.740
 hardware and software, on the evaluation deployment, on

1:33:15.740 --> 1:33:20.060
 operating a service, operating a business using actually

1:33:20.060 --> 1:33:22.860
 serving our customers all the way so that we're fully

1:33:22.860 --> 1:33:27.580
 informed about the most difficult, most important

1:33:27.580 --> 1:33:31.180
 challenges to get us to that next step of massive copy and

1:33:31.180 --> 1:33:38.860
 pasting as you said. And that's what we're doing right now.

1:33:38.860 --> 1:33:41.740
 We're incorporating all those things that we learned into

1:33:41.740 --> 1:33:44.860
 that next system that then will allow us to kind of copy and

1:33:44.860 --> 1:33:47.500
 paste all over the place and to massively scale to, you know,

1:33:47.500 --> 1:33:50.300
 more users and more locations. I mean, you know, just talk a

1:33:50.300 --> 1:33:52.380
 little bit about, you know, what does that mean along those

1:33:52.380 --> 1:33:55.020
 different dimensions? So on the hardware side, for example,

1:33:55.020 --> 1:33:57.980
 again, it's that switch from the fourth to the fifth

1:33:57.980 --> 1:34:00.380
 generation. And the fifth generation is designed to kind

1:34:00.380 --> 1:34:04.540
 of have that property. Can you say what other cities you're

1:34:04.540 --> 1:34:09.020
 thinking about? Like, I'm thinking about, sorry, we're in

1:34:09.020 --> 1:34:12.380
 San Francisco now. I thought I want to move to San Francisco,

1:34:12.380 --> 1:34:16.540
 but I'm thinking about moving to Austin. I don't know why

1:34:16.540 --> 1:34:19.580
 people are not being very nice about San Francisco currently,

1:34:19.580 --> 1:34:23.340
 but maybe it's a small, maybe it's in vogue right now.

1:34:23.340 --> 1:34:28.060
 But Austin seems, I visited there and it was, I was in a

1:34:28.060 --> 1:34:32.860
 Walmart. It's funny, these moments like turn your life.

1:34:32.860 --> 1:34:38.860
 There's this very nice woman with kind eyes, just like stopped

1:34:38.860 --> 1:34:44.460
 and said, he looks so handsome in that tie, honey, to me. This

1:34:44.460 --> 1:34:47.260
 has never happened to me in my life, but just the sweetness of

1:34:47.260 --> 1:34:49.980
 this woman is something I've never experienced, certainly on

1:34:49.980 --> 1:34:53.100
 the streets of Boston, but even in San Francisco where people

1:34:53.100 --> 1:34:57.100
 wouldn't, that's just not how they speak or think. I don't

1:34:57.100 --> 1:35:00.700
 know. There's a warmth to Austin that love. And since

1:35:00.700 --> 1:35:04.060
 Waymo does have a little bit of a history there, is that a

1:35:04.060 --> 1:35:07.980
 possibility? Is this your version of asking the question

1:35:07.980 --> 1:35:09.980
 of like, you know, Dimitri, I know you can't share your

1:35:09.980 --> 1:35:12.780
 commercial and deployment roadmap, but I'm thinking about

1:35:12.780 --> 1:35:16.300
 moving to San Francisco, Austin, like, you know, blink twice if

1:35:16.300 --> 1:35:19.900
 you think I should move to it. That's true. That's true. You

1:35:19.900 --> 1:35:23.900
 got me. You know, we've been testing all over the place. I

1:35:23.900 --> 1:35:26.860
 think we've been testing more than 25 cities. We drive

1:35:26.860 --> 1:35:31.740
 in San Francisco. We drive in, you know, Michigan for snow.

1:35:31.740 --> 1:35:34.220
 We are doing significant amount of testing in the Bay Area,

1:35:34.220 --> 1:35:37.340
 including San Francisco, which is not like, because we're

1:35:37.340 --> 1:35:40.060
 talking about the very different thing, which is like a

1:35:40.060 --> 1:35:46.380
 full on large geographic area, public service. You can't share

1:35:46.380 --> 1:35:54.140
 and you, okay. What about Moscow? When is that happening?

1:35:54.140 --> 1:35:58.700
 Take on Yandex. I'm not paying attention to those folks.

1:35:58.700 --> 1:36:02.380
 They're doing, you know, there's a lot of fun. I mean,

1:36:02.380 --> 1:36:10.540
 maybe as a way of a question, you didn't speak to sort of like

1:36:10.540 --> 1:36:15.020
 policy or like, is there tricky things with government and so

1:36:15.020 --> 1:36:20.860
 on? Like, is there other friction that you've

1:36:20.860 --> 1:36:25.260
 encountered except sort of technological friction of

1:36:25.260 --> 1:36:28.540
 solving this very difficult problem? Is there other stuff

1:36:28.540 --> 1:36:33.340
 that you have to overcome when deploying a public service in

1:36:33.340 --> 1:36:38.860
 a city? That's interesting. It's very important. So we

1:36:38.860 --> 1:36:44.540
 put significant effort in creating those partnerships and

1:36:44.540 --> 1:36:48.380
 you know, those relationships with governments at all levels,

1:36:48.380 --> 1:36:50.860
 local governments, municipalities, state level,

1:36:50.860 --> 1:36:53.900
 federal level. We've been engaged in very deep

1:36:53.900 --> 1:36:57.020
 conversations from the earliest days of our projects.

1:36:57.020 --> 1:37:01.020
 Whenever at all of these levels, whenever we go

1:37:01.020 --> 1:37:07.500
 to test or operate in a new area, we always lead

1:37:07.500 --> 1:37:10.860
 with a conversation with the local officials.

1:37:10.860 --> 1:37:13.740
 But the result of that investment is that no,

1:37:13.740 --> 1:37:16.780
 it's not challenges we have to overcome, but it is very

1:37:16.780 --> 1:37:19.980
 important that we continue to have this conversation.

1:37:19.980 --> 1:37:27.340
 Oh, yeah. I love politicians too. Okay, so Mr. Elon Musk said that

1:37:27.340 --> 1:37:32.300
 LiDAR is a crutch. What are your thoughts?

1:37:32.940 --> 1:37:36.540
 I wouldn't characterize it exactly that way. I know I think LiDAR is

1:37:36.540 --> 1:37:42.540
 very important. It is a key sensor that we use just like

1:37:42.540 --> 1:37:46.700
 other modalities, right? As we discussed, our cars use cameras, LiDAR

1:37:46.700 --> 1:37:52.700
 and radars. They are all very important. They are

1:37:52.700 --> 1:37:57.900
 at the kind of the physical level. They are very different. They have very

1:37:57.900 --> 1:38:00.300
 different, you know, physical characteristics.

1:38:00.300 --> 1:38:03.100
 Cameras are passive. LiDARs and radars are active.

1:38:03.100 --> 1:38:07.420
 Use different wavelengths. So that means they complement each other

1:38:07.420 --> 1:38:14.700
 very nicely and together combined, they can be used to

1:38:14.700 --> 1:38:20.620
 build a much safer and much more capable system.

1:38:20.620 --> 1:38:25.020
 So, you know, to me it's more of a question,

1:38:25.020 --> 1:38:28.700
 you know, why the heck would you handicap yourself and not use one

1:38:28.700 --> 1:38:32.860
 or more of those sensing modalities when they, you know, undoubtedly just make your

1:38:32.860 --> 1:38:39.100
 system more capable and safer. Now,

1:38:39.100 --> 1:38:45.180
 it, you know, what might make sense for one product or

1:38:45.180 --> 1:38:48.380
 one business might not make sense for another one.

1:38:48.380 --> 1:38:51.980
 So if you're talking about driver assist technologies, you make certain design

1:38:51.980 --> 1:38:55.260
 decisions and you make certain trade offs and make different ones if you are

1:38:55.260 --> 1:38:59.820
 building a driver that you deploy in fully driverless

1:38:59.820 --> 1:39:04.940
 vehicles. And, you know, in LiDAR specifically, when this question comes up,

1:39:04.940 --> 1:39:11.820
 I, you know, typically the criticisms that I hear or, you know, the

1:39:11.820 --> 1:39:16.060
 counterpoints is that cost and aesthetics.

1:39:16.060 --> 1:39:20.460
 And I don't find either of those, honestly, very compelling.

1:39:20.460 --> 1:39:24.380
 So on the cost side, there's nothing fundamentally prohibitive

1:39:24.380 --> 1:39:28.620
 about, you know, the cost of LiDARs. You know, radars used to be very expensive

1:39:28.620 --> 1:39:32.140
 before people started, you know, before people made certain advances in

1:39:32.140 --> 1:39:35.980
 technology and, you know, started to manufacture them at massive scale and

1:39:35.980 --> 1:39:39.740
 deploy them in vehicles, right? You know, similar with LiDARs. And this is

1:39:39.740 --> 1:39:43.260
 where the LiDARs that we have on our cars, especially the fifth generation,

1:39:43.260 --> 1:39:48.220
 you know, we've been able to make some pretty qualitative discontinuous

1:39:48.220 --> 1:39:51.580
 jumps in terms of the fundamental technology that allow us to

1:39:51.580 --> 1:39:56.380
 manufacture those things at very significant scale and at a fraction

1:39:56.380 --> 1:40:00.300
 of the cost of both our previous generation

1:40:00.300 --> 1:40:03.980
 as well as a fraction of the cost of, you know, what might be available

1:40:03.980 --> 1:40:07.100
 on the market, you know, off the shelf right now. And, you know, that improvement

1:40:07.100 --> 1:40:10.700
 will continue. So I think, you know, cost is not a

1:40:10.700 --> 1:40:14.300
 real issue. Second one is, you know, aesthetics.

1:40:14.300 --> 1:40:18.060
 You know, I don't think that's, you know, a real issue either.

1:40:18.060 --> 1:40:22.860
 Beauty is in the eye of the beholder. Yeah. You can make LiDAR sexy again.

1:40:22.860 --> 1:40:25.740
 I think you're exactly right. I think it is sexy. Like, honestly, I think form

1:40:25.740 --> 1:40:30.060
 all of function. Well, okay. You know, I was actually, somebody brought this up to

1:40:30.060 --> 1:40:34.940
 me. I mean, all forms of LiDAR, even

1:40:34.940 --> 1:40:37.580
 like the ones that are like big, you can make

1:40:37.580 --> 1:40:40.700
 look, I mean, you can make look beautiful.

1:40:40.700 --> 1:40:44.060
 There's no sense in which you can't integrate it into design.

1:40:44.060 --> 1:40:47.820
 Like, there's all kinds of awesome designs. I don't think

1:40:47.820 --> 1:40:51.260
 small and humble is beautiful. It could be

1:40:51.260 --> 1:40:55.580
 like, you know, brutalism or like, it could be

1:40:55.580 --> 1:40:59.340
 like harsh corners. I mean, like I said, like hot rods. Like, I don't like, I don't

1:40:59.340 --> 1:41:02.700
 necessarily like, like, oh man, I'm going to start so much

1:41:02.700 --> 1:41:07.420
 controversy with this. I don't like Porsches. Okay.

1:41:07.420 --> 1:41:10.700
 The Porsche 911, like everyone says it's the most beautiful.

1:41:10.700 --> 1:41:15.340
 No, no. It's like, it's like a baby car. It doesn't make any sense.

1:41:15.340 --> 1:41:18.940
 But everyone, it's beauty is in the eye of the beholder. You're already looking at

1:41:18.940 --> 1:41:24.060
 me like, what is this kid talking about? I'm happy to talk about. You're digging your

1:41:24.060 --> 1:41:27.980
 own hole. The form and function and my take on the

1:41:27.980 --> 1:41:30.940
 beauty of the hardware that we put on our vehicles,

1:41:30.940 --> 1:41:34.700
 you know, I will not comment on your Porsche monologue.

1:41:34.700 --> 1:41:39.340
 Okay. All right. So, but aesthetics, fine. But there's an underlying, like,

1:41:39.340 --> 1:41:43.900
 philosophical question behind the kind of lighter question is

1:41:43.900 --> 1:41:48.060
 like, how much of the problem can be solved

1:41:48.060 --> 1:41:51.660
 with computer vision, with machine learning?

1:41:51.660 --> 1:41:58.460
 So I think without sort of disagreements and so on,

1:41:58.460 --> 1:42:03.340
 it's nice to put it on the spectrum because Waymo is doing a lot of machine

1:42:03.340 --> 1:42:06.460
 learning as well. It's interesting to think how much of

1:42:06.460 --> 1:42:11.260
 driving, if we look at five years, 10 years, 50 years down the road,

1:42:11.260 --> 1:42:15.340
 what can be learned in almost more and more and more

1:42:15.340 --> 1:42:19.820
 end to end way. If we look at what Tesla is doing

1:42:19.820 --> 1:42:24.300
 with, as a machine learning problem, they're doing a multitask learning

1:42:24.300 --> 1:42:27.820
 thing where it's just, they break up driving into a bunch of learning tasks

1:42:27.820 --> 1:42:30.540
 and they have one single neural network and they're just collecting huge amounts

1:42:30.540 --> 1:42:33.340
 of data that's training that. I've recently hung out with George

1:42:33.340 --> 1:42:36.940
 Hotz. I don't know if you know George.

1:42:37.820 --> 1:42:41.820
 I love him so much. He's just an entertaining human being.

1:42:41.820 --> 1:42:45.340
 We were off mic talking about Hunter S. Thompson. He's the Hunter S. Thompson

1:42:45.340 --> 1:42:49.420
 of autonomous driving. Okay. So he, I didn't realize this with comma

1:42:49.420 --> 1:42:53.180
 AI, but they're like really trying to end to end.

1:42:53.180 --> 1:42:58.460
 They're the machine, like looking at the machine learning problem, they're

1:42:58.460 --> 1:43:01.580
 really not doing multitask learning, but it's

1:43:01.580 --> 1:43:05.980
 computing the drivable area as a machine learning task

1:43:05.980 --> 1:43:11.500
 and hoping that like down the line, this level two system, this driver

1:43:11.500 --> 1:43:15.340
 assistance will eventually lead to

1:43:15.340 --> 1:43:19.260
 allowing you to have a fully autonomous vehicle. Okay. There's an underlying

1:43:19.260 --> 1:43:22.540
 deep philosophical question there, technical question

1:43:22.540 --> 1:43:29.420
 of how much of driving can be learned. So LiDAR is an effective tool today

1:43:29.420 --> 1:43:33.820
 for actually deploying a successful service in Phoenix, right? That's safe,

1:43:33.820 --> 1:43:39.260
 that's reliable, et cetera, et cetera. But the question,

1:43:39.260 --> 1:43:43.100
 and I'm not saying you can't do machine learning on LiDAR, but the question is

1:43:43.100 --> 1:43:47.340
 that like how much of driving can be learned eventually.

1:43:47.340 --> 1:43:49.980
 Can we do fully autonomous? That's learned.

1:43:49.980 --> 1:43:53.340
 Yeah. You know, learning is all over the place

1:43:53.340 --> 1:43:56.620
 and plays a key role in every part of our system.

1:43:56.620 --> 1:44:01.180
 As you said, I would, you know, decouple the sensing modalities

1:44:01.180 --> 1:44:05.180
 from the, you know, ML and the software parts of it.

1:44:05.180 --> 1:44:09.740
 LiDAR, radar, cameras, like it's all machine learning.

1:44:09.740 --> 1:44:12.220
 All of the object detection classification, of course, like that's

1:44:12.220 --> 1:44:15.100
 what, you know, these modern deep nets and count nets are very

1:44:15.100 --> 1:44:19.820
 good at. You feed them raw data, massive amounts of raw data,

1:44:19.820 --> 1:44:23.900
 and that's actually what our custom build LiDARs and radars are really good

1:44:23.900 --> 1:44:25.500
 at. And radars, they don't just give you point

1:44:25.500 --> 1:44:28.060
 estimates of, you know, objects in space, they give you raw,

1:44:28.060 --> 1:44:31.660
 like, physical observations. And then you take all of that raw information,

1:44:31.660 --> 1:44:34.780
 you know, there's colors of the pixels, whether it's, you know, LiDARs returns

1:44:34.780 --> 1:44:36.780
 and some auxiliary information. It's not just distance,

1:44:36.780 --> 1:44:39.500
 right? And, you know, angle and distance is much richer information that you get

1:44:39.500 --> 1:44:42.460
 from those returns, plus really rich information from the

1:44:42.460 --> 1:44:45.820
 radars. You fuse it all together and you feed it into those massive

1:44:45.820 --> 1:44:51.340
 ML models that then, you know, lead to the best results in terms of, you

1:44:51.340 --> 1:44:55.820
 know, object detection, classification, state estimation.

1:44:55.820 --> 1:44:59.020
 So there's a side to interop, but there is a fusion. I mean, that's something

1:44:59.020 --> 1:45:01.020
 that people didn't do for a very long time,

1:45:01.020 --> 1:45:04.540
 which is like at the sensor fusion level, I guess,

1:45:04.540 --> 1:45:07.660
 like early on fusing the information together, whether

1:45:07.660 --> 1:45:11.980
 so that the the sensory information that the vehicle receives from the different

1:45:11.980 --> 1:45:15.180
 modalities or even from different cameras is

1:45:15.180 --> 1:45:19.020
 combined before it is fed into the machine learning models.

1:45:19.020 --> 1:45:21.660
 Yeah, so I think this is one of the trends you're seeing more of that you

1:45:21.660 --> 1:45:24.780
 mentioned end to end. There's different interpretation of end to end. There is

1:45:24.780 --> 1:45:27.980
 kind of the purest interpretation of I'm going to

1:45:27.980 --> 1:45:32.300
 like have one model that goes from raw sensor data to like,

1:45:32.300 --> 1:45:35.100
 you know, steering torque and, you know, gas breaks. That, you know,

1:45:35.100 --> 1:45:37.500
 that's too much. I don't think that's the right way to do it.

1:45:37.500 --> 1:45:40.620
 There's more, you know, smaller versions of end to end

1:45:40.620 --> 1:45:45.500
 where you're kind of doing more end to end learning or core training or

1:45:45.500 --> 1:45:48.700
 depropagation of kind of signals back and forth across

1:45:48.700 --> 1:45:51.900
 the different stages of your system. There's, you know, really good ways it

1:45:51.900 --> 1:45:55.180
 gets into some fairly complex design choices where on one

1:45:55.180 --> 1:45:57.980
 hand you want modularity and decomposability,

1:45:57.980 --> 1:46:01.580
 decomposability of your system. But on the other hand,

1:46:01.580 --> 1:46:05.100
 you don't want to create interfaces that are too narrow or too brittle

1:46:05.100 --> 1:46:08.380
 to engineered where you're giving up on the generality of the solution or you're

1:46:08.380 --> 1:46:12.940
 unable to properly propagate signal, you know, reach signal forward and losses

1:46:12.940 --> 1:46:17.500
 and, you know, back so you can optimize the whole system jointly.

1:46:17.500 --> 1:46:21.180
 So I would decouple and I guess what you're seeing in terms of the fusion

1:46:21.180 --> 1:46:25.580
 of the sensing data from different modalities as well as kind of fusion

1:46:25.580 --> 1:46:30.060
 at in the temporal level going more from, you know, frame by frame

1:46:30.060 --> 1:46:32.780
 where, you know, you would have one net that would do frame by frame detection

1:46:32.780 --> 1:46:35.500
 and camera and then, you know, something that does frame by frame and

1:46:35.500 --> 1:46:39.260
 lighter and then radar and then you fuse it, you know, in a weaker engineered way

1:46:39.260 --> 1:46:41.260
 later. Like the field over the last, you know,

1:46:41.260 --> 1:46:45.260
 decade has been evolving in more kind of joint fusion, more end to end models that

1:46:45.260 --> 1:46:48.060
 are, you know, solving some of these tasks, you know, jointly and there's

1:46:48.060 --> 1:46:50.860
 tremendous power in that and, you know, that's the

1:46:50.860 --> 1:46:54.700
 progression that kind of our technology, our stack has been on as well.

1:46:54.700 --> 1:46:57.980
 Now to your, you know, that so I would decouple the kind of sensing and how

1:46:57.980 --> 1:47:01.340
 that information is fused from the role of ML and the entire stack.

1:47:01.340 --> 1:47:06.460
 And, you know, I guess it's, there's trade offs and, you know, modularity and

1:47:06.460 --> 1:47:11.260
 how do you inject inductive bias into your system?

1:47:11.260 --> 1:47:15.180
 All right, this is, there's tremendous power

1:47:15.180 --> 1:47:19.660
 in being able to do that. So, you know, we have, there's no

1:47:19.660 --> 1:47:25.180
 part of our system that is not heavily, that does not heavily, you know, leverage

1:47:25.180 --> 1:47:29.580
 data driven development or state of the art ML.

1:47:29.580 --> 1:47:33.580
 But there's mapping, there's a simulator, there's perception, you know, object

1:47:33.580 --> 1:47:34.940
 level, you know, perception, whether it's

1:47:34.940 --> 1:47:38.220
 semantic understanding, prediction, decision making, you know, so forth and

1:47:38.220 --> 1:47:40.540
 so on.

1:47:42.060 --> 1:47:45.100
 It's, you know, of course, object detection and classification, like you're

1:47:45.100 --> 1:47:48.460
 finding pedestrians and cars and cyclists and, you know, cones and signs

1:47:48.460 --> 1:47:51.740
 and vegetation and being very good at estimating

1:47:51.740 --> 1:47:54.460
 kind of detection, classification, and state estimation. There's just stable

1:47:54.460 --> 1:47:57.900
 stakes, like that's step zero of this whole stack. You can be

1:47:57.900 --> 1:48:00.700
 incredibly good at that, whether you use cameras or light as a

1:48:00.700 --> 1:48:03.660
 radar, but that's just, you know, that's stable stakes, that's just step zero.

1:48:03.660 --> 1:48:06.380
 Beyond that, you get into the really interesting challenges of semantic

1:48:06.380 --> 1:48:10.140
 understanding at the perception level, you get into scene level reasoning, you

1:48:10.140 --> 1:48:13.900
 get into very deep problems that have to do with prediction and joint

1:48:13.900 --> 1:48:16.140
 prediction and interaction, so the interaction

1:48:16.140 --> 1:48:19.260
 between all the actors in the environment, pedestrians, cyclists, other

1:48:19.260 --> 1:48:22.300
 cars, and you get into decision making, right? So, how do you build a lot of

1:48:22.300 --> 1:48:26.300
 systems? So, we leverage ML very heavily in all of

1:48:26.300 --> 1:48:30.140
 these components. I do believe that the best results you

1:48:30.140 --> 1:48:33.340
 achieve by kind of using a hybrid approach and

1:48:33.340 --> 1:48:38.140
 having different types of ML, having

1:48:38.140 --> 1:48:41.580
 different models with different degrees of inductive bias

1:48:41.580 --> 1:48:45.260
 that you can have, and combining kind of model,

1:48:45.260 --> 1:48:49.180
 you know, free approaches with some model based approaches and some

1:48:49.180 --> 1:48:54.380
 rule based, physics based systems. So, you know, one example I can give

1:48:54.380 --> 1:48:58.940
 you is traffic lights. There's a problem of the detection of

1:48:58.940 --> 1:49:02.700
 traffic light state, and obviously that's a great problem for, you know, computer

1:49:02.700 --> 1:49:05.260
 vision confidence, or, you know, that's their bread and

1:49:05.260 --> 1:49:08.220
 butter, right? That's how you build that. But then the

1:49:08.220 --> 1:49:11.740
 interpretation of, you know, of a traffic light, that you're

1:49:11.740 --> 1:49:15.820
 gonna need to learn that, right? You don't need to build some,

1:49:15.820 --> 1:49:18.940
 you know, complex ML model that, you know, infers

1:49:18.940 --> 1:49:22.540
 with some, you know, precision and recall that red means stop.

1:49:22.540 --> 1:49:25.500
 Like, it's a very clear engineered signal

1:49:25.500 --> 1:49:29.500
 with very clear semantics, right? So you want to induce that bias, like how you

1:49:29.500 --> 1:49:31.740
 induce that bias, and that whether, you know, it's a

1:49:31.740 --> 1:49:36.460
 constraint or a cost, you know, function in your stack, but like

1:49:36.460 --> 1:49:40.860
 it is important to be able to inject that, like, clear semantic

1:49:40.860 --> 1:49:44.220
 signal into your stack. And, you know, that's what we do.

1:49:44.220 --> 1:49:47.340
 And, but then the question of, like, and that's when you

1:49:47.340 --> 1:49:50.860
 apply it to yourself, when you are making decisions whether you want to stop

1:49:50.860 --> 1:49:54.540
 for a red light, you know, or not.

1:49:54.540 --> 1:49:57.820
 But if you think about how other people treat traffic lights,

1:49:57.820 --> 1:50:01.260
 we're back to the ML version of that. You know they're supposed to stop

1:50:01.260 --> 1:50:02.860
 for a red light, but that doesn't mean they will.

1:50:02.860 --> 1:50:07.820
 So then you're back in the, like, very heavy

1:50:07.820 --> 1:50:11.420
 ML domain where you're picking up on, like, very subtle cues about,

1:50:11.420 --> 1:50:15.260
 you know, they have to do with the behavior of objects, pedestrians, cyclists,

1:50:15.260 --> 1:50:19.420
 cars, and the whole, you know, entire configuration of the scene

1:50:19.420 --> 1:50:22.220
 that allow you to make accurate predictions on whether they will, in

1:50:22.220 --> 1:50:27.020
 fact, stop or run a red light. So it sounds like already for Waymo,

1:50:27.020 --> 1:50:29.820
 like, machine learning is a huge part of the stack.

1:50:29.820 --> 1:50:36.300
 So it's a huge part of, like, not just, so obviously the first, the level

1:50:36.300 --> 1:50:38.860
 zero, or whatever you said, which is, like,

1:50:38.860 --> 1:50:42.380
 just the object detection of things that, you know, with no other machine learning

1:50:42.380 --> 1:50:46.380
 can do, but also starting to do prediction behavior and so on to

1:50:46.380 --> 1:50:49.660
 model the, what other, what the other parties in the

1:50:49.660 --> 1:50:51.580
 scene, entities in the scene are going to do.

1:50:51.580 --> 1:50:55.260
 So machine learning is more and more playing a role in that

1:50:55.260 --> 1:50:59.020
 as well. Of course. Oh, absolutely. I think we've been

1:50:59.020 --> 1:51:02.060
 going back to the, you know, earliest days, like, you know, DARPA,

1:51:02.060 --> 1:51:05.820
 the DARPA Grand Challenge, our team was leveraging, you know, machine

1:51:05.820 --> 1:51:08.540
 learning. It was, like, pre, you know, ImageNet, and it was a very

1:51:08.540 --> 1:51:11.660
 different type of ML, but, and I think actually it was before

1:51:11.660 --> 1:51:15.340
 my time, but the Stanford team during the Grand Challenge had a very

1:51:15.340 --> 1:51:18.940
 interesting machine learned system that would, you know, use

1:51:18.940 --> 1:51:21.340
 LiDAR and camera. We've been driving in the

1:51:21.340 --> 1:51:26.940
 desert, and it, we had built the model where it would kind of

1:51:26.940 --> 1:51:29.900
 extend the range of free space reasoning. We get a

1:51:29.900 --> 1:51:33.020
 clear signal from LiDAR, and then it had a model that said, hey, like,

1:51:33.020 --> 1:51:35.900
 this stuff on camera kind of sort of looks like this stuff in LiDAR, and I

1:51:35.900 --> 1:51:38.860
 know this stuff that I'm seeing in LiDAR, I'm very confident it's free space,

1:51:38.860 --> 1:51:43.420
 so let me extend that free space zone into the camera range that would allow

1:51:43.420 --> 1:51:45.980
 the vehicle to drive faster. And then we've been building on top of

1:51:45.980 --> 1:51:48.860
 that and kind of staying and pushing the state of the art in ML,

1:51:48.860 --> 1:51:52.620
 in all kinds of different ML over the years. And in fact,

1:51:52.620 --> 1:51:56.940
 from the early days, I think, you know, 2010 is probably the year

1:51:56.940 --> 1:52:03.500
 where Google, maybe 2011 probably, got pretty heavily involved in

1:52:03.500 --> 1:52:07.660
 machine learning, kind of deep nuts, and at that time it was probably the only

1:52:07.660 --> 1:52:11.980
 company that was very heavily investing in kind of state of the art ML and

1:52:11.980 --> 1:52:16.220
 self driving cars. And they go hand in hand.

1:52:16.220 --> 1:52:19.980
 And we've been on that journey ever since. We're doing, pushing

1:52:19.980 --> 1:52:24.060
 a lot of these areas in terms of research at Waymo, and we

1:52:24.060 --> 1:52:26.620
 collaborate very heavily with the researchers in

1:52:26.620 --> 1:52:30.060
 Alphabet, and all kinds of ML, supervised ML,

1:52:30.060 --> 1:52:34.380
 unsupervised ML, published some

1:52:34.380 --> 1:52:37.900
 interesting research papers in the space,

1:52:37.900 --> 1:52:41.180
 especially recently. It's just a super active learning as well.

1:52:41.180 --> 1:52:45.260
 Yeah, so super, super active. Of course, there's, you know, kind of the more

1:52:45.260 --> 1:52:48.940
 mature stuff, like, you know, ConvNets for, you know, object detection.

1:52:48.940 --> 1:52:52.860
 But there's some really interesting, really active work that's happening

1:52:52.860 --> 1:52:58.300
 in kind of more, you know, in bigger models and, you know,

1:52:58.300 --> 1:53:02.540
 models that have more structure to them,

1:53:02.540 --> 1:53:06.860
 you know, not just, you know, large bitmaps and reason about temporal sequences.

1:53:06.860 --> 1:53:10.700
 And some of the interesting breakthroughs that you've, you know, we've seen

1:53:10.700 --> 1:53:14.140
 in language models, right? You know, transformers,

1:53:14.140 --> 1:53:19.100
 you know, GPT3 inference. There's some really interesting applications of some

1:53:19.100 --> 1:53:21.260
 of the core breakthroughs to those problems

1:53:21.260 --> 1:53:24.540
 of, you know, behavior prediction, as well as, you know, decision making and

1:53:24.540 --> 1:53:27.900
 planning, right? You can think about it, kind of the the behavior,

1:53:27.900 --> 1:53:31.500
 how, you know, the path, the trajectories, the how people drive.

1:53:31.500 --> 1:53:34.620
 They have kind of a share, a lot of the fundamental structure,

1:53:34.620 --> 1:53:38.220
 you know, this problem. There's, you know, sequential,

1:53:38.220 --> 1:53:41.900
 you know, nature. There's a lot of structure in this representation.

1:53:41.900 --> 1:53:45.900
 There is a strong locality, kind of like in sentences, you know, words that follow

1:53:45.900 --> 1:53:48.140
 each other. They're strongly connected, but there's

1:53:48.140 --> 1:53:51.580
 also kind of larger context that doesn't have that locality, and you also see that

1:53:51.580 --> 1:53:53.740
 in driving, right? What, you know, is happening in the scene

1:53:53.740 --> 1:53:57.020
 as a whole has very strong implications on,

1:53:57.020 --> 1:54:00.940
 you know, the kind of the next step in that sequence where

1:54:00.940 --> 1:54:03.980
 whether you're, you know, predicting what other people are going to do, whether

1:54:03.980 --> 1:54:07.020
 you're making your own decisions, or whether in the simulator you're

1:54:07.020 --> 1:54:10.620
 building generative models of, you know, humans walking, cyclists

1:54:10.620 --> 1:54:14.220
 riding, and other cars driving. That's all really fascinating, like how

1:54:14.220 --> 1:54:17.340
 it's fascinating to think that transformer models and all this,

1:54:17.340 --> 1:54:21.900
 all the breakthroughs in language and NLP that might be applicable to like

1:54:21.900 --> 1:54:24.620
 driving at the higher level, at the behavioral level, that's kind of

1:54:24.620 --> 1:54:27.900
 fascinating. Let me ask about pesky little creatures

1:54:27.900 --> 1:54:32.620
 called pedestrians and cyclists. They seem, so humans are a problem. If we

1:54:32.620 --> 1:54:36.940
 can get rid of them, I would. But unfortunately, they're all sort of

1:54:36.940 --> 1:54:39.980
 a source of joy and love and beauty, so let's keep them around.

1:54:39.980 --> 1:54:43.340
 They're also our customers. For your perspective, yes, yes,

1:54:43.340 --> 1:54:46.620
 for sure. They're a source of money, very good.

1:54:46.620 --> 1:54:52.300
 But I don't even know where I was going. Oh yes,

1:54:52.300 --> 1:54:57.260
 pedestrians and cyclists, you know,

1:54:57.260 --> 1:55:00.620
 they're a fascinating injection into the system of

1:55:00.620 --> 1:55:09.020
 uncertainty of like a game theoretic dance of what to do. And also

1:55:09.020 --> 1:55:13.420
 they have perceptions of their own, and they can tweet

1:55:13.420 --> 1:55:17.500
 about your product, so you don't want to run them over

1:55:17.500 --> 1:55:21.580
 from that perspective. I mean, I don't know, I'm joking a lot, but

1:55:21.580 --> 1:55:27.340
 I think in seriousness, like, you know, pedestrians are a complicated

1:55:27.340 --> 1:55:31.340
 computer vision problem, a complicated behavioral problem. Is there something

1:55:31.340 --> 1:55:34.140
 interesting you could say about what you've learned

1:55:34.140 --> 1:55:38.380
 from a machine learning perspective, from also an autonomous vehicle,

1:55:38.380 --> 1:55:42.140
 and a product perspective about just interacting with the humans in this

1:55:42.140 --> 1:55:45.180
 world? Yeah, just to state on record, we care

1:55:45.180 --> 1:55:48.380
 deeply about the safety of pedestrians, you know, even the ones that don't have

1:55:48.380 --> 1:55:52.940
 Twitter accounts. Thank you. All right, cool.

1:55:52.940 --> 1:55:57.500
 Not me. But yes, I'm glad, I'm glad somebody does.

1:55:57.500 --> 1:56:01.340
 Okay. But you know, in all seriousness, safety

1:56:01.340 --> 1:56:07.260
 of vulnerable road users, pedestrians or cyclists, is one of our

1:56:07.260 --> 1:56:12.220
 highest priorities. We do a tremendous amount of testing

1:56:12.220 --> 1:56:16.220
 and validation, and put a very significant emphasis

1:56:16.220 --> 1:56:20.540
 on, you know, the capabilities of our systems that have to do with safety

1:56:20.540 --> 1:56:23.820
 around those unprotected vulnerable road users.

1:56:23.820 --> 1:56:27.660
 You know, cars, just, you know, discussed earlier in Phoenix, we have completely

1:56:27.660 --> 1:56:31.740
 empty cars, completely driverless cars, you know, driving in this very large area,

1:56:31.740 --> 1:56:35.260
 and you know, some people use them to, you know, go to school, so they'll drive

1:56:35.260 --> 1:56:39.660
 through school zones, right? So, kids are kind of the very special

1:56:39.660 --> 1:56:42.220
 class of those vulnerable user road users, right? You want to be,

1:56:42.220 --> 1:56:45.980
 you know, super, super safe, and super, super cautious around those. So, we take

1:56:45.980 --> 1:56:50.460
 it very, very, very seriously. And you know, what does it take to

1:56:50.460 --> 1:56:55.180
 be good at it? You know,

1:56:55.180 --> 1:57:02.060
 an incredible amount of performance across your whole stack. You know,

1:57:02.060 --> 1:57:05.820
 starts with hardware, and again, you want to use all

1:57:05.820 --> 1:57:09.500
 sensing modalities available to you. Imagine driving on a residential road

1:57:09.500 --> 1:57:13.100
 at night, and kind of making a turn, and you don't have, you know, headlights

1:57:13.100 --> 1:57:16.220
 covering some part of the space, and like, you know, a kid might

1:57:16.220 --> 1:57:20.620
 run out. And you know, lighters are amazing at that. They

1:57:20.620 --> 1:57:24.300
 see just as well in complete darkness as they do during the day, right? So, just

1:57:24.300 --> 1:57:27.900
 again, it gives you that extra,

1:57:27.900 --> 1:57:32.540
 you know, margin in terms of, you know, capability, and performance, and safety,

1:57:32.540 --> 1:57:35.420
 and quality. And in fact, we oftentimes, in these

1:57:35.420 --> 1:57:38.460
 kinds of situations, we have our system detect something,

1:57:38.460 --> 1:57:42.140
 in some cases even earlier than our trained operators in the car might do,

1:57:42.140 --> 1:57:46.620
 right? Especially, you know, in conditions like, you know, very dark nights.

1:57:46.620 --> 1:57:50.380
 So, starts with sensing, then, you know, perception

1:57:50.380 --> 1:57:54.300
 has to be incredibly good. And you have to be very, very good

1:57:54.300 --> 1:58:00.780
 at kind of detecting pedestrians in all kinds of situations, and all kinds

1:58:00.780 --> 1:58:03.580
 of environments, including, you know, people in weird poses,

1:58:03.580 --> 1:58:09.900
 people kind of running around, and you know, being partially occluded.

1:58:09.900 --> 1:58:13.180
 So, you know, that's step number one, right?

1:58:13.180 --> 1:58:17.580
 Then, you have to have in very high accuracy,

1:58:17.580 --> 1:58:21.180
 and very low latency, in terms of your reactions

1:58:21.180 --> 1:58:27.500
 to, you know, what, you know, these actors might do, right? And we've put a

1:58:27.500 --> 1:58:30.780
 tremendous amount of engineering, and tremendous amount of validation, in to

1:58:30.780 --> 1:58:35.020
 make sure our system performs properly. And, you know, oftentimes, it

1:58:35.020 --> 1:58:38.140
 does require a very strong reaction to do the safe thing. And, you know, we

1:58:38.140 --> 1:58:41.820
 actually see a lot of cases like that. That's the long tail of really rare,

1:58:41.820 --> 1:58:48.620
 you know, really, you know, crazy events that contribute to the safety

1:58:48.620 --> 1:58:52.300
 around pedestrians. Like, one example that comes to mind, that we actually

1:58:52.300 --> 1:58:56.940
 happened in Phoenix, where we were driving

1:58:56.940 --> 1:59:00.540
 along, and I think it was a 45 mile per hour road, so you have pretty high speed

1:59:00.540 --> 1:59:03.420
 traffic, and there was a sidewalk next to it, and

1:59:03.420 --> 1:59:09.100
 there was a cyclist on the sidewalk. And as we were in the right lane,

1:59:09.100 --> 1:59:13.260
 right next to the side, so it was a multi lane road, so as we got close

1:59:13.260 --> 1:59:17.180
 to the cyclist on the sidewalk, it was a woman, you know, she tripped and fell.

1:59:17.180 --> 1:59:20.540
 Just, you know, fell right into the path of our vehicle, right?

1:59:20.540 --> 1:59:25.820
 And our, you know, car, you know, this was actually with a

1:59:25.820 --> 1:59:29.820
 test driver, our test drivers, did exactly the right thing.

1:59:29.820 --> 1:59:33.100
 They kind of reacted, and came to stop. It requires both very strong steering,

1:59:33.100 --> 1:59:37.020
 and, you know, strong application of the brake. And then we simulated what our

1:59:37.020 --> 1:59:39.260
 system would have done in that situation, and it did, you know,

1:59:39.260 --> 1:59:43.180
 exactly the same thing. And that speaks to, you know, all of

1:59:43.180 --> 1:59:46.620
 those components of really good state estimation and

1:59:46.620 --> 1:59:49.020
 tracking. And, like, imagine, you know, a person

1:59:49.020 --> 1:59:52.140
 on a bike, and they're falling over, and they're doing that right in front of you,

1:59:52.140 --> 1:59:54.300
 right? So you have to be really, like, things are changing. The appearance of

1:59:54.300 --> 1:59:57.820
 that whole thing is changing, right? And a person goes one way, they're falling on

1:59:57.820 --> 2:00:00.380
 the road, they're, you know, being flat on the ground in front of

2:00:00.380 --> 2:00:03.340
 you. You know, the bike goes flying the other direction.

2:00:03.340 --> 2:00:06.060
 Like, the two objects that used to be one, they're now, you know,

2:00:06.060 --> 2:00:09.020
 are splitting apart, and the car has to, like, detect all of that.

2:00:09.020 --> 2:00:12.620
 Like, milliseconds matter, and it doesn't, you know, it's not good enough to just

2:00:12.620 --> 2:00:15.660
 brake. You have to, like, steer and brake, and there's traffic around you.

2:00:15.660 --> 2:00:19.180
 So, like, it all has to come together, and it was really great

2:00:19.180 --> 2:00:22.060
 to see in this case, and other cases like that, that we're actually seeing in the

2:00:22.060 --> 2:00:25.100
 wild, that our system is, you know, performing

2:00:25.100 --> 2:00:28.620
 exactly the way that we would have liked, and is able to,

2:00:28.620 --> 2:00:30.620
 you know, avoid collisions like this.

2:00:30.620 --> 2:00:32.780
 That's such an exciting space for robotics.

2:00:32.780 --> 2:00:37.500
 Like, in that split second to make decisions of life and death.

2:00:37.500 --> 2:00:41.580
 I don't know. The stakes are high, in a sense, but it's also beautiful

2:00:41.580 --> 2:00:47.020
 that for somebody who loves artificial intelligence, the possibility that an AI

2:00:47.020 --> 2:00:49.980
 system might be able to save a human life.

2:00:49.980 --> 2:00:53.740
 That's kind of exciting as a problem, like, to wake up.

2:00:53.740 --> 2:00:57.420
 It's terrifying, probably, for an engineer to wake up,

2:00:57.420 --> 2:01:01.020
 and to think about, but it's also exciting because it's, like,

2:01:01.020 --> 2:01:05.420
 it's in your hands. Let me try to ask a question that's often brought up about

2:01:05.420 --> 2:01:09.420
 autonomous vehicles, and it might be fun to see if you have

2:01:09.420 --> 2:01:14.620
 anything interesting to say, which is about the trolley problem.

2:01:14.620 --> 2:01:19.260
 So, a trolley problem is an interesting philosophical construct

2:01:19.260 --> 2:01:23.260
 that highlights, and there's many others like it,

2:01:23.260 --> 2:01:29.900
 of the difficult ethical decisions that we humans have before us in this

2:01:29.900 --> 2:01:34.060
 complicated world. So, specifically is the choice

2:01:34.060 --> 2:01:39.020
 between if you are forced to choose to kill

2:01:39.020 --> 2:01:42.700
 a group X of people versus a group Y of people, like

2:01:42.700 --> 2:01:48.220
 one person. If you did nothing, you would kill one person, but if

2:01:48.220 --> 2:01:51.340
 you would kill five people, and if you decide to swerve out of the way, you

2:01:51.340 --> 2:01:55.180
 would only kill one person. Do you do nothing, or you choose to do

2:01:55.180 --> 2:01:58.060
 something? You can construct all kinds of, sort of,

2:01:58.060 --> 2:02:05.500
 ethical experiments of this kind that, I think, at least on a positive note,

2:02:05.500 --> 2:02:09.660
 inspire you to think about, like, introspect

2:02:09.660 --> 2:02:16.220
 what are the physics of our morality, and there's usually not

2:02:16.220 --> 2:02:20.700
 good answers there. I think people love it because it's just an exciting

2:02:20.700 --> 2:02:24.060
 thing to think about. I think people who build autonomous

2:02:24.060 --> 2:02:30.060
 vehicles usually roll their eyes, because this is not,

2:02:30.060 --> 2:02:34.060
 this one as constructed, this, like, literally never comes up

2:02:34.060 --> 2:02:38.300
 in reality. You never have to choose between killing

2:02:38.300 --> 2:02:41.660
 one or, like, one of two groups of people,

2:02:41.660 --> 2:02:48.780
 but I wonder if you can speak to, is there some something interesting

2:02:48.780 --> 2:02:52.620
 to you as an engineer of autonomous vehicles that's within the trolley

2:02:52.620 --> 2:02:55.740
 problem, or maybe more generally, are there

2:02:55.740 --> 2:02:58.940
 difficult ethical decisions that you find

2:02:58.940 --> 2:03:03.340
 that an algorithm must make? On the specific version of the trolley problem,

2:03:03.340 --> 2:03:07.900
 which one would you do, if you're driving? The question itself

2:03:07.900 --> 2:03:11.340
 is a profound question, because we humans ourselves

2:03:11.340 --> 2:03:18.700
 cannot answer, and that's the very point. I would kill both.

2:03:18.700 --> 2:03:21.340
 Yeah, humans, I think you're exactly right in that, you know, humans are not

2:03:21.340 --> 2:03:24.460
 particularly good. I think they're kind of phrased as, like, what would a computer do,

2:03:24.460 --> 2:03:28.540
 but, like, humans, you know, are not very good, and actually oftentimes

2:03:28.540 --> 2:03:32.620
 I think that, you know, freezing and kind of not doing anything, because,

2:03:32.620 --> 2:03:35.500
 like, you've taken a few extra milliseconds to just process, and then

2:03:35.500 --> 2:03:38.700
 you end up, like, doing the worst of the possible outcomes, right? So,

2:03:38.700 --> 2:03:42.220
 I do think that, as you've pointed out, it can be

2:03:42.220 --> 2:03:45.660
 a bit of a distraction, and it can be a bit of a kind of red herring. I think

2:03:45.660 --> 2:03:47.820
 it's an interesting, you know, discussion

2:03:47.820 --> 2:03:51.580
 in the realm of philosophy, right? But in terms of

2:03:51.580 --> 2:03:54.780
 what, you know, how that affects the actual

2:03:54.780 --> 2:03:57.660
 engineering and deployment of self driving vehicles,

2:03:57.660 --> 2:04:02.780
 it's not how you go about building a system, right? We've talked

2:04:02.780 --> 2:04:06.460
 about how you engineer a system, how you, you know, go about evaluating

2:04:06.460 --> 2:04:09.820
 the different components and, you know, the safety of the entire thing.

2:04:09.820 --> 2:04:13.740
 How do you kind of inject the, you know, various

2:04:13.740 --> 2:04:17.580
 model based, safety based arguments, and, like, yes, you reason at parts of the

2:04:17.580 --> 2:04:20.540
 system, you know, you reason about the

2:04:20.540 --> 2:04:24.220
 probability of a collision, the severity of that collision, right?

2:04:24.220 --> 2:04:27.180
 And that is incorporated, and there's, you know, you have to properly reason

2:04:27.180 --> 2:04:29.500
 about the uncertainty that flows through the system, right? So,

2:04:29.500 --> 2:04:34.540
 you know, those, you know, factors definitely play a role in how

2:04:34.540 --> 2:04:36.700
 the cars then behave, but they tend to be more

2:04:36.700 --> 2:04:39.740
 of, like, the emergent behavior. And what you see, like, you're absolutely right

2:04:39.740 --> 2:04:43.740
 that these, you know, clear theoretical problems that they, you

2:04:43.740 --> 2:04:46.940
 know, you don't encounter that in the system, and really kind of being

2:04:46.940 --> 2:04:49.980
 back to our previous discussion of, like, what, you know, what, you

2:04:49.980 --> 2:04:53.900
 know, which one do you choose? Well, you know, oftentimes, like,

2:04:53.900 --> 2:04:57.420
 you made a mistake earlier. Like, you shouldn't be in that situation

2:04:57.420 --> 2:05:00.620
 in the first place, right? And in reality, the system comes up.

2:05:00.620 --> 2:05:03.740
 If you build a very good, safe, and capable driver,

2:05:03.740 --> 2:05:08.380
 you have enough, you know, clues in the environment that you

2:05:08.380 --> 2:05:11.340
 drive defensively, so you don't put yourself in that situation, right? And

2:05:11.340 --> 2:05:14.060
 again, you know, it has, you know, this, if you go back to that analogy of, you

2:05:14.060 --> 2:05:16.860
 know, precision and recoil, like, okay, you can make a, you know, very hard trade

2:05:16.860 --> 2:05:19.500
 off, but like, neither answer is really good.

2:05:19.500 --> 2:05:22.460
 But what instead you focus on is kind of moving

2:05:22.460 --> 2:05:26.140
 the whole curve up, and then you focus on building the right capability on the

2:05:26.140 --> 2:05:28.620
 right defensive driving, so that, you know, you don't put yourself in the

2:05:28.620 --> 2:05:32.380
 situation like this. I don't know if you have a good answer

2:05:32.380 --> 2:05:35.420
 for this, but people love it when I ask this question

2:05:35.420 --> 2:05:42.460
 about books. Are there books in your life that you've enjoyed,

2:05:42.460 --> 2:05:47.100
 philosophical, fiction, technical, that had a big impact on you as an engineer or

2:05:47.100 --> 2:05:50.300
 as a human being? You know, everything from science fiction

2:05:50.300 --> 2:05:53.500
 to a favorite textbook. Is there three books that stand out that

2:05:53.500 --> 2:05:57.340
 you can think of? Three books. So I would, you know, that

2:05:57.340 --> 2:06:02.860
 impacted me, I would say,

2:06:02.860 --> 2:06:06.380
 and this one is, you probably know it well,

2:06:06.380 --> 2:06:11.420
 but not generally well known, I think, in the U.S., or kind of

2:06:11.420 --> 2:06:16.620
 internationally, The Master and Margarita. It's one of, actually, my

2:06:16.620 --> 2:06:20.860
 favorite books. It is, you know, by

2:06:20.860 --> 2:06:26.300
 Russian, it's a novel by Russian author Mikhail Bulgakov, and it's just, it's a

2:06:26.300 --> 2:06:28.220
 great book. It's one of those books that you can, like,

2:06:28.220 --> 2:06:32.300
 reread your entire life, and it's very accessible. You can read it as a kid,

2:06:32.300 --> 2:06:35.900
 and, like, it's, you know, the plot is interesting. It's, you know, the

2:06:35.900 --> 2:06:38.140
 devil, you know, visiting the Soviet Union,

2:06:38.140 --> 2:06:41.980
 and, you know, but it, like, you read it, reread it

2:06:41.980 --> 2:06:46.060
 at different stages of your life, and you enjoy it for

2:06:46.060 --> 2:06:49.580
 different, very different reasons, and you keep finding, like, deeper and deeper

2:06:49.580 --> 2:06:52.220
 meaning, and, you know, kind of affected, you know,

2:06:52.220 --> 2:06:57.580
 had a, definitely had an, like, imprint on me, you know, mostly from the,

2:06:57.580 --> 2:07:00.940
 probably kind of the cultural, stylistic aspect. Like, it makes you think one of

2:07:00.940 --> 2:07:04.300
 those books that, you know, is good and makes you think, but also has,

2:07:04.300 --> 2:07:07.740
 like, this really, you know, silly, quirky, dark sense of, you know,

2:07:07.740 --> 2:07:10.140
 humor. It captures the Russian soul more than

2:07:10.140 --> 2:07:13.020
 many, perhaps, many other books. On that, like, slight note,

2:07:13.020 --> 2:07:17.180
 just out of curiosity, one of the saddest things is I've read that book

2:07:17.180 --> 2:07:22.460
 in English. Did you, by chance, read it in English or in Russian?

2:07:22.460 --> 2:07:26.060
 In Russian, only in Russian, and I actually, that is a question I had,

2:07:26.060 --> 2:07:30.780
 kind of posed to myself every once in a while, like, I wonder how well it

2:07:30.780 --> 2:07:33.420
 translates, if it translates at all, and there's the

2:07:33.420 --> 2:07:35.980
 language aspect of it, and then there's the cultural aspect, so

2:07:35.980 --> 2:07:39.260
 I, actually, I'm not sure if, you know, either of those would

2:07:39.260 --> 2:07:43.740
 work well in English. Now, I forget their names, but, so, when the COVID lifts a

2:07:43.740 --> 2:07:48.780
 little bit, I'm traveling to Paris for several reasons. One is just, I've

2:07:48.780 --> 2:07:50.700
 never been to Paris, I want to go to Paris, but

2:07:50.700 --> 2:07:57.020
 there's the most famous translators of Dostoevsky, Tolstoy, of most of

2:07:57.020 --> 2:08:00.540
 Russian literature live there. There's a couple, they're famous,

2:08:00.540 --> 2:08:03.980
 a man and a woman, and I'm going to, sort of, have a series of conversations with

2:08:03.980 --> 2:08:06.780
 them, and in preparation for that, I'm starting

2:08:06.780 --> 2:08:10.380
 to read Dostoevsky in Russian, so I'm really embarrassed to say that I read

2:08:10.380 --> 2:08:13.820
 this, everything I've read in Russian literature of, like,

2:08:13.820 --> 2:08:18.540
 serious depth has been in English, even though

2:08:18.540 --> 2:08:21.820
 I can also read, I mean, obviously, in Russian, but

2:08:21.820 --> 2:08:26.540
 for some reason, it seemed,

2:08:26.940 --> 2:08:31.420
 in the optimization of life, it seemed the improper decision to do, to read in

2:08:31.420 --> 2:08:35.020
 Russian, like, you know, like, I don't need to,

2:08:35.020 --> 2:08:38.700
 I need to think in English, not in Russian, but now I'm changing my mind on

2:08:38.700 --> 2:08:41.340
 that, and so, the question of how well I translate, it's a

2:08:41.340 --> 2:08:43.900
 really fun to method one, like, even with Dostoevsky.

2:08:43.900 --> 2:08:47.340
 So, from what I understand, Dostoevsky translates easier,

2:08:47.340 --> 2:08:52.380
 others don't as much. Obviously, the poetry doesn't translate as well,

2:08:52.380 --> 2:08:57.740
 I'm also the music big fan of Vladimir Vosotsky,

2:08:57.740 --> 2:09:02.700
 he doesn't obviously translate well, people have tried,

2:09:02.700 --> 2:09:06.300
 but mastermind, I don't know, I don't know about that one, I just know in

2:09:06.300 --> 2:09:10.140
 English, you know, as fun as hell in English, so, so, but

2:09:10.140 --> 2:09:13.340
 it's a curious question, and I want to study it rigorously from both the

2:09:13.340 --> 2:09:16.940
 machine learning aspect, and also because I want to do a

2:09:16.940 --> 2:09:21.980
 couple of interviews in Russia, that

2:09:21.980 --> 2:09:27.100
 I'm still unsure of how to properly conduct an interview

2:09:27.100 --> 2:09:30.380
 across a language barrier, it's a fascinating question

2:09:30.380 --> 2:09:34.060
 that ultimately communicates to an American audience. There's a few

2:09:34.060 --> 2:09:39.260
 Russian people that I think are truly special human beings,

2:09:39.260 --> 2:09:44.780
 and I feel, like, I sometimes encounter this with some

2:09:44.780 --> 2:09:48.300
 incredible scientists, and maybe you encounter this

2:09:48.300 --> 2:09:52.780
 as well at some point in your life, that it feels like because of the language

2:09:52.780 --> 2:09:57.660
 barrier, their ideas are lost to history. It's a sad thing, I think about, like,

2:09:57.660 --> 2:10:01.500
 Chinese scientists, or even authors that, like,

2:10:01.500 --> 2:10:05.820
 that we don't, in an English speaking world, don't get to appreciate

2:10:05.820 --> 2:10:09.180
 some, like, the depth of the culture because it's lost in translation,

2:10:09.180 --> 2:10:13.260
 and I feel like I would love to show that to the world,

2:10:13.260 --> 2:10:16.940
 like, I'm just some idiot, but because I have this,

2:10:16.940 --> 2:10:20.860
 like, at least some semblance of skill in speaking Russian,

2:10:20.860 --> 2:10:25.020
 I feel like, and I know how to record stuff on a video camera,

2:10:25.020 --> 2:10:28.700
 I feel like I want to catch, like, Grigori Perlman, who's a mathematician, I'm not

2:10:28.700 --> 2:10:31.740
 sure if you're familiar with him, I want to talk to him, like, he's a

2:10:31.740 --> 2:10:35.980
 fascinating mind, and to bring him to a wider audience in English speaking

2:10:35.980 --> 2:10:40.060
 will be fascinating, but that requires to be rigorous about this question

2:10:40.060 --> 2:10:46.380
 of how well Bulgakov translates. I mean, I know it's a silly

2:10:46.380 --> 2:10:50.940
 concept, but it's a fundamental one, because how do you translate, and

2:10:50.940 --> 2:10:54.940
 that's the thing that Google Translate is also facing

2:10:54.940 --> 2:10:59.020
 as a more machine learning problem, but I wonder as a more

2:10:59.020 --> 2:11:03.020
 bigger problem for AI, how do we capture the magic

2:11:03.020 --> 2:11:08.860
 that's there in the language? I think that's a really interesting,

2:11:08.860 --> 2:11:12.540
 really challenging problem. If you do read it, Master and Margarita

2:11:12.540 --> 2:11:16.700
 in English, sorry, in Russian, I'd be curious

2:11:16.700 --> 2:11:20.620
 to get your opinion, and I think part of it is language, but part of it's just,

2:11:20.620 --> 2:11:23.260
 you know, centuries of culture, that, you know, the cultures are

2:11:23.260 --> 2:11:28.060
 different, so it's hard to connect that.

2:11:28.060 --> 2:11:31.420
 Okay, so that was my first one, right? You had two more. The second one I

2:11:31.420 --> 2:11:35.660
 would probably pick is the science fiction by the

2:11:35.660 --> 2:11:38.460
 Strogatsky brothers. You know, it's up there with, you know,

2:11:38.460 --> 2:11:43.340
 Isaac Asimov and, you know, Ray Bradbury and, you know, company. The

2:11:43.340 --> 2:11:47.740
 Strogatsky brothers kind of appealed more to me. I think it made more of an

2:11:47.740 --> 2:11:53.500
 impression on me growing up. I apologize if I'm

2:11:53.500 --> 2:11:57.100
 showing my complete ignorance. I'm so weak on sci fi. What did

2:11:57.100 --> 2:12:04.060
 they write? Oh, Roadside Picnic,

2:12:04.060 --> 2:12:07.580
 Heart to Be a God,

2:12:07.580 --> 2:12:14.700
 Beetle in an Ant Hill, Monday Starts on Saturday. Like, it's

2:12:14.700 --> 2:12:17.500
 not just science fiction. It also has very interesting, you know,

2:12:17.500 --> 2:12:21.580
 interpersonal and societal questions, and some of the

2:12:21.580 --> 2:12:27.820
 language is just completely hilarious.

2:12:27.820 --> 2:12:31.500
 That's the one. Oh, interesting. Monday Starts on Saturday. So,

2:12:31.500 --> 2:12:36.300
 I need to read. Okay, oh boy. You put that in the category of science fiction?

2:12:36.300 --> 2:12:39.900
 That one is, I mean, this was more of a silly,

2:12:39.900 --> 2:12:43.260
 you know, humorous work. I mean, there is kind of...

2:12:43.260 --> 2:12:46.380
 It's profound too, right? Science fiction, right? It's about, you know, this

2:12:46.380 --> 2:12:50.620
 research institute, and it has deep parallels to

2:12:50.620 --> 2:12:53.660
 serious research, but the setting, of course,

2:12:53.660 --> 2:12:56.380
 is that they're working on, you know, magic, right? And there's a

2:12:56.380 --> 2:13:00.300
 lot of stuff. And that's their style, right?

2:13:00.300 --> 2:13:03.260
 And, you know, other books are very different, right? You know,

2:13:03.260 --> 2:13:07.100
 Heart to Be a God, right? It's about kind of this higher society being injected

2:13:07.100 --> 2:13:09.660
 into this primitive world, and how they operate there,

2:13:09.660 --> 2:13:13.420
 and some of the very deep ethical questions there,

2:13:13.420 --> 2:13:16.540
 right? And, like, they've got this full spectrum. Some is, you know, more about

2:13:16.540 --> 2:13:19.580
 kind of more adventure style. But, like, I enjoy all of

2:13:19.580 --> 2:13:21.820
 their books. There's just, you know, probably a couple.

2:13:21.820 --> 2:13:24.780
 Actually, one I think that they consider their most important work.

2:13:24.780 --> 2:13:29.660
 I think it's The Snail on a Hill. I'm not exactly sure how it

2:13:29.660 --> 2:13:32.620
 translates. I tried reading a couple times. I still don't get it.

2:13:32.620 --> 2:13:36.540
 But everything else I fully enjoyed. And, like, for one of my birthdays as a kid, I

2:13:36.540 --> 2:13:40.060
 got, like, their entire collection, like, occupied a giant shelf in my room, and

2:13:40.060 --> 2:13:42.220
 then, like, over the holidays, I just, like,

2:13:42.220 --> 2:13:44.700
 you know, my parents couldn't drag me out of the room, and I read the whole thing

2:13:44.700 --> 2:13:49.500
 cover to cover. And I really enjoyed it.

2:13:49.500 --> 2:13:52.540
 And that's one more. For the third one, you know, maybe a little bit

2:13:52.540 --> 2:13:56.700
 darker, but, you know, comes to mind is Orwell's

2:13:56.700 --> 2:14:01.180
 1984. And, you know, you asked what made an

2:14:01.180 --> 2:14:03.900
 impression on me and the books that people should read. That one, I think,

2:14:03.900 --> 2:14:06.860
 falls in the category of both. You know, definitely it's one of those

2:14:06.860 --> 2:14:11.100
 books that you read, and you just kind of, you know, put it

2:14:11.100 --> 2:14:16.460
 down and you stare in space for a while. You know, that kind of work. I think

2:14:16.460 --> 2:14:19.980
 there's, you know, lessons there. People should

2:14:19.980 --> 2:14:24.220
 not ignore. And, you know, nowadays, with, like,

2:14:24.220 --> 2:14:26.060
 everything that's happening in the world, I,

2:14:26.060 --> 2:14:29.420
 like, can't help it, but, you know, have my mind jump to some,

2:14:29.420 --> 2:14:34.220
 you know, parallels with what Orwell described. And, like, there's this whole,

2:14:34.220 --> 2:14:38.460
 you know, concept of double think and ignoring logic and, you know, holding

2:14:38.460 --> 2:14:41.820
 completely contradictory opinions in your mind and not have that not bother

2:14:41.820 --> 2:14:44.140
 you and, you know, sticking to the party line

2:14:44.140 --> 2:14:48.220
 at all costs. Like, you know, there's something there.

2:14:48.220 --> 2:14:52.940
 If anything, 2020 has taught me, and I'm a huge fan of Animal Farm, which is a

2:14:52.940 --> 2:14:57.900
 kind of friendly, as a friend of 1984 by Orwell.

2:14:57.900 --> 2:15:03.660
 It's kind of another thought experiment of how our society

2:15:03.660 --> 2:15:07.340
 may go in directions that we wouldn't like it to go.

2:15:07.340 --> 2:15:14.300
 But if anything that's been kind of heartbreaking to an

2:15:14.300 --> 2:15:18.860
 optimist about 2020 is that

2:15:18.940 --> 2:15:22.140
 that society is kind of fragile. Like, we have this,

2:15:22.140 --> 2:15:25.900
 this is a special little experiment we have going on.

2:15:25.900 --> 2:15:32.300
 And not, it's not unbreakable. Like, we should be careful to, like, preserve

2:15:32.300 --> 2:15:36.380
 whatever the special thing we have going on. I mean, I think 1984

2:15:36.380 --> 2:15:39.820
 and these books, The Brave New World, they're

2:15:39.820 --> 2:15:43.660
 helpful in thinking, like, stuff can go wrong

2:15:43.660 --> 2:15:48.380
 in nonobvious ways. And it's, like, it's up to us to preserve it.

2:15:48.380 --> 2:15:51.980
 And it's, like, it's a responsibility. It's been weighing heavy on me because, like,

2:15:51.980 --> 2:15:57.580
 for some reason, like, more than my mom follows me on Twitter and I

2:15:57.580 --> 2:15:59.980
 feel like I have, like, now somehow a

2:15:59.980 --> 2:16:03.100
 responsibility to

2:16:03.100 --> 2:16:07.980
 do this world. And it dawned on me that, like,

2:16:07.980 --> 2:16:12.300
 me and millions of others are, like, the little ants

2:16:12.300 --> 2:16:17.020
 that maintain this little colony, right? So we have a responsibility not to

2:16:17.020 --> 2:16:20.060
 be, I don't know what the right analogy is, but

2:16:20.060 --> 2:16:23.420
 I'll put a flamethrower to the place. We want to

2:16:23.420 --> 2:16:27.900
 not do that. And there's interesting complicated ways of doing that as 1984

2:16:27.900 --> 2:16:29.820
 shows. It could be through bureaucracy. It could

2:16:29.820 --> 2:16:33.180
 be through incompetence. It could be through misinformation.

2:16:33.180 --> 2:16:36.460
 It could be through division and toxicity.

2:16:36.460 --> 2:16:39.980
 I'm a huge believer in, like, that love will be

2:16:39.980 --> 2:16:46.460
 the, somehow, the solution. So, love and robots. Love and robots, yeah.

2:16:46.460 --> 2:16:49.340
 I think you're exactly right. Unfortunately, I think it's less of a

2:16:49.340 --> 2:16:51.980
 flamethrower type of thing. It's more of a,

2:16:51.980 --> 2:16:55.100
 in many cases, it's going to be more of a slow boil. And that's the

2:16:55.100 --> 2:17:00.220
 danger. Let me ask, it's a fun thing to make

2:17:00.220 --> 2:17:05.100
 a world class roboticist, engineer, and leader uncomfortable with a

2:17:05.100 --> 2:17:09.660
 ridiculous question about life. What is the meaning of life,

2:17:09.660 --> 2:17:14.700
 Dimitri, from a robotics and a human perspective?

2:17:14.700 --> 2:17:19.500
 You only have a couple minutes, or one minute to answer, so.

2:17:19.820 --> 2:17:23.180
 I don't know if that makes it more difficult or easier, actually.

2:17:23.180 --> 2:17:29.740
 You know, they're very tempted to quote one of the

2:17:29.740 --> 2:17:36.060
 stories by Isaac Asimov, actually. Actually, titled,

2:17:36.060 --> 2:17:39.900
 appropriately titled, The Last Question. It's a short story where, you know, the

2:17:39.900 --> 2:17:42.860
 plot is that, you know, humans build this supercomputer,

2:17:42.860 --> 2:17:46.220
 you know, this AI intelligence, and, you know, once it

2:17:46.220 --> 2:17:49.660
 gets powerful enough, they pose this question to it, you know,

2:17:49.660 --> 2:17:54.380
 how can the entropy in the universe be reduced, right? So the computer replies,

2:17:54.380 --> 2:17:58.140
 as of yet, insufficient information to give a meaningful answer,

2:17:58.140 --> 2:18:00.940
 right? And then, you know, thousands of years go by, and they keep posing the

2:18:00.940 --> 2:18:03.980
 same question, and the computer, you know, gets more and more powerful, and keeps

2:18:03.980 --> 2:18:06.540
 giving the same answer, you know, as of yet, insufficient

2:18:06.540 --> 2:18:09.580
 information to give a meaningful answer, or something along those lines,

2:18:09.580 --> 2:18:12.940
 right? And then, you know, it keeps, you know, happening, and

2:18:12.940 --> 2:18:16.060
 happening, you fast forward, like, millions of years into the future, and,

2:18:16.060 --> 2:18:19.100
 you know, billions of years, and, like, at some point, it's just the only entity in

2:18:19.100 --> 2:18:21.580
 the universe, it's, like, absorbed all humanity,

2:18:21.580 --> 2:18:24.460
 and all knowledge in the universe, and it, like, keeps posing the same question

2:18:24.460 --> 2:18:28.700
 to itself, and, you know, finally, it gets to the

2:18:28.700 --> 2:18:31.900
 point where it is able to answer that question, but, of course, at that point,

2:18:31.900 --> 2:18:34.700
 you know, there's, you know, the heat death of the universe has occurred, and

2:18:34.700 --> 2:18:37.500
 that's the only entity, and there's nobody else to provide that

2:18:37.500 --> 2:18:40.140
 answer to, so the only thing it can do is to,

2:18:40.140 --> 2:18:43.980
 you know, answer it by demonstration, so, like, you know, it recreates the big bang,

2:18:43.980 --> 2:18:47.100
 right, and resets the clock, right?

2:18:47.100 --> 2:18:50.540
 But, like, you know, I can try to give kind of a

2:18:50.540 --> 2:18:53.340
 different version of the answer, you know, maybe

2:18:53.340 --> 2:18:56.780
 not on the behalf of all humanity, I think that that might be a little

2:18:56.780 --> 2:19:00.300
 presumptuous for me to speak about the meaning of life on the behalf of all

2:19:00.300 --> 2:19:03.420
 humans, but at least, you know, personally,

2:19:03.420 --> 2:19:06.940
 it changes, right? I think if you think about kind of what

2:19:06.940 --> 2:19:13.660
 gives, you know, you and your life meaning and purpose, and kind of

2:19:13.660 --> 2:19:18.460
 what drives you, it seems to

2:19:18.460 --> 2:19:22.060
 change over time, right, and that lifespan

2:19:22.060 --> 2:19:25.180
 of, you know, kind of your existence, you know, when

2:19:25.180 --> 2:19:27.980
 just when you just enter this world, right, it's all about kind of new

2:19:27.980 --> 2:19:33.180
 experiences, right? You get, like, new smells, new sounds, new emotions, right,

2:19:33.180 --> 2:19:36.380
 and, like, that's what's driving you, right? You're experiencing

2:19:36.380 --> 2:19:40.140
 new amazing things, right, and that's magical, right? That's pretty

2:19:40.140 --> 2:19:43.100
 pretty awesome, right? That gives you kind of meaning.

2:19:43.100 --> 2:19:47.740
 Then, you know, you get a little bit older, you start more intentionally

2:19:47.740 --> 2:19:51.020
 learning about things, right? I guess, actually, before you start intentionally

2:19:51.020 --> 2:19:53.740
 learning, it's probably fun. Fun is a thing that gives you kind of

2:19:53.740 --> 2:19:56.780
 meaning and purpose and purpose and the thing you optimize for, right?

2:19:56.780 --> 2:20:01.020
 And, like, fun is good. Then you get, you know, start learning, and I guess that

2:20:01.020 --> 2:20:05.660
 this joy of comprehension

2:20:05.660 --> 2:20:09.500
 and discovery is another thing that, you know, gives you

2:20:09.500 --> 2:20:12.940
 meaning and purpose and drives you, right? Then, you know, you

2:20:12.940 --> 2:20:17.420
 learn enough stuff and you want to give some of it back, right? And so

2:20:17.420 --> 2:20:20.460
 impact and contributions back to, you know, technology or society,

2:20:20.460 --> 2:20:24.860
 you know, people, you know, local or more globally

2:20:24.860 --> 2:20:28.620
 becomes a new thing that, you know, drives a lot of kind of your behavior

2:20:28.620 --> 2:20:31.900
 and is something that gives you purpose and

2:20:31.900 --> 2:20:35.260
 that you derive, you know, positive feedback from, right?

2:20:35.260 --> 2:20:38.460
 You know, then you go and so on and so forth. You go through various stages of

2:20:38.460 --> 2:20:43.420
 life. If you have kids,

2:20:43.420 --> 2:20:46.220
 like, that definitely changes your perspective on things. You know, I have

2:20:46.220 --> 2:20:48.940
 three that definitely flips some bits in your

2:20:48.940 --> 2:20:52.220
 head in terms of, you know, what you care about and what you

2:20:52.220 --> 2:20:54.940
 optimize for and, you know, what matters, what doesn't matter, right?

2:20:54.940 --> 2:20:58.140
 So, you know, and so on and so forth, right? And I,

2:20:58.140 --> 2:21:02.380
 it seems to me that, you know, it's all of those things and as

2:21:02.380 --> 2:21:06.700
 kind of you go through life, you know,

2:21:06.700 --> 2:21:10.140
 you want these to be additive, right? New experiences,

2:21:10.140 --> 2:21:14.460
 fun, learning, impact. Like, you want to, you know, be accumulating.

2:21:14.460 --> 2:21:17.820
 I don't want to, you know, stop having fun or, you know, experiencing new things and

2:21:17.820 --> 2:21:20.300
 I think it's important that, you know, it just kind of becomes

2:21:20.300 --> 2:21:23.660
 additive as opposed to a replacement or subtraction.

2:21:23.660 --> 2:21:27.500
 But, you know, those fewest problems as far as I got, but, you know, ask me in a

2:21:27.500 --> 2:21:30.220
 few years, I might have one or two more to add to the list.

2:21:30.220 --> 2:21:34.540
 And before you know it, time is up, just like it is for this conversation,

2:21:34.540 --> 2:21:38.460
 but hopefully it was a fun ride. It was a huge honor to meet you.

2:21:38.460 --> 2:21:43.900
 As you know, I've been a fan of yours and a fan of Google Self Driving Car and

2:21:43.900 --> 2:21:47.420
 Waymo for a long time. I can't wait. I mean, it's one of the

2:21:47.420 --> 2:21:50.300
 most exciting, if we look back in the 21st century, I

2:21:50.300 --> 2:21:53.180
 truly believe it'll be one of the most exciting things we

2:21:53.180 --> 2:21:57.100
 descendants of apes have created on this earth. So,

2:21:57.100 --> 2:22:00.540
 I'm a huge fan and I can't wait to see what you do

2:22:00.540 --> 2:22:04.460
 next. Thanks so much for talking to me. Thanks, thanks for having me and it's a

2:22:04.460 --> 2:22:08.540
 also a huge fan doing work, honestly, and I really

2:22:08.540 --> 2:22:11.260
 enjoyed it. Thank you. Thanks for listening to this

2:22:11.260 --> 2:22:14.620
 conversation with Dmitry Dolgov and thank you to our sponsors,

2:22:14.620 --> 2:22:19.340
 Triolabs, a company that helps businesses apply machine learning to

2:22:19.340 --> 2:22:23.100
 solve real world problems, Blinkist, an app I use for reading

2:22:23.100 --> 2:22:27.420
 through summaries of books, BetterHelp, online therapy with a licensed

2:22:27.420 --> 2:22:30.860
 professional, and CashApp, the app I use to send money to

2:22:30.860 --> 2:22:33.260
 friends. Please check out these sponsors in the

2:22:33.260 --> 2:22:37.180
 description to get a discount and to support this podcast. If you

2:22:37.180 --> 2:22:40.380
 enjoy this thing, subscribe on YouTube, review it with Five Stars

2:22:40.380 --> 2:22:44.140
 and Upper Podcast, follow on Spotify, support on Patreon,

2:22:44.140 --> 2:22:47.740
 or connect with me on Twitter at Lex Friedman. And now,

2:22:47.740 --> 2:22:51.420
 let me leave you with some words from Isaac Asimov.

2:22:51.420 --> 2:22:55.660
 Science can amuse and fascinate us all, but it is engineering

2:22:55.660 --> 2:22:59.980
 that changes the world. Thank you for listening and hope to see you

2:22:59.980 --> 2:23:04.060
 next time.

