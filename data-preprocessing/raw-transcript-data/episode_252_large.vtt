WEBVTT

00:00.000 --> 00:02.540
 The following is a conversation with Elon Musk,

00:02.540 --> 00:06.980
 his third time on this, The Lex Friedman Podcast.

00:06.980 --> 00:08.440
 Yeah, make yourself comfortable.

00:08.440 --> 00:09.280
 Boo.

00:09.280 --> 00:10.120
 No, wow, okay.

00:10.120 --> 00:12.760
 You don't do the headphone thing?

00:12.760 --> 00:13.600
 No.

00:13.600 --> 00:14.420
 Okay.

00:14.420 --> 00:15.840
 I mean, how close do I need to get up to the same place?

00:15.840 --> 00:17.560
 The closer you are, the sexier you sound.

00:17.560 --> 00:18.400
 Hey, babe.

00:18.400 --> 00:19.220
 What's up?

00:19.220 --> 00:20.880
 Can't get enough of your love, baby.

00:20.880 --> 00:25.080
 I'm gonna clip that out

00:25.080 --> 00:26.720
 any time somebody messages me about it.

00:26.720 --> 00:30.120
 If you want my body and you think I'm sexy,

00:30.120 --> 00:32.080
 come right out and tell me so.

00:33.040 --> 00:36.000
 Do, do, do, do, do.

00:36.000 --> 00:38.840
 So good.

00:38.840 --> 00:39.680
 So good.

00:39.680 --> 00:41.880
 Okay, serious mode activate.

00:41.880 --> 00:43.240
 All right.

00:43.240 --> 00:44.360
 Serious mode.

00:44.360 --> 00:45.200
 Come on, you're Russian.

00:45.200 --> 00:46.020
 You can be serious.

00:46.020 --> 00:46.860
 Yeah, I know.

00:46.860 --> 00:47.700
 Everyone's serious all the time in Russia.

00:47.700 --> 00:50.600
 Yeah, yeah, we'll get there.

00:50.600 --> 00:51.920
 We'll get there.

00:51.920 --> 00:52.760
 Yeah.

00:52.760 --> 00:53.600
 It's gotten soft.

00:53.600 --> 00:57.760
 Allow me to say that the SpaceX launch

00:57.760 --> 01:00.960
 of human beings to orbit on May 30th, 2020

01:02.160 --> 01:03.920
 was seen by many as the first step

01:03.920 --> 01:07.440
 in a new era of human space exploration.

01:07.440 --> 01:10.620
 These human space flight missions were a beacon of hope

01:10.620 --> 01:12.740
 to me and to millions over the past two years

01:12.740 --> 01:14.680
 as our world has been going through

01:14.680 --> 01:18.080
 one of the most difficult periods in recent human history.

01:18.080 --> 01:21.720
 We saw, we see the rise of division, fear, cynicism,

01:21.720 --> 01:24.620
 and the loss of common humanity

01:24.620 --> 01:26.380
 right when it is needed most.

01:26.380 --> 01:29.080
 So first, Elon, let me say thank you

01:29.080 --> 01:30.620
 for giving the world hope and reason

01:30.620 --> 01:32.520
 to be excited about the future.

01:32.520 --> 01:34.200
 Oh, it's kind of easy to say.

01:34.200 --> 01:35.480
 I do want to do that.

01:35.480 --> 01:38.320
 Humanity has obviously a lot of issues

01:38.320 --> 01:42.620
 and people at times do bad things,

01:42.620 --> 01:47.260
 but despite all that, I love humanity

01:47.260 --> 01:52.260
 and I think we should make sure we do everything we can

01:52.560 --> 01:54.640
 to have a good future and an exciting future

01:54.640 --> 01:58.320
 and one where that maximizes the happiness of the people.

01:58.320 --> 02:00.960
 Let me ask about Crew Dragon Demo 2.

02:00.960 --> 02:03.480
 So that first flight with humans on board,

02:04.360 --> 02:06.160
 how did you feel leading up to that launch?

02:06.160 --> 02:07.280
 Were you scared?

02:07.280 --> 02:08.260
 Were you excited?

02:08.260 --> 02:09.640
 What was going through your mind?

02:09.640 --> 02:11.380
 So much was at stake.

02:11.380 --> 02:14.700
 Yeah, no, that was extremely stressful, no question.

02:14.700 --> 02:19.140
 We obviously could not let them down in any way.

02:19.140 --> 02:24.140
 So extremely stressful, I'd say, to say the least.

02:25.940 --> 02:28.380
 I was confident that at the time that we launched

02:28.380 --> 02:33.380
 that no one could think of anything at all to do

02:34.140 --> 02:36.500
 that would improve the probability of success.

02:36.500 --> 02:40.100
 And we racked our brains to think of any possible way

02:40.100 --> 02:41.420
 to improve the probability of success.

02:41.420 --> 02:44.820
 We could not think of anything more, nor could NASA.

02:44.820 --> 02:48.060
 And so that's just the best that we could do.

02:48.060 --> 02:51.860
 So then we went ahead and launched.

02:51.860 --> 02:54.340
 Now, I'm not a religious person,

02:54.340 --> 02:59.340
 but I nonetheless got on my knees and prayed for that mission.

03:00.340 --> 03:01.940
 Were you able to sleep?

03:01.940 --> 03:02.780
 No.

03:02.780 --> 03:07.780
 How did it feel when it was a success,

03:10.820 --> 03:12.780
 first when the launch was a success,

03:12.780 --> 03:15.580
 and when they returned back home or back to Earth?

03:16.940 --> 03:18.060
 It was a great relief.

03:20.380 --> 03:23.480
 Yeah, for high stress situations,

03:23.480 --> 03:26.060
 I find it's not so much elation as relief.

03:26.060 --> 03:31.060
 And I think once, as we got more comfortable

03:33.440 --> 03:34.600
 and proved out the systems,

03:34.600 --> 03:39.600
 because we really got to make sure everything works,

03:41.280 --> 03:43.200
 it was definitely a lot more enjoyable

03:43.200 --> 03:46.520
 with the subsequent astronaut missions.

03:46.520 --> 03:50.160
 And I thought the Inspiration mission

03:50.160 --> 03:53.960
 was actually a very inspiring Inspiration 4 mission.

03:53.960 --> 03:57.040
 I'd encourage people to watch the Inspiration documentary

03:57.040 --> 03:58.880
 on Netflix, it's actually really good.

04:00.280 --> 04:03.360
 And it really isn't, I was actually inspired by that.

04:03.360 --> 04:07.760
 And so that one I felt I was kind of able

04:07.760 --> 04:09.320
 to enjoy the actual mission

04:09.320 --> 04:10.880
 and not just be super stressed all the time.

04:10.880 --> 04:13.760
 So for people that somehow don't know,

04:13.760 --> 04:17.240
 it's the all civilian, first time all civilian

04:17.240 --> 04:19.400
 out to space, out to orbit.

04:19.400 --> 04:22.480
 Yeah, and it was I think the highest orbit

04:22.480 --> 04:26.120
 that in like, I don't know, 30 or 40 years or something.

04:26.120 --> 04:29.480
 The only one that was higher was the one shuttle,

04:29.480 --> 04:32.160
 sorry, a Hubble servicing mission.

04:32.160 --> 04:35.980
 And then before that, it would have been Apollo in 72.

04:37.840 --> 04:39.180
 It's pretty wild.

04:39.180 --> 04:40.520
 So it's cool, it's good.

04:40.520 --> 04:45.520
 I think as a species, we want to be continuing

04:46.620 --> 04:50.000
 to do better and reach higher ground.

04:50.000 --> 04:52.880
 And like, I think it would be tragic, extremely tragic

04:52.880 --> 04:57.000
 if Apollo was the high watermark for humanity,

04:57.000 --> 04:58.800
 and that's as far as we ever got.

05:00.080 --> 05:05.080
 And it's concerning that here we are 49 years

05:06.360 --> 05:09.060
 after the last mission to the moon.

05:09.060 --> 05:14.060
 And so almost half a century, and we've not been back.

05:14.060 --> 05:16.260
 And that's worrying.

05:16.260 --> 05:20.280
 It's like, does that mean we've peaked as a civilization

05:20.280 --> 05:21.120
 or what?

05:21.120 --> 05:24.480
 So like, I think we got to get back to the moon

05:24.480 --> 05:27.160
 and build a base there, a science base.

05:27.160 --> 05:28.880
 I think we could learn a lot about the nature

05:28.880 --> 05:31.120
 of the universe if we have a proper science base

05:31.120 --> 05:35.520
 on the moon, like we have a science base in Antarctica

05:35.520 --> 05:38.080
 and many other parts of the world.

05:38.080 --> 05:41.880
 And so that's like, I think the next big thing,

05:41.880 --> 05:45.680
 we've got to have like a serious moon base

05:45.680 --> 05:49.960
 and then get people to Mars and get out there

05:49.960 --> 05:52.580
 and be a spacefaring civilization.

05:52.580 --> 05:54.500
 I'll ask you about some of those details,

05:54.500 --> 05:57.520
 but since you're so busy with the hard engineering

05:57.520 --> 06:00.440
 challenges of everything that's involved,

06:00.440 --> 06:03.000
 are you still able to marvel at the magic of it all,

06:03.000 --> 06:05.880
 of space travel, of every time the rocket goes up,

06:05.880 --> 06:08.120
 especially when it's a crewed mission?

06:08.120 --> 06:11.440
 Or are you just so overwhelmed with all the challenges

06:11.440 --> 06:12.560
 that you have to solve?

06:12.560 --> 06:16.280
 And actually, sort of to add to that,

06:16.280 --> 06:19.240
 the reason I wanted to ask this question of May 30th,

06:19.240 --> 06:21.840
 it's been some time, so you can look back

06:21.840 --> 06:23.520
 and think about the impact already.

06:23.520 --> 06:26.360
 It's already, at the time it was an engineering problem

06:26.360 --> 06:29.320
 maybe, now it's becoming a historic moment.

06:29.320 --> 06:31.640
 Like it's a moment that, how many moments

06:31.640 --> 06:33.720
 will be remembered about the 21st century?

06:33.720 --> 06:37.300
 To me, that or something like that,

06:37.300 --> 06:39.780
 maybe Inspiration4, one of those would be remembered

06:39.780 --> 06:44.200
 as the early steps of a new age of space exploration.

06:44.200 --> 06:46.640
 Yeah, I mean, during the launches itself,

06:46.640 --> 06:49.120
 so I mean, I think maybe some people will know,

06:49.120 --> 06:50.200
 but a lot of people don't know,

06:50.200 --> 06:52.120
 is like I'm actually the chief engineer of SpaceX,

06:52.120 --> 06:57.120
 so I've signed off on pretty much all the design decisions.

06:59.040 --> 07:03.120
 And so if there's something that goes wrong

07:03.120 --> 07:09.960
 with that vehicle, it's fundamentally my fault, so.

07:09.960 --> 07:13.800
 So I'm really just thinking about all the things that,

07:13.800 --> 07:15.600
 like, so when I see the rocket,

07:15.600 --> 07:17.320
 I see all the things that could go wrong

07:17.320 --> 07:18.520
 and the things that could be better

07:18.520 --> 07:21.480
 and the same with the Dragon spacecraft.

07:21.480 --> 07:23.440
 It's like, other people will see,

07:23.440 --> 07:25.600
 oh, this is a spacecraft or a rocket

07:25.600 --> 07:27.040
 and this looks really cool.

07:27.040 --> 07:29.600
 I'm like, I've like a readout of like,

07:29.600 --> 07:32.600
 these are the risks, these are the problems,

07:32.600 --> 07:33.840
 that's what I see.

07:33.840 --> 07:36.320
 Like, choo choo choo choo choo choo choo.

07:36.320 --> 07:38.840
 So it's not what other people see

07:38.840 --> 07:40.400
 when they see the product, you know.

07:40.400 --> 07:43.040
 So let me ask you then to analyze Starship

07:43.040 --> 07:44.360
 in that same way.

07:44.360 --> 07:46.520
 I know you have, you'll talk about,

07:46.520 --> 07:49.560
 in more detail about Starship in the near future, perhaps.

07:49.560 --> 07:51.800
 Yeah, we can talk about it now if you want.

07:51.800 --> 07:54.120
 But just in that same way, like you said,

07:54.120 --> 07:57.620
 you see, when you see a rocket,

07:57.620 --> 07:59.160
 you see sort of a list of risks.

07:59.160 --> 08:01.040
 In that same way, you said that Starship

08:01.040 --> 08:03.240
 is a really hard problem.

08:03.240 --> 08:05.480
 So there's many ways I can ask this,

08:05.480 --> 08:09.560
 but if you magically could solve one problem perfectly,

08:09.560 --> 08:11.740
 one engineering problem perfectly,

08:11.740 --> 08:13.000
 which one would it be?

08:13.000 --> 08:14.120
 On Starship?

08:14.120 --> 08:15.540
 On, sorry, on Starship.

08:15.540 --> 08:18.040
 So is it maybe related to the efficiency,

08:18.040 --> 08:21.360
 the engine, the weight of the different components,

08:21.360 --> 08:22.680
 the complexity of various things,

08:22.680 --> 08:26.720
 maybe the controls of the crazy thing it has to do to land?

08:26.720 --> 08:30.080
 No, it's actually by far the biggest thing

08:30.080 --> 08:33.720
 absorbing my time is engine production.

08:35.080 --> 08:36.480
 Not the design of the engine,

08:36.480 --> 08:41.480
 but I've often said prototypes are easy, production is hard.

08:45.360 --> 08:48.680
 So we have the most advanced rocket engine

08:48.680 --> 08:50.240
 that's ever been designed.

08:52.720 --> 08:55.260
 Cause I'd say currently the best rocket engine ever

08:55.260 --> 09:00.260
 is probably the RD180 or RD170,

09:00.320 --> 09:02.600
 that's the Russian engine basically.

09:03.640 --> 09:06.760
 And still, I think an engine should only count

09:06.760 --> 09:09.000
 if it's gotten something to orbit.

09:09.000 --> 09:12.720
 So our engine has not gotten anything to orbit yet,

09:12.720 --> 09:14.600
 but it is, it's the first engine

09:14.600 --> 09:19.600
 that's actually better than the Russian RD engines,

09:20.240 --> 09:22.680
 which were amazing design.

09:22.680 --> 09:24.120
 So you're talking about Raptor engine.

09:24.120 --> 09:25.520
 What makes it amazing?

09:25.520 --> 09:29.280
 What are the different aspects of it that make it,

09:29.280 --> 09:31.920
 like what are you the most excited about

09:31.920 --> 09:35.080
 if the whole thing works in terms of efficiency,

09:35.080 --> 09:37.120
 all those kinds of things?

09:37.120 --> 09:42.000
 Well, it's, the Raptor is a full flow

09:42.000 --> 09:46.360
 staged combustion engine

09:46.360 --> 09:50.840
 and it's operating at a very high chamber pressure.

09:50.840 --> 09:54.800
 So one of the key figures of merit, perhaps the key figure

09:54.800 --> 09:58.680
 of merit is what is the chamber pressure

09:58.680 --> 10:00.800
 at which the rocket engine can operate?

10:00.800 --> 10:03.040
 That's the combustion chamber pressure.

10:03.040 --> 10:06.960
 So Raptor is designed to operate at 300 bar,

10:06.960 --> 10:10.320
 possibly maybe higher, that's 300 atmospheres.

10:10.320 --> 10:15.320
 So the record right now for operational engine

10:15.880 --> 10:17.880
 is the RD engine that I mentioned, the Russian RD,

10:17.880 --> 10:21.300
 which is I believe around 267 bar.

10:22.560 --> 10:25.840
 And the difficulty of the chamber pressure

10:25.840 --> 10:27.860
 is increases on a nonlinear basis.

10:27.860 --> 10:32.860
 So 10% more chamber pressure is more like 50% more difficult.

10:36.960 --> 10:38.640
 But that chamber pressure is,

10:38.640 --> 10:42.920
 that is what allows you to get a very high power density

10:44.400 --> 10:45.220
 for the engine.

10:45.220 --> 10:50.220
 So enabling a very high thrust to weight ratio

10:53.300 --> 10:57.020
 and a very high specific impulse.

10:57.020 --> 10:59.820
 So specific impulse is like a measure of the efficiency

10:59.820 --> 11:01.700
 of a rocket engine.

11:01.700 --> 11:06.700
 It's really the effect of exhaust velocity

11:07.820 --> 11:10.120
 of the gas coming out of the engine.

11:10.120 --> 11:15.120
 So with a very high chamber pressure,

11:17.160 --> 11:21.600
 you can have a compact engine

11:21.600 --> 11:24.000
 that nonetheless has a high expansion ratio,

11:24.000 --> 11:29.000
 which is the ratio between the exit nozzle and the throat.

11:31.800 --> 11:35.640
 So you see a rocket engine's got sort of like a hourglass

11:35.640 --> 11:38.120
 shape, it's like a chamber and then it necks down

11:38.120 --> 11:41.880
 and there's a nozzle and the ratio of the exit diameter

11:41.880 --> 11:45.960
 to the throat is the expansion ratio.

11:45.960 --> 11:49.640
 So why is it such a hard engine to manufacture at scale?

11:51.400 --> 11:53.120
 It's very complex.

11:53.120 --> 11:55.200
 So a lot of, what is complexity mean here?

11:55.200 --> 11:56.760
 There's a lot of components involved.

11:56.760 --> 12:01.760
 There's a lot of components and a lot of unique materials

12:02.400 --> 12:07.040
 that, so we had to invent several alloys

12:07.040 --> 12:09.600
 that don't exist in order to make this engine work.

12:11.520 --> 12:13.040
 It's a materials problem too.

12:14.560 --> 12:19.560
 It's a materials problem and in a staged combustion,

12:19.760 --> 12:21.200
 a full flow staged combustion,

12:21.200 --> 12:24.160
 there are many feedback loops in the system.

12:24.160 --> 12:29.160
 So basically you've got propellants and hot gas

12:31.820 --> 12:36.820
 flowing simultaneously to so many different places

12:36.820 --> 12:41.820
 on the engine and they all have a recursive effect

12:42.740 --> 12:43.700
 on each other.

12:43.700 --> 12:45.780
 So you change one thing here, it has a recursive effect

12:45.780 --> 12:47.420
 here, it changes something over there

12:47.420 --> 12:51.380
 and it's quite hard to control.

12:52.740 --> 12:54.980
 Like there's a reason no one's made this before.

12:58.700 --> 13:03.700
 And the reason we're doing a staged combustion full flow

13:03.700 --> 13:08.700
 is because it has the highest possible efficiency.

13:12.740 --> 13:17.740
 So in order to make a fully reusable rocket,

13:19.380 --> 13:23.200
 which that's the really the holy grail of orbital rocketry.

13:25.340 --> 13:28.420
 You have to have, everything's gotta be the best.

13:28.420 --> 13:30.900
 It's gotta be the best engine, the best airframe,

13:30.900 --> 13:35.900
 the best heat shield, extremely light avionics,

13:38.100 --> 13:40.620
 very clever control mechanisms.

13:40.620 --> 13:45.100
 You've got to shed mass in any possible way that you can.

13:45.100 --> 13:47.620
 For example, we are, instead of putting landing legs

13:47.620 --> 13:49.580
 on the booster and ship, we are gonna catch them

13:49.580 --> 13:53.220
 with a tower to save the weight of the landing legs.

13:53.220 --> 13:58.220
 So that's like, I mean, we're talking about catching

13:58.220 --> 14:03.220
 the largest flying object ever made with,

14:03.340 --> 14:06.660
 on a giant tower with chopstick arms.

14:06.660 --> 14:09.660
 It's like Karate Kid with the fly, but much bigger.

14:12.060 --> 14:12.900
 I mean, pulling something like that home.

14:12.900 --> 14:15.460
 This probably won't work the first time.

14:17.620 --> 14:19.820
 Anyway, so this is bananas, this is banana stuff.

14:19.820 --> 14:23.100
 So you mentioned that you doubt, well, not you doubt,

14:23.100 --> 14:26.460
 but there's days or moments when you doubt

14:26.460 --> 14:28.340
 that this is even possible.

14:28.340 --> 14:30.120
 It's so difficult.

14:30.120 --> 14:34.120
 The possible part is, well, at this point,

14:35.420 --> 14:37.480
 I think we will get Starship to work.

14:41.380 --> 14:42.860
 There's a question of timing.

14:42.860 --> 14:45.460
 How long will it take us to do this?

14:45.460 --> 14:47.700
 How long will it take us to actually achieve

14:47.700 --> 14:49.060
 full and rapid reusability?

14:50.500 --> 14:52.680
 Cause it will take probably many launches

14:52.680 --> 14:56.760
 before we are able to have full and rapid reusability.

14:57.660 --> 15:01.540
 But I can say that the physics pencils out.

15:01.540 --> 15:06.540
 Like we're not, like at this point,

15:06.620 --> 15:10.100
 I'd say we're confident that, like let's say,

15:10.100 --> 15:12.620
 I'm very confident success is in the set

15:12.620 --> 15:13.820
 of all possible outcomes.

15:13.820 --> 15:18.340
 For a while there, I was not convinced

15:18.340 --> 15:20.820
 that success was in the set of possible outcomes,

15:20.820 --> 15:23.820
 which is very important actually.

15:23.820 --> 15:28.820
 But so we're saying there's a chance.

15:29.700 --> 15:31.500
 I'm saying there's a chance, exactly.

15:33.260 --> 15:37.100
 Just not sure how long it will take.

15:38.260 --> 15:39.580
 We have a very talented team.

15:39.580 --> 15:42.080
 They're working night and day to make it happen.

15:43.420 --> 15:48.260
 And like I said, the critical thing to achieve

15:48.260 --> 15:49.540
 for the revolution in space flight

15:49.540 --> 15:52.020
 and for humanity to be a spacefaring civilization

15:52.020 --> 15:54.380
 is to have a fully and rapidly reusable rocket,

15:54.380 --> 15:55.220
 orbital rocket.

15:56.500 --> 15:58.820
 There's not even been any orbital rocket

15:58.820 --> 16:00.100
 that's been fully reusable ever.

16:00.100 --> 16:05.040
 And this has always been the holy grail of rocketry.

16:06.020 --> 16:09.700
 And many smart people, very smart people,

16:09.700 --> 16:12.700
 have tried to do this before and they've not succeeded.

16:12.700 --> 16:16.100
 So, cause it's such a hard problem.

16:16.100 --> 16:21.100
 What's your source of belief in situations like this?

16:21.220 --> 16:23.580
 When the engineering problem is so difficult,

16:23.580 --> 16:27.500
 there's a lot of experts, many of whom you admire,

16:27.500 --> 16:29.220
 who have failed in the past.

16:29.220 --> 16:34.220
 And a lot of people, a lot of experts,

16:37.220 --> 16:38.860
 maybe journalists, all the kind of,

16:38.860 --> 16:40.900
 the public in general have a lot of doubt

16:40.900 --> 16:42.820
 about whether it's possible.

16:42.820 --> 16:47.340
 And you yourself know that even if it's a non null set,

16:47.340 --> 16:50.580
 non empty set of success, it's still unlikely

16:50.580 --> 16:52.140
 or very difficult.

16:52.140 --> 16:54.740
 Like where do you go to, both personally,

16:55.780 --> 16:58.920
 intellectually as an engineer, as a team,

16:58.920 --> 17:00.860
 like for source of strength needed

17:00.860 --> 17:02.560
 to sort of persevere through this.

17:02.560 --> 17:18.560
 And to keep going with the project, take it to completion.

17:18.560 --> 17:19.800
 A source of strength.

17:19.800 --> 17:21.880
 That's really not how I think about things.

17:23.600 --> 17:25.000
 I mean, for me, it's simply this,

17:25.000 --> 17:28.080
 this is something that is important to get done

17:28.080 --> 17:32.480
 and we should just keep doing it or die trying.

17:32.480 --> 17:35.920
 And I don't need a source of strength.

17:35.920 --> 17:39.000
 So quitting is not even like...

17:39.000 --> 17:41.520
 That's not, it's not in my nature.

17:41.520 --> 17:45.080
 And I don't care about optimism or pessimism.

17:46.240 --> 17:47.880
 Fuck that, we're gonna get it done.

17:47.880 --> 17:48.800
 Gonna get it done.

17:51.720 --> 17:55.160
 Can you then zoom back in to specific problems

17:55.160 --> 17:58.300
 with Starship or any engineering problems you work on?

17:58.300 --> 18:00.900
 Can you try to introspect your particular

18:00.900 --> 18:03.520
 biological neural network, your thinking process

18:03.520 --> 18:06.120
 and describe how you think through problems,

18:06.120 --> 18:07.960
 the different engineering and design problems?

18:07.960 --> 18:10.080
 Is there like a systematic process?

18:10.080 --> 18:11.880
 You've spoken about first principles thinking,

18:11.880 --> 18:14.140
 but is there a kind of process to it?

18:14.140 --> 18:19.140
 Well, yeah, like saying like physics is a law

18:19.200 --> 18:21.640
 and everything else is a recommendation.

18:21.640 --> 18:23.200
 Like I've met a lot of people that can break the law,

18:23.200 --> 18:25.700
 but I haven't met anyone who could break physics.

18:25.700 --> 18:30.700
 So the first, for any kind of technology problem,

18:32.520 --> 18:34.560
 you have to sort of just make sure

18:34.560 --> 18:37.880
 you're not violating physics.

18:39.960 --> 18:44.960
 And first principles analysis, I think,

18:45.120 --> 18:49.200
 is something that can be applied to really any walk of life,

18:49.200 --> 18:50.040
 anything really.

18:50.040 --> 18:54.680
 It's really just saying, let's boil something down

18:54.680 --> 18:58.440
 to the most fundamental principles.

18:58.440 --> 19:00.840
 The things that we are most confident are true

19:00.840 --> 19:02.480
 at a foundational level.

19:02.480 --> 19:05.040
 And that sets your axiomatic base,

19:05.040 --> 19:07.000
 and then you reason up from there,

19:07.000 --> 19:09.040
 and then you cross check your conclusion

19:09.040 --> 19:11.340
 against the axiomatic truths.

19:13.680 --> 19:18.120
 So some basics in physics would be like,

19:18.120 --> 19:20.280
 are you violating conservation of energy or momentum

19:20.280 --> 19:21.400
 or something like that?

19:21.400 --> 19:26.400
 Then it's not gonna work.

19:29.560 --> 19:32.960
 So that's just to establish, is it possible?

19:32.960 --> 19:34.760
 And another good physics tool

19:34.760 --> 19:36.480
 is thinking about things in the limit.

19:36.480 --> 19:38.360
 If you take a particular thing

19:38.360 --> 19:41.680
 and you scale it to a very large number

19:41.680 --> 19:44.840
 or to a very small number, how do things change?

19:45.960 --> 19:48.920
 Both like in number of things you manufacture

19:48.920 --> 19:51.640
 or something like that, and then in time?

19:51.640 --> 19:55.920
 Yeah, like let's say take an example of like manufacturing,

19:55.920 --> 19:59.140
 which I think is just a very underrated problem.

20:00.400 --> 20:05.140
 And like I said, it's much harder to

20:06.720 --> 20:09.520
 take an advanced technology product

20:09.520 --> 20:11.000
 and bring it into volume manufacturing

20:11.000 --> 20:12.880
 than it is to design it in the first place.

20:12.880 --> 20:14.480
 My orders of magnitude.

20:14.480 --> 20:17.800
 So let's say you're trying to figure out

20:17.800 --> 20:22.800
 is like why is this part or product expensive?

20:23.960 --> 20:27.400
 Is it because of something fundamentally foolish

20:27.400 --> 20:31.320
 that we're doing or is it because our volume is too low?

20:31.320 --> 20:32.840
 And so then you say, okay, well, what if our volume

20:32.840 --> 20:34.220
 was a million units a year?

20:34.220 --> 20:35.740
 Is it still expensive?

20:35.740 --> 20:38.120
 That's what I mean by thinking about things in the limit.

20:38.120 --> 20:40.140
 If it's still expensive at a million units a year,

20:40.140 --> 20:42.500
 then volume is not the reason why your thing is expensive.

20:42.500 --> 20:44.700
 There's something fundamental about the design.

20:44.700 --> 20:47.440
 And then you then can focus on reducing the complexity

20:47.440 --> 20:48.280
 or something like that in the design.

20:48.280 --> 20:50.600
 You gotta change the design to change the part

20:50.600 --> 20:55.600
 to be something that is not fundamentally expensive.

20:56.440 --> 20:58.920
 But like that's a common thing in rocketry

20:58.920 --> 21:01.840
 because the unit volume is relatively low.

21:01.840 --> 21:04.120
 And so a common excuse would be,

21:04.120 --> 21:06.480
 well, it's expensive because our unit volume is low.

21:06.480 --> 21:08.760
 And if we were in like automotive or something like that,

21:08.760 --> 21:10.920
 or consumer electronics, then our costs would be lower.

21:10.920 --> 21:13.080
 I'm like, okay, so let's say

21:13.080 --> 21:14.620
 now you're making a million units a year.

21:14.620 --> 21:16.080
 Is it still expensive?

21:16.080 --> 21:20.560
 If the answer is yes, then economies of scale

21:20.560 --> 21:22.120
 are not the issue.

21:22.120 --> 21:24.120
 Do you throw into manufacturing,

21:24.120 --> 21:26.080
 do you throw like supply chain?

21:26.080 --> 21:28.520
 Talked about resources and materials and stuff like that.

21:28.520 --> 21:29.960
 Do you throw that into the calculation

21:29.960 --> 21:31.800
 of trying to reason from first principles,

21:31.800 --> 21:34.640
 like how we're gonna make the supply chain work here?

21:34.640 --> 21:35.760
 Yeah, yeah.

21:35.760 --> 21:37.840
 And then the cost of materials, things like that.

21:37.840 --> 21:38.960
 Or is that too much?

21:38.960 --> 21:41.880
 Exactly, so like a good example,

21:41.880 --> 21:44.440
 I think of thinking about things in the limit

21:44.440 --> 21:49.440
 is if you take any machine or whatever,

21:54.360 --> 21:56.040
 like take a rocket or whatever,

21:56.040 --> 22:01.040
 and say, if you look at the raw materials in the rocket,

22:03.640 --> 22:07.560
 so you're gonna have like aluminum, steel, titanium,

22:07.560 --> 22:12.560
 Inconel, specialty alloys, copper,

22:12.560 --> 22:17.560
 and you say, what's the weight of the constituent elements,

22:19.200 --> 22:20.440
 of each of these elements,

22:20.440 --> 22:22.600
 and what is their raw material value?

22:22.600 --> 22:25.720
 And that sets the asymptotic limit

22:25.720 --> 22:29.480
 for how low the cost of the vehicle can be

22:29.480 --> 22:31.800
 unless you change the materials.

22:31.800 --> 22:33.960
 So, and then when you do that,

22:33.960 --> 22:35.760
 I call it like maybe the magic one number

22:35.760 --> 22:36.600
 or something like that.

22:36.600 --> 22:38.760
 So that would be like, if you had the,

22:40.320 --> 22:42.440
 like just a pile of these raw materials,

22:42.440 --> 22:43.680
 again, you could wave the magic wand

22:43.680 --> 22:45.920
 and rearrange the atoms into the final shape.

22:47.160 --> 22:49.600
 That would be the lowest possible cost

22:49.600 --> 22:51.000
 that you could make this thing for

22:51.000 --> 22:52.720
 unless you change the materials.

22:52.720 --> 22:56.400
 So then, and that is almost always a very low number.

22:57.760 --> 23:01.200
 So then what's actually causing things to be expensive

23:01.200 --> 23:04.080
 is how you put the atoms into the desired shape.

23:06.000 --> 23:09.000
 Yeah, actually, if you don't mind me taking a tiny tangent,

23:10.120 --> 23:11.440
 I often talk to Jim Keller,

23:11.440 --> 23:14.440
 who's somebody that worked with you as a friend.

23:14.440 --> 23:17.760
 Jim was, yeah, did great work at Tesla.

23:17.760 --> 23:20.480
 So I suppose he carries the flame

23:20.480 --> 23:22.680
 with the same kind of thinking

23:22.680 --> 23:24.560
 that you're talking about now.

23:26.240 --> 23:27.880
 And I guess I see that same thing

23:27.880 --> 23:31.800
 at Tesla and SpaceX folks who work there,

23:31.800 --> 23:33.760
 they kind of learn this way of thinking

23:33.760 --> 23:36.720
 and it kind of becomes obvious almost.

23:36.720 --> 23:39.680
 But anyway, I had argument, not argument,

23:39.680 --> 23:44.680
 but he educated me about how cheap it might be

23:44.800 --> 23:46.600
 to manufacture a Tesla bot.

23:46.600 --> 23:48.360
 We just, we had an argument.

23:48.360 --> 23:52.120
 How can you reduce the cost of scale of producing a robot?

23:52.120 --> 23:55.960
 Because I've gotten the chance to interact quite a bit,

23:55.960 --> 23:59.480
 obviously, in the academic circles with humanoid robots

23:59.480 --> 24:01.800
 and then Boston Dynamics and stuff like that.

24:01.800 --> 24:04.520
 And they're very expensive to build.

24:04.520 --> 24:07.600
 And then Jim kind of schooled me on saying like,

24:07.600 --> 24:10.080
 okay, like this kind of first principles thinking

24:10.080 --> 24:12.640
 of how can we get the cost of manufacturing down?

24:13.760 --> 24:14.720
 I suppose you do that,

24:14.720 --> 24:17.640
 you have done that kind of thinking for Tesla bot

24:17.640 --> 24:21.880
 and for all kinds of complex systems

24:21.880 --> 24:23.680
 that are traditionally seen as complex.

24:23.680 --> 24:26.280
 And you say, okay, how can we simplify everything down?

24:27.160 --> 24:30.320
 Yeah, I mean, I think if you are really good

24:30.320 --> 24:34.640
 at manufacturing, you can basically make,

24:34.640 --> 24:36.560
 at high volume, you can basically make anything

24:36.560 --> 24:40.640
 for a cost that asymptotically approaches

24:40.640 --> 24:42.880
 the raw material value of the constituents

24:42.880 --> 24:46.480
 plus any intellectual property that you need to do license.

24:46.480 --> 24:47.320
 Anything.

24:49.440 --> 24:50.280
 But it's hard.

24:50.280 --> 24:51.840
 It's not like that's a very hard thing to do,

24:51.840 --> 24:54.720
 but it is possible for anything.

24:54.720 --> 24:57.480
 Anything in volume can be made, like I said,

24:57.480 --> 25:00.440
 for a cost that asymptotically approaches

25:00.440 --> 25:02.760
 this raw material constituents

25:02.760 --> 25:05.360
 plus intellectual property license rights.

25:05.360 --> 25:08.520
 So what'll often happen in trying to design a product

25:08.520 --> 25:11.960
 is people will start with the tools and parts

25:11.960 --> 25:14.520
 and methods that they are familiar with

25:14.520 --> 25:17.480
 and then try to create the product

25:17.480 --> 25:19.560
 using their existing tools and methods.

25:21.240 --> 25:25.080
 The other way to think about it is actually imagine the,

25:25.080 --> 25:28.800
 try to imagine the platonic ideal of the perfect product

25:28.800 --> 25:31.360
 or technology, whatever it might be.

25:31.360 --> 25:35.680
 And say, what is the perfect arrangement of atoms

25:35.680 --> 25:38.560
 that would be the best possible product?

25:38.560 --> 25:39.840
 And now let us try to figure out

25:39.840 --> 25:41.540
 how to get the atoms in that shape.

25:43.880 --> 25:45.600
 I mean, it sounds,

25:48.000 --> 25:50.400
 it's almost like a Rick and Morty absurd

25:50.400 --> 25:52.080
 until you start to really think about it

25:52.080 --> 25:56.440
 and you really should think about it in this way

25:56.440 --> 25:59.000
 because everything else is kind of,

25:59.000 --> 26:03.000
 if you think, you might fall victim to the momentum

26:03.000 --> 26:04.440
 of the way things were done in the past

26:04.440 --> 26:06.000
 unless you think in this way.

26:06.000 --> 26:07.680
 Well, just as a function of inertia,

26:07.680 --> 26:10.640
 people will want to use the same tools and methods

26:10.640 --> 26:12.040
 that they are familiar with.

26:13.720 --> 26:15.720
 That's what they'll do by default.

26:15.720 --> 26:18.720
 And then that will lead to an outcome of things

26:18.720 --> 26:20.480
 that can be made with those tools and methods

26:20.480 --> 26:23.880
 but is unlikely to be the platonic ideal

26:23.880 --> 26:25.040
 of the perfect product.

26:25.040 --> 26:28.360
 So then, so that's why it's good to think of things

26:28.360 --> 26:29.200
 in both directions.

26:29.200 --> 26:31.240
 So like, what can we build with the tools that we have?

26:31.240 --> 26:34.600
 But then, but also what is the perfect,

26:34.600 --> 26:36.320
 the theoretical perfect product look like?

26:36.320 --> 26:38.600
 And that theoretical perfect part

26:38.600 --> 26:39.560
 is gonna be a moving target

26:39.560 --> 26:41.720
 because as you learn more,

26:41.720 --> 26:45.960
 the definition of that perfect product will change

26:45.960 --> 26:47.720
 because you don't actually know what the perfect product is,

26:47.720 --> 26:52.120
 but you can successfully approximate a more perfect product.

26:52.120 --> 26:54.520
 So think about it like that and then saying,

26:54.520 --> 26:57.320
 okay, now what tools, methods, materials,

26:57.320 --> 27:00.400
 whatever do we need to create

27:00.400 --> 27:02.360
 in order to get the atoms in that shape?

27:03.880 --> 27:07.880
 But people rarely think about it that way.

27:07.880 --> 27:09.440
 But it's a powerful tool.

27:10.480 --> 27:13.840
 I should mention that the brilliant Siobhan Ziles

27:13.840 --> 27:17.840
 is hanging out with us in case you hear a voice

27:17.840 --> 27:22.840
 of wisdom from outside, from up above.

27:23.720 --> 27:26.320
 Okay, so let me ask you about Mars.

27:26.320 --> 27:28.520
 You mentioned it would be great for science

27:28.520 --> 27:32.960
 to put a base on the moon to do some research.

27:32.960 --> 27:36.760
 But the truly big leap, again,

27:36.760 --> 27:38.920
 in this category of seemingly impossible,

27:38.920 --> 27:41.880
 is to put a human being on Mars.

27:41.880 --> 27:44.920
 When do you think SpaceX will land a human being on Mars?

27:44.920 --> 27:49.920
 Hmm, best case is about five years, worst case, 10 years.

27:54.480 --> 27:57.480
 Okay, so I'm gonna ask you about Mars.

27:57.480 --> 28:00.480
 You mentioned it would be great for science

28:00.480 --> 28:03.480
 to put a base on the moon to do some research.

28:03.480 --> 28:05.680
 But the truly big leap, again,

28:05.680 --> 28:08.760
 in this category of seemingly impossible,

28:08.760 --> 28:11.240
 is to put a human being on Mars.

28:11.240 --> 28:15.040
 But the truly big leap, again,

28:15.040 --> 28:18.040
 in this category of seemingly impossible,

28:18.040 --> 28:20.480
 is to put a human being on Mars.

28:20.480 --> 28:23.480
 When do you think SpaceX will land a human being on Mars?

28:23.480 --> 28:28.480
 Hmm, best case is about five years, worst case, 10 years.

28:31.760 --> 28:34.520
 What are the determining factors, would you say,

28:34.520 --> 28:36.240
 from an engineering perspective?

28:36.240 --> 28:37.840
 Or is that not the bottlenecks?

28:37.840 --> 28:40.760
 I don't know, order of magnitude or something like that.

28:40.760 --> 28:43.000
 It's a lot, it's really next level.

28:43.000 --> 28:48.000
 So, and the fundamental optimization of Starship

28:49.200 --> 28:51.480
 is minimizing cost per ton to orbit,

28:51.480 --> 28:54.760
 and ultimately cost per ton to the surface of Mars.

28:54.760 --> 28:56.320
 This may seem like a mercantile objective,

28:56.320 --> 28:59.240
 but it is actually the thing that needs to be optimized.

29:00.320 --> 29:04.000
 Like there is a certain cost per ton to the surface of Mars

29:04.000 --> 29:07.920
 where we can afford to establish a self sustaining city,

29:08.800 --> 29:11.560
 and then above that, we cannot afford to do it.

29:12.760 --> 29:16.760
 So right now, you couldn't fly to Mars for a trillion dollars.

29:16.760 --> 29:19.120
 No amount of money could get you a ticket to Mars.

29:19.120 --> 29:21.520
 So we need to get that above,

29:22.440 --> 29:25.360
 to get that something that is actually possible at all.

29:27.760 --> 29:32.240
 But that's, we don't just want to have,

29:32.240 --> 29:33.800
 with Mars, flags and footprints,

29:33.800 --> 29:35.880
 and then not come back for a half century,

29:35.880 --> 29:37.920
 like we did with the Moon.

29:37.920 --> 29:42.920
 In order to pass a very important, great filter,

29:43.360 --> 29:45.680
 I think we need to be a multi planet species.

29:48.320 --> 29:51.400
 This may sound somewhat esoteric to a lot of people,

29:51.400 --> 29:55.680
 but eventually, given enough time,

29:57.160 --> 30:00.480
 there's something, Earth is likely to experience

30:00.480 --> 30:05.200
 some calamity that could be something

30:05.200 --> 30:06.720
 that humans do to themselves,

30:06.720 --> 30:09.400
 or an external event like happened to the dinosaurs.

30:12.680 --> 30:17.680
 But eventually, if none of that happens,

30:17.920 --> 30:21.240
 and somehow magically we keep going,

30:21.240 --> 30:24.520
 then the Sun is gradually expanding,

30:24.520 --> 30:26.600
 and will engulf the Earth,

30:26.600 --> 30:31.080
 and probably Earth gets too hot for life

30:31.080 --> 30:34.800
 in about 500 million years.

30:34.800 --> 30:37.280
 It's a long time, but that's only 10% longer

30:37.280 --> 30:39.360
 than Earth has been around.

30:39.360 --> 30:43.240
 And so if you think about the current situation,

30:43.240 --> 30:45.640
 it's really remarkable, and kind of hard to believe,

30:45.640 --> 30:50.120
 but Earth's been around four and a half billion years,

30:50.120 --> 30:52.360
 and this is the first time in four and a half billion years

30:52.360 --> 30:55.720
 that it's been possible to extend life beyond Earth.

30:55.720 --> 30:58.120
 And that window of opportunity may be open

30:58.120 --> 30:59.520
 for a long time, and I hope it is,

30:59.520 --> 31:01.740
 but it also may be open for a short time.

31:01.740 --> 31:06.740
 And I think it is wise for us to act quickly

31:09.440 --> 31:13.800
 while the window is open, just in case it closes.

31:13.800 --> 31:17.800
 Yeah, the existence of nuclear weapons, pandemics,

31:17.800 --> 31:22.800
 all kinds of threats should kind of give us some motivation.

31:22.800 --> 31:27.800
 I mean, civilization could die with a bang or a whimper.

31:29.560 --> 31:32.760
 If it dies as a demographic collapse,

31:32.760 --> 31:35.160
 then it's more of a whimper, obviously.

31:35.160 --> 31:38.480
 And if it's World War III, it's more of a bang.

31:38.480 --> 31:40.040
 But these are all risks.

31:41.160 --> 31:42.000
 I mean, it's important to think of these things

31:42.000 --> 31:46.240
 and just think of things like probabilities, not certainties.

31:46.240 --> 31:47.680
 There's a certain probability

31:47.680 --> 31:50.200
 that something bad will happen on Earth.

31:50.200 --> 31:53.200
 I think most likely the future will be good.

31:53.200 --> 31:56.400
 But there's like, let's say for argument's sake,

31:56.400 --> 32:00.920
 a 1% chance per century of a civilization ending event.

32:00.920 --> 32:03.080
 Like that was Stephen Hawking's estimate.

32:05.080 --> 32:07.720
 I think he might be right about that.

32:07.720 --> 32:12.720
 So then we should basically think of this

32:15.440 --> 32:16.760
 being a multi planet species,

32:16.760 --> 32:18.600
 just like taking out a planet from the sky,

32:18.600 --> 32:20.720
 multi planet species, just like taking out insurance

32:20.720 --> 32:21.560
 for life itself.

32:21.560 --> 32:23.280
 Like life insurance for life.

32:27.280 --> 32:29.360
 It's turned into an infomercial real quick.

32:29.360 --> 32:31.600
 Life insurance for life, yes.

32:31.600 --> 32:36.240
 And we can bring the creatures from,

32:36.240 --> 32:38.040
 plants and animals from Earth to Mars

32:38.040 --> 32:39.440
 and breathe life into the planet

32:41.280 --> 32:43.720
 and have a second planet with life.

32:44.640 --> 32:46.000
 That would be great.

32:46.000 --> 32:47.560
 They can't bring themselves there.

32:47.560 --> 32:50.000
 So if we don't bring them to Mars,

32:50.000 --> 32:52.280
 then they will just for sure all die

32:52.280 --> 32:54.320
 when the sun expands anyway.

32:54.320 --> 32:56.200
 And then that'll be it.

32:56.200 --> 32:58.560
 What do you think is the most difficult aspect

32:58.560 --> 33:00.960
 of building a civilization on Mars?

33:00.960 --> 33:03.760
 Terraforming Mars, like from an engineering perspective,

33:03.760 --> 33:07.240
 from a financial perspective, human perspective,

33:07.240 --> 33:12.240
 to get a large number of folks there

33:13.000 --> 33:15.880
 who will never return back to Earth?

33:15.880 --> 33:17.160
 No, they could certainly return.

33:17.160 --> 33:18.440
 Some will return back to Earth.

33:18.440 --> 33:19.960
 They will choose to stay there

33:19.960 --> 33:21.280
 for the rest of their lives.

33:21.280 --> 33:22.160
 Yeah, many will.

33:23.760 --> 33:28.440
 But we need the spaceships back,

33:28.440 --> 33:29.480
 like the ones that go to Mars.

33:29.480 --> 33:30.320
 We need them back.

33:30.320 --> 33:32.680
 So you can hop on if you want.

33:32.680 --> 33:34.960
 But we can't just not have the spaceships come back.

33:34.960 --> 33:35.800
 Those things are expensive.

33:35.800 --> 33:36.640
 We need them back.

33:36.640 --> 33:38.760
 I'd like to come back and do another trip.

33:38.760 --> 33:40.680
 I mean, do you think about the terraforming aspect,

33:40.680 --> 33:41.640
 like actually building?

33:41.640 --> 33:44.560
 Are you so focused right now on the spaceships part

33:44.560 --> 33:46.960
 that's so critical to get to Mars?

33:46.960 --> 33:48.200
 We absolutely, if you can't get there,

33:48.200 --> 33:49.600
 nothing else matters.

33:49.600 --> 33:53.040
 So, and like I said, we can't get there

33:53.040 --> 33:54.680
 at some extraordinarily high cost.

33:54.680 --> 33:57.800
 I mean, the current cost of, let's say,

33:57.800 --> 34:00.080
 one ton to the surface of Mars

34:00.080 --> 34:02.680
 is on the order of a billion dollars.

34:02.680 --> 34:04.840
 So, because you don't just need the rocket

34:04.840 --> 34:05.680
 and the launch and everything,

34:05.680 --> 34:09.840
 you need like heat shield, you need guidance system,

34:09.840 --> 34:12.280
 you need deep space communications,

34:12.280 --> 34:15.000
 you need some kind of landing system.

34:15.000 --> 34:19.520
 So, like rough approximation would be a billion dollars

34:19.520 --> 34:22.200
 per ton to the surface of Mars right now.

34:22.200 --> 34:26.880
 This is obviously way too expensive

34:26.880 --> 34:29.060
 to create a self sustaining civilization.

34:30.680 --> 34:35.680
 So we need to improve that by at least a factor of 1,000.

34:36.840 --> 34:38.440
 A million per ton?

34:38.440 --> 34:40.880
 Yes, ideally much less than a million ton.

34:40.880 --> 34:44.280
 But if it's not, like it's gotta be,

34:44.280 --> 34:45.200
 so you have to say like, well,

34:45.200 --> 34:47.960
 how much can society afford to spend

34:47.960 --> 34:52.400
 or just want to spend on a self sustaining city on Mars?

34:52.400 --> 34:53.780
 The self sustaining part is important.

34:53.780 --> 34:56.240
 Like it's just the key threshold,

34:57.720 --> 35:01.360
 the great filter will have been passed

35:01.360 --> 35:05.280
 when the city on Mars can survive

35:05.280 --> 35:07.720
 even if the spaceships from Earth stop coming

35:07.720 --> 35:09.960
 for any reason, doesn't matter what the reason is,

35:09.960 --> 35:12.120
 but if they stop coming for any reason,

35:12.120 --> 35:13.720
 will it die out or will it not?

35:13.720 --> 35:16.400
 And if there's even one critical ingredient missing,

35:16.400 --> 35:18.320
 then it still doesn't count.

35:18.320 --> 35:20.440
 It's like, you know, if you're on a long sea voyage

35:20.440 --> 35:22.520
 and you've got everything except vitamin C,

35:23.920 --> 35:26.280
 it's only a matter of time, you know, you're gonna die.

35:26.280 --> 35:28.880
 So we're gonna get a Mars city

35:28.880 --> 35:30.840
 to the point where it's self sustaining.

35:32.260 --> 35:33.880
 I'm not sure this will really happen in my lifetime,

35:33.880 --> 35:37.320
 but I hope to see it at least have a lot of momentum.

35:37.320 --> 35:38.520
 And then you could say, okay,

35:38.520 --> 35:40.380
 what is the minimum tonnage necessary

35:40.380 --> 35:44.060
 to have a self sustaining city?

35:44.060 --> 35:46.480
 And there's a lot of uncertainty about this.

35:46.480 --> 35:48.560
 You could say like, I don't know,

35:48.560 --> 35:50.440
 it's probably at least a million tons

35:52.040 --> 35:55.440
 cause you have to set up a lot of infrastructure on Mars.

35:55.440 --> 35:58.680
 Like I said, you can't be missing anything

35:58.680 --> 36:00.960
 that in order to be self sustaining,

36:00.960 --> 36:02.420
 you can't be missing, like you need,

36:02.420 --> 36:07.120
 you know, semiconductor, fabs, you need iron ore refineries,

36:07.120 --> 36:09.360
 like you need lots of things, you know.

36:09.360 --> 36:13.640
 So, and Mars is not super hospitable.

36:13.640 --> 36:15.680
 It's the least inhospitable planet,

36:15.680 --> 36:18.120
 but it's definitely a fixer of a planet.

36:18.120 --> 36:19.360
 Outside of Earth.

36:19.360 --> 36:20.200
 Yes.

36:20.200 --> 36:21.020
 Earth is pretty good.

36:21.020 --> 36:22.280
 Earth is like easy, yeah.

36:22.280 --> 36:25.480
 And also I should, we should clarify in the solar system.

36:25.480 --> 36:26.600
 Yes, in the solar system.

36:26.600 --> 36:29.760
 There might be nice like vacation spots.

36:29.760 --> 36:31.160
 There might be some great planets out there,

36:31.160 --> 36:32.760
 but it's hopeless.

36:32.760 --> 36:33.800
 Too hard to get there?

36:33.800 --> 36:37.440
 Yeah, way, way, way, way, way too hard to say the least.

36:37.440 --> 36:39.720
 Let me push back on that, not really a push back,

36:39.720 --> 36:42.000
 but a quick curve ball of a question.

36:42.000 --> 36:44.840
 So you did mention physics as the first starting point.

36:44.840 --> 36:49.840
 So, general relativity allows for wormholes.

36:51.400 --> 36:53.080
 They technically can exist.

36:53.080 --> 36:55.720
 Do you think those can ever be leveraged

36:55.720 --> 36:58.080
 by humans to travel fast in the speed of light?

36:59.400 --> 37:03.600
 Well, the wormhole thing is debatable.

37:03.600 --> 37:08.200
 The, we currently do not know of any means

37:08.200 --> 37:10.200
 of going faster than the speed of light.

37:11.760 --> 37:16.760
 There is like, there are some ideas about having space.

37:21.400 --> 37:26.400
 Like so, you can only move at the speed of light

37:26.520 --> 37:30.740
 through space, but if you can make space itself move,

37:30.740 --> 37:34.520
 that's like, that's warping space.

37:36.140 --> 37:39.580
 Space is capable of moving faster than the speed of light.

37:39.580 --> 37:40.740
 Right.

37:40.740 --> 37:42.100
 Like the universe in the Big Bang,

37:42.100 --> 37:44.180
 the universe expanded at much,

37:44.180 --> 37:46.820
 much more than the speed of light by a lot.

37:46.820 --> 37:47.660
 Yeah.

37:48.820 --> 37:53.820
 So, but the, if this is possible,

37:56.380 --> 37:58.660
 the amount of energy required to warp space

37:58.660 --> 38:03.100
 is so gigantic, it boggles the mind.

38:03.100 --> 38:05.060
 So all the work you've done with propulsion,

38:05.060 --> 38:08.100
 how much innovation is possible with rocket propulsion?

38:08.100 --> 38:11.140
 Is this, I mean, you've seen it all,

38:11.140 --> 38:13.540
 and you're constantly innovating in every aspect.

38:14.420 --> 38:15.340
 How much is possible?

38:15.340 --> 38:17.380
 Like how much, can you get 10x somehow?

38:17.380 --> 38:19.660
 Is there something in there in physics

38:19.660 --> 38:21.260
 that you can get significant improvement

38:21.260 --> 38:22.620
 in terms of efficiency of engines

38:22.620 --> 38:24.640
 and all those kinds of things?

38:24.640 --> 38:27.940
 Well, as I was saying, really the Holy Grail

38:27.940 --> 38:31.500
 is a fully and rapidly reusable orbital system.

38:33.020 --> 38:38.020
 So right now, the Falcon 9

38:38.100 --> 38:41.460
 is the only reusable rocket out there,

38:41.460 --> 38:44.300
 but the booster comes back and lands,

38:44.300 --> 38:46.180
 and you've seen the videos,

38:46.180 --> 38:47.620
 and we get the nose cone fairing back,

38:47.620 --> 38:49.740
 but we do not get the upper stage back.

38:49.740 --> 38:54.260
 So that means that we have a minimum cost

38:54.260 --> 38:56.660
 of building an upper stage.

38:56.660 --> 38:59.300
 And you can think of like a two stage rocket

38:59.300 --> 39:00.700
 of sort of like two airplanes,

39:00.700 --> 39:03.180
 like a big airplane and a small airplane.

39:03.180 --> 39:04.600
 And we get the big airplane back,

39:04.600 --> 39:05.860
 but not the small airplane.

39:05.860 --> 39:07.860
 And so it still costs a lot.

39:07.860 --> 39:11.300
 So that upper stage is at least $10 million.

39:13.300 --> 39:15.460
 And then the degree of,

39:15.460 --> 39:19.060
 the booster is not as rapidly and completely reusable

39:19.060 --> 39:20.900
 as we'd like in order of the fairings.

39:20.900 --> 39:25.180
 So our kind of minimum marginal cost

39:25.180 --> 39:27.180
 and our counting overhead for per flight

39:27.180 --> 39:32.180
 is on the order of 15 to $20 million, maybe.

39:33.400 --> 39:38.060
 So that's extremely good for,

39:38.060 --> 39:40.660
 it's by far better than any rocket ever in history.

39:41.660 --> 39:45.380
 But with full and rapid reusability,

39:45.380 --> 39:48.840
 we can reduce the cost per ton to orbit

39:48.840 --> 39:51.880
 by a factor of 100.

39:51.880 --> 39:54.200
 But just think of it like,

39:54.200 --> 39:58.800
 like imagine if you had an aircraft or something or a car.

39:58.800 --> 40:02.920
 And if you had to buy a new car

40:02.920 --> 40:05.080
 every time you went for a drive,

40:05.080 --> 40:06.760
 that would be very expensive.

40:06.760 --> 40:08.320
 It'd be silly, frankly.

40:08.320 --> 40:13.320
 But in fact, you just refuel the car or recharge the car.

40:13.480 --> 40:18.480
 And that makes your trip like,

40:18.480 --> 40:20.320
 I don't know, a thousand times cheaper.

40:20.320 --> 40:23.800
 So it's the same for rockets.

40:23.800 --> 40:27.240
 If you, it's very difficult to make this complex machine

40:27.240 --> 40:28.680
 that can go to orbit.

40:28.680 --> 40:30.560
 And so if you cannot reuse it

40:30.560 --> 40:32.800
 and have to throw even any part of,

40:32.800 --> 40:34.000
 any significant part of it away,

40:34.000 --> 40:36.640
 that massively increases the cost.

40:36.640 --> 40:40.320
 So, you know, Starship in theory

40:40.320 --> 40:44.360
 could do a cost per launch of like a million,

40:44.360 --> 40:46.400
 maybe $2 million or something like that.

40:46.400 --> 40:51.400
 And put over a hundred tons in orbit, which is crazy.

40:53.560 --> 40:55.960
 Yeah, that's incredible.

40:55.960 --> 40:58.200
 So you're saying like it's by far the biggest bang

40:58.200 --> 41:00.720
 for the buck is to make it fully reusable

41:00.720 --> 41:04.160
 versus like some kind of brilliant breakthrough

41:04.160 --> 41:05.760
 in theoretical physics.

41:05.760 --> 41:07.680
 No, no, there's no, there's no brilliant break.

41:07.680 --> 41:11.240
 No, there's no, just make the rocket reusable.

41:11.240 --> 41:13.520
 This is an extremely difficult engineering problem.

41:13.520 --> 41:14.360
 Got it.

41:14.360 --> 41:17.960
 No new physics is required.

41:17.960 --> 41:19.320
 Just brilliant engineering.

41:19.320 --> 41:22.200
 Let me ask a slightly philosophical fun question.

41:22.200 --> 41:24.840
 Gotta ask, I know you're focused on getting to Mars,

41:24.840 --> 41:27.240
 but once we're there on Mars, what do you,

41:27.240 --> 41:32.240
 what form of government, economic system, political system

41:32.360 --> 41:33.760
 do you think would work best

41:33.760 --> 41:37.200
 for an early civilization of humans?

41:37.200 --> 41:41.560
 Is, I mean, the interesting reason to talk about this stuff,

41:41.560 --> 41:44.320
 it also helps people dream about the future.

41:44.320 --> 41:45.560
 I know you're really focused

41:45.560 --> 41:48.040
 about the short term engineering dream,

41:48.040 --> 41:49.320
 but it's like, I don't know,

41:49.320 --> 41:51.840
 there's something about imagining an actual civilization

41:51.840 --> 41:55.440
 on Mars that gives people, really gives people hope.

41:55.440 --> 41:57.680
 Well, it would be a new frontier and an opportunity

41:57.680 --> 41:59.640
 to rethink the whole nature of government,

41:59.640 --> 42:02.680
 just as was done in the creation of the United States.

42:02.680 --> 42:14.680
 So, I mean, I would suggest having a direct democracy,

42:14.680 --> 42:16.200
 people vote directly on things

42:16.200 --> 42:18.440
 as opposed to representative democracy.

42:18.440 --> 42:21.520
 So representative democracy, I think,

42:21.520 --> 42:25.120
 is too subject to a special interest

42:25.120 --> 42:29.580
 and a coercion of the politicians and that kind of thing.

42:29.580 --> 42:34.580
 So I'd recommend that there was just direct democracy,

42:39.360 --> 42:42.680
 people vote on laws, the population votes on laws themselves,

42:42.680 --> 42:44.520
 and then the laws must be short enough

42:44.520 --> 42:46.880
 that people can understand them.

42:46.880 --> 42:49.920
 Yeah, and then like keeping a well informed populace,

42:49.920 --> 42:52.240
 like really being transparent about all the information,

42:52.240 --> 42:53.080
 about what they're voting for.

42:53.080 --> 42:54.800
 Yeah, absolute transparency.

42:54.800 --> 42:57.480
 Yeah, and not make it as annoying as those cookies

42:57.480 --> 42:59.920
 we have to accept. Accept cookies.

42:59.920 --> 43:01.760
 I've always, like, you know,

43:01.760 --> 43:03.640
 there's like always like a slight amount of trepidation

43:03.640 --> 43:05.360
 when you click accept cookies,

43:05.360 --> 43:07.280
 like I feel as though there's like perhaps

43:07.280 --> 43:10.680
 like a very tiny chance that it'll open a portal to hell

43:10.680 --> 43:12.320
 or something like that.

43:12.320 --> 43:13.800
 That's exactly how I feel.

43:13.800 --> 43:16.640
 Why do they, why do they keep wanting me to accept it?

43:16.640 --> 43:19.000
 What do they want with this cookie?

43:19.000 --> 43:21.020
 Like somebody got upset with accepting cookies

43:21.020 --> 43:23.040
 or something somewhere, who cares?

43:23.040 --> 43:26.920
 Like so annoying to keep accepting all these cookies.

43:26.920 --> 43:29.600
 To me, this is just a great example.

43:29.600 --> 43:30.720
 Yes, you can have my damn cookie.

43:30.720 --> 43:32.360
 I don't care, whatever.

43:32.360 --> 43:33.720
 Heard it from Ilhan first.

43:33.720 --> 43:35.960
 He accepts all your damn cookies.

43:35.960 --> 43:38.920
 Yeah, and stop asking me.

43:40.240 --> 43:41.400
 It's annoying.

43:41.400 --> 43:46.000
 Yeah, it's one example of implementation

43:46.000 --> 43:50.200
 of a good idea done really horribly.

43:50.200 --> 43:51.480
 Yeah, it's somebody who was like,

43:51.480 --> 43:54.920
 there's some good intentions of like privacy or whatever,

43:54.920 --> 43:57.320
 but now everyone's just has to accept cookies

43:57.320 --> 43:59.360
 and it's not, you know, you have billions of people

43:59.360 --> 44:00.720
 who have to keep clicking accept cookie.

44:00.720 --> 44:02.440
 It's super annoying.

44:02.440 --> 44:05.000
 Then we just accept the damn cookie, it's fine.

44:05.000 --> 44:08.660
 There is like, I think a fundamental problem that we're,

44:08.660 --> 44:11.300
 because we've not really had a major,

44:12.320 --> 44:14.360
 like a world war or something like that in a while.

44:14.360 --> 44:16.920
 And obviously we would like to not have world wars.

44:18.320 --> 44:19.840
 There's not been a cleansing function

44:19.840 --> 44:21.420
 for rules and regulations.

44:21.420 --> 44:24.340
 So wars did have, you know, some sort of aligning

44:24.340 --> 44:27.580
 in that there would be a reset on rules

44:27.580 --> 44:29.660
 and regulations after a war.

44:29.660 --> 44:30.500
 So World Wars I and II,

44:30.500 --> 44:32.980
 there were huge resets on rules and regulations.

44:34.260 --> 44:37.900
 Now, if society does not have a war

44:37.900 --> 44:39.460
 and there's no cleansing function

44:39.460 --> 44:41.420
 or garbage collection for rules and regulations,

44:41.420 --> 44:43.500
 then rules and regulations will accumulate every year

44:43.500 --> 44:45.020
 because they're immortal.

44:45.020 --> 44:48.580
 There's no actual, humans die, but the laws don't.

44:48.580 --> 44:51.660
 So we need a garbage collection function

44:51.660 --> 44:52.980
 for rules and regulations.

44:52.980 --> 44:55.660
 They should not just be immortal

44:55.660 --> 44:57.340
 because some of the rules and regulations

44:57.340 --> 45:00.260
 that are put in place will be counterproductive,

45:00.260 --> 45:02.140
 done with good intentions, but counterproductive.

45:02.140 --> 45:03.940
 Sometimes not done with good intentions.

45:03.940 --> 45:08.940
 So if rules and regulations just accumulate every year

45:09.100 --> 45:10.740
 and you get more and more of them,

45:10.740 --> 45:13.220
 then eventually you won't be able to do anything.

45:13.220 --> 45:15.180
 You're just like Gulliver with, you know,

45:15.180 --> 45:17.420
 tied down by thousands of little structures

45:17.420 --> 45:19.620
 by thousands of little strings.

45:19.620 --> 45:24.620
 And we see that in, you know, US and like basically

45:26.460 --> 45:30.300
 all economies that have been around for a while

45:31.460 --> 45:35.340
 and regulators and legislators create new rules

45:35.340 --> 45:36.580
 and regulations every year,

45:36.580 --> 45:38.740
 but they don't put effort into removing them.

45:38.740 --> 45:40.300
 And I think that's very important that we put effort

45:40.300 --> 45:42.100
 into removing rules and regulations.

45:44.020 --> 45:45.620
 But it gets tough because you get special interests

45:45.620 --> 45:48.900
 that then are dependent on, like they have, you know,

45:48.900 --> 45:51.940
 a vested interest in that whatever rule and regulation

45:51.940 --> 45:55.620
 and then they fight to not get it removed.

45:57.620 --> 46:00.940
 Yeah, so I mean, I guess the problem with the constitution

46:00.940 --> 46:04.140
 is it's kind of like C versus Java

46:04.140 --> 46:06.740
 because it doesn't have any garbage collection built in.

46:06.740 --> 46:09.020
 I think there should be, when you first said

46:09.020 --> 46:10.860
 the metaphor of garbage collection, I loved it.

46:10.860 --> 46:12.380
 Yeah, from a coding standpoint.

46:12.380 --> 46:14.340
 From a coding standpoint, yeah, yeah.

46:14.340 --> 46:16.900
 It would be interesting if the laws themselves

46:16.900 --> 46:19.020
 kind of had a built in thing

46:19.020 --> 46:20.700
 where they kind of die after a while

46:20.700 --> 46:23.660
 unless somebody explicitly publicly defends them.

46:23.660 --> 46:26.220
 So that's sort of, it's not like somebody has to kill them.

46:26.220 --> 46:28.100
 They kind of die themselves.

46:28.100 --> 46:29.180
 They disappear.

46:29.180 --> 46:30.020
 Yeah.

46:32.540 --> 46:36.220
 Not to defend Java or anything, but you know, C++,

46:36.220 --> 46:38.540
 you know, you could also have a great garbage collection

46:38.540 --> 46:39.940
 in Python and so on.

46:39.940 --> 46:43.740
 Yeah, so yeah, something needs to happen

46:43.740 --> 46:48.580
 or just the civilization's arteries just harden over time

46:48.580 --> 46:50.820
 and you can just get less and less done

46:50.820 --> 46:53.220
 because there's just a rule against everything.

46:54.820 --> 46:57.900
 So I think like, I don't know, for Mars or whatever,

46:57.900 --> 47:00.260
 I'd say, or even for, you know, obviously for Earth as well,

47:00.260 --> 47:02.620
 like I think there should be an active process

47:02.620 --> 47:04.940
 for removing rules and regulations

47:04.940 --> 47:07.100
 and questioning their existence.

47:07.100 --> 47:10.260
 Just like if we've got a function

47:10.260 --> 47:11.580
 for creating rules and regulations,

47:11.580 --> 47:13.140
 because rules and regulations can also think of us

47:13.140 --> 47:15.780
 like they're like software or lines of code

47:15.780 --> 47:18.860
 for operating civilization.

47:18.860 --> 47:21.300
 That's the rules and regulations.

47:21.300 --> 47:22.980
 So it's not like we shouldn't have rules and regulations,

47:22.980 --> 47:27.100
 but you have code accumulation, but no code removal.

47:27.100 --> 47:31.460
 And so it just gets to become basically archaic bloatware

47:31.460 --> 47:32.300
 after a while.

47:33.620 --> 47:37.900
 And it's just, it makes it hard for things to progress.

47:37.900 --> 47:40.460
 So I don't know, maybe Mars, you'd have like,

47:40.460 --> 47:44.300
 you know, any given law must have a sunset, you know,

47:44.300 --> 47:49.300
 and require active voting to keep it up there, you know.

47:52.140 --> 47:54.660
 And I actually also say like, and these are just,

47:54.660 --> 47:57.020
 I don't know, recommendations or thoughts,

47:58.260 --> 48:00.700
 ultimately will be up to the people on Mars to decide.

48:00.700 --> 48:05.700
 But I think it should be easier to remove a law

48:06.660 --> 48:08.620
 than to add one because of the,

48:08.620 --> 48:10.700
 just to overcome the inertia of laws.

48:10.700 --> 48:15.220
 So maybe it's like, for argument's sake,

48:15.220 --> 48:19.780
 you need like say 60% vote to have a law take effect,

48:19.780 --> 48:22.020
 but only a 40% vote to remove it.

48:23.420 --> 48:26.620
 So let me be the guy, you posted a meme on Twitter recently

48:26.620 --> 48:30.140
 where there's like a row of urinals,

48:30.140 --> 48:33.100
 and a guy just walks all the way across,

48:33.100 --> 48:34.620
 and he tells you about crypto.

48:36.300 --> 48:38.340
 I mean, that's happened to me so many times.

48:38.340 --> 48:40.420
 I think maybe even literally.

48:40.420 --> 48:41.820
 Yeah.

48:41.820 --> 48:43.500
 Do you think, technologically speaking,

48:43.500 --> 48:47.340
 there's any room for ideas of smart contracts or so on?

48:47.340 --> 48:49.300
 Because you mentioned laws.

48:49.300 --> 48:52.980
 That's an interesting use of things like smart contracts

48:52.980 --> 48:56.000
 to implement the laws by which governments function.

48:57.300 --> 48:58.980
 Like something built on Ethereum,

48:58.980 --> 49:03.980
 or maybe a dog coin that enables smart contracts somehow.

49:04.860 --> 49:08.260
 I don't quite understand this whole smart contract thing.

49:08.260 --> 49:09.100
 You know.

49:09.960 --> 49:15.020
 I mean, I'm too dumb to understand smart contracts.

49:15.020 --> 49:16.020
 That's a good line.

49:17.900 --> 49:21.460
 I mean, my general approach to any kind of deal or whatever

49:21.460 --> 49:23.780
 is just make sure there's clarity of understanding.

49:23.780 --> 49:25.820
 That's the most important thing.

49:25.820 --> 49:29.660
 And just keep any kind of deal very short and simple,

49:29.660 --> 49:33.460
 plain language, and just make sure everyone understands

49:33.460 --> 49:36.580
 this is the deal, is it clear?

49:36.580 --> 49:40.660
 And what are the consequences if various things

49:40.660 --> 49:41.500
 don't happen?

49:42.620 --> 49:47.220
 But usually deals are, business deals or whatever,

49:47.220 --> 49:50.940
 are way too long and complex and overly lawyered

49:50.940 --> 49:51.880
 and pointlessly.

49:52.740 --> 49:57.020
 You mentioned that Doge is the people's coin.

49:57.020 --> 49:57.860
 Yeah.

49:57.860 --> 49:59.660
 And you said that you were literally going,

49:59.660 --> 50:04.660
 SpaceX may consider literally putting a Doge coin

50:04.660 --> 50:09.660
 on the moon, is this something you're still considering?

50:09.940 --> 50:13.620
 Mars, perhaps, do you think there's some chance,

50:13.620 --> 50:16.060
 we've talked about political systems on Mars,

50:16.060 --> 50:20.220
 that Doge coin is the official currency of Mars

50:20.220 --> 50:21.580
 at some point in the future?

50:22.580 --> 50:25.700
 Well, I think Mars itself will need to have

50:25.700 --> 50:29.060
 a different currency because you can't synchronize

50:29.060 --> 50:32.660
 due to speed of light, or not easily.

50:32.660 --> 50:35.020
 So it must be completely stand alone from Earth.

50:36.480 --> 50:41.340
 Well, yeah, because Mars is, at closest approach,

50:41.340 --> 50:43.020
 it's four light minutes away, roughly,

50:43.020 --> 50:45.580
 and then at furthest approach, it's roughly

50:45.580 --> 50:48.480
 20 light minutes away, maybe a little more.

50:50.100 --> 50:52.980
 So you can't really have something synchronizing

50:52.980 --> 50:55.500
 if you've got a 20 minute speed of light issue,

50:55.500 --> 50:58.180
 if it's got a one minute blockchain.

50:58.180 --> 50:59.980
 It's not gonna synchronize properly.

50:59.980 --> 51:03.960
 So Mars, I don't know if Mars would have

51:03.960 --> 51:07.640
 a cryptocurrency as a thing, but probably, seems likely.

51:07.640 --> 51:10.240
 But it would be some kind of localized thing on Mars.

51:12.280 --> 51:13.880
 And you let the people decide.

51:14.800 --> 51:16.500
 Yeah, absolutely.

51:17.680 --> 51:20.720
 The future of Mars should be up to the Martians.

51:20.720 --> 51:25.720
 Yeah, so, I think the cryptocurrency thing

51:25.720 --> 51:30.720
 is an interesting approach to reducing

51:30.800 --> 51:35.800
 the error in the database that is called money.

51:41.400 --> 51:42.920
 I think I have a pretty deep understanding

51:42.920 --> 51:46.720
 of what money actually is on a practical day to day basis

51:46.720 --> 51:48.040
 because of PayPal.

51:50.400 --> 51:52.920
 We really got in deep there.

51:52.920 --> 51:57.560
 And right now, the money system, actually,

51:57.560 --> 52:01.120
 for practical purposes, is really a bunch

52:01.120 --> 52:05.780
 of heterogeneous mainframes running old COBOL.

52:07.480 --> 52:08.720
 Okay, you mean literally.

52:08.720 --> 52:10.840
 That's literally what's happening.

52:10.840 --> 52:11.680
 In batch mode.

52:12.760 --> 52:13.600
 Okay.

52:13.600 --> 52:14.420
 In batch mode.

52:14.420 --> 52:16.720
 Yeah, pretty the poor bastards who have

52:16.720 --> 52:19.000
 to maintain that code.

52:19.000 --> 52:22.240
 Okay, that's a pain.

52:22.240 --> 52:24.240
 Not even Fortran, it's COBOL.

52:24.240 --> 52:25.080
 It's COBOL.

52:26.040 --> 52:30.120
 And the banks are still buying mainframes in 2021

52:30.120 --> 52:32.020
 and running ancient COBOL code.

52:33.000 --> 52:37.800
 And the Federal Reserve is probably even older

52:37.800 --> 52:39.280
 than what the banks have, and they have

52:39.280 --> 52:41.080
 an old COBOL mainframe.

52:41.880 --> 52:46.880
 And so, the government effectively has editing privileges

52:47.080 --> 52:48.660
 on the money database.

52:48.660 --> 52:53.660
 And they use those editing privileges to make more money,

52:53.740 --> 52:55.420
 whatever they want.

52:55.420 --> 52:59.060
 And this increases the error in the database that is money.

52:59.060 --> 53:00.820
 So, I think money should really be viewed

53:00.820 --> 53:03.580
 through the lens of information theory.

53:03.580 --> 53:08.300
 And so, it's kind of like an internet connection.

53:08.300 --> 53:12.580
 Like what's the bandwidth, total bit rate,

53:12.580 --> 53:16.420
 what is the latency, jitter, packet drop,

53:16.420 --> 53:21.420
 you know, errors in network communication.

53:21.820 --> 53:24.460
 Just think of money like that, basically.

53:24.460 --> 53:26.460
 I think that's probably the right way to think of it.

53:26.460 --> 53:31.460
 And then say what system from an information theory

53:31.460 --> 53:34.380
 standpoint allows an economy to function the best.

53:35.620 --> 53:40.620
 And, you know, crypto is an attempt to reduce

53:40.620 --> 53:45.620
 the error in money that is contributed

53:48.780 --> 53:53.300
 by governments diluting the money supply

53:53.300 --> 53:57.220
 as basically a pernicious form of taxation.

53:58.900 --> 54:01.900
 So, both policy in terms of with inflation

54:01.900 --> 54:05.780
 and actual like technological COBOL,

54:05.780 --> 54:08.880
 like cryptocurrency takes us into the 21st century

54:08.880 --> 54:10.740
 in terms of the actual systems

54:10.740 --> 54:12.140
 that allow you to do the transaction,

54:12.140 --> 54:14.180
 to store wealth, all those kinds of things.

54:16.920 --> 54:18.580
 Like I said, just think of money as information.

54:18.580 --> 54:20.900
 People often will think of money

54:20.900 --> 54:22.740
 as having power in and of itself.

54:24.100 --> 54:24.980
 It does not.

54:24.980 --> 54:28.980
 Money is information and it does not have power

54:28.980 --> 54:29.900
 in and of itself.

54:31.460 --> 54:35.060
 Like, you know, applying the physics tools

54:35.060 --> 54:37.540
 of thinking about things in the limit is helpful.

54:37.540 --> 54:39.980
 If you are stranded on a tropical island

54:41.020 --> 54:45.500
 and you have a trillion dollars, it's useless

54:47.660 --> 54:50.540
 because there's no resource allocation.

54:50.540 --> 54:52.660
 Money is a database for resource allocation,

54:52.660 --> 54:55.020
 but there's no resource to allocate except yourself.

54:55.020 --> 54:56.060
 So, money is useless.

55:01.020 --> 55:04.100
 If you're stranded on a desert island with no food,

55:04.100 --> 55:09.100
 all the Bitcoin in the world will not stop you

55:10.340 --> 55:11.180
 from starving.

55:12.420 --> 55:17.420
 So, just think of money as a database

55:20.820 --> 55:24.020
 for resource allocation across time and space.

55:24.980 --> 55:29.980
 And then what system, in what form

55:29.980 --> 55:34.980
 should that database or data system,

55:37.020 --> 55:39.060
 what would be most effective?

55:39.060 --> 55:41.380
 Now, there is a fundamental issue

55:41.380 --> 55:44.940
 with say Bitcoin in its current form

55:46.020 --> 55:48.660
 in that the transaction volume is very limited

55:50.700 --> 55:55.700
 and the latency for a properly confirmed transaction

55:55.700 --> 55:58.100
 is too long, much longer than you'd like.

55:58.100 --> 56:02.460
 So, it's actually not great from a transaction volume

56:02.460 --> 56:04.460
 standpoint or a latency standpoint.

56:07.260 --> 56:12.260
 So, it is perhaps useful to solve an aspect

56:12.420 --> 56:17.420
 of the money database problem, which is the sort of store

56:17.420 --> 56:22.020
 of wealth or an accounting of relative obligations,

56:22.020 --> 56:27.020
 I suppose, but it is not useful as a currency,

56:27.500 --> 56:28.780
 as a day to day currency.

56:28.780 --> 56:31.260
 But people have proposed different technological solutions.

56:31.260 --> 56:32.100
 Like Lightning.

56:32.100 --> 56:34.740
 Yeah, Lightning Network and the layer two technologies

56:34.740 --> 56:35.580
 on top of that.

56:35.580 --> 56:38.820
 I mean, it seems to be all kind of a trade off,

56:38.820 --> 56:41.060
 but the point is, it's kind of brilliant to say

56:41.060 --> 56:42.460
 that just think about information,

56:42.460 --> 56:44.100
 think about what kind of database,

56:44.100 --> 56:45.860
 what kind of infrastructure enables

56:45.860 --> 56:46.700
 that exchange of information.

56:46.700 --> 56:49.180
 Yeah, just say like you're operating an economy

56:49.180 --> 56:54.180
 and you need to have some thing that allows

56:55.260 --> 56:59.740
 for the efficient, to have efficient value ratios

56:59.740 --> 57:01.380
 between products and services.

57:01.380 --> 57:03.220
 So, you've got this massive number of products

57:03.220 --> 57:06.380
 and services and you need to, you can't just barter.

57:06.380 --> 57:09.620
 It's just like, that would be extremely unwieldy.

57:09.620 --> 57:13.420
 So, you need something that gives you the ratio

57:13.420 --> 57:18.420
 of exchange between goods and services.

57:20.540 --> 57:22.580
 And then something that allows you

57:22.580 --> 57:26.580
 to shift obligations across time, like debt.

57:26.580 --> 57:29.180
 Debt and equity shift obligations across time.

57:29.180 --> 57:31.340
 Then what does the best job of that?

57:33.300 --> 57:36.020
 Part of the reason why I think there's some

57:36.020 --> 57:38.660
 Merit Doge coin, even though it was obviously created

57:38.660 --> 57:43.660
 as a joke, is that it actually does have

57:44.100 --> 57:48.340
 a much higher transaction volume capability than Bitcoin.

57:49.820 --> 57:53.180
 And the costs of doing a transaction,

57:53.180 --> 57:55.980
 the Doge coin fee is very low.

57:55.980 --> 57:58.220
 Like right now, if you want to do a Bitcoin transaction,

57:58.220 --> 58:00.460
 the price of doing that transaction is very high.

58:00.460 --> 58:04.340
 So, you could not use it effectively for most things.

58:04.340 --> 58:09.340
 And nor could it even scale to a high volume.

58:11.860 --> 58:15.260
 And when Bitcoin started, I guess around 2008

58:15.260 --> 58:18.740
 or something like that, the internet connections

58:18.740 --> 58:20.820
 were much worse than they are today.

58:20.820 --> 58:23.620
 Like Order of magnitude, I mean,

58:23.620 --> 58:26.860
 just way, way worse in 2008.

58:26.860 --> 58:31.860
 So, like having a small block size or whatever

58:31.860 --> 58:36.860
 is, and a long synchronization time made sense in 2008.

58:37.820 --> 58:41.380
 But 2021, or fast forward 10 years,

58:41.380 --> 58:45.380
 it's like comically low.

58:50.540 --> 58:55.540
 And I think there's some value to having a linear increase

58:55.580 --> 58:58.620
 in the amount of currency that is generated.

58:58.620 --> 59:01.060
 So, because some amount of the currency,

59:01.060 --> 59:05.060
 like if a currency is too deflationary,

59:05.060 --> 59:10.060
 or I should say, if a currency is expected

59:10.140 --> 59:11.700
 to increase in value over time,

59:11.700 --> 59:14.220
 there's reluctance to spend it.

59:14.220 --> 59:17.380
 Because you're like, oh, I'll just hold it and not spend it

59:17.380 --> 59:19.380
 because it's scarcity is increasing with time.

59:19.380 --> 59:22.180
 So, if I spend it now, then I will regret spending it.

59:22.180 --> 59:24.980
 So, I will just, you know, hodl it.

59:24.980 --> 59:27.980
 But if there's some dilution of the currency occurring

59:27.980 --> 59:29.860
 over time, that's more of an incentive

59:29.860 --> 59:31.260
 to use it as a currency.

59:31.260 --> 59:36.260
 So, those coins, somewhat randomly has just a fixed,

59:39.220 --> 59:43.220
 a number of sort of coins or hash strings

59:43.220 --> 59:46.540
 that are generated every year.

59:46.540 --> 59:49.860
 So, there's some inflation, but it's not a percentage base.

59:49.860 --> 59:54.340
 It's a percentage of the total amount of money

59:54.340 --> 59:55.700
 it's a fixed number.

59:55.700 --> 59:58.380
 So, the percentage of inflation

59:58.380 --> 1:00:00.140
 will necessarily decline over time.

1:00:02.700 --> 1:00:06.340
 So, I'm not saying that it's like the ideal system

1:00:06.340 --> 1:00:09.500
 for a currency, but I think it actually is

1:00:09.500 --> 1:00:13.460
 just fundamentally better than anything else I've seen

1:00:13.460 --> 1:00:16.420
 just by accident, so.

1:00:16.420 --> 1:00:19.740
 I like how you said around 2008.

1:00:19.740 --> 1:00:23.420
 So, you're not, you know, some people suggested

1:00:23.420 --> 1:00:24.820
 you might be Satoshi Nakamoto.

1:00:24.820 --> 1:00:26.340
 You've previously said you're not.

1:00:26.340 --> 1:00:27.180
 Let me ask.

1:00:27.180 --> 1:00:28.780
 You're not for sure.

1:00:28.780 --> 1:00:30.140
 Would you tell us if you were?

1:00:30.140 --> 1:00:30.980
 Yes.

1:00:30.980 --> 1:00:31.820
 Okay.

1:00:33.460 --> 1:00:34.780
 Do you think it's a feature or a bug

1:00:34.780 --> 1:00:37.260
 that he's anonymous or she or they?

1:00:38.980 --> 1:00:41.700
 It's an interesting kind of quirk of human history

1:00:41.700 --> 1:00:43.580
 that there is a particular technology

1:00:43.580 --> 1:00:46.140
 that is a completely anonymous inventor

1:00:46.140 --> 1:00:51.140
 or creator.

1:01:03.380 --> 1:01:08.020
 Well, I mean, you can look at the evolution of ideas

1:01:10.060 --> 1:01:11.860
 before the launch of Bitcoin

1:01:11.860 --> 1:01:16.860
 and see who wrote, you know, about those ideas.

1:01:19.540 --> 1:01:21.900
 And then, like, I don't know exactly,

1:01:21.900 --> 1:01:24.180
 obviously I don't know who created Bitcoin

1:01:24.180 --> 1:01:25.180
 for practical purposes,

1:01:25.180 --> 1:01:28.980
 but the evolution of ideas is pretty clear for that.

1:01:28.980 --> 1:01:31.940
 And like, it seems as though like Nick Szabo

1:01:31.940 --> 1:01:35.260
 is probably more than anyone else responsible

1:01:35.260 --> 1:01:37.100
 for the evolution of those ideas.

1:01:37.100 --> 1:01:41.100
 So, he claims not to be Satoshi Nakamoto,

1:01:41.100 --> 1:01:44.460
 but I'm not sure that's neither here nor there,

1:01:44.460 --> 1:01:47.700
 but he seems to be the one more responsible

1:01:47.700 --> 1:01:50.340
 for the ideas behind Bitcoin than anyone else.

1:01:50.340 --> 1:01:52.820
 So, it's not perhaps like singular figures

1:01:52.820 --> 1:01:55.740
 aren't even as important as the figures involved

1:01:55.740 --> 1:01:58.100
 in the evolution of ideas, the Leto thing, so.

1:01:58.100 --> 1:01:58.940
 Yeah.

1:01:58.940 --> 1:02:02.260
 Yeah, it's, you know, perhaps it's sad

1:02:02.260 --> 1:02:03.100
 to think about history,

1:02:03.100 --> 1:02:06.340
 but maybe most names will be forgotten anyway.

1:02:06.340 --> 1:02:07.460
 What is a name anyway?

1:02:07.460 --> 1:02:11.260
 It's a name attached to an idea.

1:02:11.260 --> 1:02:13.700
 What does it even mean, really?

1:02:13.700 --> 1:02:16.260
 I think Shakespeare had a thing about roses and stuff,

1:02:16.260 --> 1:02:17.220
 whatever he said.

1:02:17.220 --> 1:02:19.500
 A rose by any other name, it smells sweet.

1:02:22.420 --> 1:02:24.340
 I got Elon to quote Shakespeare.

1:02:24.340 --> 1:02:26.900
 I feel like I accomplished something today.

1:02:26.900 --> 1:02:28.860
 Shall I compare thee to a summer's day?

1:02:28.860 --> 1:02:30.820
 What?

1:02:30.820 --> 1:02:31.820
 I'm gonna clip that out.

1:02:31.820 --> 1:02:33.980
 I said it to people.

1:02:33.980 --> 1:02:38.980
 Not more temperate and more fair.

1:02:39.060 --> 1:02:40.620
 Autopilot.

1:02:40.620 --> 1:02:41.460
 Tesla autopilot.

1:02:46.140 --> 1:02:48.500
 Tesla autopilot has been through an incredible journey

1:02:48.500 --> 1:02:50.540
 over the past six years,

1:02:50.540 --> 1:02:52.780
 or perhaps even longer in the minds of,

1:02:52.780 --> 1:02:55.180
 in your mind and the minds of many involved.

1:02:57.020 --> 1:02:58.980
 Yeah, I think that's where we first like connected really

1:02:58.980 --> 1:03:01.900
 was the autopilot stuff, autonomy and.

1:03:01.900 --> 1:03:05.220
 The whole journey was incredible to me to watch.

1:03:05.220 --> 1:03:10.220
 I was, because I knew, well, part of it is I was at MIT

1:03:10.340 --> 1:03:13.180
 and I knew the difficulty of computer vision.

1:03:13.180 --> 1:03:15.780
 And I knew the whole, I had a lot of colleagues and friends

1:03:15.780 --> 1:03:18.420
 about the DARPA challenge and knew how difficult it is.

1:03:18.420 --> 1:03:20.140
 And so there was a natural skepticism

1:03:20.140 --> 1:03:23.700
 when I first drove a Tesla with the initial system

1:03:23.700 --> 1:03:25.140
 based on Mobileye.

1:03:25.140 --> 1:03:28.900
 I thought there's no way, so first when I got in,

1:03:28.900 --> 1:03:32.580
 I thought there's no way this car could maintain,

1:03:32.580 --> 1:03:35.900
 like stay in the lane and create a comfortable experience.

1:03:35.900 --> 1:03:39.500
 So my intuition initially was that the lane keeping problem

1:03:39.500 --> 1:03:41.780
 is way too difficult to solve.

1:03:41.780 --> 1:03:43.900
 Oh, lane keeping, yeah, that's relatively easy.

1:03:43.900 --> 1:03:47.820
 Well, like, but not this, but solve in the way

1:03:47.820 --> 1:03:50.860
 that we just, we talked about previous is prototype

1:03:50.860 --> 1:03:54.380
 versus a thing that actually creates a pleasant experience

1:03:54.380 --> 1:03:57.460
 over hundreds of thousands of miles or millions.

1:03:57.460 --> 1:04:00.460
 Yeah, so we had to wrap a lot of code

1:04:00.460 --> 1:04:01.740
 around the Mobileye thing.

1:04:01.740 --> 1:04:04.380
 It doesn't just work by itself.

1:04:04.380 --> 1:04:06.420
 I mean, that's part of the story

1:04:06.420 --> 1:04:07.980
 of how you approach things sometimes.

1:04:07.980 --> 1:04:09.680
 Sometimes you do things from scratch.

1:04:09.680 --> 1:04:12.340
 Sometimes at first you kind of see what's out there

1:04:12.340 --> 1:04:14.340
 and then you decide to do from scratch.

1:04:14.340 --> 1:04:17.180
 That was one of the boldest decisions I've seen

1:04:17.180 --> 1:04:18.820
 is both in the hardware and the software

1:04:18.820 --> 1:04:21.020
 to decide to eventually go from scratch.

1:04:21.020 --> 1:04:22.700
 I thought, again, I was skeptical

1:04:22.700 --> 1:04:24.500
 of whether that's going to be able to work out

1:04:24.500 --> 1:04:26.860
 because it's such a difficult problem.

1:04:26.860 --> 1:04:28.900
 And so it was an incredible journey

1:04:28.900 --> 1:04:31.460
 what I see now with everything,

1:04:31.460 --> 1:04:33.220
 the hardware, the compute, the sensors,

1:04:33.220 --> 1:04:37.300
 the things I maybe care and love about most

1:04:37.300 --> 1:04:40.060
 is the stuff that Andre Karpathy is leading

1:04:40.060 --> 1:04:41.740
 with the data set selection,

1:04:41.740 --> 1:04:43.100
 the whole data engine process,

1:04:43.100 --> 1:04:45.020
 the neural network architectures,

1:04:45.020 --> 1:04:47.300
 the way that's in the real world,

1:04:47.300 --> 1:04:49.380
 that network is tested, validated,

1:04:49.380 --> 1:04:50.900
 all the different test sets,

1:04:52.340 --> 1:04:54.740
 versus the ImageNet model of computer vision,

1:04:54.740 --> 1:04:58.340
 like what's in academia is like real world

1:04:58.340 --> 1:04:59.720
 artificial intelligence.

1:05:01.340 --> 1:05:04.220
 And Andre's awesome and obviously plays an important role,

1:05:04.220 --> 1:05:07.780
 but we have a lot of really talented people driving things.

1:05:09.540 --> 1:05:12.760
 And Ashok is actually the head of autopilot engineering.

1:05:14.820 --> 1:05:16.380
 Andre's the director of AI.

1:05:16.380 --> 1:05:17.700
 AI stuff, yeah, yeah.

1:05:17.700 --> 1:05:20.580
 So yeah, I'm aware that there's an incredible team

1:05:20.580 --> 1:05:22.060
 of just a lot going on.

1:05:22.060 --> 1:05:26.060
 Yeah, obviously people will give me too much credit

1:05:26.060 --> 1:05:28.700
 and they'll give Andre too much credit, so.

1:05:28.700 --> 1:05:31.700
 And people should realize how much is going on

1:05:31.700 --> 1:05:32.540
 under the hood.

1:05:32.540 --> 1:05:34.940
 Yeah, it's just a lot of really talented people.

1:05:36.540 --> 1:05:40.020
 The Tesla Autopilot AI team is extremely talented.

1:05:40.020 --> 1:05:42.620
 It's like some of the smartest people in the world.

1:05:43.700 --> 1:05:45.060
 So yeah, we're getting it done.

1:05:45.060 --> 1:05:47.660
 What are some insights you've gained

1:05:47.660 --> 1:05:51.300
 over those five, six years of autopilot

1:05:51.300 --> 1:05:54.260
 about the problem of autonomous driving?

1:05:54.260 --> 1:05:58.580
 So you leaped in having some sort of

1:05:58.580 --> 1:06:00.820
 first principles kinds of intuitions,

1:06:00.820 --> 1:06:05.340
 but nobody knows how difficult the problem, like the problem.

1:06:05.340 --> 1:06:07.140
 I thought the self driving problem would be hard,

1:06:07.140 --> 1:06:08.980
 but it was harder than I thought.

1:06:08.980 --> 1:06:09.980
 It's not like I thought it would be easy.

1:06:09.980 --> 1:06:10.800
 I thought it would be very hard,

1:06:10.800 --> 1:06:14.220
 but it was actually way harder than even that.

1:06:14.220 --> 1:06:17.060
 So, I mean, what it comes down to at the end of the day

1:06:17.060 --> 1:06:18.760
 is to solve self driving,

1:06:18.760 --> 1:06:23.760
 you basically need to recreate what humans do to drive,

1:06:28.680 --> 1:06:31.480
 which is humans drive with optical sensors,

1:06:31.480 --> 1:06:33.800
 eyes and biological neural nets.

1:06:34.880 --> 1:06:36.420
 And so in order to,

1:06:36.420 --> 1:06:39.120
 that's how the entire road system is designed to work

1:06:39.120 --> 1:06:43.920
 with basically passive optical and neural nets,

1:06:45.360 --> 1:06:46.200
 biologically.

1:06:46.200 --> 1:06:47.920
 And now that we need to,

1:06:47.920 --> 1:06:50.080
 so for actually for full self driving to work,

1:06:50.080 --> 1:06:52.080
 we have to recreate that in digital form.

1:06:52.960 --> 1:06:57.960
 So we have to, that means cameras with advanced neural nets

1:07:01.520 --> 1:07:06.400
 in silicon form, and then it will obviously solve

1:07:06.400 --> 1:07:08.000
 for full self driving.

1:07:08.000 --> 1:07:09.000
 That's the only way.

1:07:09.000 --> 1:07:10.320
 I don't think there's any other way.

1:07:10.320 --> 1:07:12.920
 But the question is what aspects of human nature

1:07:12.920 --> 1:07:15.560
 do you have to encode into the machine, right?

1:07:15.560 --> 1:07:18.720
 Do you have to solve the perception problem, like detect?

1:07:18.720 --> 1:07:21.320
 And then you first realize

1:07:21.320 --> 1:07:23.080
 what is the perception problem for driving,

1:07:23.080 --> 1:07:25.400
 like all the kinds of things you have to be able to see,

1:07:25.400 --> 1:07:27.900
 like what do we even look at when we drive?

1:07:27.900 --> 1:07:32.440
 There's, I just recently heard Andre talked about MIT

1:07:32.440 --> 1:07:33.720
 about car doors.

1:07:33.720 --> 1:07:36.080
 I think it was the world's greatest talk of all time

1:07:36.080 --> 1:07:41.080
 about car doors, the fine details of car doors.

1:07:41.380 --> 1:07:44.440
 Like what is even an open car door, man?

1:07:44.440 --> 1:07:46.880
 So like the ontology of that,

1:07:46.880 --> 1:07:48.000
 that's a perception problem.

1:07:48.000 --> 1:07:49.880
 We humans solve that perception problem,

1:07:49.880 --> 1:07:51.640
 and Tesla has to solve that problem.

1:07:51.640 --> 1:07:53.380
 And then there's the control and the planning

1:07:53.380 --> 1:07:54.980
 coupled with the perception.

1:07:54.980 --> 1:07:58.280
 You have to figure out like what's involved in driving,

1:07:58.280 --> 1:08:00.880
 like especially in all the different edge cases.

1:08:02.320 --> 1:08:06.540
 And then, I mean, maybe you can comment on this,

1:08:06.540 --> 1:08:10.920
 how much game theoretic kind of stuff needs to be involved

1:08:10.920 --> 1:08:12.700
 at a four way stop sign.

1:08:12.700 --> 1:08:17.700
 As humans, when we drive, our actions affect the world.

1:08:18.060 --> 1:08:20.780
 Like it changes how others behave.

1:08:20.780 --> 1:08:23.340
 Most autonomous driving, if you,

1:08:23.340 --> 1:08:27.420
 you're usually just responding to the scene

1:08:27.420 --> 1:08:31.360
 as opposed to like really asserting yourself in the scene.

1:08:31.360 --> 1:08:32.200
 Do you think?

1:08:33.140 --> 1:08:37.580
 I think these sort of control logic conundrums

1:08:37.580 --> 1:08:39.340
 are not the hard part.

1:08:39.340 --> 1:08:43.580
 The, you know, let's see.

1:08:45.540 --> 1:08:46.860
 What do you think is the hard part

1:08:46.860 --> 1:08:50.580
 in this whole beautiful, complex problem?

1:08:50.580 --> 1:08:52.980
 So it's a lot of freaking software, man.

1:08:52.980 --> 1:08:54.380
 A lot of smart lines of code.

1:08:57.300 --> 1:08:59.660
 For sure, in order to have,

1:09:01.340 --> 1:09:03.980
 create an accurate vector space.

1:09:03.980 --> 1:09:08.280
 So like you're coming from image space,

1:09:08.280 --> 1:09:12.540
 which is like this flow of photons,

1:09:12.540 --> 1:09:14.380
 you're going to the camera, cameras,

1:09:14.380 --> 1:09:19.380
 and then you have this massive bitstream

1:09:21.220 --> 1:09:26.220
 in image space, and then you have to effectively compress

1:09:29.580 --> 1:09:34.580
 a massive bitstream corresponding to photons

1:09:34.580 --> 1:09:38.500
 that knocked off an electron in a camera sensor

1:09:38.500 --> 1:09:42.500
 and turn that bitstream into a vector space.

1:09:44.860 --> 1:09:47.860
 By vector space, I mean like, you know,

1:09:47.860 --> 1:09:52.860
 you've got cars and humans and lane lines and curves

1:09:55.060 --> 1:09:59.380
 and traffic lights and that kind of thing.

1:09:59.380 --> 1:10:03.180
 Once you've got all of that in your head,

1:10:03.180 --> 1:10:08.180
 once you have an accurate vector space,

1:10:08.500 --> 1:10:11.680
 the control problem is similar to that of a video game,

1:10:11.680 --> 1:10:14.100
 like a Grand Theft Auto of Cyberpunk,

1:10:14.100 --> 1:10:16.260
 if you have accurate vector space.

1:10:16.260 --> 1:10:18.300
 It's the control problem is,

1:10:18.300 --> 1:10:20.300
 I wouldn't say it's trivial, it's not trivial,

1:10:20.300 --> 1:10:25.300
 but it's not like some insurmountable thing.

1:10:29.020 --> 1:10:32.140
 Having an accurate vector space is very difficult.

1:10:32.140 --> 1:10:35.540
 Yeah, I think we humans don't give enough respect

1:10:35.540 --> 1:10:37.900
 to how incredible the human perception system is

1:10:37.900 --> 1:10:42.460
 to mapping the raw photons to the vector space

1:10:42.460 --> 1:10:44.660
 representation in our heads.

1:10:44.660 --> 1:10:47.460
 Your brain is doing an incredible amount of processing

1:10:48.360 --> 1:10:51.360
 and giving you an image that is a very cleaned up image.

1:10:51.360 --> 1:10:53.380
 Like when we look around here, we see,

1:10:53.380 --> 1:10:55.340
 like you see color in the corners of your eyes,

1:10:55.340 --> 1:10:59.420
 but actually your eyes have very few cones,

1:10:59.420 --> 1:11:02.260
 like cone receptors in the peripheral vision.

1:11:02.260 --> 1:11:05.660
 Your eyes are painting color in the peripheral vision.

1:11:05.660 --> 1:11:06.500
 You don't realize it,

1:11:06.500 --> 1:11:09.060
 but their eyes are actually painting color

1:11:09.060 --> 1:11:12.260
 and your eyes also have like this blood vessels

1:11:12.260 --> 1:11:14.660
 and all sorts of gnarly things and there's a blind spot,

1:11:14.660 --> 1:11:16.380
 but do you see your blind spot?

1:11:16.380 --> 1:11:21.180
 No, your brain is painting in the missing, the blind spot.

1:11:21.180 --> 1:11:24.980
 You're gonna do these things online where you look here

1:11:24.980 --> 1:11:27.380
 and look at this point and then look at this point

1:11:27.380 --> 1:11:30.460
 and if it's in your blind spot,

1:11:30.460 --> 1:11:33.660
 your brain will just fill in the missing bits.

1:11:33.660 --> 1:11:35.500
 The peripheral vision is so cool.

1:11:35.500 --> 1:11:38.060
 It makes you realize all the illusions for vision sciences,

1:11:38.060 --> 1:11:40.620
 so it makes you realize just how incredible the brain is.

1:11:40.620 --> 1:11:42.660
 The brain is doing crazy amount of post processing

1:11:42.660 --> 1:11:44.780
 on the vision signals for your eyes.

1:11:45.820 --> 1:11:46.660
 It's insane.

1:11:49.180 --> 1:11:51.940
 And then even once you get all those vision signals,

1:11:51.940 --> 1:11:56.260
 your brain is constantly trying to forget

1:11:56.260 --> 1:11:57.660
 as much as possible.

1:11:57.660 --> 1:11:59.500
 So human memory is,

1:11:59.500 --> 1:12:01.900
 perhaps the weakest thing about the brain is memory.

1:12:01.900 --> 1:12:05.260
 So because memory is so expensive to our brain

1:12:05.260 --> 1:12:06.660
 and so limited,

1:12:06.660 --> 1:12:09.740
 your brain is trying to forget as much as possible

1:12:09.740 --> 1:12:12.140
 and distill the things that you see

1:12:12.140 --> 1:12:16.500
 into the smallest amounts of information possible.

1:12:16.500 --> 1:12:19.340
 So your brain is trying to not just get to a vector space,

1:12:19.340 --> 1:12:22.860
 but get to a vector space that is the smallest possible

1:12:22.860 --> 1:12:25.020
 vector space of only relevant objects.

1:12:26.620 --> 1:12:29.860
 And I think you can sort of look inside your brain

1:12:29.860 --> 1:12:31.540
 or at least I can,

1:12:31.540 --> 1:12:35.380
 when you drive down the road and try to think about

1:12:35.380 --> 1:12:38.940
 what your brain is actually doing consciously.

1:12:38.940 --> 1:12:43.940
 And it's like you'll see a car,

1:12:44.540 --> 1:12:46.860
 because you don't have cameras,

1:12:46.860 --> 1:12:48.940
 I don't have eyes in the back of your head or a side.

1:12:48.940 --> 1:12:53.940
 So you basically have like two cameras on a slow gimbal.

1:13:00.460 --> 1:13:01.700
 And eyesight is not that great.

1:13:01.700 --> 1:13:04.300
 Okay, human eyes are like,

1:13:04.300 --> 1:13:05.700
 and people are constantly distracted

1:13:05.700 --> 1:13:07.180
 and thinking about things and texting

1:13:07.180 --> 1:13:09.260
 and doing all sorts of things they shouldn't do in a car,

1:13:09.260 --> 1:13:10.940
 changing the radio station.

1:13:10.940 --> 1:13:15.060
 So having arguments is like,

1:13:15.060 --> 1:13:20.060
 so when's the last time you looked right and left

1:13:22.540 --> 1:13:27.060
 and rearward, or even diagonally forward

1:13:27.060 --> 1:13:30.140
 to actually refresh your vector space?

1:13:30.140 --> 1:13:32.740
 So you're glancing around and what your mind is doing

1:13:32.740 --> 1:13:37.300
 is trying to distill the relevant vectors,

1:13:37.300 --> 1:13:40.220
 basically objects with a position and motion.

1:13:40.220 --> 1:13:45.220
 And then editing that down to the least amount

1:13:48.140 --> 1:13:49.940
 that's necessary for you to drive.

1:13:49.940 --> 1:13:53.260
 It does seem to be able to edit it down

1:13:53.260 --> 1:13:55.780
 or compress it even further into things like concepts.

1:13:55.780 --> 1:13:57.660
 So it's not, it's like it goes beyond,

1:13:57.660 --> 1:14:01.260
 the human mind seems to go sometimes beyond vector space

1:14:01.260 --> 1:14:05.080
 to sort of space of concepts to where you'll see a thing.

1:14:05.080 --> 1:14:07.520
 It's no longer represented spatially somehow.

1:14:07.520 --> 1:14:10.060
 It's almost like a concept that you should be aware of.

1:14:10.060 --> 1:14:12.300
 Like if this is a school zone,

1:14:12.300 --> 1:14:14.940
 you'll remember that as a concept,

1:14:14.940 --> 1:14:16.420
 which is a weird thing to represent,

1:14:16.420 --> 1:14:17.480
 but perhaps for driving,

1:14:17.480 --> 1:14:20.460
 you don't need to fully represent those things.

1:14:20.460 --> 1:14:25.460
 Or maybe you get those kind of indirectly.

1:14:25.860 --> 1:14:27.740
 You need to establish vector space

1:14:27.740 --> 1:14:32.740
 and then actually have predictions for those vector spaces.

1:14:32.740 --> 1:14:37.740
 So if you drive past, say a bus and you see that there's people,

1:14:47.340 --> 1:14:48.500
 before you drove past the bus,

1:14:48.500 --> 1:14:50.580
 you saw people crossing or some,

1:14:50.580 --> 1:14:52.820
 just imagine there's like a large truck

1:14:52.820 --> 1:14:54.220
 or something blocking site.

1:14:55.500 --> 1:14:57.420
 But before you came up to the truck,

1:14:57.420 --> 1:15:00.700
 you saw that there were some kids about to cross the road

1:15:00.700 --> 1:15:01.540
 in front of the truck.

1:15:01.540 --> 1:15:03.260
 Now you can no longer see the kids,

1:15:03.260 --> 1:15:06.300
 but you would now know, okay,

1:15:06.300 --> 1:15:09.020
 those kids are probably gonna pass by the truck

1:15:09.020 --> 1:15:12.020
 and cross the road, even though you cannot see them.

1:15:12.020 --> 1:15:15.980
 So you have to have memory,

1:15:17.340 --> 1:15:19.100
 you have to need to remember that there were kids there

1:15:19.100 --> 1:15:21.820
 and you need to have some forward prediction

1:15:21.820 --> 1:15:25.660
 of what their position will be at the time of relevance.

1:15:25.660 --> 1:15:28.540
 So with occlusions and computer vision,

1:15:28.540 --> 1:15:30.840
 when you can't see an object anymore,

1:15:30.840 --> 1:15:33.460
 even when it just walks behind a tree and reappears,

1:15:33.460 --> 1:15:35.100
 that's a really, really,

1:15:35.100 --> 1:15:37.100
 I mean, at least in academic literature,

1:15:37.100 --> 1:15:40.620
 it's tracking through occlusions, it's very difficult.

1:15:40.620 --> 1:15:41.940
 Yeah, we're doing it.

1:15:41.940 --> 1:15:42.780
 I understand this.

1:15:42.780 --> 1:15:43.600
 Yeah.

1:15:43.600 --> 1:15:44.440
 So some of it.

1:15:44.440 --> 1:15:45.420
 It's like object permanence,

1:15:45.420 --> 1:15:47.700
 like same thing happens with humans with neural nets.

1:15:47.700 --> 1:15:50.100
 Like when like a toddler grows up,

1:15:50.100 --> 1:15:54.420
 like there's a point in time where they develop,

1:15:54.420 --> 1:15:56.220
 they have a sense of object permanence.

1:15:56.220 --> 1:15:59.580
 So before a certain age, if you have a ball or a toy

1:15:59.580 --> 1:16:01.180
 or whatever, and you put it behind your back

1:16:01.180 --> 1:16:03.360
 and you pop it out, if they don't,

1:16:03.360 --> 1:16:04.660
 before they have object permanence,

1:16:04.660 --> 1:16:05.860
 it's like a new thing every time.

1:16:05.860 --> 1:16:08.260
 It's like, whoa, this toy went poof, just fared

1:16:08.260 --> 1:16:09.980
 and now it's back again and they can't believe it.

1:16:09.980 --> 1:16:12.140
 And that they can play peekaboo all day long

1:16:12.140 --> 1:16:13.940
 because peekaboo is fresh every time.

1:16:13.940 --> 1:16:18.220
 But then we figured out object permanence,

1:16:18.220 --> 1:16:20.360
 then they realize, oh no, the object is not gone,

1:16:20.360 --> 1:16:21.760
 it's just behind your back.

1:16:22.660 --> 1:16:26.380
 Sometimes I wish we never did figure out object permanence.

1:16:26.380 --> 1:16:28.260
 Yeah, so that's a...

1:16:28.260 --> 1:16:31.620
 So that's an important problem to solve.

1:16:31.620 --> 1:16:33.960
 Yes, so like an important evolution

1:16:33.960 --> 1:16:35.740
 of the neural nets in the car is

1:16:39.740 --> 1:16:43.860
 memory across both time and space.

1:16:43.860 --> 1:16:47.100
 So now you can't remember, like you have to say,

1:16:47.100 --> 1:16:48.960
 like how long do you want to remember things for?

1:16:48.960 --> 1:16:53.260
 And there's a cost to remembering things for a long time.

1:16:53.260 --> 1:16:55.740
 So you can like run out of memory

1:16:55.740 --> 1:16:58.600
 to try to remember too much for too long.

1:16:58.600 --> 1:17:01.420
 And then you also have things that are stale

1:17:01.420 --> 1:17:03.540
 if you remember them for too long.

1:17:03.540 --> 1:17:06.880
 And then you also need things that are remembered over time.

1:17:06.880 --> 1:17:10.640
 So even if you like say have like,

1:17:10.640 --> 1:17:14.580
 for our good sake, five seconds of memory on a time basis,

1:17:14.580 --> 1:17:17.100
 but like let's say you're parked at a light

1:17:17.100 --> 1:17:20.660
 and you saw, use a pedestrian example,

1:17:20.660 --> 1:17:25.220
 that people were waiting to cross the road

1:17:25.220 --> 1:17:27.820
 and you can't quite see them because of an occlusion,

1:17:28.900 --> 1:17:31.820
 but they might wait for a minute before the light changes

1:17:31.820 --> 1:17:33.180
 for them to cross the road.

1:17:33.180 --> 1:17:36.940
 You still need to remember that that's where they were

1:17:36.940 --> 1:17:38.420
 and that they're probably going

1:17:38.420 --> 1:17:40.580
 to cross the road type of thing.

1:17:40.580 --> 1:17:44.740
 So even if that exceeds your time based memory,

1:17:44.740 --> 1:17:47.000
 it should not exceed your space memory.

1:17:48.140 --> 1:17:50.500
 And I just think the data engine side of that,

1:17:50.500 --> 1:17:53.540
 so getting the data to learn all of the concepts

1:17:53.540 --> 1:17:56.180
 that you're saying now is an incredible process.

1:17:56.180 --> 1:17:58.380
 It's this iterative process of just,

1:17:58.380 --> 1:18:00.740
 it's this hydranet of many.

1:18:00.740 --> 1:18:01.580
 Hydranet.

1:18:03.380 --> 1:18:05.420
 We're changing the name to something else.

1:18:05.420 --> 1:18:09.700
 Okay, I'm sure it'll be equally as Rick and Morty like.

1:18:09.700 --> 1:18:11.460
 There's a lot of, yeah.

1:18:11.460 --> 1:18:13.780
 We've rearchitected the neural net,

1:18:14.700 --> 1:18:17.980
 the neural nets in the cars so many times it's crazy.

1:18:17.980 --> 1:18:20.020
 Oh, so every time there's a new major version,

1:18:20.020 --> 1:18:21.940
 you'll rename it to something more ridiculous

1:18:21.940 --> 1:18:25.060
 or memorable and beautiful, sorry.

1:18:25.060 --> 1:18:26.360
 Not ridiculous, of course.

1:18:28.140 --> 1:18:32.500
 If you see the full array of neural nets

1:18:32.500 --> 1:18:34.180
 that are operating in the cars,

1:18:34.180 --> 1:18:35.540
 it kind of boggles the mind.

1:18:36.420 --> 1:18:38.500
 There's so many layers, it's crazy.

1:18:39.860 --> 1:18:41.920
 So, yeah.

1:18:43.620 --> 1:18:48.620
 And we started off with simple neural nets

1:18:48.620 --> 1:18:53.300
 that were basically image recognition

1:18:53.300 --> 1:18:56.620
 on a single frame from a single camera

1:18:56.620 --> 1:19:00.700
 and then trying to knit those together

1:19:00.700 --> 1:19:05.700
 with C, I should say we're really primarily running C here

1:19:07.680 --> 1:19:10.000
 because C++ is too much overhead

1:19:10.000 --> 1:19:11.660
 and we have our own C compiler.

1:19:11.660 --> 1:19:13.580
 So to get maximum performance,

1:19:13.580 --> 1:19:15.780
 we actually wrote our own C compiler

1:19:15.780 --> 1:19:18.060
 and are continuing to optimize our C compiler

1:19:18.060 --> 1:19:20.060
 for maximum efficiency.

1:19:20.060 --> 1:19:23.100
 In fact, we've just recently done a new river

1:19:23.100 --> 1:19:25.260
 on our C compiler that'll compile directly

1:19:25.260 --> 1:19:26.980
 to our autopilot hardware.

1:19:26.980 --> 1:19:28.960
 If you want to compile the whole thing down

1:19:28.960 --> 1:19:32.660
 with your own compiler, like so efficiency here,

1:19:32.660 --> 1:19:33.940
 because there's all kinds of compute,

1:19:33.940 --> 1:19:37.340
 there's CPU, GPU, there's like basic types of things

1:19:37.340 --> 1:19:39.140
 and you have to somehow figure out the scheduling

1:19:39.140 --> 1:19:40.140
 across all of those things.

1:19:40.140 --> 1:19:43.320
 And so you're compiling the code down that does all, okay.

1:19:44.500 --> 1:19:46.900
 So that's why there's a lot of people involved.

1:19:46.900 --> 1:19:50.620
 There's a lot of hardcore software engineering

1:19:50.620 --> 1:19:54.700
 at a very sort of bare metal level

1:19:54.700 --> 1:19:57.280
 because we're trying to do a lot of compute

1:19:57.280 --> 1:20:02.280
 that's constrained to our full self driving computer.

1:20:03.040 --> 1:20:06.780
 So we want to try to have the highest frames per second

1:20:07.740 --> 1:20:12.740
 possible in a sort of very finite amount of compute

1:20:14.580 --> 1:20:15.420
 and power.

1:20:15.420 --> 1:20:20.420
 So we really put a lot of effort into the efficiency

1:20:20.940 --> 1:20:21.780
 of our compute.

1:20:23.820 --> 1:20:26.060
 And so there's actually a lot of work done

1:20:26.060 --> 1:20:29.660
 by some very talented software engineers at Tesla

1:20:29.660 --> 1:20:33.140
 that at a very foundational level

1:20:33.140 --> 1:20:35.260
 to improve the efficiency of compute

1:20:35.260 --> 1:20:38.940
 and how we use the trip accelerators,

1:20:38.940 --> 1:20:43.180
 which are basically, you know,

1:20:43.180 --> 1:20:45.420
 doing matrix math dot products,

1:20:45.420 --> 1:20:47.340
 like a bazillion dot products.

1:20:47.340 --> 1:20:49.580
 And it's like, what are neural nets?

1:20:49.580 --> 1:20:53.200
 It's like compute wise, like 99% dot products.

1:20:54.340 --> 1:20:57.100
 So, you know.

1:20:57.100 --> 1:20:59.700
 And you want to achieve as many high frame rates

1:20:59.700 --> 1:21:00.580
 like a video game.

1:21:00.580 --> 1:21:05.020
 You want full resolution, high frame rate.

1:21:05.020 --> 1:21:10.020
 High frame rate, low latency, low jitter.

1:21:10.020 --> 1:21:15.020
 So I think one of the things we're moving towards now

1:21:18.340 --> 1:21:22.540
 is no post processing of the image

1:21:22.540 --> 1:21:26.700
 through the image signal processor.

1:21:26.700 --> 1:21:31.700
 So like what happens for cameras is that,

1:21:32.580 --> 1:21:34.040
 almost all cameras is they,

1:21:35.740 --> 1:21:37.700
 there's a lot of post processing done

1:21:37.700 --> 1:21:40.260
 in order to make pictures look pretty.

1:21:40.260 --> 1:21:43.540
 And so we don't care about pictures looking pretty.

1:21:43.540 --> 1:21:45.380
 We just want the data.

1:21:45.380 --> 1:21:48.780
 So we're moving just raw photon counts.

1:21:48.780 --> 1:21:53.780
 So the system will, like the image that the computer sees

1:21:55.060 --> 1:21:57.860
 is actually much more than what you'd see

1:21:57.860 --> 1:21:59.100
 if you represented it on a camera.

1:21:59.100 --> 1:22:00.780
 It's got much more data.

1:22:00.780 --> 1:22:02.560
 And even in a very low light conditions,

1:22:02.560 --> 1:22:05.220
 you can see that there's a small photon count difference

1:22:05.220 --> 1:22:08.780
 between this spot here and that spot there,

1:22:08.780 --> 1:22:09.620
 which means that,

1:22:09.620 --> 1:22:12.020
 so it can see in the dark incredibly well

1:22:12.900 --> 1:22:15.100
 because it can detect these tiny differences

1:22:15.100 --> 1:22:16.140
 in photon counts.

1:22:16.980 --> 1:22:19.820
 Like much better than you could possibly imagine.

1:22:20.780 --> 1:22:25.780
 So, and then we also save 13 milliseconds on a latency.

1:22:27.420 --> 1:22:28.260
 So.

1:22:29.260 --> 1:22:31.220
 From removing the post processing on the image?

1:22:31.220 --> 1:22:32.060
 Yes.

1:22:32.060 --> 1:22:32.900
 It's like,

1:22:32.900 --> 1:22:34.300
 it's incredible.

1:22:34.300 --> 1:22:35.820
 Cause we've got eight cameras

1:22:35.820 --> 1:22:39.780
 and then there's roughly, I don't know,

1:22:39.780 --> 1:22:41.980
 one and a half milliseconds or so,

1:22:41.980 --> 1:22:46.220
 maybe 1.6 milliseconds of latency for each camera.

1:22:46.220 --> 1:22:51.220
 And so like going to just,

1:22:53.380 --> 1:22:56.300
 basically bypassing the image processor

1:22:56.300 --> 1:22:58.240
 gets us back 13 milliseconds of latency,

1:22:58.240 --> 1:22:59.320
 which is important.

1:22:59.320 --> 1:23:03.440
 And we track latency all the way from, you know,

1:23:03.440 --> 1:23:06.760
 photon hits the camera to, you know,

1:23:06.760 --> 1:23:08.920
 all the steps that it's got to go through to get,

1:23:08.920 --> 1:23:12.280
 you know, go through the various neural nets

1:23:12.280 --> 1:23:13.320
 and the C code.

1:23:13.320 --> 1:23:16.540
 And there's a little bit of C++ there as well.

1:23:17.800 --> 1:23:20.200
 Well, I can maybe a lot, but it,

1:23:20.200 --> 1:23:23.080
 the core stuff is heavy duty computers all in C.

1:23:25.120 --> 1:23:28.840
 And so we track that latency all the way

1:23:28.840 --> 1:23:33.480
 to an output command to the drive unit to accelerate

1:23:33.480 --> 1:23:36.600
 the brakes just to slow down the steering,

1:23:36.600 --> 1:23:38.920
 you know, turn left or right.

1:23:38.920 --> 1:23:40.680
 So, cause you go to output a command

1:23:40.680 --> 1:23:41.800
 that's got to go to a controller.

1:23:41.800 --> 1:23:44.120
 And like some of these controllers have an update frequency

1:23:44.120 --> 1:23:46.480
 that's maybe 10 Hertz or something like that,

1:23:46.480 --> 1:23:47.320
 which is slow.

1:23:47.320 --> 1:23:50.240
 That's like now you lose a hundred milliseconds potentially.

1:23:50.240 --> 1:23:53.960
 So, so then we want to update the,

1:23:55.040 --> 1:23:58.720
 the drivers on the like say steering and braking control

1:23:58.720 --> 1:24:02.920
 to have more like a hundred Hertz instead of 10 Hertz.

1:24:02.920 --> 1:24:04.400
 And then you've got a 10 millisecond latency

1:24:04.400 --> 1:24:06.400
 instead of a hundred millisecond worst case latency.

1:24:06.400 --> 1:24:09.560
 And actually jitter is more of a challenge than latency.

1:24:09.560 --> 1:24:11.000
 Cause latency is like, you can, you can,

1:24:11.000 --> 1:24:13.120
 you can anticipate and predict, but if you're,

1:24:13.120 --> 1:24:14.920
 but if you've got a stack up of things going

1:24:14.920 --> 1:24:17.880
 from the camera to the, to the computer through

1:24:17.880 --> 1:24:19.400
 then a series of other computers,

1:24:19.400 --> 1:24:22.120
 and finally to an actuator on the car,

1:24:22.120 --> 1:24:26.960
 if you have a stack up of tolerances of timing tolerances,

1:24:26.960 --> 1:24:29.000
 then you can have quite a variable latency,

1:24:29.000 --> 1:24:30.360
 which is called jitter.

1:24:30.360 --> 1:24:34.840
 And, and that makes it hard to, to, to anticipate exactly

1:24:34.840 --> 1:24:37.480
 what, how you should turn the car or accelerate,

1:24:37.480 --> 1:24:40.640
 because if you've got maybe a hundred,

1:24:40.640 --> 1:24:42.440
 50, 200 milliseconds of jitter,

1:24:42.440 --> 1:24:45.560
 then you could be off by, you know, up to 0.2 seconds.

1:24:45.560 --> 1:24:47.560
 And this can make, this could make a big difference.

1:24:47.560 --> 1:24:50.160
 So you have to interpolate somehow to, to, to,

1:24:50.160 --> 1:24:52.400
 to deal with the effects of jitter.

1:24:52.400 --> 1:24:56.940
 So that you can make like robust control decisions.

1:24:57.920 --> 1:25:01.620
 You have to, so the jitters and the sensor information,

1:25:01.620 --> 1:25:05.000
 or the jitter can occur at any stage in the pipeline.

1:25:05.000 --> 1:25:07.760
 You can, if you have just, if you have fixed latency,

1:25:07.760 --> 1:25:11.960
 you can anticipate and, and like say, okay,

1:25:11.960 --> 1:25:16.560
 we know that our information is for argument's sake,

1:25:16.560 --> 1:25:19.000
 150 milliseconds stale.

1:25:19.000 --> 1:25:22.840
 Like, so for, for, for 150 milliseconds

1:25:22.840 --> 1:25:27.840
 from photon second camera to where you can measure a change

1:25:28.500 --> 1:25:30.840
 in the acceleration of the vehicle.

1:25:33.720 --> 1:25:38.080
 So then, then you can say, okay, well, we're going to enter,

1:25:38.080 --> 1:25:39.400
 we know it's 150 milliseconds.

1:25:39.400 --> 1:25:40.960
 So we're going to take that into account

1:25:40.960 --> 1:25:44.240
 and, and compensate for that latency.

1:25:44.240 --> 1:25:47.320
 However, if you've got then 150 milliseconds of latency

1:25:47.320 --> 1:25:49.200
 plus a hundred milliseconds of jitter,

1:25:49.200 --> 1:25:50.760
 that's which could be anywhere from zero,

1:25:50.760 --> 1:25:52.240
 zero to a hundred milliseconds on top.

1:25:52.240 --> 1:25:55.720
 So, so then your latency could be from 150 to 250 milliseconds.

1:25:55.720 --> 1:25:56.680
 Now you've got a hundred milliseconds

1:25:56.680 --> 1:25:58.040
 that you don't know what to do with.

1:25:58.040 --> 1:25:59.960
 And that's basically random.

1:26:01.440 --> 1:26:04.260
 So getting rid of jitter is extremely important.

1:26:04.260 --> 1:26:05.840
 And that affects your control decisions

1:26:05.840 --> 1:26:07.400
 and all those kinds of things.

1:26:07.400 --> 1:26:08.240
 Okay.

1:26:09.160 --> 1:26:11.140
 Yeah, the car's just going to fundamentally maneuver better

1:26:11.140 --> 1:26:12.040
 with lower jitter.

1:26:12.920 --> 1:26:13.760
 Got it.

1:26:13.760 --> 1:26:16.760
 The cars will maneuver with superhuman ability

1:26:16.760 --> 1:26:19.000
 and reaction time much faster than a human.

1:26:20.360 --> 1:26:24.320
 I mean, I think over time the autopilot,

1:26:24.320 --> 1:26:26.400
 full self driving will be capable of maneuvers

1:26:26.400 --> 1:26:31.400
 that are far more than what like James Bond could do

1:26:34.840 --> 1:26:36.360
 in like the best movie type of thing.

1:26:36.360 --> 1:26:38.620
 That's exactly what I was imagining in my mind,

1:26:38.620 --> 1:26:40.240
 as you said it.

1:26:40.240 --> 1:26:41.820
 It's like an impossible maneuvers

1:26:41.820 --> 1:26:43.020
 that a human couldn't do.

1:26:43.020 --> 1:26:45.080
 Yeah, so.

1:26:45.080 --> 1:26:48.720
 Well, let me ask sort of looking back the six years,

1:26:48.720 --> 1:26:50.280
 looking out into the future,

1:26:50.280 --> 1:26:51.840
 based on your current understanding,

1:26:51.840 --> 1:26:53.640
 how hard do you think this,

1:26:53.640 --> 1:26:55.360
 this full self driving problem,

1:26:55.360 --> 1:26:58.820
 when do you think Tesla will solve level four FSD?

1:27:01.040 --> 1:27:02.340
 I mean, it's looking quite likely

1:27:02.340 --> 1:27:03.640
 that it will be next year.

1:27:05.520 --> 1:27:07.040
 And what does the solution look like?

1:27:07.040 --> 1:27:10.200
 Is it the current pool of FSD beta candidates,

1:27:10.200 --> 1:27:13.120
 they start getting greater and greater

1:27:13.120 --> 1:27:15.760
 as they have been degrees of autonomy,

1:27:15.760 --> 1:27:17.760
 and then there's a certain level

1:27:17.760 --> 1:27:20.800
 beyond which they can do their own,

1:27:20.800 --> 1:27:21.840
 they can read a book.

1:27:22.880 --> 1:27:23.840
 Yeah, so.

1:27:25.720 --> 1:27:26.880
 I mean, you can see that anybody

1:27:26.880 --> 1:27:30.640
 who's been following the full self driving beta closely

1:27:30.640 --> 1:27:35.260
 will see that the rate of disengagements

1:27:35.260 --> 1:27:37.440
 has been dropping rapidly.

1:27:37.440 --> 1:27:40.720
 So like disengagement be where the driver intervenes

1:27:40.720 --> 1:27:44.000
 to prevent the car from doing something dangerous,

1:27:44.000 --> 1:27:44.940
 potentially, so.

1:27:49.740 --> 1:27:53.240
 So the interventions per million miles

1:27:53.240 --> 1:27:55.920
 has been dropping dramatically at some point.

1:27:57.520 --> 1:28:01.040
 And that trend looks like it happens next year

1:28:01.040 --> 1:28:06.040
 is that the probability of an accident on FSD

1:28:06.040 --> 1:28:09.520
 is less than that of the average human,

1:28:09.520 --> 1:28:12.480
 and then significantly less than that of the average human.

1:28:13.480 --> 1:28:18.480
 So it certainly appears like we will get there next year.

1:28:21.080 --> 1:28:24.000
 Then of course, then there's gonna be a case of,

1:28:24.000 --> 1:28:26.040
 okay, well, we now have to prove this to regulators

1:28:26.040 --> 1:28:28.640
 and prove it to, you know, and we want a standard

1:28:28.640 --> 1:28:31.160
 that is not just equivalent to a human,

1:28:31.160 --> 1:28:33.480
 but much better than the average human.

1:28:33.480 --> 1:28:35.560
 I think it's gotta be at least two or three times

1:28:35.560 --> 1:28:39.000
 two or three times higher safety than a human.

1:28:39.000 --> 1:28:41.360
 So two or three times lower probability of injury

1:28:41.360 --> 1:28:44.840
 than a human before we would actually say like,

1:28:44.840 --> 1:28:46.800
 okay, it's okay to go, it's not gonna be equivalent,

1:28:46.800 --> 1:28:48.340
 it's gonna be much better.

1:28:48.340 --> 1:28:53.340
 So if you look, FSD 10.6 just came out recently,

1:28:53.400 --> 1:28:57.120
 10.7 is on the way, maybe 11 is on the way,

1:28:57.120 --> 1:28:58.440
 so we're in the future.

1:28:58.440 --> 1:29:01.040
 Yeah, we were hoping to get 11 out this year,

1:29:01.040 --> 1:29:06.040
 but 11 actually has a whole bunch of fundamental rewrites

1:29:07.180 --> 1:29:09.460
 on the neural net architecture,

1:29:10.840 --> 1:29:14.040
 and some fundamental improvements

1:29:14.040 --> 1:29:18.280
 in creating vector space, so.

1:29:19.680 --> 1:29:22.240
 There is some fundamental like leap

1:29:22.240 --> 1:29:24.000
 that really deserves the 11,

1:29:24.000 --> 1:29:25.160
 I mean, that's a pretty cool number.

1:29:25.160 --> 1:29:29.920
 Yeah, 11 would be a single stack

1:29:29.920 --> 1:29:32.160
 for all, you know, one stack to rule them all.

1:29:36.160 --> 1:29:38.600
 But there are just some really fundamental

1:29:40.520 --> 1:29:42.120
 neural net architecture changes

1:29:43.760 --> 1:29:47.720
 that will allow for much more capability,

1:29:47.720 --> 1:29:51.080
 but at first they're gonna have issues.

1:29:51.080 --> 1:29:54.680
 So like we have this working on like sort of alpha software

1:29:54.680 --> 1:29:59.680
 and it's good, but it's basically taking a whole bunch

1:30:00.880 --> 1:30:05.360
 of C, C++ code and leading a massive amount of C++ code

1:30:05.360 --> 1:30:06.440
 and replacing it with a neural net.

1:30:06.440 --> 1:30:09.160
 And Andre makes this point a lot,

1:30:09.160 --> 1:30:12.480
 which is like neural nets are kind of eating software.

1:30:12.480 --> 1:30:15.880
 Over time there's like less and less conventional software,

1:30:15.880 --> 1:30:18.240
 more and more neural net, which is still software,

1:30:18.240 --> 1:30:21.120
 but it's, you know, still comes out the lines of software,

1:30:21.120 --> 1:30:25.320
 but it's more neural net stuff

1:30:25.320 --> 1:30:29.800
 and less, you know, heuristics basically.

1:30:33.080 --> 1:30:38.080
 More matrix based stuff and less heuristics based stuff.

1:30:38.080 --> 1:30:47.080
 And, you know, like one of the big changes will be,

1:30:47.080 --> 1:30:54.080
 like right now the neural nets will deliver

1:30:54.080 --> 1:31:00.080
 a giant bag of points to the C++ or C and C++ code.

1:31:00.080 --> 1:31:03.080
 We call it the giant bag of points.

1:31:03.080 --> 1:31:08.080
 And it's like, so you've got a pixel and something associated

1:31:08.080 --> 1:31:09.080
 with that pixel.

1:31:09.080 --> 1:31:11.080
 Like this pixel is probably car.

1:31:11.080 --> 1:31:13.080
 The pixel is probably lane line.

1:31:13.080 --> 1:31:16.080
 Then you've got to assemble this giant bag of points

1:31:16.080 --> 1:31:21.080
 in the C code and turn it into vectors.

1:31:21.080 --> 1:31:26.080
 And it does a pretty good job of it, but it's,

1:31:26.080 --> 1:31:30.080
 we want to just, you know,

1:31:30.080 --> 1:31:35.080
 we need another layer of neural nets on top of that

1:31:35.080 --> 1:31:40.080
 to take the giant bag of points and distill that down

1:31:40.080 --> 1:31:45.080
 to a vector space in the neural net part of the software,

1:31:45.080 --> 1:31:48.080
 as opposed to the heuristics part of the software.

1:31:48.080 --> 1:31:51.080
 This is a big improvement.

1:31:51.080 --> 1:31:52.080
 Neural nets all the way down.

1:31:52.080 --> 1:31:53.080
 That's what you want.

1:31:53.080 --> 1:31:58.080
 It's not even all neural nets, but this will be just a,

1:31:58.080 --> 1:32:01.080
 this is a game changer to not have the bag of points,

1:32:01.080 --> 1:32:04.080
 the giant bag of points that has to be assembled

1:32:04.080 --> 1:32:09.080
 with many lines of C++ and have the,

1:32:09.080 --> 1:32:12.080
 and have a neural net just assemble those into a vector.

1:32:12.080 --> 1:32:19.080
 So the neural net is outputting much, much less data.

1:32:19.080 --> 1:32:22.080
 It's outputting, this is a lane line.

1:32:22.080 --> 1:32:23.080
 This is a curb.

1:32:23.080 --> 1:32:24.080
 This is drivable space.

1:32:24.080 --> 1:32:25.080
 This is a car.

1:32:25.080 --> 1:32:29.080
 This is a pedestrian or a cyclist or something like that.

1:32:29.080 --> 1:32:35.080
 It's outputting, it's really outputting proper vectors

1:32:35.080 --> 1:32:39.080
 to the C, C++ control code,

1:32:39.080 --> 1:32:50.080
 as opposed to the sort of constructing the vectors in C,

1:32:50.080 --> 1:32:52.080
 which we've done, I think, quite a good job of,

1:32:52.080 --> 1:32:55.080
 but we're kind of hitting a local maximum

1:32:55.080 --> 1:32:59.080
 on how well the C can do this.

1:32:59.080 --> 1:33:02.080
 So this is really a big deal.

1:33:02.080 --> 1:33:04.080
 And just all of the networks in the car

1:33:04.080 --> 1:33:06.080
 need to move to surround video.

1:33:06.080 --> 1:33:11.080
 There's still some legacy networks that are not surround video.

1:33:11.080 --> 1:33:14.080
 And all of the training needs to move to surround video

1:33:14.080 --> 1:33:16.080
 and the efficiency of the training,

1:33:16.080 --> 1:33:18.080
 it needs to get better and it is.

1:33:18.080 --> 1:33:25.080
 And then we need to move everything to raw photon counts

1:33:25.080 --> 1:33:29.080
 as opposed to processed images,

1:33:29.080 --> 1:33:31.080
 which is quite a big reset on the training

1:33:31.080 --> 1:33:35.080
 because the system's trained on post processed images.

1:33:35.080 --> 1:33:38.080
 So we need to redo all the training

1:33:38.080 --> 1:33:41.080
 to train against the raw photon counts

1:33:41.080 --> 1:33:43.080
 instead of the post processed image.

1:33:43.080 --> 1:33:46.080
 So ultimately, it's kind of reducing the complexity

1:33:46.080 --> 1:33:47.080
 of the whole thing.

1:33:47.080 --> 1:33:50.080
 So reducing the...

1:33:50.080 --> 1:33:52.080
 Lines of code will actually go lower.

1:33:52.080 --> 1:33:54.080
 Yeah, that's fascinating.

1:33:54.080 --> 1:33:56.080
 So you're doing fusion of all the sensors

1:33:56.080 --> 1:33:58.080
 and reducing the complexity of having to deal with these...

1:33:58.080 --> 1:33:59.080
 Fusion of the cameras.

1:33:59.080 --> 1:34:00.080
 Fusion of the cameras, really.

1:34:00.080 --> 1:34:03.080
 Right, yes.

1:34:03.080 --> 1:34:05.080
 Same with humans.

1:34:05.080 --> 1:34:07.080
 Well, I guess we've got ears too.

1:34:07.080 --> 1:34:11.080
 Yeah, we'll actually need to incorporate sound as well

1:34:11.080 --> 1:34:14.080
 because you need to listen for ambulance sirens

1:34:14.080 --> 1:34:20.080
 or fire trucks or somebody yelling at you or something.

1:34:20.080 --> 1:34:21.080
 I don't know.

1:34:21.080 --> 1:34:24.080
 There's a little bit of audio that needs to be incorporated as well.

1:34:24.080 --> 1:34:26.080
 Do you need to go back for a break?

1:34:26.080 --> 1:34:27.080
 Yeah, sure, let's take a break.

1:34:27.080 --> 1:34:28.080
 Okay.

1:34:28.080 --> 1:34:33.080
 Honestly, frankly, the ideas are the easy thing

1:34:33.080 --> 1:34:35.080
 and the implementation is the hard thing.

1:34:35.080 --> 1:34:37.080
 The idea of going to the moon is the easy part.

1:34:37.080 --> 1:34:39.080
 Not going to the moon is the hard part.

1:34:39.080 --> 1:34:40.080
 It's the hard part.

1:34:40.080 --> 1:34:42.080
 And there's a lot of hardcore engineering

1:34:42.080 --> 1:34:46.080
 that's got to get done at the hardware and software level.

1:34:46.080 --> 1:34:48.080
 Like I said, optimizing the C compiler

1:34:48.080 --> 1:34:55.080
 and just cutting out latency everywhere.

1:34:55.080 --> 1:34:59.080
 If we don't do this, the system will not work properly.

1:34:59.080 --> 1:35:02.080
 So the work of the engineers doing this,

1:35:02.080 --> 1:35:05.080
 they are like the unsung heroes,

1:35:05.080 --> 1:35:08.080
 but they are critical to the success of the situation.

1:35:08.080 --> 1:35:09.080
 I think you made it clear.

1:35:09.080 --> 1:35:11.080
 I mean, at least to me, it's super exciting.

1:35:11.080 --> 1:35:15.080
 Everything that's going on outside of what Andre is doing.

1:35:15.080 --> 1:35:17.080
 Just the whole infrastructure, the software.

1:35:17.080 --> 1:35:19.080
 I mean, everything is going on with Data Engine,

1:35:19.080 --> 1:35:21.080
 whatever it's called.

1:35:21.080 --> 1:35:24.080
 The whole process is just work of art to me.

1:35:24.080 --> 1:35:26.080
 The sheer scale of it boggles my mind.

1:35:26.080 --> 1:35:29.080
 Like the training, the amount of work done with,

1:35:29.080 --> 1:35:33.080
 like we've written all this custom software for training and labeling

1:35:33.080 --> 1:35:34.080
 and to do auto labeling.

1:35:34.080 --> 1:35:38.080
 Auto labeling is essential.

1:35:38.080 --> 1:35:42.080
 Because especially when you've got surround video, it's very difficult.

1:35:42.080 --> 1:35:48.080
 To label surround video from scratch is extremely difficult.

1:35:48.080 --> 1:35:52.080
 Like take a human such a long time to even label one video clip,

1:35:52.080 --> 1:35:54.080
 like several hours.

1:35:54.080 --> 1:36:00.080
 Or the auto label, basically we just apply heavy duty,

1:36:00.080 --> 1:36:06.080
 like a lot of compute to the video clips to preassign

1:36:06.080 --> 1:36:09.080
 and guess what all the things are that are going on in the surround video.

1:36:09.080 --> 1:36:10.080
 And then there's like correcting it.

1:36:10.080 --> 1:36:11.080
 Yeah.

1:36:11.080 --> 1:36:13.080
 And then all the human has to do is like tweet,

1:36:13.080 --> 1:36:16.080
 like say, adjust what is incorrect.

1:36:16.080 --> 1:36:21.080
 This is like increases productivity by in fact a hundred or more.

1:36:21.080 --> 1:36:22.080
 Yeah.

1:36:22.080 --> 1:36:25.080
 So you've presented Tesla Bot as primarily useful in the factory.

1:36:25.080 --> 1:36:28.080
 First of all, I think humanoid robots are incredible.

1:36:28.080 --> 1:36:32.080
 From a fan of robotics, I think the elegance of movement

1:36:32.080 --> 1:36:38.080
 that humanoid robots, that bipedal robots show are just so cool.

1:36:38.080 --> 1:36:40.080
 So it's really interesting that you're working on this

1:36:40.080 --> 1:36:44.080
 and also talking about applying the same kind of all the ideas,

1:36:44.080 --> 1:36:46.080
 some of which we've talked about with Data Engine,

1:36:46.080 --> 1:36:49.080
 all the things that we're talking about with Tesla Autopilot,

1:36:49.080 --> 1:36:54.080
 just transferring that over to just yet another robotics problem.

1:36:54.080 --> 1:36:57.080
 I have to ask, since I care about human robot interaction,

1:36:57.080 --> 1:36:59.080
 so the human side of that.

1:36:59.080 --> 1:37:01.080
 So you've talked about mostly in the factory.

1:37:01.080 --> 1:37:06.080
 Do you see part of this problem that Tesla Bot has to solve

1:37:06.080 --> 1:37:10.080
 is interacting with humans and potentially having a place like in the home.

1:37:10.080 --> 1:37:15.080
 So interacting, not just not replacing labor, but also like, I don't know,

1:37:15.080 --> 1:37:18.080
 being a friend or an assistant or something like that.

1:37:18.080 --> 1:37:27.080
 Yeah, I think the possibilities are endless.

1:37:27.080 --> 1:37:32.080
 It's not quite in Tesla's primary mission direction

1:37:32.080 --> 1:37:34.080
 of accelerating sustainable energy,

1:37:34.080 --> 1:37:38.080
 but it is an extremely useful thing that we can do for the world,

1:37:38.080 --> 1:37:44.080
 which is to make a useful humanoid robot that is capable of interacting with the world

1:37:44.080 --> 1:37:49.080
 and helping in many different ways.

1:37:49.080 --> 1:37:59.080
 I think if you say extrapolate to many years in the future,

1:37:59.080 --> 1:38:04.080
 I think work will become optional.

1:38:04.080 --> 1:38:10.080
 There's a lot of jobs that if people weren't paid to do it,

1:38:10.080 --> 1:38:12.080
 they wouldn't do it.

1:38:12.080 --> 1:38:14.080
 It's not fun necessarily.

1:38:14.080 --> 1:38:16.080
 If you're washing dishes all day,

1:38:16.080 --> 1:38:19.080
 it's like, you know, even if you really like washing dishes,

1:38:19.080 --> 1:38:22.080
 you really want to do it for eight hours a day every day.

1:38:22.080 --> 1:38:25.080
 Probably not.

1:38:25.080 --> 1:38:27.080
 And then there's like dangerous work.

1:38:27.080 --> 1:38:30.080
 And basically, if it's dangerous, boring,

1:38:30.080 --> 1:38:34.080
 it has like potential for repetitive stress injury, that kind of thing.

1:38:34.080 --> 1:38:40.080
 Then that's really where humanoid robots would add the most value initially.

1:38:40.080 --> 1:38:47.080
 So that's what we're aiming for is for the humanoid robots to do jobs

1:38:47.080 --> 1:38:51.080
 that people don't voluntarily want to do.

1:38:51.080 --> 1:38:53.080
 And then we'll have to pair that obviously

1:38:53.080 --> 1:38:56.080
 with some kind of universal basic income in the future.

1:38:56.080 --> 1:39:00.080
 So I think.

1:39:00.080 --> 1:39:05.080
 So do you see a world when there's like hundreds of millions of Tesla bots

1:39:05.080 --> 1:39:12.080
 doing different, performing different tasks throughout the world?

1:39:12.080 --> 1:39:14.080
 Yeah, I haven't really thought about it that far into the future,

1:39:14.080 --> 1:39:17.080
 but I guess there may be something like that.

1:39:17.080 --> 1:39:20.080
 So.

1:39:20.080 --> 1:39:22.080
 Can I ask a wild question?

1:39:22.080 --> 1:39:25.080
 So the number of Tesla cars has been accelerating.

1:39:25.080 --> 1:39:28.080
 There's been close to two million produced.

1:39:28.080 --> 1:39:30.080
 Many of them have autopilot.

1:39:30.080 --> 1:39:32.080
 I think we're over two million now.

1:39:32.080 --> 1:39:36.080
 Do you think there will ever be a time when there will be more Tesla bots

1:39:36.080 --> 1:39:40.080
 than Tesla cars?

1:39:40.080 --> 1:39:42.080
 Yeah.

1:39:42.080 --> 1:39:44.080
 Actually, it's funny you asked this question,

1:39:44.080 --> 1:39:47.080
 because normally I do try to think pretty far into the future,

1:39:47.080 --> 1:39:52.080
 but I haven't really thought that far into the future with the Tesla bot,

1:39:52.080 --> 1:39:55.080
 or it's codenamed Optimus.

1:39:55.080 --> 1:39:59.080
 I call it Optimus subprime.

1:39:59.080 --> 1:40:04.080
 It's not like a giant transformer robot.

1:40:04.080 --> 1:40:07.080
 So.

1:40:07.080 --> 1:40:14.080
 But it's meant to be a general purpose, helpful bot.

1:40:14.080 --> 1:40:18.080
 And basically, like the things that we're basically like, like,

1:40:18.080 --> 1:40:25.080
 Tesla, I think is the has the most advanced real world AI

1:40:25.080 --> 1:40:26.080
 for interacting with the real world,

1:40:26.080 --> 1:40:30.080
 which are developed as a function of to make self driving work.

1:40:30.080 --> 1:40:36.080
 And so along with custom hardware and like a lot of, you know,

1:40:36.080 --> 1:40:39.080
 hardcore low level software to have it run efficiently

1:40:39.080 --> 1:40:43.080
 and be power efficient because it's one thing to do neural nets

1:40:43.080 --> 1:40:45.080
 if you've got a gigantic server room with 10,000 computers.

1:40:45.080 --> 1:40:48.080
 But now let's say you just you have to now distill that down

1:40:48.080 --> 1:40:53.080
 into one computer that's running at low power in a humanoid robot or a car.

1:40:53.080 --> 1:40:54.080
 That's actually very difficult.

1:40:54.080 --> 1:40:59.080
 A lot of hardcore software work is required for that.

1:40:59.080 --> 1:41:05.080
 So since we're kind of like solving the navigate the real world

1:41:05.080 --> 1:41:08.080
 with neural nets problem for cars,

1:41:08.080 --> 1:41:10.080
 which are kind of robots with four wheels,

1:41:10.080 --> 1:41:14.080
 then it's like kind of a natural extension of that is to put it

1:41:14.080 --> 1:41:20.080
 in a robot with arms and legs and actuators.

1:41:20.080 --> 1:41:31.080
 So like the two hard things are like you basically need to make the

1:41:31.080 --> 1:41:34.080
 have the robot be intelligent enough to interact in a sensible way

1:41:34.080 --> 1:41:36.080
 with the environment.

1:41:36.080 --> 1:41:43.080
 So you need real world AI and you need to be very good at manufacturing,

1:41:43.080 --> 1:41:44.080
 which is a very hard problem.

1:41:44.080 --> 1:41:50.080
 Tesla is very good at manufacturing and also has the real world AI.

1:41:50.080 --> 1:41:58.080
 So making the humanoid robot work is basically means developing custom motors

1:41:58.080 --> 1:42:04.080
 and sensors that are different from what a car would use.

1:42:04.080 --> 1:42:10.080
 But we've also we have I think we have the best expertise

1:42:10.080 --> 1:42:15.080
 in developing advanced electric motors and power electronics.

1:42:15.080 --> 1:42:22.080
 So it just has to be for humanoid robot application on a car.

1:42:22.080 --> 1:42:25.080
 Still, you do talk about love sometimes.

1:42:25.080 --> 1:42:26.080
 So let me ask.

1:42:26.080 --> 1:42:29.080
 This isn't like for like sex robots or something like that.

1:42:29.080 --> 1:42:30.080
 Love is the answer.

1:42:30.080 --> 1:42:33.080
 Yes.

1:42:33.080 --> 1:42:36.080
 There is something compelling to us, not compelling,

1:42:36.080 --> 1:42:41.080
 but we connect with humanoid robots or even like robots like with the dog

1:42:41.080 --> 1:42:43.080
 and shapes of dogs.

1:42:43.080 --> 1:42:48.080
 It just it seems like, you know, there's a huge amount of loneliness in this world.

1:42:48.080 --> 1:42:51.080
 All of us seek companionship with other humans, friendship

1:42:51.080 --> 1:42:52.080
 and all those kinds of things.

1:42:52.080 --> 1:42:56.080
 We have a lot of here in Austin, a lot of people have dogs.

1:42:56.080 --> 1:43:01.080
 There seems to be a huge opportunity to also have robots that decrease

1:43:01.080 --> 1:43:09.080
 the amount of loneliness in the world or help us humans connect with each other.

1:43:09.080 --> 1:43:12.080
 So in a way that dogs can.

1:43:12.080 --> 1:43:14.080
 Do you think about that with TeslaBot at all?

1:43:14.080 --> 1:43:19.080
 Or is it really focused on the problem of performing specific tasks,

1:43:19.080 --> 1:43:23.080
 not connecting with humans?

1:43:23.080 --> 1:43:27.080
 I mean, to be honest, I have not actually thought about it from the companionship

1:43:27.080 --> 1:43:31.080
 standpoint, but I think it actually would end up being it could be actually

1:43:31.080 --> 1:43:34.080
 a very good companion.

1:43:34.080 --> 1:43:44.080
 And it could develop like a personality over time that is that is like unique,

1:43:44.080 --> 1:43:47.080
 like, you know, it's not like they're just all the robots are the same

1:43:47.080 --> 1:43:53.080
 and that personality could evolve to be, you know,

1:43:53.080 --> 1:43:59.080
 match the owner or the, you know, yes, the owner.

1:43:59.080 --> 1:44:02.080
 Well, whatever you want to call it.

1:44:02.080 --> 1:44:05.080
 The other half, right?

1:44:05.080 --> 1:44:06.080
 In the same way that friends do.

1:44:06.080 --> 1:44:09.080
 See, I think that's a huge opportunity.

1:44:09.080 --> 1:44:14.080
 Yeah, no, that's interesting.

1:44:14.080 --> 1:44:19.080
 Because, you know, like there's a Japanese phrase I like, Wabi Sabi,

1:44:19.080 --> 1:44:23.080
 you know, the subtle imperfections are what makes something special.

1:44:23.080 --> 1:44:28.080
 And the subtle imperfections of the personality of the robot mapped

1:44:28.080 --> 1:44:34.080
 to the subtle imperfections of the robot's human friend.

1:44:34.080 --> 1:44:36.080
 I don't know, owner sounds like maybe the wrong word,

1:44:36.080 --> 1:44:42.080
 but could actually make an incredible buddy, basically.

1:44:42.080 --> 1:44:43.080
 In that way, the imperfections.

1:44:43.080 --> 1:44:46.080
 Like R2D2 or like a C3PO sort of thing, you know.

1:44:46.080 --> 1:44:49.080
 So from a machine learning perspective,

1:44:49.080 --> 1:44:53.080
 I think the flaws being a feature is really nice.

1:44:53.080 --> 1:44:57.080
 You could be quite terrible at being a robot for quite a while

1:44:57.080 --> 1:45:00.080
 in the general home environment or in general world.

1:45:00.080 --> 1:45:02.080
 And that's kind of adorable.

1:45:02.080 --> 1:45:06.080
 And that's like, those are your flaws and you fall in love with those flaws.

1:45:06.080 --> 1:45:09.080
 So it's very different than autonomous driving

1:45:09.080 --> 1:45:13.080
 where it's a very high stakes environment you cannot mess up.

1:45:13.080 --> 1:45:17.080
 And so it's more fun to be a robot in the home.

1:45:17.080 --> 1:45:21.080
 Yeah, in fact, if you think of like C3PO and R2D2,

1:45:21.080 --> 1:45:24.080
 like they actually had a lot of like flaws and imperfections

1:45:24.080 --> 1:45:29.080
 and silly things and they would argue with each other.

1:45:29.080 --> 1:45:32.080
 Were they actually good at doing anything?

1:45:32.080 --> 1:45:34.080
 I'm not exactly sure.

1:45:34.080 --> 1:45:38.080
 They definitely added a lot to the story.

1:45:38.080 --> 1:45:43.080
 But there's sort of quirky elements and, you know,

1:45:43.080 --> 1:45:45.080
 that they would like make mistakes and do things.

1:45:45.080 --> 1:45:52.080
 It was like it made them relatable, I don't know, enduring.

1:45:52.080 --> 1:45:59.080
 So yeah, I think that that could be something that probably would happen.

1:45:59.080 --> 1:46:03.080
 But our initial focus is just to make it useful.

1:46:03.080 --> 1:46:06.080
 So I'm confident we'll get it done.

1:46:06.080 --> 1:46:08.080
 I'm not sure what the exact timeframe is,

1:46:08.080 --> 1:46:11.080
 but like we'll probably have, I don't know,

1:46:11.080 --> 1:46:15.080
 a decent prototype towards the end of next year or something like that.

1:46:15.080 --> 1:46:19.080
 And it's cool that it's connected to Tesla, the car.

1:46:19.080 --> 1:46:22.080
 Yeah, it's using a lot of, you know,

1:46:22.080 --> 1:46:25.080
 it would use the autopilot inference computer

1:46:25.080 --> 1:46:29.080
 and a lot of the training that we've done for cars

1:46:29.080 --> 1:46:32.080
 in terms of recognizing real world things

1:46:32.080 --> 1:46:38.080
 could be applied directly to the robot.

1:46:38.080 --> 1:46:42.080
 But there's a lot of custom actuators and sensors that need to be developed.

1:46:42.080 --> 1:46:47.080
 And an extra module on top of the vector space for love.

1:46:47.080 --> 1:46:48.080
 Yeah.

1:46:48.080 --> 1:46:51.080
 That's what I'm saying.

1:46:51.080 --> 1:46:53.080
 We can add that to the car too.

1:46:53.080 --> 1:46:55.080
 That's true.

1:46:55.080 --> 1:46:57.080
 Yeah, it could be useful in all environments.

1:46:57.080 --> 1:46:59.080
 Like you said, a lot of people argue in the car,

1:46:59.080 --> 1:47:02.080
 so maybe we can help them out.

1:47:02.080 --> 1:47:03.080
 You're a student of history,

1:47:03.080 --> 1:47:06.080
 fan of Dan Carlin's Hardcore History podcast.

1:47:06.080 --> 1:47:07.080
 Yeah, it's great.

1:47:07.080 --> 1:47:08.080
 Greatest podcast ever?

1:47:08.080 --> 1:47:11.080
 Yeah, I think it is actually.

1:47:11.080 --> 1:47:14.080
 It almost doesn't really count as a podcast.

1:47:14.080 --> 1:47:16.080
 It's more like an audio book.

1:47:16.080 --> 1:47:18.080
 So you were on the podcast with Dan.

1:47:18.080 --> 1:47:21.080
 I just had a chat with him about it.

1:47:21.080 --> 1:47:23.080
 He said you guys went military and all that kind of stuff.

1:47:23.080 --> 1:47:32.080
 Yeah, it was basically, it should be titled Engineer Wars, essentially.

1:47:32.080 --> 1:47:36.080
 Like when there's a rapid change in the rate of technology,

1:47:36.080 --> 1:47:43.080
 then engineering plays a pivotal role in victory and battle.

1:47:43.080 --> 1:47:45.080
 How far back in history did you go?

1:47:45.080 --> 1:47:47.080
 Did you go World War II?

1:47:47.080 --> 1:47:55.080
 Well, it was supposed to be a deep dive on fighters and bomber technology in World War II,

1:47:55.080 --> 1:47:58.080
 but that ended up being more wide ranging than that,

1:47:58.080 --> 1:48:04.080
 because I just went down the total rathole of studying all of the fighters and bombers of World War II

1:48:04.080 --> 1:48:10.080
 and the constant rock, paper, scissors game that one country would make this plane,

1:48:10.080 --> 1:48:15.080
 then it would make a plane to beat that, and that country would make a plane to beat that.

1:48:15.080 --> 1:48:18.080
 And really what matters is the pace of innovation

1:48:18.080 --> 1:48:25.080
 and also access to high quality fuel and raw materials.

1:48:25.080 --> 1:48:29.080
 Germany had some amazing designs, but they couldn't make them

1:48:29.080 --> 1:48:31.080
 because they couldn't get the raw materials,

1:48:31.080 --> 1:48:37.080
 and they had a real problem with the oil and fuel, basically.

1:48:37.080 --> 1:48:40.080
 The fuel quality was extremely variable.

1:48:40.080 --> 1:48:42.080
 So the design wasn't the bottleneck?

1:48:42.080 --> 1:48:47.080
 Yeah, the U.S. had kickass fuel that was very consistent.

1:48:47.080 --> 1:48:50.080
 The problem is if you make a very high performance aircraft engine,

1:48:50.080 --> 1:48:59.080
 in order to make it high performance, the fuel, the aviation gas,

1:48:59.080 --> 1:49:07.080
 has to be a consistent mixture and it has to have a high octane.

1:49:07.080 --> 1:49:11.080
 High octane is the most important thing, but it also can't have impurities and stuff

1:49:11.080 --> 1:49:14.080
 because you'll foul up the engine.

1:49:14.080 --> 1:49:16.080
 And Germany just never had good access to oil.

1:49:16.080 --> 1:49:22.080
 They tried to get it by invading the Caucasus, but that didn't work too well.

1:49:22.080 --> 1:49:23.080
 It never worked so well.

1:49:23.080 --> 1:49:24.080
 It didn't work out for them.

1:49:24.080 --> 1:49:26.080
 See you, Jerry.

1:49:26.080 --> 1:49:28.080
 Nice to meet you.

1:49:28.080 --> 1:49:31.080
 Germany was always struggling with basically shitty oil,

1:49:31.080 --> 1:49:37.080
 and they couldn't count on high quality fuel for their aircraft,

1:49:37.080 --> 1:49:43.080
 so they had to have all these additives and stuff.

1:49:43.080 --> 1:49:48.080
 Whereas the U.S. had awesome fuel, and they provided that to Britain as well.

1:49:48.080 --> 1:49:53.080
 So that allowed the British and the Americans to design aircraft engines

1:49:53.080 --> 1:49:58.080
 that were super high performance, better than anything else in the world.

1:49:58.080 --> 1:50:01.080
 Germany could design the engines, they just didn't have the fuel.

1:50:01.080 --> 1:50:06.080
 And then also the quality of the aluminum alloys that they were getting

1:50:06.080 --> 1:50:09.080
 was also not that great.

1:50:09.080 --> 1:50:11.080
 Is this like, you talked about all this with Dan?

1:50:11.080 --> 1:50:12.080
 Yep.

1:50:12.080 --> 1:50:13.080
 Awesome.

1:50:13.080 --> 1:50:16.080
 Broadly looking at history, when you look at Genghis Khan,

1:50:16.080 --> 1:50:22.080
 when you look at Stalin, Hitler, the darkest moments of human history,

1:50:22.080 --> 1:50:24.080
 what do you take away from those moments?

1:50:24.080 --> 1:50:28.080
 Does it help you gain insight about human nature, about human behavior today,

1:50:28.080 --> 1:50:32.080
 whether it's the wars or the individuals or just the behavior of people,

1:50:32.080 --> 1:50:41.080
 any aspects of history?

1:50:41.080 --> 1:50:49.080
 Yeah, I find history fascinating.

1:50:49.080 --> 1:50:54.080
 There's just a lot of incredible things that have been done, good and bad,

1:50:54.080 --> 1:51:06.080
 that they help you understand the nature of civilization and individuals.

1:51:06.080 --> 1:51:09.080
 Does it make you sad that humans do these kinds of things to each other?

1:51:09.080 --> 1:51:15.080
 You look at the 20th century, World War II, the cruelty, the abuse of power,

1:51:15.080 --> 1:51:20.080
 talk about communism, Marxism, and Stalin.

1:51:20.080 --> 1:51:24.080
 I mean, there's a lot of human history.

1:51:24.080 --> 1:51:28.080
 Most of it is actually people just getting on with their lives,

1:51:28.080 --> 1:51:35.080
 and it's not like human history is just nonstop war and disaster.

1:51:35.080 --> 1:51:38.080
 Those are actually just, those are intermittent and rare.

1:51:38.080 --> 1:51:46.080
 If they weren't, then humans would soon cease to exist.

1:51:46.080 --> 1:51:50.080
 But it's just that wars tend to be written about a lot,

1:51:50.080 --> 1:51:54.080
 whereas something being like, well,

1:51:54.080 --> 1:51:58.080
 a normal year where nothing major happened doesn't get written about much.

1:51:58.080 --> 1:52:04.080
 But that's, most people just like farming and kind of living their life,

1:52:04.080 --> 1:52:09.080
 being a villager somewhere.

1:52:09.080 --> 1:52:16.080
 And every now and again, there's a war.

1:52:16.080 --> 1:52:23.080
 And I would have to say, there aren't very many books where I just had to stop reading

1:52:23.080 --> 1:52:26.080
 because it was just too dark.

1:52:26.080 --> 1:52:32.080
 But the book about Stalin, The Court of the Red Tsar, I had to stop reading.

1:52:32.080 --> 1:52:37.080
 It was just too dark and rough.

1:52:37.080 --> 1:52:39.080
 Yeah.

1:52:39.080 --> 1:52:44.080
 The 30s, there's a lot of lessons there to me,

1:52:44.080 --> 1:52:48.080
 in particular that it feels like humans,

1:52:48.080 --> 1:52:53.080
 like all of us have that, it's the old Solzhenitsyn line,

1:52:53.080 --> 1:52:56.080
 that the line between good and evil runs through the heart of every man,

1:52:56.080 --> 1:52:59.080
 that all of us are capable of evil, all of us are capable of good.

1:52:59.080 --> 1:53:04.080
 It's almost like this kind of responsibility that all of us have

1:53:04.080 --> 1:53:07.080
 to tend towards the good.

1:53:07.080 --> 1:53:11.080
 And so to me, looking at history is almost like an example of,

1:53:11.080 --> 1:53:16.080
 look, you have some charismatic leader that convinces you of things.

1:53:16.080 --> 1:53:21.080
 It's too easy, based on that story, to do evil onto each other,

1:53:21.080 --> 1:53:23.080
 onto your family, onto others.

1:53:23.080 --> 1:53:26.080
 And so it's like our responsibility to do good.

1:53:26.080 --> 1:53:29.080
 It's not like now is somehow different from history.

1:53:29.080 --> 1:53:32.080
 That can happen again. All of it can happen again.

1:53:32.080 --> 1:53:35.080
 And yes, most of the time, you're right,

1:53:35.080 --> 1:53:39.080
 the optimistic view here is mostly people are just living life.

1:53:39.080 --> 1:53:44.080
 And as you've often memed about, the quality of life was way worse

1:53:44.080 --> 1:53:47.080
 back in the day, and this keeps improving over time

1:53:47.080 --> 1:53:49.080
 through innovation, through technology.

1:53:49.080 --> 1:53:54.080
 But still, it's somehow notable that these blimps of atrocities happen.

1:53:54.080 --> 1:53:56.080
 Sure.

1:53:56.080 --> 1:54:02.080
 Yeah, I mean, life was really tough for most of history.

1:54:02.080 --> 1:54:07.080
 I mean, for most of human history, a good year would be one

1:54:07.080 --> 1:54:11.080
 where not that many people in your village died of the plague,

1:54:11.080 --> 1:54:16.080
 starvation, freezing to death, or being killed by a neighboring village.

1:54:16.080 --> 1:54:18.080
 It's like, well, it wasn't that bad.

1:54:18.080 --> 1:54:20.080
 It was only like we lost 5% this year.

1:54:20.080 --> 1:54:23.080
 That was a good year.

1:54:23.080 --> 1:54:25.080
 That would be par for the course.

1:54:25.080 --> 1:54:28.080
 Just not starving to death would have been the primary goal

1:54:28.080 --> 1:54:31.080
 of most people throughout history,

1:54:31.080 --> 1:54:34.080
 is making sure we'll have enough food to last through the winter

1:54:34.080 --> 1:54:36.080
 and not freeze or whatever.

1:54:36.080 --> 1:54:42.080
 So, now food is plentiful.

1:54:42.080 --> 1:54:46.080
 I have an obesity problem.

1:54:46.080 --> 1:54:50.080
 Well, yeah, the lesson there is to be grateful for the way things are now

1:54:50.080 --> 1:54:53.080
 for some of us.

1:54:53.080 --> 1:54:56.080
 We've spoken about this offline.

1:54:56.080 --> 1:55:00.080
 I'd love to get your thought about it here.

1:55:00.080 --> 1:55:05.080
 If I sat down for a long form in person conversation with the President of Russia,

1:55:05.080 --> 1:55:10.080
 Vladimir Putin, would you potentially want to call in for a few minutes

1:55:10.080 --> 1:55:15.080
 to join in on a conversation with him, moderated and translated by me?

1:55:15.080 --> 1:55:16.080
 Sure, yeah.

1:55:16.080 --> 1:55:19.080
 Sure, I'd be happy to do that.

1:55:19.080 --> 1:55:22.080
 You've shown interest in the Russian language.

1:55:22.080 --> 1:55:27.080
 Is this grounded in your interest in history of linguistics, culture, general curiosity?

1:55:27.080 --> 1:55:29.080
 I think it sounds cool.

1:55:29.080 --> 1:55:32.080
 Sounds cool and that looks cool.

1:55:32.080 --> 1:55:39.080
 Well, it takes a moment to read Cyrillic.

1:55:39.080 --> 1:55:43.080
 Once you know what the Cyrillic characters stand for,

1:55:43.080 --> 1:55:47.080
 actually then reading Russian becomes a lot easier

1:55:47.080 --> 1:55:49.080
 because there are a lot of words that are actually the same.

1:55:49.080 --> 1:55:54.080
 Like bank is bank.

1:55:54.080 --> 1:55:59.080
 So find the words that are exactly the same and now you start to understand Cyrillic.

1:55:59.080 --> 1:56:06.080
 If you can sound it out, there's at least some commonality of words.

1:56:06.080 --> 1:56:09.080
 What about the culture?

1:56:09.080 --> 1:56:12.080
 You love great engineering, physics.

1:56:12.080 --> 1:56:14.080
 There's a tradition of the sciences there.

1:56:14.080 --> 1:56:17.080
 You look at the 20th century from rocketry.

1:56:17.080 --> 1:56:24.080
 Some of the greatest rockets, some of the space exploration has been done in the former Soviet Union.

1:56:24.080 --> 1:56:27.080
 So do you draw inspiration from that history?

1:56:27.080 --> 1:56:33.080
 Just how this culture that in many ways, one of the sad things is because of the language,

1:56:33.080 --> 1:56:37.080
 a lot of it is lost to history because it's not translated.

1:56:37.080 --> 1:56:40.080
 Because it is in some ways an isolated culture.

1:56:40.080 --> 1:56:45.080
 It flourishes within its borders.

1:56:45.080 --> 1:56:51.080
 So do you draw inspiration from those folks, from the history of science engineering there?

1:56:51.080 --> 1:57:02.080
 The Soviet Union, Russia and Ukraine as well have a really strong history in space flight.

1:57:02.080 --> 1:57:11.080
 Some of the most advanced and impressive things in history were done by the Soviet Union.

1:57:11.080 --> 1:57:20.080
 So one cannot help but admire the impressive rocket technology that was developed.

1:57:20.080 --> 1:57:29.080
 After the fall of the Soviet Union, there's much less that then happened.

1:57:29.080 --> 1:57:46.080
 But still things are happening, but it's not quite at the frenetic pace that it was happening before the Soviet Union kind of dissolved into separate republics.

1:57:46.080 --> 1:57:52.080
 Yeah, there's Roscosmos, the Russian agency.

1:57:52.080 --> 1:57:57.080
 I look forward to a time when those countries with China are working together.

1:57:57.080 --> 1:58:00.080
 The United States are all working together.

1:58:00.080 --> 1:58:02.080
 Maybe a little bit of friendly competition.

1:58:02.080 --> 1:58:04.080
 I think friendly competition is good.

1:58:04.080 --> 1:58:09.080
 Governments are slow and the only thing slower than one government is a collection of governments.

1:58:09.080 --> 1:58:16.080
 So the Olympics would be boring if everyone just crossed the finishing line at the same time.

1:58:16.080 --> 1:58:18.080
 Nobody would watch.

1:58:18.080 --> 1:58:22.080
 And people wouldn't try hard to run fast and stuff.

1:58:22.080 --> 1:58:25.080
 So I think friendly competition is a good thing.

1:58:25.080 --> 1:58:29.080
 This is also a good place to give a shout out to a video titled,

1:58:29.080 --> 1:58:34.080
 The Entire Soviet Rocket Engine Family Tree by Tim Dodd, AKA Everyday Astronaut.

1:58:34.080 --> 1:58:35.080
 It's like an hour and a half.

1:58:35.080 --> 1:58:38.080
 It gives the full history of Soviet rockets.

1:58:38.080 --> 1:58:41.080
 And people should definitely go check out and support Tim in general.

1:58:41.080 --> 1:58:45.080
 That guy is super excited about the future, super excited about space flight.

1:58:45.080 --> 1:58:50.080
 Every time I see anything by him, I just have a stupid smile on my face because he's so excited about stuff.

1:58:50.080 --> 1:58:53.080
 Yeah, Tim Dodd is really great.

1:58:53.080 --> 1:59:02.080
 If you're interested in anything to do with space, he's, in terms of explaining rocket technology to your average person, he's awesome.

1:59:02.080 --> 1:59:04.080
 The best, I'd say.

1:59:04.080 --> 1:59:14.080
 And I should say like the part of the reason like I switched us from like Rafter at one point was going to be a hydrogen engine.

1:59:14.080 --> 1:59:17.080
 But hydrogen has a lot of challenges.

1:59:17.080 --> 1:59:18.080
 It's very low density.

1:59:18.080 --> 1:59:19.080
 It's a deep cryogen.

1:59:19.080 --> 1:59:23.080
 So it's only liquid at a very, very close to absolute zero.

1:59:23.080 --> 1:59:26.080
 Requires a lot of insulation.

1:59:26.080 --> 1:59:30.080
 So it is a lot of challenges there.

1:59:30.080 --> 1:59:35.080
 And I was actually reading a bit about Russian rocket engine developments.

1:59:35.080 --> 1:59:50.080
 And at least the impression I had was that the Soviet Union, Russia and Ukraine primarily were actually in the process of switching to Methalox.

1:59:50.080 --> 1:59:55.080
 And there were some interesting tests and data for ISP.

1:59:55.080 --> 2:00:01.080
 Like they were able to get like up to like a 380 second ISP with the Methalox engine.

2:00:01.080 --> 2:00:05.080
 And I was like, well, OK, that's actually really impressive.

2:00:05.080 --> 2:00:19.080
 So I think you could actually get a much lower cost, like in optimizing cost per ton to orbit, cost per ton to Mars.

2:00:19.080 --> 2:00:25.080
 I think Methalox is the way to go.

2:00:25.080 --> 2:00:32.080
 And I was partly inspired by the Russian work on the test stands with Methalox engines.

2:00:32.080 --> 2:00:35.080
 And now for something completely different.

2:00:35.080 --> 2:00:41.080
 Do you mind doing a bit of a meme review in the spirit of the great, the powerful PewDiePie?

2:00:41.080 --> 2:00:42.080
 Let's say 1 to 11.

2:00:42.080 --> 2:00:45.080
 Just go over a few documents printed out.

2:00:45.080 --> 2:00:46.080
 We can try.

2:00:46.080 --> 2:00:49.080
 Let's try this.

2:00:49.080 --> 2:00:56.080
 I present to you document numero uno.

2:00:56.080 --> 2:00:58.080
 I don't know. OK.

2:00:58.080 --> 2:01:03.080
 Vladimir Impaler discovers marshmallows.

2:01:03.080 --> 2:01:07.080
 That's not bad.

2:01:07.080 --> 2:01:11.080
 So you get it? Because he likes impaling things.

2:01:11.080 --> 2:01:12.080
 Yes, I get it.

2:01:12.080 --> 2:01:14.080
 I don't know, three, whatever.

2:01:14.080 --> 2:01:19.080
 That's not very good.

2:01:19.080 --> 2:01:28.080
 This is grounded in some engineering, some history.

2:01:28.080 --> 2:01:31.080
 Yeah, give us an eight out of ten.

2:01:31.080 --> 2:01:33.080
 What do you think about nuclear power?

2:01:33.080 --> 2:01:34.080
 I'm in favor of nuclear power.

2:01:34.080 --> 2:01:47.080
 I think in a place that is not subject to extreme natural disasters, I think nuclear power is a great way to generate electricity.

2:01:47.080 --> 2:01:51.080
 I don't think we should be shutting down nuclear power stations.

2:01:51.080 --> 2:01:53.080
 Yeah, but what about Chernobyl?

2:01:53.080 --> 2:01:56.080
 Exactly.

2:01:56.080 --> 2:02:04.080
 I think there's a lot of fear of radiation and stuff.

2:02:04.080 --> 2:02:13.080
 The problem is a lot of people just don't study engineering or physics.

2:02:13.080 --> 2:02:16.080
 Just the word radiation just sounds scary.

2:02:16.080 --> 2:02:21.080
 They can't calibrate what radiation means.

2:02:21.080 --> 2:02:30.080
 But radiation is much less dangerous than you think.

2:02:30.080 --> 2:02:41.080
 For example, Fukushima, when the Fukushima problem happened due to the tsunami,

2:02:41.080 --> 2:02:46.080
 I got people in California asking me if they should worry about radiation from Fukushima.

2:02:46.080 --> 2:02:51.080
 I'm like, definitely not, not even slightly, not at all.

2:02:51.080 --> 2:02:54.080
 That is crazy.

2:02:54.080 --> 2:03:09.080
 Just to show this is how the danger is so much overplayed compared to what it really is that I actually flew to Fukushima.

2:03:09.080 --> 2:03:28.080
 I donated a solar power system for a water treatment plant, and I made a point of eating locally grown vegetables on TV in Fukushima.

2:03:28.080 --> 2:03:31.080
 I'm still alive.

2:03:31.080 --> 2:03:36.080
 So it's not even that the risk of these events is low, but the impact of them is...

2:03:36.080 --> 2:03:38.080
 The impact is greatly exaggerated.

2:03:38.080 --> 2:03:40.080
 It's human nature.

2:03:40.080 --> 2:03:42.080
 People don't know what radiation is.

2:03:42.080 --> 2:03:46.080
 I've had people ask me, what about radiation from cell phones causing brain cancer?

2:03:46.080 --> 2:03:49.080
 I'm like, when you say radiation, do you mean photons or particles?

2:03:49.080 --> 2:03:52.080
 They're like, I don't know, what do you mean photons or particles?

2:03:52.080 --> 2:03:56.080
 Do you mean, let's say, photons?

2:03:56.080 --> 2:03:59.080
 What frequency or wavelength?

2:03:59.080 --> 2:04:01.080
 And they're like, I have no idea.

2:04:01.080 --> 2:04:04.080
 Do you know that everything's radiating all the time?

2:04:04.080 --> 2:04:06.080
 What do you mean?

2:04:06.080 --> 2:04:08.080
 Like, yeah, everything's radiating all the time.

2:04:08.080 --> 2:04:12.080
 Photons are being emitted by all objects all the time, basically.

2:04:12.080 --> 2:04:21.080
 And if you want to know what it means to stand in front of nuclear fire, go outside.

2:04:21.080 --> 2:04:28.080
 The sun is a gigantic thermonuclear reactor that you're staring right at it.

2:04:28.080 --> 2:04:29.080
 Are you still alive?

2:04:29.080 --> 2:04:30.080
 Yes.

2:04:30.080 --> 2:04:32.080
 Okay, amazing.

2:04:32.080 --> 2:04:39.080
 Yeah, I guess radiation is one of the words that could be used as a tool to fear monger by certain people.

2:04:39.080 --> 2:04:40.080
 That's it.

2:04:40.080 --> 2:04:42.080
 I think people just don't understand.

2:04:42.080 --> 2:04:46.080
 I mean, that's the way to fight that fear, I suppose, is to understand, is to learn.

2:04:46.080 --> 2:04:50.080
 Yeah, just say, okay, how many people have actually died from nuclear accidents?

2:04:50.080 --> 2:04:51.080
 It's practically nothing.

2:04:51.080 --> 2:04:57.080
 And say, how many people have died from coal plants?

2:04:57.080 --> 2:04:59.080
 And it's a very big number.

2:04:59.080 --> 2:05:05.080
 So, like, obviously we should not be starting up coal plants and shutting down nuclear plants.

2:05:05.080 --> 2:05:08.080
 It just doesn't make any sense at all.

2:05:08.080 --> 2:05:15.080
 Coal plants, like, I don't know, 100 to 1,000 times worse for health than nuclear power plants.

2:05:15.080 --> 2:05:16.080
 You want to go to the next one?

2:05:16.080 --> 2:05:20.080
 This is really bad.

2:05:20.080 --> 2:05:24.080
 It's 90, 180, and 360 degrees.

2:05:24.080 --> 2:05:25.080
 Everybody loves the math.

2:05:25.080 --> 2:05:28.080
 Nobody gives a shit about 270.

2:05:28.080 --> 2:05:30.080
 It's not super funny.

2:05:30.080 --> 2:05:32.080
 I don't know, like, 203.

2:05:32.080 --> 2:05:37.080
 This is not a, you know, LOL situation.

2:05:37.080 --> 2:05:43.080
 Yeah.

2:05:43.080 --> 2:05:44.080
 That was pretty good.

2:05:44.080 --> 2:05:48.080
 The United States oscillating between establishing and destroying dictatorships.

2:05:48.080 --> 2:05:49.080
 It's like a metronome.

2:05:49.080 --> 2:05:50.080
 Is that a metronome?

2:05:50.080 --> 2:05:53.080
 Yeah, it's out of 7 out of 10.

2:05:53.080 --> 2:05:54.080
 It's kind of true.

2:05:54.080 --> 2:05:57.080
 Oh, yeah, this is kind of personal for me.

2:05:57.080 --> 2:05:58.080
 Next one.

2:05:58.080 --> 2:05:59.080
 Oh, man.

2:05:59.080 --> 2:06:00.080
 Is this Leica?

2:06:00.080 --> 2:06:01.080
 Yeah.

2:06:01.080 --> 2:06:02.080
 Well, no.

2:06:02.080 --> 2:06:03.080
 Or it's like referring to Leica or something?

2:06:03.080 --> 2:06:06.080
 As Leica's, like, husband.

2:06:06.080 --> 2:06:07.080
 Husband.

2:06:07.080 --> 2:06:08.080
 Yeah, yeah.

2:06:08.080 --> 2:06:09.080
 Hello.

2:06:09.080 --> 2:06:10.080
 Yes, this is dog.

2:06:10.080 --> 2:06:11.080
 Your wife was launched into space.

2:06:11.080 --> 2:06:16.080
 And then the last one is him with his eyes closed and a bottle of vodka.

2:06:16.080 --> 2:06:17.080
 Yeah.

2:06:17.080 --> 2:06:18.080
 Leica didn't come back.

2:06:18.080 --> 2:06:19.080
 No.

2:06:19.080 --> 2:06:24.080
 They don't tell you the full story of, you know, what the impact they had on the loved

2:06:24.080 --> 2:06:25.080
 ones.

2:06:25.080 --> 2:06:26.080
 True.

2:06:26.080 --> 2:06:27.080
 Yeah.

2:06:27.080 --> 2:06:28.080
 It's like 711 for me.

2:06:28.080 --> 2:06:29.080
 Sure.

2:06:29.080 --> 2:06:30.080
 The Soviet shadow.

2:06:30.080 --> 2:06:31.080
 Oh, yeah.

2:06:31.080 --> 2:06:33.400
 This keeps going on the Russian theme.

2:06:33.400 --> 2:06:35.160
 First man in space.

2:06:35.160 --> 2:06:36.160
 Nobody cares.

2:06:36.160 --> 2:06:37.160
 First man on the moon.

2:06:37.160 --> 2:06:38.160
 Well, I think people do care.

2:06:38.160 --> 2:06:39.160
 No, I know.

2:06:39.160 --> 2:06:40.160
 But...

2:06:40.160 --> 2:06:41.160
 There is...

2:06:41.160 --> 2:06:45.760
 Yuri Gagarin's name will be forever in history, I think.

2:06:45.760 --> 2:06:52.080
 There is something special about placing, like, stepping foot onto another totally foreign

2:06:52.080 --> 2:06:53.080
 land.

2:06:53.080 --> 2:06:56.960
 It's not the journey, like, people that explore the oceans.

2:06:56.960 --> 2:07:01.400
 It's not as important to explore the oceans as to land on a whole new continent.

2:07:01.400 --> 2:07:02.400
 Yeah.

2:07:02.400 --> 2:07:05.080
 Well, this is about you.

2:07:05.080 --> 2:07:08.200
 Oh, yeah, I'd love to get your comment on this.

2:07:08.200 --> 2:07:14.680
 Elon Musk, after sending 6.6 billion dollars to the UN to end world hunger, you have three

2:07:14.680 --> 2:07:15.680
 hours.

2:07:15.680 --> 2:07:24.240
 Yeah, well, I mean, obviously, 6 billion dollars is not going to end world hunger.

2:07:24.240 --> 2:07:30.680
 So I mean, the reality is at this point, the world is producing far more food than it can

2:07:30.680 --> 2:07:31.680
 really consume.

2:07:31.680 --> 2:07:35.920
 Like, we don't have a caloric constraint at this point.

2:07:35.920 --> 2:07:43.280
 So where there is hunger, it is almost always due to, like, civil war or strife or some

2:07:43.280 --> 2:07:52.860
 like, it's not a thing that is extremely rare for it to be just a matter of, like, lack

2:07:52.860 --> 2:07:53.860
 of money.

2:07:53.860 --> 2:07:59.000
 It's like, you know, it's like some civil war in some country and like one part of the

2:07:59.000 --> 2:08:03.040
 country is literally trying to starve the other part of the country.

2:08:03.040 --> 2:08:05.840
 So it's much more complex than something that money could solve.

2:08:05.840 --> 2:08:07.440
 It's geopolitics.

2:08:07.440 --> 2:08:09.840
 It's a lot of things.

2:08:09.840 --> 2:08:10.840
 It's human nature.

2:08:10.840 --> 2:08:11.840
 It's governments.

2:08:11.840 --> 2:08:14.520
 It's money, monetary systems, all that kind of stuff.

2:08:14.520 --> 2:08:17.640
 Yeah, food is extremely cheap these days.

2:08:17.640 --> 2:08:26.720
 It's like, I mean, the US at this point, you know, among low income families, obesity is

2:08:26.720 --> 2:08:27.960
 actually another problem.

2:08:27.960 --> 2:08:31.120
 It's not, like, obesity, it's not hunger.

2:08:31.120 --> 2:08:34.660
 It's like too much, you know, too many calories.

2:08:34.660 --> 2:08:37.880
 So it's not that nobody's hungry anywhere.

2:08:37.880 --> 2:08:43.960
 It's just, this is not a simple matter of adding money and solving it.

2:08:43.960 --> 2:08:44.960
 Hmm.

2:08:44.960 --> 2:08:48.800
 What do you think that one gets?

2:08:48.800 --> 2:08:49.800
 It's getting...

2:08:49.800 --> 2:08:50.800
 Two.

2:08:50.800 --> 2:08:57.560
 We're just going after empires, world, where did you get those artifacts?

2:08:57.560 --> 2:08:58.560
 The British Museum.

2:08:58.560 --> 2:09:01.600
 Shout out to Monty Python.

2:09:01.600 --> 2:09:02.600
 We found them.

2:09:02.600 --> 2:09:03.600
 Yeah.

2:09:03.600 --> 2:09:05.800
 The British Museum is pretty great.

2:09:05.800 --> 2:09:10.200
 I mean, admittedly Britain did take these historical artifacts from all around the world

2:09:10.200 --> 2:09:16.120
 and put them in London, but, you know, it's not like people can't go see them.

2:09:16.120 --> 2:09:23.480
 So it is a convenient place to see these ancient artifacts is London for, you know, for a large

2:09:23.480 --> 2:09:25.080
 segment of the world.

2:09:25.080 --> 2:09:29.640
 So I think, you know, on balance, the British Museum is a net good, although I'm sure a

2:09:29.640 --> 2:09:31.440
 lot of countries will argue about that.

2:09:31.440 --> 2:09:32.440
 Yeah.

2:09:32.440 --> 2:09:35.960
 It's like you want to make these historical artifacts accessible to as many people as

2:09:35.960 --> 2:09:41.520
 possible and the British Museum, I think, does a good job of that.

2:09:41.520 --> 2:09:45.200
 Even if there's a darker aspect to like the history of empire in general, whatever the

2:09:45.200 --> 2:09:52.120
 empire is, however things were done, it is the history that happened.

2:09:52.120 --> 2:09:54.280
 You can't sort of erase that history, unfortunately.

2:09:54.280 --> 2:09:55.920
 You could just become better in the future.

2:09:55.920 --> 2:09:57.520
 That's the point.

2:09:57.520 --> 2:09:58.520
 Yeah.

2:09:58.520 --> 2:10:04.040
 I mean, it's like, well, how are we going to pass moral judgment on these things?

2:10:04.040 --> 2:10:10.400
 Like it's like if, you know, if one is going to judge, say, the Russian Empire, you've

2:10:10.400 --> 2:10:14.720
 got to judge, you know, what everyone was doing at the time and how were the British

2:10:14.720 --> 2:10:18.200
 relative to everyone.

2:10:18.200 --> 2:10:22.640
 And I think the British would actually get like a relatively good grade, relatively good

2:10:22.640 --> 2:10:30.040
 grade, not in absolute terms, but compared to what everyone else was doing, they were

2:10:30.040 --> 2:10:31.600
 not the worst.

2:10:31.600 --> 2:10:35.400
 Like I said, you got to look at these things in the context of the history at the time

2:10:35.400 --> 2:10:39.200
 and say, what were the alternatives and what are you comparing it against?

2:10:39.200 --> 2:10:47.220
 And I do not think it would be the case that Britain would get a bad grade when looking

2:10:47.220 --> 2:10:48.560
 at history at the time.

2:10:48.560 --> 2:10:56.280
 You know, if you judge history from, you know, from what is morally acceptable today, you

2:10:56.280 --> 2:10:58.400
 basically are going to give everyone a failing grade.

2:10:58.400 --> 2:10:59.400
 I'm not clear.

2:10:59.400 --> 2:11:04.720
 It's not, I don't think anyone would get a passing grade in their morality of like you

2:11:04.720 --> 2:11:08.960
 could go back 300 years ago, like who's getting a passing grade?

2:11:08.960 --> 2:11:10.760
 Basically no one.

2:11:10.760 --> 2:11:16.280
 And we might not get a passing grade from generations that come after us.

2:11:16.280 --> 2:11:18.880
 What does that one get?

2:11:18.880 --> 2:11:19.880
 Sure.

2:11:19.880 --> 2:11:20.880
 Six, seven.

2:11:20.880 --> 2:11:21.880
 For the Monty Python, maybe.

2:11:21.880 --> 2:11:22.880
 I always love Monty Python.

2:11:22.880 --> 2:11:23.880
 They're great.

2:11:23.880 --> 2:11:28.880
 The Life of Brian and the Quest of the Holy Grail are incredible.

2:11:28.880 --> 2:11:29.880
 Yeah.

2:11:29.880 --> 2:11:30.880
 Yeah.

2:11:30.880 --> 2:11:31.880
 Yeah.

2:11:31.880 --> 2:11:32.880
 Those serious eyebrows.

2:11:32.880 --> 2:11:37.480
 How important do you think is facial hair to great leadership?

2:11:37.480 --> 2:11:41.480
 Well, you got a new haircut.

2:11:41.480 --> 2:11:42.480
 How does that affect your leadership?

2:11:42.480 --> 2:11:43.480
 I don't know.

2:11:43.480 --> 2:11:44.480
 Hopefully not.

2:11:44.480 --> 2:11:45.480
 It doesn't.

2:11:45.480 --> 2:11:46.480
 Is that the second no one?

2:11:46.480 --> 2:11:47.480
 Yeah.

2:11:47.480 --> 2:11:48.480
 The second is no one.

2:11:48.480 --> 2:11:49.480
 There is no one competing with Brezhnev.

2:11:49.480 --> 2:11:50.480
 No one, too.

2:11:50.480 --> 2:11:51.480
 Those are like epic eyebrows.

2:11:51.480 --> 2:11:52.480
 Yeah.

2:11:52.480 --> 2:11:53.480
 Sure.

2:11:53.480 --> 2:11:54.480
 That's ridiculous.

2:11:54.480 --> 2:11:55.480
 Give it a six or seven, I don't know.

2:11:55.480 --> 2:11:56.480
 I like this Shakespearean analysis of memes.

2:11:56.480 --> 2:11:57.480
 Brezhnev, he had a flair for drama as well.

2:11:57.480 --> 2:11:58.480
 Like, you know, showmanship.

2:11:58.480 --> 2:11:59.480
 Yeah.

2:11:59.480 --> 2:12:00.480
 Yeah.

2:12:00.480 --> 2:12:01.480
 It must come from the eyebrows.

2:12:01.480 --> 2:12:02.480
 All right.

2:12:02.480 --> 2:12:20.960
 Invention, great engineering, look what I invented, that's the best thing since ripped

2:12:20.960 --> 2:12:21.960
 up bread.

2:12:21.960 --> 2:12:22.960
 Yeah.

2:12:22.960 --> 2:12:29.000
 Because they invented sliced bread, am I just explaining memes at this point?

2:12:29.000 --> 2:12:40.000
 This is where my life has become a meme, what it like, you know, like a scribe that like

2:12:40.000 --> 2:12:44.120
 runs around with the kings and just like writes down memes.

2:12:44.120 --> 2:12:46.280
 I mean, when was the cheeseburger invented?

2:12:46.280 --> 2:12:47.280
 That's like an epic invention.

2:12:47.280 --> 2:12:48.280
 Yeah.

2:12:48.280 --> 2:12:49.280
 Like, like, wow.

2:12:49.280 --> 2:12:50.280
 Yeah.

2:12:50.280 --> 2:12:57.680
 Versus just like a burger or a burger, I guess a burger in general is like, you know, then

2:12:57.680 --> 2:13:01.800
 there's like, what is a burger, what's a sandwich, and then you start getting a pizza sandwich

2:13:01.800 --> 2:13:05.720
 and what is the original, it gets into an ontology argument.

2:13:05.720 --> 2:13:06.720
 Yeah.

2:13:06.720 --> 2:13:08.920
 But everybody knows like if you order like a burger or cheeseburger or whatever and you

2:13:08.920 --> 2:13:14.000
 like, you got like, you know, tomato and some lettuce and onions and whatever and, you know,

2:13:14.000 --> 2:13:16.040
 mayor and ketchup and mustard, it's like epic.

2:13:16.040 --> 2:13:17.040
 Yeah.

2:13:17.040 --> 2:13:21.080
 But I'm sure they've had bread and meat separately for a long time and it was kind of a burger

2:13:21.080 --> 2:13:25.960
 on the same plate, but somebody who actually combined them into the same thing and then

2:13:25.960 --> 2:13:29.040
 you bite it and hold it makes it convenient.

2:13:29.040 --> 2:13:30.040
 It's a materials problem.

2:13:30.040 --> 2:13:31.040
 Yeah.

2:13:31.040 --> 2:13:33.520
 Like your hands don't get dirty and whatever.

2:13:33.520 --> 2:13:34.520
 Yeah.

2:13:34.520 --> 2:13:35.520
 It's brilliant.

2:13:35.520 --> 2:13:43.280
 Well, that is not what I would have guessed, but everyone knows like you, if you order

2:13:43.280 --> 2:13:46.800
 a cheeseburger, you know what you're getting, you know, it's not like some obtuse, like,

2:13:46.800 --> 2:13:52.800
 well, I wonder what I'll get, you know, um, you know, uh, fries are, I mean, great.

2:13:52.800 --> 2:13:56.320
 I mean, they were the devil, but fries are awesome.

2:13:56.320 --> 2:14:00.320
 And uh, yeah, pizza is incredible.

2:14:00.320 --> 2:14:05.480
 Food innovation doesn't get enough love, I guess is what we're getting at.

2:14:05.480 --> 2:14:06.480
 Great.

2:14:06.480 --> 2:14:13.520
 Um, uh, what about the, uh, Matthew McConaughey, Austinite here, uh, president Kennedy, do

2:14:13.520 --> 2:14:15.200
 you know how to put men on the moon yet?

2:14:15.200 --> 2:14:16.200
 NASA?

2:14:16.200 --> 2:14:17.200
 No.

2:14:17.200 --> 2:14:19.440
 President Kennedy, it'd be a lot cooler if you did.

2:14:19.440 --> 2:14:25.360
 Pretty much sure, six, six or seven, I suppose.

2:14:25.360 --> 2:14:26.360
 All right.

2:14:26.360 --> 2:14:30.880
 And this is the last one that's funny.

2:14:30.880 --> 2:14:35.680
 Someone drew a bunch of dicks all over the walls, 16 chapel boys bath.

2:14:35.680 --> 2:14:36.680
 Sure.

2:14:36.680 --> 2:14:37.680
 I'll give it a nine.

2:14:37.680 --> 2:14:38.680
 It's super.

2:14:38.680 --> 2:14:39.680
 It's really true.

2:14:39.680 --> 2:14:40.680
 All right.

2:14:40.680 --> 2:14:41.680
 This is our highest ranking meme for today.

2:14:41.680 --> 2:14:42.680
 I mean, it's true.

2:14:42.680 --> 2:14:44.880
 Like, how do they get away with it?

2:14:44.880 --> 2:14:45.880
 Lots of nakedness.

2:14:45.880 --> 2:14:49.640
 I mean, dick pics are, I mean, just something throughout history.

2:14:49.640 --> 2:14:53.080
 Uh, as long as people can draw things, there's been a dick pic.

2:14:53.080 --> 2:14:55.080
 It's a staple of human history.

2:14:55.080 --> 2:14:56.080
 It's a staple.

2:14:56.080 --> 2:14:58.840
 It's just throughout human history.

2:14:58.840 --> 2:15:00.680
 You tweeted that you aspire to comedy.

2:15:00.680 --> 2:15:02.640
 You're friends with Joe Rogan.

2:15:02.640 --> 2:15:08.080
 Might you, uh, do a short standup comedy set at some point in the future, maybe, um, open

2:15:08.080 --> 2:15:09.640
 for Joe, something like that.

2:15:09.640 --> 2:15:10.640
 Is that, is that...

2:15:10.640 --> 2:15:11.640
 Really?

2:15:11.640 --> 2:15:12.640
 Standup?

2:15:12.640 --> 2:15:13.640
 Actual, just full on standup?

2:15:13.640 --> 2:15:14.640
 Full on standup.

2:15:14.640 --> 2:15:15.640
 Is that in there or is that...

2:15:15.640 --> 2:15:22.840
 It's extremely difficult if, uh, at least that's what, uh, like Joe says and the comedians

2:15:22.840 --> 2:15:23.840
 say.

2:15:23.840 --> 2:15:24.840
 Huh.

2:15:24.840 --> 2:15:32.880
 I wonder if I could, um, I mean, I, I, you know, I, I have done standup for friends,

2:15:32.880 --> 2:15:40.320
 just, uh, impromptu, you know, I'll get, get on like a roof, uh, and they, they do laugh,

2:15:40.320 --> 2:15:41.440
 but they're our friends too.

2:15:41.440 --> 2:15:45.320
 So I don't know if, if you've got to call, you know, like a room of strangers, are they

2:15:45.320 --> 2:15:51.400
 going to actually also find it funny, but I could try, see what happens.

2:15:51.400 --> 2:15:53.560
 I think you'd learn something either way.

2:15:53.560 --> 2:15:54.560
 Um, yeah.

2:15:54.560 --> 2:16:00.080
 I kind of love, um, both the, when you bomb and when, when you do great, just watching

2:16:00.080 --> 2:16:03.320
 people, how they deal with it, it's so difficult.

2:16:03.320 --> 2:16:07.000
 It's so, you're so fragile up there.

2:16:07.000 --> 2:16:09.760
 It's just you and you, you think you're going to be funny.

2:16:09.760 --> 2:16:14.400
 And when it completely falls flat, it's just, it's beautiful to see people deal with like

2:16:14.400 --> 2:16:15.400
 that.

2:16:15.400 --> 2:16:16.400
 Yeah.

2:16:16.400 --> 2:16:17.400
 I might have enough material to do standup.

2:16:17.400 --> 2:16:21.240
 I've never thought about it, but I might have enough material.

2:16:21.240 --> 2:16:25.480
 Um, I don't know, like 15 minutes or something.

2:16:25.480 --> 2:16:26.480
 Oh yeah.

2:16:26.480 --> 2:16:27.480
 Yeah.

2:16:27.480 --> 2:16:28.480
 Do a Netflix special.

2:16:28.480 --> 2:16:29.480
 Netflix special.

2:16:29.480 --> 2:16:30.480
 Sure.

2:16:30.480 --> 2:16:36.240
 Um, what's your favorite Rick and Morty concept, uh, just to spring that on you.

2:16:36.240 --> 2:16:39.960
 Is there, there's a lot of sort of scientific engineering ideas explored there.

2:16:39.960 --> 2:16:42.880
 There's the butter robot.

2:16:42.880 --> 2:16:44.800
 That's a great, uh, that's a great show.

2:16:44.800 --> 2:16:45.800
 Um, yeah.

2:16:45.800 --> 2:16:47.440
 Rick and Morty is awesome.

2:16:47.440 --> 2:16:50.560
 Somebody that's exactly like you from an alternate dimension showed up there.

2:16:50.560 --> 2:16:51.560
 Elon Tusk.

2:16:51.560 --> 2:16:52.560
 Yeah, that's right.

2:16:52.560 --> 2:16:53.560
 That you voiced.

2:16:53.560 --> 2:16:54.560
 Yeah.

2:16:54.560 --> 2:16:57.200
 Rick and Morty certainly explores a lot of interesting concepts.

2:16:57.200 --> 2:17:00.200
 Uh, so like what's the favorite one?

2:17:00.200 --> 2:17:01.200
 I don't know.

2:17:01.200 --> 2:17:04.960
 The butter robot certainly is, uh, you know, it's like, it's certainly possible to have

2:17:04.960 --> 2:17:06.840
 too much sentience in a device.

2:17:06.840 --> 2:17:12.080
 Um, like you don't want to have your toast to be like a super genius toaster.

2:17:12.080 --> 2:17:15.280
 It's going to hate, hate life cause all it could do is make his toast.

2:17:15.280 --> 2:17:19.720
 But if it's like, you don't want to have like super intelligent stuck in a very limited

2:17:19.720 --> 2:17:20.720
 device.

2:17:20.720 --> 2:17:24.960
 Um, do you think it's too easy from a, if we're talking about from the engineering perspective

2:17:24.960 --> 2:17:30.760
 of super intelligence, like with Marvin the robot, like, is it, it seems like it might

2:17:30.760 --> 2:17:33.800
 be very easy to engineer just the depressed robot.

2:17:33.800 --> 2:17:40.760
 Like it's not obvious to engineer and robot that's going to find a fulfilling existence.

2:17:40.760 --> 2:17:47.160
 Sometimes humans I suppose, but, um, I wonder if that's like the default, if you don't do

2:17:47.160 --> 2:17:52.800
 a good job on building a robot, it's going to be sad a lot.

2:17:52.800 --> 2:17:58.160
 Well we can reprogram robots easier than we can reprogram humans.

2:17:58.160 --> 2:18:06.520
 So I guess if you let it evolve without tinkering, then it might get a sad, uh, but you can change

2:18:06.520 --> 2:18:13.000
 the optimization function and have it be a cheery robot.

2:18:13.000 --> 2:18:17.560
 You uh, like I mentioned with, with SpaceX, you give a lot of people hope and a lot of

2:18:17.560 --> 2:18:18.560
 people look up to you.

2:18:18.560 --> 2:18:19.920
 Millions of people look up to you.

2:18:19.920 --> 2:18:26.400
 Uh, if we think about young people in high school, maybe in college, um, what advice

2:18:26.400 --> 2:18:31.680
 would you give to them about if they want to try to do something big in this world,

2:18:31.680 --> 2:18:33.680
 they want to really have a big positive impact.

2:18:33.680 --> 2:18:39.440
 What advice would you give them about their career, maybe about life in general?

2:18:39.440 --> 2:18:40.440
 Try to be useful.

2:18:40.440 --> 2:18:46.480
 Um, you know, do things that are useful to your fellow human beings, to the world.

2:18:46.480 --> 2:18:48.480
 It's very hard to be useful.

2:18:48.480 --> 2:18:51.680
 Um, very hard.

2:18:51.680 --> 2:18:57.200
 Um, you know, are you contributing more than you consume?

2:18:57.200 --> 2:19:05.760
 You know, like, uh, like can you try to have a positive net contribution to society?

2:19:05.760 --> 2:19:11.600
 Um, I think that's the thing to aim for, you know, not, not to try to be sort of a leader

2:19:11.600 --> 2:19:15.200
 for just for the sake of being a leader or whatever.

2:19:15.200 --> 2:19:22.080
 Um, a lot of time people, a lot of times the people you want as leaders are the people

2:19:22.080 --> 2:19:24.440
 who don't want to be leaders.

2:19:24.440 --> 2:19:36.640
 So, um, if you live a useful life, that is a good life, a life worth having lived.

2:19:36.640 --> 2:19:45.040
 Um, you know, and I, like I said, I would, I would encourage people to use the mental

2:19:45.040 --> 2:19:48.360
 tools of physics and apply them broadly in life.

2:19:48.360 --> 2:19:49.360
 There are the best tools.

2:19:49.360 --> 2:19:54.040
 When you think about education and self education, what do you recommend?

2:19:54.040 --> 2:20:01.920
 So there's the university, there's a self study, there is a hands on sort of finding

2:20:01.920 --> 2:20:05.840
 a company or a place or a set of people that do the thing you're passionate about and joining

2:20:05.840 --> 2:20:08.080
 them as early as possible.

2:20:08.080 --> 2:20:13.520
 Um, there's, uh, taking a road trip across Europe for a few years and writing some poetry,

2:20:13.520 --> 2:20:18.440
 which, uh, which, which trajectory do you suggest?

2:20:18.440 --> 2:20:24.000
 In terms of learning about how you can become useful, as you mentioned, how you can have

2:20:24.000 --> 2:20:27.320
 the most positive impact.

2:20:27.320 --> 2:20:39.160
 Well, I encourage people to read a lot of books, just read, basically try to ingest

2:20:39.160 --> 2:20:46.000
 as much information as you can, uh, and try to also just develop a good general knowledge.

2:20:46.000 --> 2:20:54.320
 Um, so, so you at least have like a rough lay of the land of the knowledge landscape.

2:20:54.320 --> 2:20:59.040
 Like try to learn a little bit about a lot of things, um, cause you might not know what

2:20:59.040 --> 2:21:00.040
 you're really interested in.

2:21:00.040 --> 2:21:04.280
 How would you know what you're really interested in if you at least aren't like doing a peripheral

2:21:04.280 --> 2:21:10.240
 explore exploration of broadly of, of the knowledge landscape?

2:21:10.240 --> 2:21:17.320
 Um, and you talk to people from different walks of life and different, uh, industries

2:21:17.320 --> 2:21:27.520
 and professions and skills and occupations, like just try to learn as much as possible.

2:21:27.520 --> 2:21:31.120
 Man's search for meaning.

2:21:31.120 --> 2:21:32.920
 Isn't the whole thing a search for meaning?

2:21:32.920 --> 2:21:33.920
 Yeah.

2:21:33.920 --> 2:21:38.920
 What's the meaning of life and all, you know, but just generally, like I said, I would encourage

2:21:38.920 --> 2:21:45.760
 people to read broadly, um, in many different subject areas, um, and, and, and then try

2:21:45.760 --> 2:21:51.580
 to find something where there's an overlap of your talents and, and what you're interested

2:21:51.580 --> 2:21:52.580
 in.

2:21:52.580 --> 2:21:56.120
 So people may, may, may be good at something, but, or they may have skill at a particular

2:21:56.120 --> 2:21:57.760
 thing, but they don't like doing it.

2:21:57.760 --> 2:22:04.680
 Um, so you want to try to find a thing where you have your, that's a good, a good, a combination

2:22:04.680 --> 2:22:12.600
 of, of your, of the things that you're inherently good at, but you also like doing, um, and,

2:22:12.600 --> 2:22:13.600
 um,

2:22:13.600 --> 2:22:18.680
 And reading is a super fast shortcut to, to figure out which, where are you, you both

2:22:18.680 --> 2:22:19.680
 good at it.

2:22:19.680 --> 2:22:22.680
 You like doing it and it will actually have positive impact.

2:22:22.680 --> 2:22:25.040
 Well, you got to learn about things somehow.

2:22:25.040 --> 2:22:31.320
 So read, reading a broad range, just really read, read it.

2:22:31.320 --> 2:22:38.760
 You know, one point was that kid I read through the encyclopedia, uh, so that was pretty helpful.

2:22:38.760 --> 2:22:44.280
 Um, and, uh, there are also things that I didn't even know existed a lot, so obviously

2:22:44.280 --> 2:22:45.280
 and

2:22:45.280 --> 2:22:46.280
 It's like as broad as it gets.

2:22:46.280 --> 2:22:51.240
 Encyclopedias were digestible, I think, uh, you know, whatever, 40 years ago.

2:22:51.240 --> 2:22:58.360
 Um, so, um, you know, maybe read through the, the condensed version of the encyclopedia

2:22:58.360 --> 2:22:59.360
 of Britannica.

2:22:59.360 --> 2:23:05.120
 And that, um, you can always like skip subjects or you read a few paragraphs and you know

2:23:05.120 --> 2:23:07.560
 you're not interested, just jump to the next one.

2:23:07.560 --> 2:23:12.360
 That sort of read the encyclopedia or scan, skim, skim through it.

2:23:12.360 --> 2:23:19.920
 Um, and, um, but I, you know, I put a lot of stock and certainly have a lot of respect

2:23:19.920 --> 2:23:27.480
 for someone who puts in an honest day's work, uh, to do useful things and, and just generally

2:23:27.480 --> 2:23:34.560
 to have like a, not a zero sum mindset, um, or, uh, like have, have more of a grow the

2:23:34.560 --> 2:23:35.840
 pie mindset.

2:23:35.840 --> 2:23:42.280
 Like the, if you, if you sort of say like when, when I see people like perhaps, um,

2:23:42.280 --> 2:23:48.320
 including some very smart people kind of taking an attitude of, uh, like, like, like doing

2:23:48.320 --> 2:23:53.640
 things that seem like morally questionable, it's often because they have at a base sort

2:23:53.640 --> 2:23:57.480
 of axiomatic level, a zero sum mindset.

2:23:57.480 --> 2:24:02.720
 Um, and, and they, without realizing it, they don't realize they have a zero sum mindset

2:24:02.720 --> 2:24:04.920
 or at least that they don't realize it consciously.

2:24:04.920 --> 2:24:09.260
 Um, and so if you have a zero sum mindset, then the only way to get ahead is by taking

2:24:09.260 --> 2:24:11.520
 things from others.

2:24:11.520 --> 2:24:17.160
 If it's like, if the, if the, if the pie is fixed, then the only way to have more pie

2:24:17.160 --> 2:24:19.360
 is to take someone else's pie.

2:24:19.360 --> 2:24:20.680
 But this is false.

2:24:20.680 --> 2:24:24.120
 Like obviously the pie has grown dramatically over time, the economic pie.

2:24:24.120 --> 2:24:31.640
 Um, so the reality, in reality you can have the, so overuse this analogy, if you have

2:24:31.640 --> 2:24:37.960
 a lot of, there's a lot of pie, pie is not fixed.

2:24:37.960 --> 2:24:43.640
 Um, uh, so you really want to make sure you don't, you're not operating, um, without realizing

2:24:43.640 --> 2:24:47.960
 it from a zero sum mindset where, where the only way to get ahead is to take things from

2:24:47.960 --> 2:24:48.960
 others.

2:24:48.960 --> 2:24:52.040
 And you take, try to take things from others, which is not, not good.

2:24:52.040 --> 2:25:02.280
 It's much better to work on, uh, adding to the economic pie, maybe, you know, so creating,

2:25:02.280 --> 2:25:06.200
 like I said, create, creating more than you consume, uh, doing more than you.

2:25:06.200 --> 2:25:07.200
 Yeah.

2:25:07.200 --> 2:25:08.960
 Um, so that's, that's a big deal.

2:25:08.960 --> 2:25:15.040
 Um, I think there's like, you know, a fair number of people in, in finance that, uh,

2:25:15.040 --> 2:25:16.680
 do have a bit of a zero sum mindset.

2:25:16.680 --> 2:25:19.160
 I mean, it's all walks of life.

2:25:19.160 --> 2:25:25.680
 I've seen that one of the, one of the reasons, uh, Rogan inspires me is he celebrates others

2:25:25.680 --> 2:25:26.680
 a lot.

2:25:26.680 --> 2:25:29.400
 There's not, not creating a constant competition.

2:25:29.400 --> 2:25:31.400
 Like there's a scarcity of resources.

2:25:31.400 --> 2:25:36.800
 What happens when you celebrate others and you promote others, the ideas of others, it,

2:25:36.800 --> 2:25:38.960
 it, uh, it actually grows that pie.

2:25:38.960 --> 2:25:45.280
 I mean, it, every, like the, uh, the resource, the resources become less scarce and that,

2:25:45.280 --> 2:25:46.760
 that applies in a lot of kinds of domains.

2:25:46.760 --> 2:25:51.600
 It applies in academia where a lot of people are very, uh, see some funding for academic

2:25:51.600 --> 2:25:54.120
 research is zero sum and it is not.

2:25:54.120 --> 2:25:58.080
 If you celebrate each other, if you make, if you get everybody to be excited about AI,

2:25:58.080 --> 2:26:02.440
 about physics, above mathematics, I think it, there'll be more and more funding and

2:26:02.440 --> 2:26:03.640
 I think everybody wins.

2:26:03.640 --> 2:26:04.640
 Yeah.

2:26:04.640 --> 2:26:05.640
 That applies, I think broadly.

2:26:05.640 --> 2:26:06.640
 Uh,

2:26:06.640 --> 2:26:08.680
 yeah, yeah, exactly.

2:26:08.680 --> 2:26:15.240
 So last, last question about love and meaning, uh, what is the role of love?

2:26:15.240 --> 2:26:21.720
 In the human condition, broadly and more specific to you, how has love, romantic love or otherwise

2:26:21.720 --> 2:26:27.360
 made you a better person, a better human being?

2:26:27.360 --> 2:26:28.360
 Better engineer?

2:26:28.360 --> 2:26:31.520
 Now you're asking really perplexing questions.

2:26:31.520 --> 2:26:41.240
 Um, it's hard to give a, I mean, there are many books, poems and songs written about

2:26:41.240 --> 2:26:50.960
 what is love and what is, what exactly, you know, um, you know, what is love, baby don't

2:26:50.960 --> 2:26:51.960
 hurt me.

2:26:51.960 --> 2:26:53.920
 Um, that's one of the great ones.

2:26:53.920 --> 2:26:54.920
 Yes.

2:26:54.920 --> 2:26:55.920
 Yeah.

2:26:55.920 --> 2:26:58.120
 You've, you've earlier quoted Shakespeare, but that that's really up there.

2:26:58.120 --> 2:26:59.120
 Yeah.

2:26:59.120 --> 2:27:02.120
 Love is a many splinter thing.

2:27:02.120 --> 2:27:07.960
 Uh, I mean there's, um, it's cause we've talked about so many inspiring things like be useful

2:27:07.960 --> 2:27:13.720
 in the world, sort of like solve problems, alleviate suffering, but it seems like connection

2:27:13.720 --> 2:27:20.320
 between humans is a source, you know, it's a, it's a source of joys, a source of meaning

2:27:20.320 --> 2:27:24.160
 and that, that's what love is, friendship, love.

2:27:24.160 --> 2:27:29.040
 I just wonder if you think about that kind of thing where you talk about preserving the

2:27:29.040 --> 2:27:35.240
 light of human consciousness and us becoming a multi planetary, multi planetary species.

2:27:35.240 --> 2:27:43.240
 I mean, to me at least, um, that, that means like if we're just alone and conscious and

2:27:43.240 --> 2:27:48.880
 intelligent and it doesn't mean nearly as much as if we're with others, right?

2:27:48.880 --> 2:27:54.560
 And there's some magic created when we're together, the, uh, the, the friendship of

2:27:54.560 --> 2:27:55.560
 it.

2:27:55.560 --> 2:27:59.440
 And I think the highest form of it is love, which I think broadly is, is much bigger than

2:27:59.440 --> 2:28:05.800
 just sort of romantic, but also yes, romantic love and, um, family and those kinds of things.

2:28:05.800 --> 2:28:10.420
 Well, I mean, the reason I guess I care about us becoming multi planet species in a space

2:28:10.420 --> 2:28:19.600
 frank civilization is foundationally, I love humanity, um, and, and so I wish to see it

2:28:19.600 --> 2:28:27.520
 prosper and do great things and be happy and, um, and if I did not love humanity, I would

2:28:27.520 --> 2:28:29.640
 not care about these things.

2:28:29.640 --> 2:28:35.080
 So when you look at the whole of it, the human history, all the people who's ever lived,

2:28:35.080 --> 2:28:41.840
 all the people live now, it's pretty, we're, we're okay.

2:28:41.840 --> 2:28:44.960
 On the whole, we're pretty interesting bunch.

2:28:44.960 --> 2:28:45.960
 Yeah.

2:28:45.960 --> 2:28:51.680
 All things considered, and I've read a lot of history, including the darkest, worst parts

2:28:51.680 --> 2:28:59.560
 of it, and, uh, despite all that, I think on balance, I still love humanity.

2:28:59.560 --> 2:29:03.280
 You joked about it with the 42, uh, what do you, what do you think is the meaning of this

2:29:03.280 --> 2:29:05.600
 whole thing?

2:29:05.600 --> 2:29:08.520
 Is like, is there a non numerical representation?

2:29:08.520 --> 2:29:09.520
 Yeah.

2:29:09.520 --> 2:29:13.120
 Well, really, I think what Douglas Adams was saying in Hitchhiker's Guide to the Galaxy

2:29:13.120 --> 2:29:22.000
 is that, um, the universe is the answer and, uh, what we really need to figure out are

2:29:22.000 --> 2:29:27.680
 what questions to ask about the answer that is the universe and that the question is the

2:29:27.680 --> 2:29:28.680
 really the hard part.

2:29:28.680 --> 2:29:33.640
 And if you can properly frame the question, then the answer, relatively speaking, is easy.

2:29:33.640 --> 2:29:40.840
 Uh, so, so, so therefore, if you want to understand what questions to ask about the universe,

2:29:40.840 --> 2:29:45.860
 you want to understand the meaning of life, we need to expand the scope and scale of consciousness

2:29:45.860 --> 2:29:51.120
 so that we're better able to understand the nature of the universe and, and understand

2:29:51.120 --> 2:29:52.580
 the meaning of life.

2:29:52.580 --> 2:30:00.240
 And ultimately, the most important part would be to ask the right question, thereby elevating

2:30:00.240 --> 2:30:07.880
 the role of the interviewer as the most important human in the room.

2:30:07.880 --> 2:30:12.880
 Good questions are, you know, it's a hard, it's hard to come up with good questions.

2:30:12.880 --> 2:30:13.880
 Absolutely.

2:30:13.880 --> 2:30:20.000
 Um, but yeah, like, it's like that, that is the foundation of my philosophy is that, um,

2:30:20.000 --> 2:30:27.240
 I am curious about the nature of the universe and, uh, you know, and obviously I will die.

2:30:27.240 --> 2:30:31.840
 I don't know when I'll die, but I won't live forever.

2:30:31.840 --> 2:30:36.560
 Um, but I would like to know that we are on a path to understanding the nature of the

2:30:36.560 --> 2:30:40.040
 universe and the meaning of life and what questions to ask about the answer that is

2:30:40.040 --> 2:30:41.040
 the universe.

2:30:41.040 --> 2:30:46.160
 And, um, and so if we expand the scope and scale of humanity and consciousness in general,

2:30:46.160 --> 2:30:51.960
 um, which includes silicon consciousness, then that, you know, that, that, that seems

2:30:51.960 --> 2:30:53.920
 like a fundamentally good thing.

2:30:53.920 --> 2:31:00.520
 Elon, like I said, um, I'm deeply grateful that you would spend your extremely valuable

2:31:00.520 --> 2:31:07.040
 time with me today and also that you have given millions of people hope in this difficult

2:31:07.040 --> 2:31:11.380
 time, this divisive time, in this, uh, cynical time.

2:31:11.380 --> 2:31:14.160
 So I hope you do continue doing what you're doing.

2:31:14.160 --> 2:31:15.160
 Thank you so much for talking today.

2:31:15.160 --> 2:31:16.160
 Oh, you're welcome.

2:31:16.160 --> 2:31:18.880
 Uh, thanks for your excellent questions.

2:31:18.880 --> 2:31:21.280
 Thanks for listening to this conversation with Elon Musk.

2:31:21.280 --> 2:31:25.480
 To support this podcast, please check out our sponsors in the description.

2:31:25.480 --> 2:31:29.640
 And now let me leave you with some words from Elon Musk himself.

2:31:29.640 --> 2:31:35.840
 When something is important enough, you do it, even if the odds are not in your favor.

2:31:35.840 --> 2:32:02.560
 Thank you for listening and hope to see you next time.

