WEBVTT

00:00.000 --> 00:02.720
 The following is a conversation with Yann LeCun,

00:02.720 --> 00:04.560
 his second time on the podcast.

00:04.560 --> 00:09.180
 He is the chief AI scientist at Meta, formerly Facebook,

00:09.180 --> 00:13.080
 professor at NYU, touring award winner,

00:13.080 --> 00:15.640
 one of the seminal figures in the history

00:15.640 --> 00:18.480
 of machine learning and artificial intelligence,

00:18.480 --> 00:21.960
 and someone who is brilliant and opinionated

00:21.960 --> 00:23.440
 in the best kind of way.

00:23.440 --> 00:26.000
 And so it was always fun to talk to him.

00:26.000 --> 00:28.000
 This is the Lex Friedman podcast.

00:28.000 --> 00:29.960
 To support it, please check out our sponsors

00:29.960 --> 00:31.220
 in the description.

00:31.220 --> 00:35.040
 And now, here's my conversation with Yann LeCun.

00:36.160 --> 00:37.600
 You cowrote the article,

00:37.600 --> 00:40.900
 Self Supervised Learning, the Dark Matter of Intelligence.

00:40.900 --> 00:43.720
 Great title, by the way, with Ishan Mizra.

00:43.720 --> 00:46.640
 So let me ask, what is self supervised learning,

00:46.640 --> 00:49.920
 and why is it the dark matter of intelligence?

00:49.920 --> 00:51.720
 I'll start by the dark matter part.

00:53.120 --> 00:55.680
 There is obviously a kind of learning

00:55.680 --> 00:59.880
 that humans and animals are doing

00:59.880 --> 01:02.800
 that we currently are not reproducing properly

01:02.800 --> 01:04.660
 with machines or with AI, right?

01:04.660 --> 01:08.480
 So the most popular approaches to machine learning today are,

01:08.480 --> 01:09.660
 or paradigms, I should say,

01:09.660 --> 01:12.720
 are supervised learning and reinforcement learning.

01:12.720 --> 01:15.120
 And they are extremely inefficient.

01:15.120 --> 01:17.620
 Supervised learning requires many samples

01:17.620 --> 01:19.760
 for learning anything.

01:19.760 --> 01:22.760
 And reinforcement learning requires a ridiculously large

01:22.760 --> 01:27.300
 number of trial and errors for a system to learn anything.

01:29.320 --> 01:31.560
 And that's why we don't have self driving cars.

01:32.960 --> 01:34.760
 That was a big leap from one to the other.

01:34.760 --> 01:38.760
 Okay, so that, to solve difficult problems,

01:38.760 --> 01:42.360
 you have to have a lot of human annotation

01:42.360 --> 01:44.080
 for supervised learning to work.

01:44.080 --> 01:45.520
 And to solve those difficult problems

01:45.520 --> 01:46.680
 with reinforcement learning,

01:46.680 --> 01:50.240
 you have to have some way to maybe simulate that problem

01:50.240 --> 01:52.720
 such that you can do that large scale kind of learning

01:52.720 --> 01:54.420
 that reinforcement learning requires.

01:54.420 --> 01:58.320
 Right, so how is it that most teenagers can learn

01:58.320 --> 02:02.280
 to drive a car in about 20 hours of practice,

02:02.280 --> 02:07.280
 whereas even with millions of hours of simulated practice,

02:07.400 --> 02:09.220
 a self driving car can't actually learn

02:09.220 --> 02:10.700
 to drive itself properly.

02:12.120 --> 02:13.920
 And so obviously we're missing something, right?

02:13.920 --> 02:15.600
 And it's quite obvious for a lot of people

02:15.600 --> 02:19.760
 that the immediate response you get from many people is,

02:19.760 --> 02:22.840
 well, humans use their background knowledge

02:22.840 --> 02:25.820
 to learn faster, and they're right.

02:25.820 --> 02:28.280
 Now, how was that background knowledge acquired?

02:28.280 --> 02:30.080
 And that's the big question.

02:30.080 --> 02:34.040
 So now you have to ask, how do babies

02:34.040 --> 02:37.120
 in the first few months of life learn how the world works?

02:37.120 --> 02:38.240
 Mostly by observation,

02:38.240 --> 02:40.960
 because they can hardly act in the world.

02:40.960 --> 02:42.560
 And they learn an enormous amount

02:42.560 --> 02:43.840
 of background knowledge about the world

02:43.840 --> 02:47.960
 that may be the basis of what we call common sense.

02:47.960 --> 02:51.280
 This type of learning is not learning a task.

02:51.280 --> 02:53.680
 It's not being reinforced for anything.

02:53.680 --> 02:57.300
 It's just observing the world and figuring out how it works.

02:58.400 --> 03:01.240
 Building world models, learning world models.

03:01.240 --> 03:02.120
 How do we do this?

03:02.120 --> 03:04.560
 And how do we reproduce this in machines?

03:04.560 --> 03:09.520
 So self supervised learning is one instance

03:09.520 --> 03:13.120
 or one attempt at trying to reproduce this kind of learning.

03:13.120 --> 03:16.400
 Okay, so you're looking at just observation,

03:16.400 --> 03:18.720
 so not even the interacting part of a child.

03:18.720 --> 03:21.600
 It's just sitting there watching mom and dad walk around,

03:21.600 --> 03:23.480
 pick up stuff, all of that.

03:23.480 --> 03:25.520
 That's what we mean about background knowledge.

03:25.520 --> 03:27.520
 Perhaps not even watching mom and dad,

03:27.520 --> 03:30.000
 just watching the world go by.

03:30.000 --> 03:31.920
 Just having eyes open or having eyes closed

03:31.920 --> 03:34.480
 or the very act of opening and closing eyes

03:34.480 --> 03:36.280
 that the world appears and disappears,

03:36.280 --> 03:37.860
 all that basic information.

03:39.120 --> 03:43.160
 And you're saying in order to learn to drive,

03:43.160 --> 03:45.840
 like the reason humans are able to learn to drive quickly,

03:45.840 --> 03:47.360
 some faster than others,

03:47.360 --> 03:48.680
 is because of the background knowledge.

03:48.680 --> 03:51.760
 They're able to watch cars operate in the world

03:51.760 --> 03:53.640
 in the many years leading up to it,

03:53.640 --> 03:55.760
 the physics of basic objects, all that kind of stuff.

03:55.760 --> 03:56.600
 That's right.

03:56.600 --> 03:57.440
 I mean, the basic physics of objects,

03:57.440 --> 04:00.880
 you don't even need to know how a car works, right?

04:00.880 --> 04:02.500
 Because that you can learn fairly quickly.

04:02.500 --> 04:03.840
 I mean, the example I use very often

04:03.840 --> 04:05.800
 is you're driving next to a cliff.

04:06.680 --> 04:10.560
 And you know in advance because of your understanding

04:10.560 --> 04:13.200
 of intuitive physics that if you turn the wheel

04:13.200 --> 04:15.080
 to the right, the car will veer to the right,

04:15.080 --> 04:17.560
 will run off the cliff, fall off the cliff,

04:17.560 --> 04:20.400
 and nothing good will come out of this, right?

04:20.400 --> 04:23.760
 But if you are a sort of tabularized

04:23.760 --> 04:25.100
 reinforcement learning system

04:25.100 --> 04:27.040
 that doesn't have a model of the world,

04:28.160 --> 04:30.500
 you have to repeat falling off this cliff

04:30.500 --> 04:32.800
 thousands of times before you figure out it's a bad idea.

04:32.800 --> 04:34.560
 And then a few more thousand times

04:34.560 --> 04:36.960
 before you figure out how to not do it.

04:36.960 --> 04:38.480
 And then a few more million times

04:38.480 --> 04:39.800
 before you figure out how to not do it

04:39.800 --> 04:42.520
 in every situation you ever encounter.

04:42.520 --> 04:45.800
 So self supervised learning still has to have

04:45.800 --> 04:50.560
 some source of truth being told to it by somebody.

04:50.560 --> 04:54.560
 So you have to figure out a way without human assistance

04:54.560 --> 04:56.600
 or without significant amount of human assistance

04:56.600 --> 04:59.100
 to get that truth from the world.

04:59.100 --> 05:03.980
 So the mystery there is how much signal is there?

05:03.980 --> 05:06.280
 How much truth is there that the world gives you?

05:06.280 --> 05:08.160
 Whether it's the human world,

05:08.160 --> 05:10.020
 like you watch YouTube or something like that,

05:10.020 --> 05:12.960
 or it's the more natural world.

05:12.960 --> 05:14.920
 So how much signal is there?

05:14.920 --> 05:16.280
 So here's the trick.

05:16.280 --> 05:20.120
 There is way more signal in sort of a self supervised

05:20.120 --> 05:22.500
 setting than there is in either a supervised

05:22.500 --> 05:24.520
 or reinforcement setting.

05:24.520 --> 05:28.380
 And this is going to my analogy of the cake.

05:30.280 --> 05:32.320
 The cake as someone has called it,

05:32.320 --> 05:36.000
 where when you try to figure out how much information

05:36.000 --> 05:37.840
 you ask the machine to predict

05:37.840 --> 05:41.040
 and how much feedback you give the machine at every trial,

05:41.040 --> 05:41.880
 in reinforcement learning,

05:41.880 --> 05:43.340
 you give the machine a single scalar.

05:43.340 --> 05:45.400
 You tell the machine you did good, you did bad.

05:45.400 --> 05:49.640
 And you only tell this to the machine once in a while.

05:49.640 --> 05:51.440
 When I say you, it could be the universe

05:51.440 --> 05:52.880
 telling the machine, right?

05:54.120 --> 05:55.840
 But it's just one scalar.

05:55.840 --> 05:57.160
 And so as a consequence,

05:57.160 --> 05:59.600
 you cannot possibly learn something very complicated

05:59.600 --> 06:01.120
 without many, many, many trials

06:01.120 --> 06:04.760
 where you get many, many feedbacks of this type.

06:04.760 --> 06:08.880
 Supervised learning, you give a few bits to the machine

06:08.880 --> 06:10.220
 at every sample.

06:11.280 --> 06:15.720
 Let's say you're training a system on recognizing images

06:15.720 --> 06:17.680
 on ImageNet with 1000 categories,

06:17.680 --> 06:20.920
 that's a little less than 10 bits of information per sample.

06:22.180 --> 06:24.640
 But self supervised learning, here is the setting.

06:24.640 --> 06:26.360
 Ideally, we don't know how to do this yet,

06:26.360 --> 06:31.360
 but ideally you would show a machine a segment of video

06:31.680 --> 06:34.200
 and then stop the video and ask the machine to predict

06:34.200 --> 06:35.600
 what's going to happen next.

06:37.640 --> 06:38.720
 And so we let the machine predict

06:38.720 --> 06:41.400
 and then you let time go by

06:41.400 --> 06:44.340
 and show the machine what actually happened

06:44.340 --> 06:47.920
 and hope the machine will learn to do a better job

06:47.920 --> 06:49.400
 at predicting next time around.

06:49.400 --> 06:51.580
 There's a huge amount of information you give the machine

06:51.580 --> 06:53.560
 because it's an entire video clip

06:54.680 --> 06:59.280
 of the future after the video clip you fed it

06:59.280 --> 07:00.280
 in the first place.

07:00.280 --> 07:05.120
 So both for language and for vision, there's a subtle,

07:05.120 --> 07:06.920
 seemingly trivial construction,

07:06.920 --> 07:08.520
 but maybe that's representative

07:08.520 --> 07:10.620
 of what is required to create intelligence,

07:10.620 --> 07:12.880
 which is filling the gap.

07:13.720 --> 07:17.760
 So it sounds dumb, but can you,

07:19.760 --> 07:22.080
 it is possible you could solve all of intelligence

07:22.080 --> 07:25.280
 in this way, just for both language,

07:25.280 --> 07:28.800
 just give a sentence and continue it

07:28.800 --> 07:31.160
 or give a sentence and there's a gap in it,

07:32.080 --> 07:35.720
 some words blanked out and you fill in what words go there.

07:35.720 --> 07:39.200
 For vision, you give a sequence of images

07:39.200 --> 07:40.960
 and predict what's going to happen next,

07:40.960 --> 07:43.840
 or you fill in what happened in between.

07:43.840 --> 07:46.960
 Do you think it's possible that formulation alone

07:48.600 --> 07:50.980
 as a signal for self supervised learning

07:50.980 --> 07:53.640
 can solve intelligence for vision and language?

07:53.640 --> 07:56.320
 I think that's the best shot at the moment.

07:56.320 --> 07:59.120
 So whether this will take us all the way

07:59.120 --> 08:01.760
 to human level intelligence or something,

08:01.760 --> 08:04.840
 or just cat level intelligence is not clear,

08:04.840 --> 08:07.340
 but among all the possible approaches

08:07.340 --> 08:09.520
 that people have proposed, I think it's our best shot.

08:09.520 --> 08:14.520
 So I think this idea of an intelligent system

08:14.640 --> 08:18.880
 filling in the blanks, either predicting the future,

08:18.880 --> 08:22.180
 inferring the past, filling in missing information,

08:23.760 --> 08:25.200
 I'm currently filling the blank

08:25.200 --> 08:26.680
 of what is behind your head

08:26.680 --> 08:30.600
 and what your head looks like from the back,

08:30.600 --> 08:33.760
 because I have basic knowledge about how humans are made.

08:33.760 --> 08:36.360
 And I don't know what you're going to say,

08:36.360 --> 08:37.280
 at which point you're going to speak,

08:37.280 --> 08:38.960
 whether you're going to move your head this way or that way,

08:38.960 --> 08:40.280
 which way you're going to look,

08:40.280 --> 08:42.080
 but I know you're not going to just dematerialize

08:42.080 --> 08:44.920
 and reappear three meters down the hall,

08:46.280 --> 08:49.520
 because I know what's possible and what's impossible

08:49.520 --> 08:51.160
 according to intuitive physics.

08:51.160 --> 08:53.280
 You have a model of what's possible and what's impossible

08:53.280 --> 08:55.080
 and then you'd be very surprised if it happens

08:55.080 --> 08:57.840
 and then you'll have to reconstruct your model.

08:57.840 --> 08:59.600
 Right, so that's the model of the world.

08:59.600 --> 09:02.240
 It's what tells you, what fills in the blanks.

09:02.240 --> 09:04.960
 So given your partial information about the state

09:04.960 --> 09:07.160
 of the world, given by your perception,

09:08.080 --> 09:11.360
 your model of the world fills in the missing information

09:11.360 --> 09:13.760
 and that includes predicting the future,

09:13.760 --> 09:16.880
 re predicting the past, filling in things

09:16.880 --> 09:18.400
 you don't immediately perceive.

09:18.400 --> 09:22.280
 And that doesn't have to be purely generic vision

09:22.280 --> 09:24.340
 or visual information or generic language.

09:24.340 --> 09:28.920
 You can go to specifics like predicting

09:28.920 --> 09:31.120
 what control decision you make when you're driving

09:31.120 --> 09:35.620
 in a lane, you have a sequence of images from a vehicle

09:35.620 --> 09:39.640
 and then you have information if you record it on video

09:39.640 --> 09:43.680
 where the car ended up going so you can go back in time

09:43.680 --> 09:45.520
 and predict where the car went

09:45.520 --> 09:46.680
 based on the visual information.

09:46.680 --> 09:49.440
 That's very specific, domain specific.

09:49.440 --> 09:51.480
 Right, but the question is whether we can come up

09:51.480 --> 09:56.480
 with sort of a generic method for training machines

09:57.000 --> 09:59.840
 to do this kind of prediction or filling in the blanks.

09:59.840 --> 10:04.720
 So right now, this type of approach has been unbelievably

10:04.720 --> 10:08.200
 successful in the context of natural language processing.

10:08.200 --> 10:10.440
 Every modern natural language processing is pre trained

10:10.440 --> 10:13.720
 in self supervised manner to fill in the blanks.

10:13.720 --> 10:16.400
 You show it a sequence of words, you remove 10% of them

10:16.400 --> 10:17.940
 and then you train some gigantic neural net

10:17.940 --> 10:20.320
 to predict the words that are missing.

10:20.320 --> 10:22.760
 And once you've pre trained that network,

10:22.760 --> 10:26.600
 you can use the internal representation learned by it

10:26.600 --> 10:30.480
 as input to something that you train supervised

10:30.480 --> 10:32.240
 or whatever.

10:32.240 --> 10:33.400
 That's been incredibly successful.

10:33.400 --> 10:37.600
 Not so successful in images, although it's making progress

10:37.600 --> 10:42.600
 and it's based on sort of manual data augmentation.

10:42.600 --> 10:43.560
 We can go into this later,

10:43.560 --> 10:47.200
 but what has not been successful yet is training from video.

10:47.200 --> 10:49.440
 So getting a machine to learn to represent

10:49.440 --> 10:52.800
 the visual world, for example, by just watching video.

10:52.800 --> 10:54.800
 Nobody has really succeeded in doing this.

10:54.800 --> 10:57.520
 Okay, well, let's kind of give a high level overview.

10:57.520 --> 11:02.360
 What's the difference in kind and in difficulty

11:02.360 --> 11:03.960
 between vision and language?

11:03.960 --> 11:08.280
 So you said people haven't been able to really

11:08.280 --> 11:10.480
 kind of crack the problem of vision open

11:10.480 --> 11:11.960
 in terms of self supervised learning,

11:11.960 --> 11:13.800
 but that may not be necessarily

11:13.800 --> 11:15.840
 because it's fundamentally more difficult.

11:15.840 --> 11:18.720
 Maybe like when we're talking about achieving,

11:18.720 --> 11:22.320
 like passing the Turing test in the full spirit

11:22.320 --> 11:24.920
 of the Turing test in language might be harder than vision.

11:24.920 --> 11:26.400
 That's not obvious.

11:26.400 --> 11:29.440
 So in your view, which is harder

11:29.440 --> 11:31.960
 or perhaps are they just the same problem?

11:31.960 --> 11:34.840
 When the farther we get to solving each,

11:34.840 --> 11:36.720
 the more we realize it's all the same thing.

11:36.720 --> 11:37.680
 It's all the same cake.

11:37.680 --> 11:40.200
 I think what I'm looking for are methods

11:40.200 --> 11:43.600
 that make them look essentially like the same cake,

11:43.600 --> 11:44.800
 but currently they're not.

11:44.800 --> 11:48.480
 And the main issue with learning world models

11:48.480 --> 11:53.120
 or learning predictive models is that the prediction

11:53.120 --> 11:55.880
 is never a single thing

11:55.880 --> 11:59.240
 because the world is not entirely predictable.

11:59.240 --> 12:00.680
 It may be deterministic or stochastic.

12:00.680 --> 12:02.960
 We can get into the philosophical discussion about it,

12:02.960 --> 12:05.280
 but even if it's deterministic,

12:05.280 --> 12:07.440
 it's not entirely predictable.

12:07.440 --> 12:11.760
 And so if I play a short video clip

12:11.760 --> 12:14.160
 and then I ask you to predict what's going to happen next,

12:14.160 --> 12:16.360
 there's many, many plausible continuations

12:16.360 --> 12:20.520
 for that video clip and the number of continuation grows

12:20.520 --> 12:23.920
 with the interval of time that you're asking the system

12:23.920 --> 12:26.480
 to make a prediction for.

12:26.480 --> 12:29.880
 And so one big question with self supervised learning

12:29.880 --> 12:32.320
 is how you represent this uncertainty,

12:32.320 --> 12:35.200
 how you represent multiple discrete outcomes,

12:35.200 --> 12:37.120
 how you represent a sort of continuum

12:37.120 --> 12:40.400
 of possible outcomes, et cetera.

12:40.400 --> 12:45.200
 And if you are sort of a classical machine learning person,

12:45.200 --> 12:47.880
 you say, oh, you just represent a distribution, right?

12:49.120 --> 12:52.560
 And that we know how to do when we're predicting words,

12:52.560 --> 12:53.720
 missing words in the text,

12:53.720 --> 12:56.840
 because you can have a neural net give a score

12:56.840 --> 12:58.640
 for every word in the dictionary.

12:58.640 --> 13:02.480
 It's a big list of numbers, maybe 100,000 or so.

13:02.480 --> 13:05.280
 And you can turn them into a probability distribution

13:05.280 --> 13:07.640
 that tells you when I say a sentence,

13:09.880 --> 13:13.000
 the cat is chasing the blank in the kitchen.

13:13.000 --> 13:15.840
 There are only a few words that make sense there.

13:15.840 --> 13:18.360
 It could be a mouse or it could be a lizard spot

13:18.360 --> 13:19.920
 or something like that, right?

13:21.560 --> 13:25.840
 And if I say the blank is chasing the blank in the Savannah,

13:25.840 --> 13:27.840
 you also have a bunch of plausible options

13:27.840 --> 13:29.200
 for those two words, right?

13:30.960 --> 13:33.640
 Because you have kind of a underlying reality

13:33.640 --> 13:36.240
 that you can refer to to sort of fill in those blanks.

13:38.080 --> 13:42.040
 So you cannot say for sure in the Savannah,

13:42.040 --> 13:44.480
 if it's a lion or a cheetah or whatever,

13:44.480 --> 13:49.480
 you cannot know if it's a zebra or a do or whatever,

13:49.560 --> 13:50.960
 wildebeest, the same thing.

13:55.360 --> 13:56.840
 But you can represent the uncertainty

13:56.840 --> 13:58.520
 by just a long list of numbers.

13:58.520 --> 14:01.800
 Now, if I do the same thing with video,

14:01.800 --> 14:04.360
 when I ask you to predict a video clip,

14:04.360 --> 14:07.400
 it's not a discrete set of potential frames.

14:07.400 --> 14:10.000
 You have to have somewhere representing

14:10.000 --> 14:13.520
 a sort of infinite number of plausible continuations

14:13.520 --> 14:17.480
 of multiple frames in a high dimensional continuous space.

14:17.480 --> 14:20.520
 And we just have no idea how to do this properly.

14:20.520 --> 14:22.880
 Fine night, high dimensional.

14:22.880 --> 14:23.720
 So like you,

14:23.720 --> 14:25.320
 It's finite high dimensional, yes.

14:25.320 --> 14:26.240
 Just like the words,

14:26.240 --> 14:31.240
 they try to get it down to a small finite set

14:32.200 --> 14:34.240
 of like under a million, something like that.

14:34.240 --> 14:35.080
 Something like that.

14:35.080 --> 14:38.320
 I mean, it's kind of ridiculous that we're doing

14:38.320 --> 14:40.840
 a distribution over every single possible word

14:40.840 --> 14:42.880
 for language and it works.

14:42.880 --> 14:45.280
 It feels like that's a really dumb way to do it.

14:46.480 --> 14:49.720
 Like there seems to be like there should be

14:49.720 --> 14:52.920
 some more compressed representation

14:52.920 --> 14:55.040
 of the distribution of the words.

14:55.040 --> 14:56.120
 You're right about that.

14:56.120 --> 14:58.880
 And so do you have any interesting ideas

14:58.880 --> 15:01.880
 about how to represent all of reality in a compressed way

15:01.880 --> 15:03.800
 such that you can form a distribution over it?

15:03.800 --> 15:06.200
 That's one of the big questions, how do you do that?

15:06.200 --> 15:08.440
 Right, I mean, what's kind of another thing

15:08.440 --> 15:13.080
 that really is stupid about, I shouldn't say stupid,

15:13.080 --> 15:15.560
 but like simplistic about current approaches

15:15.560 --> 15:19.360
 to self supervised learning in NLP in text

15:19.360 --> 15:21.920
 is that not only do you represent

15:21.920 --> 15:23.840
 a giant distribution over words,

15:23.840 --> 15:25.680
 but for multiple words that are missing,

15:25.680 --> 15:27.680
 those distributions are essentially independent

15:27.680 --> 15:28.520
 of each other.

15:30.200 --> 15:33.040
 And you don't pay too much of a price for this.

15:33.040 --> 15:37.840
 So you can't, so the system in the sentence

15:37.840 --> 15:41.280
 that I gave earlier, if it gives a certain probability

15:41.280 --> 15:44.800
 for a lion and cheetah, and then a certain probability

15:44.800 --> 15:49.800
 for gazelle, wildebeest and zebra,

15:51.960 --> 15:54.800
 those two probabilities are independent of each other.

15:55.960 --> 15:58.040
 And it's not the case that those things are independent.

15:58.040 --> 16:01.480
 Lions actually attack like bigger animals than cheetahs.

16:01.480 --> 16:05.960
 So there's a huge independent hypothesis in this process,

16:05.960 --> 16:07.800
 which is not actually true.

16:07.800 --> 16:09.880
 The reason for this is that we don't know

16:09.880 --> 16:13.000
 how to represent properly distributions

16:13.000 --> 16:16.240
 over combinatorial sequences of symbols,

16:16.240 --> 16:19.000
 essentially because the number grows exponentially

16:19.000 --> 16:21.320
 with the length of the symbols.

16:21.320 --> 16:22.760
 And so we have to use tricks for this,

16:22.760 --> 16:26.400
 but those techniques can get around,

16:26.400 --> 16:27.800
 like don't even deal with it.

16:27.800 --> 16:31.760
 So the big question is would there be some sort

16:31.760 --> 16:35.640
 of abstract latent representation of text

16:35.640 --> 16:40.640
 that would say that when I switch lion for gazelle,

16:40.680 --> 16:45.480
 lion for cheetah, I also have to switch zebra for gazelle?

16:45.480 --> 16:48.720
 Yeah, so this independence assumption,

16:48.720 --> 16:51.160
 let me throw some criticism at you that I often hear

16:51.160 --> 16:52.920
 and see how you respond.

16:52.920 --> 16:56.000
 So this kind of filling in the blanks is just statistics.

16:56.000 --> 16:57.880
 You're not learning anything

16:58.880 --> 17:01.600
 like the deep underlying concepts.

17:01.600 --> 17:05.640
 You're just mimicking stuff from the past.

17:05.640 --> 17:08.560
 You're not learning anything new such that you can use it

17:08.560 --> 17:10.800
 to generalize about the world.

17:11.960 --> 17:14.120
 Or okay, let me just say the crude version,

17:14.120 --> 17:16.200
 which is just statistics.

17:16.200 --> 17:18.320
 It's not intelligence.

17:18.320 --> 17:19.640
 What do you have to say to that?

17:19.640 --> 17:20.880
 What do you usually say to that

17:20.880 --> 17:22.640
 if you kind of hear this kind of thing?

17:22.640 --> 17:23.960
 I don't get into those discussions

17:23.960 --> 17:26.760
 because they are kind of pointless.

17:26.760 --> 17:28.760
 So first of all, it's quite possible

17:28.760 --> 17:30.480
 that intelligence is just statistics.

17:30.480 --> 17:32.760
 It's just statistics of a particular kind.

17:32.760 --> 17:35.480
 Yes, this is the philosophical question.

17:35.480 --> 17:38.400
 It's kind of is it possible

17:38.400 --> 17:40.280
 that intelligence is just statistics?

17:40.280 --> 17:43.520
 Yeah, but what kind of statistics?

17:43.520 --> 17:46.200
 So if you are asking the question,

17:47.160 --> 17:50.680
 are the models of the world that we learn,

17:50.680 --> 17:52.320
 do they have some notion of causality?

17:52.320 --> 17:53.400
 Yes.

17:53.400 --> 17:56.240
 So if the criticism comes from people who say,

17:57.200 --> 17:59.440
 current machine learning system don't care about causality,

17:59.440 --> 18:03.120
 which by the way is wrong, I agree with them.

18:04.600 --> 18:06.560
 Your model of the world should have your actions

18:06.560 --> 18:09.080
 as one of the inputs.

18:09.080 --> 18:11.400
 And that will drive you to learn causal models of the world

18:11.400 --> 18:15.080
 where you know what intervention in the world

18:15.080 --> 18:16.720
 will cause what result.

18:16.720 --> 18:19.400
 Or you can do this by observation of other agents

18:19.400 --> 18:22.520
 acting in the world and observing the effect.

18:22.520 --> 18:24.240
 Other humans, for example.

18:24.240 --> 18:28.440
 So I think at some level of description,

18:28.440 --> 18:30.240
 intelligence is just statistics.

18:31.680 --> 18:35.200
 But that doesn't mean you don't have models

18:35.200 --> 18:40.080
 that have deep mechanistic explanation for what goes on.

18:40.080 --> 18:41.760
 The question is how do you learn them?

18:41.760 --> 18:44.440
 That's the question I'm interested in.

18:44.440 --> 18:49.360
 Because a lot of people who actually voice their criticism

18:49.360 --> 18:51.040
 say that those mechanistic model

18:51.040 --> 18:52.640
 have to come from someplace else.

18:52.640 --> 18:54.040
 They have to come from human designers,

18:54.040 --> 18:56.200
 they have to come from I don't know what.

18:56.200 --> 18:57.880
 And obviously we learn them.

18:59.280 --> 19:01.800
 Or if we don't learn them as an individual,

19:01.800 --> 19:04.920
 nature learn them for us using evolution.

19:04.920 --> 19:07.160
 So regardless of what you think,

19:07.160 --> 19:09.400
 those processes have been learned somehow.

19:10.240 --> 19:12.920
 So if you look at the human brain,

19:12.920 --> 19:14.640
 just like when we humans introspect

19:14.640 --> 19:16.320
 about how the brain works,

19:16.320 --> 19:20.240
 it seems like when we think about what is intelligence,

19:20.240 --> 19:22.440
 we think about the high level stuff,

19:22.440 --> 19:23.960
 like the models we've constructed,

19:23.960 --> 19:25.560
 concepts like cognitive science,

19:25.560 --> 19:28.720
 like concepts of memory and reasoning module,

19:28.720 --> 19:31.160
 almost like these high level modules.

19:32.360 --> 19:34.400
 Is this serve as a good analogy?

19:35.400 --> 19:40.400
 Like are we ignoring the dark matter,

19:40.720 --> 19:43.560
 the basic low level mechanisms?

19:43.560 --> 19:45.800
 Just like we ignore the way the operating system works,

19:45.800 --> 19:49.640
 we're just using the high level software.

19:49.640 --> 19:52.720
 We're ignoring that at the low level,

19:52.720 --> 19:56.440
 the neural network might be doing something like statistics.

19:56.440 --> 19:59.120
 Like meaning, sorry to use this word

19:59.120 --> 20:00.560
 probably incorrectly and crudely,

20:00.560 --> 20:03.320
 but doing this kind of fill in the gap kind of learning

20:03.320 --> 20:05.720
 and just kind of updating the model constantly

20:05.720 --> 20:09.240
 in order to be able to support the raw sensory information

20:09.240 --> 20:11.360
 to predict it and then adjust to the prediction

20:11.360 --> 20:12.400
 when it's wrong.

20:12.400 --> 20:15.840
 But like when we look at our brain at the high level,

20:15.840 --> 20:18.320
 it feels like we're doing, like we're playing chess,

20:18.320 --> 20:22.240
 like we're like playing with high level concepts

20:22.240 --> 20:23.680
 and we're stitching them together

20:23.680 --> 20:26.000
 and we're putting them into longterm memory.

20:26.000 --> 20:28.280
 But really what's going underneath

20:28.280 --> 20:30.160
 is something we're not able to introspect,

20:30.160 --> 20:34.440
 which is this kind of simple, large neural network

20:34.440 --> 20:36.000
 that's just filling in the gaps.

20:36.000 --> 20:37.120
 Right, well, okay.

20:37.120 --> 20:39.760
 So there's a lot of questions and a lot of answers there.

20:39.760 --> 20:40.600
 Okay, so first of all,

20:40.600 --> 20:42.680
 there's a whole school of thought in neuroscience,

20:42.680 --> 20:45.240
 computational neuroscience in particular,

20:45.240 --> 20:47.760
 that likes the idea of predictive coding,

20:47.760 --> 20:50.080
 which is really related to the idea

20:50.080 --> 20:52.040
 I was talking about in self supervised learning.

20:52.040 --> 20:53.520
 So everything is about prediction.

20:53.520 --> 20:56.320
 The essence of intelligence is the ability to predict

20:56.320 --> 20:58.840
 and everything the brain does is trying to predict,

20:59.920 --> 21:02.120
 predict everything from everything else.

21:02.120 --> 21:04.760
 Okay, and that's really sort of the underlying principle,

21:04.760 --> 21:07.800
 if you want, that self supervised learning

21:07.800 --> 21:10.640
 is trying to kind of reproduce this idea of prediction

21:10.640 --> 21:13.080
 as kind of an essential mechanism

21:13.080 --> 21:16.320
 of task independent learning, if you want.

21:16.320 --> 21:19.320
 The next step is what kind of intelligence

21:19.320 --> 21:21.120
 are you interested in reproducing?

21:21.120 --> 21:24.640
 And of course, we all think about trying to reproduce

21:24.640 --> 21:28.320
 sort of high level cognitive processes in humans,

21:28.320 --> 21:30.400
 but like with machines, we're not even at the level

21:30.400 --> 21:35.400
 of even reproducing the learning processes in a cat brain.

21:37.160 --> 21:39.360
 The most intelligent or intelligent systems

21:39.360 --> 21:41.960
 don't have as much common sense as a house cat.

21:43.200 --> 21:45.160
 So how is it that cats learn?

21:45.160 --> 21:47.920
 And cats don't do a whole lot of reasoning.

21:47.920 --> 21:49.600
 They certainly have causal models.

21:49.600 --> 21:53.600
 They certainly have, because many cats can figure out

21:53.600 --> 21:56.600
 how they can act on the world to get what they want.

21:56.600 --> 22:01.600
 They certainly have a fantastic model of intuitive physics,

22:01.800 --> 22:04.560
 certainly the dynamics of their own bodies,

22:04.560 --> 22:06.880
 but also of praise and things like that.

22:06.880 --> 22:09.880
 So they're pretty smart.

22:09.880 --> 22:12.400
 They only do this with about 800 million neurons.

22:12.400 --> 22:17.400
 We are not anywhere close to reproducing this kind of thing.

22:17.920 --> 22:21.320
 So to some extent, I could say,

22:21.320 --> 22:24.960
 let's not even worry about like the high level cognition

22:26.280 --> 22:27.960
 and kind of longterm planning and reasoning

22:27.960 --> 22:30.120
 that humans can do until we figure out like,

22:30.120 --> 22:32.520
 can we even reproduce what cats are doing?

22:32.520 --> 22:37.000
 Now that said, this ability to learn world models,

22:37.000 --> 22:41.560
 I think is the key to the possibility of learning machines

22:41.560 --> 22:43.160
 that can also reason.

22:43.160 --> 22:45.640
 So whenever I give a talk, I say there are three challenges

22:45.640 --> 22:47.320
 in the three main challenges in machine learning.

22:47.320 --> 22:49.920
 The first one is getting machines to learn

22:49.920 --> 22:51.800
 to represent the world

22:51.800 --> 22:53.960
 and I'm proposing self supervised learning.

22:54.840 --> 22:58.000
 The second is getting machines to reason

22:58.000 --> 22:59.240
 in ways that are compatible

22:59.240 --> 23:01.640
 with essentially gradient based learning

23:01.640 --> 23:04.240
 because this is what deep learning is all about really.

23:05.280 --> 23:06.640
 And the third one is something

23:06.640 --> 23:07.640
 we have no idea how to solve,

23:07.640 --> 23:09.480
 at least I have no idea how to solve

23:09.480 --> 23:14.360
 is can we get machines to learn hierarchical representations

23:14.360 --> 23:15.960
 of action plans?

23:17.920 --> 23:18.760
 We know how to train them

23:18.760 --> 23:21.080
 to learn hierarchical representations of perception

23:22.200 --> 23:23.680
 with convolutional nets and things like that

23:23.680 --> 23:26.040
 and transformers, but what about action plans?

23:26.040 --> 23:28.280
 Can we get them to spontaneously learn

23:28.280 --> 23:30.480
 good hierarchical representations of actions?

23:30.480 --> 23:32.400
 Also gradient based.

23:32.400 --> 23:35.880
 Yeah, all of that needs to be somewhat differentiable

23:35.880 --> 23:38.720
 so that you can apply sort of gradient based learning,

23:38.720 --> 23:40.920
 which is really what deep learning is about.

23:42.080 --> 23:46.760
 So it's background, knowledge, ability to reason

23:46.760 --> 23:50.520
 in a way that's differentiable

23:50.520 --> 23:53.840
 that is somehow connected, deeply integrated

23:53.840 --> 23:55.480
 with that background knowledge

23:55.480 --> 23:57.600
 or builds on top of that background knowledge

23:57.600 --> 23:59.120
 and then given that background knowledge

23:59.120 --> 24:02.360
 be able to make hierarchical plans in the world.

24:02.360 --> 24:05.480
 So if you take classical optimal control,

24:05.480 --> 24:07.000
 there's something in classical optimal control

24:07.000 --> 24:10.520
 called model predictive control.

24:10.520 --> 24:13.840
 And it's been around since the early sixties.

24:13.840 --> 24:16.840
 NASA uses that to compute trajectories of rockets.

24:16.840 --> 24:20.600
 And the basic idea is that you have a predictive model

24:20.600 --> 24:21.840
 of the rocket, let's say,

24:21.840 --> 24:25.440
 or whatever system you intend to control,

24:25.440 --> 24:28.360
 which given the state of the system at time T

24:28.360 --> 24:31.640
 and given an action that you're taking the system.

24:31.640 --> 24:33.520
 So for a rocket to be thrust

24:33.520 --> 24:35.600
 and all the controls you can have,

24:35.600 --> 24:37.280
 it gives you the state of the system

24:37.280 --> 24:38.800
 at time T plus Delta T, right?

24:38.800 --> 24:41.520
 So basically a differential equation, something like that.

24:43.520 --> 24:45.240
 And if you have this model

24:45.240 --> 24:48.720
 and you have this model in the form of some sort of neural net

24:48.720 --> 24:50.960
 or some sort of a set of formula

24:50.960 --> 24:52.920
 that you can back propagate gradient through,

24:52.920 --> 24:55.240
 you can do what's called model predictive control

24:55.240 --> 24:57.680
 or gradient based model predictive control.

24:57.680 --> 25:02.680
 So you can unroll that model in time.

25:02.680 --> 25:07.680
 You feed it a hypothesized sequence of actions.

25:08.080 --> 25:10.760
 And then you have some objective function

25:10.760 --> 25:13.240
 that measures how well at the end of the trajectory,

25:13.240 --> 25:16.240
 the system has succeeded or matched what you wanted to do.

25:17.240 --> 25:18.280
 Is it a robot harm?

25:18.280 --> 25:20.680
 Have you grasped the object you want to grasp?

25:20.680 --> 25:23.360
 If it's a rocket, are you at the right place

25:23.360 --> 25:26.120
 near the space station, things like that.

25:26.120 --> 25:28.040
 And by back propagation through time,

25:28.040 --> 25:30.080
 and again, this was invented in the 1960s,

25:30.080 --> 25:34.040
 by optimal control theorists, you can figure out

25:34.040 --> 25:36.160
 what is the optimal sequence of actions

25:36.160 --> 25:41.160
 that will get my system to the best final state.

25:42.040 --> 25:44.560
 So that's a form of reasoning.

25:44.560 --> 25:45.640
 It's basically planning.

25:45.640 --> 25:48.160
 And a lot of planning systems in robotics

25:48.160 --> 25:49.600
 are actually based on this.

25:49.600 --> 25:53.160
 And you can think of this as a form of reasoning.

25:53.160 --> 25:57.040
 So to take the example of the teenager driving a car,

25:57.040 --> 26:00.120
 you have a pretty good dynamical model of the car.

26:00.120 --> 26:01.280
 It doesn't need to be very accurate.

26:01.280 --> 26:03.840
 But you know, again, that if you turn the wheel

26:03.840 --> 26:05.080
 to the right and there is a cliff,

26:05.080 --> 26:06.520
 you're gonna run off the cliff, right?

26:06.520 --> 26:08.000
 You don't need to have a very accurate model

26:08.000 --> 26:09.080
 to predict that.

26:09.080 --> 26:10.640
 And you can run this in your mind

26:10.640 --> 26:13.080
 and decide not to do it for that reason.

26:13.080 --> 26:14.480
 Because you can predict in advance

26:14.480 --> 26:15.600
 that the result is gonna be bad.

26:15.600 --> 26:17.960
 So you can sort of imagine different scenarios

26:17.960 --> 26:21.560
 and then employ or take the first step

26:21.560 --> 26:23.360
 in the scenario that is most favorable

26:23.360 --> 26:24.960
 and then repeat the process again.

26:24.960 --> 26:27.120
 The scenario that is most favorable

26:27.120 --> 26:28.480
 and then repeat the process of planning.

26:28.480 --> 26:31.280
 That's called receding horizon model predictive control.

26:31.280 --> 26:35.280
 So even all those things have names going back decades.

26:36.480 --> 26:40.680
 And so if you're not a classical optimal control,

26:40.680 --> 26:42.960
 the model of the world is not generally learned.

26:44.360 --> 26:46.240
 Sometimes a few parameters you have to identify.

26:46.240 --> 26:47.800
 That's called systems identification.

26:47.800 --> 26:52.640
 But generally, the model is mostly deterministic

26:52.640 --> 26:53.920
 and mostly built by hand.

26:53.920 --> 26:55.920
 So the question of AI,

26:55.920 --> 26:58.760
 I think the big challenge of AI for the next decade

26:58.760 --> 27:01.120
 is how do we get machines to learn predictive models

27:01.120 --> 27:03.720
 of the world that deal with uncertainty

27:03.720 --> 27:05.840
 and deal with the real world in all this complexity?

27:05.840 --> 27:08.160
 So it's not just the trajectory of a rocket,

27:08.160 --> 27:10.240
 which you can reduce to first principles.

27:10.240 --> 27:13.040
 It's not even just the trajectory of a robot arm,

27:13.040 --> 27:16.320
 which again, you can model by careful mathematics.

27:16.320 --> 27:17.200
 But it's everything else,

27:17.200 --> 27:18.880
 everything we observe in the world.

27:18.880 --> 27:20.120
 People, behavior,

27:20.120 --> 27:25.120
 physical systems that involve collective phenomena,

27:25.800 --> 27:30.800
 like water or trees and branches in a tree or something

27:31.880 --> 27:36.680
 or complex things that humans have no trouble

27:36.680 --> 27:38.520
 developing abstract representations

27:38.520 --> 27:39.840
 and predictive model for,

27:39.840 --> 27:41.600
 but we still don't know how to do with machines.

27:41.600 --> 27:43.880
 Where do you put in these three,

27:43.880 --> 27:46.180
 maybe in the planning stages,

27:46.180 --> 27:50.660
 the game theoretic nature of this world,

27:50.660 --> 27:52.980
 where your actions not only respond

27:52.980 --> 27:55.540
 to the dynamic nature of the world, the environment,

27:55.540 --> 27:57.500
 but also affect it.

27:57.500 --> 27:59.860
 So if there's other humans involved,

27:59.860 --> 28:02.220
 is this point number four,

28:02.220 --> 28:03.420
 or is it somehow integrated

28:03.420 --> 28:05.820
 into the hierarchical representation of action

28:05.820 --> 28:06.660
 in your view?

28:06.660 --> 28:07.500
 I think it's integrated.

28:07.500 --> 28:11.580
 It's just that now your model of the world has to deal with,

28:11.580 --> 28:13.100
 it just makes it more complicated.

28:13.100 --> 28:15.600
 The fact that humans are complicated

28:15.600 --> 28:17.220
 and not easily predictable,

28:17.220 --> 28:19.860
 that makes your model of the world much more complicated,

28:19.860 --> 28:21.340
 that much more complicated.

28:21.340 --> 28:22.380
 Well, there's a chess,

28:22.380 --> 28:25.300
 I mean, I suppose chess is an analogy.

28:25.300 --> 28:27.740
 So multicolored tree search.

28:28.860 --> 28:32.040
 There's a, I go, you go, I go, you go.

28:32.040 --> 28:35.580
 Like Andre Capote recently gave a talk at MIT

28:35.580 --> 28:36.980
 about car doors.

28:37.900 --> 28:39.280
 I think there's some machine learning too,

28:39.280 --> 28:40.780
 but mostly car doors.

28:40.780 --> 28:43.340
 And there's a dynamic nature to the car,

28:43.340 --> 28:44.700
 like the person opening the door,

28:44.700 --> 28:46.900
 checking, I mean, he wasn't talking about that.

28:46.900 --> 28:48.420
 He was talking about the perception problem

28:48.420 --> 28:50.940
 of what the ontology of what defines a car door,

28:50.940 --> 28:52.940
 this big philosophical question.

28:52.940 --> 28:54.060
 But to me, it was interesting

28:54.060 --> 28:57.300
 because it's obvious that the person opening the car doors,

28:57.300 --> 28:59.580
 they're trying to get out, like here in New York,

28:59.580 --> 29:01.400
 trying to get out of the car.

29:01.400 --> 29:03.580
 You slowing down is going to signal something.

29:03.580 --> 29:05.380
 You speeding up is gonna signal something,

29:05.380 --> 29:06.460
 and that's a dance.

29:06.460 --> 29:10.140
 It's a asynchronous chess game.

29:10.140 --> 29:10.980
 I don't know.

29:10.980 --> 29:15.980
 So it feels like it's not just,

29:16.900 --> 29:18.780
 I mean, I guess you can integrate all of them

29:18.780 --> 29:21.300
 to one giant model, like the entirety

29:21.300 --> 29:24.340
 of these little interactions.

29:24.340 --> 29:25.740
 Because it's not as complicated as chess.

29:25.740 --> 29:27.120
 It's just like a little dance.

29:27.120 --> 29:28.800
 We do like a little dance together,

29:28.800 --> 29:29.980
 and then we figure it out.

29:29.980 --> 29:32.500
 Well, in some ways it's way more complicated than chess

29:32.500 --> 29:36.020
 because it's continuous, it's uncertain

29:36.020 --> 29:37.260
 in a continuous manner.

29:38.220 --> 29:39.860
 It doesn't feel more complicated.

29:39.860 --> 29:41.060
 But it doesn't feel more complicated

29:41.060 --> 29:43.660
 because that's what we've evolved to solve.

29:43.660 --> 29:45.480
 This is the kind of problem we've evolved to solve.

29:45.480 --> 29:46.400
 And so we're good at it

29:46.400 --> 29:49.300
 because nature has made us good at it.

29:50.500 --> 29:52.340
 Nature has not made us good at chess.

29:52.340 --> 29:54.180
 We completely suck at chess.

29:55.700 --> 29:57.980
 In fact, that's why we designed it as a game,

29:57.980 --> 29:59.020
 is to be challenging.

30:00.340 --> 30:02.580
 And if there is something that recent progress

30:02.580 --> 30:05.580
 in chess and Go has made us realize

30:05.580 --> 30:07.900
 is that humans are really terrible at those things,

30:07.900 --> 30:09.660
 like really bad.

30:09.660 --> 30:11.540
 There was a story right before AlphaGo

30:11.540 --> 30:15.220
 that the best Go players thought

30:15.220 --> 30:18.520
 there were maybe two or three stones behind an ideal player

30:18.520 --> 30:19.720
 that they would call God.

30:20.700 --> 30:23.700
 In fact, no, there are like nine or 10 stones behind.

30:23.700 --> 30:25.340
 I mean, we're just bad.

30:25.340 --> 30:27.420
 So we're not good at,

30:27.420 --> 30:30.340
 and it's because we have limited working memory.

30:30.340 --> 30:32.980
 We're not very good at doing this tree exploration

30:32.980 --> 30:36.780
 that computers are much better at doing than we are.

30:36.780 --> 30:37.940
 But we are much better

30:37.940 --> 30:40.620
 at learning differentiable models to the world.

30:40.620 --> 30:43.820
 I mean, I said differentiable in a kind of,

30:43.820 --> 30:46.420
 I should say not differentiable in the sense that

30:46.420 --> 30:47.480
 we went back far through it,

30:47.480 --> 30:50.500
 but in the sense that our brain has some mechanism

30:50.500 --> 30:54.060
 for estimating gradients of some kind.

30:54.060 --> 30:56.540
 And that's what makes us efficient.

30:56.540 --> 31:01.540
 So if you have an agent that consists of a model

31:02.180 --> 31:04.380
 of the world, which in the human brain

31:04.380 --> 31:08.340
 is basically the entire front half of your brain,

31:08.340 --> 31:10.220
 an objective function,

31:10.220 --> 31:14.440
 which in humans is a combination of two things.

31:14.440 --> 31:17.660
 There is your sort of intrinsic motivation module,

31:17.660 --> 31:19.140
 which is in the basal ganglia,

31:19.140 --> 31:20.100
 the base of your brain.

31:20.100 --> 31:22.540
 That's the thing that measures pain and hunger

31:22.540 --> 31:23.360
 and things like that,

31:23.360 --> 31:26.860
 like immediate feelings and emotions.

31:28.020 --> 31:30.780
 And then there is the equivalent

31:30.780 --> 31:32.620
 of what people in reinforcement learning call a critic,

31:32.620 --> 31:36.100
 which is a sort of module that predicts ahead

31:36.100 --> 31:41.100
 what the outcome of a situation will be.

31:41.940 --> 31:43.840
 And so it's not a cost function,

31:43.840 --> 31:45.460
 but it's sort of not an objective function,

31:45.460 --> 31:49.020
 but it's sort of a train predictor

31:49.020 --> 31:50.980
 of the ultimate objective function.

31:50.980 --> 31:52.620
 And that also is differentiable.

31:52.620 --> 31:54.660
 And so if all of this is differentiable,

31:54.660 --> 31:59.660
 your cost function, your critic, your world model,

31:59.660 --> 32:03.100
 then you can use gradient based type methods

32:03.100 --> 32:05.820
 to do planning, to do reasoning, to do learning,

32:05.820 --> 32:08.140
 to do all the things that we'd like

32:08.140 --> 32:11.840
 an intelligent agent to do.

32:11.840 --> 32:14.180
 And gradient based learning,

32:14.180 --> 32:15.340
 like what's your intuition?

32:15.340 --> 32:18.420
 That's probably at the core of what can solve intelligence.

32:18.420 --> 32:23.420
 So you don't need like logic based reasoning in your view.

32:25.620 --> 32:27.260
 I don't know how to make logic based reasoning

32:27.260 --> 32:29.780
 compatible with efficient learning.

32:31.020 --> 32:32.300
 Okay, I mean, there is a big question,

32:32.300 --> 32:33.900
 perhaps a philosophical question.

32:33.900 --> 32:35.220
 I mean, it's not that philosophical,

32:35.220 --> 32:40.020
 but that we can ask is that all the learning algorithms

32:40.020 --> 32:43.300
 we know from engineering and computer science

32:43.300 --> 32:45.780
 proceed by optimizing some objective function.

32:48.340 --> 32:49.940
 So one question we may ask is,

32:51.780 --> 32:54.740
 does learning in the brain minimize an objective function?

32:54.740 --> 32:57.340
 I mean, it could be a composite

32:57.340 --> 32:58.500
 of multiple objective functions,

32:58.500 --> 33:00.300
 but it's still an objective function.

33:01.420 --> 33:04.660
 Second, if it does optimize an objective function,

33:04.660 --> 33:09.660
 does it do it by some sort of gradient estimation?

33:09.940 --> 33:10.860
 It doesn't need to be a back prop,

33:10.860 --> 33:14.820
 but some way of estimating the gradient in efficient manner

33:14.820 --> 33:17.020
 whose complexity is on the same order of magnitude

33:17.020 --> 33:20.800
 as actually running the inference.

33:20.800 --> 33:24.060
 Because you can't afford to do things

33:24.060 --> 33:26.540
 like perturbing a weight in your brain

33:26.540 --> 33:28.100
 to figure out what the effect is.

33:28.100 --> 33:30.780
 And then sort of, you can do sort of

33:30.780 --> 33:33.300
 estimating gradient by perturbation.

33:33.300 --> 33:35.460
 To me, it seems very implausible

33:35.460 --> 33:40.460
 that the brain uses some sort of zeroth order black box

33:41.060 --> 33:43.000
 gradient free optimization,

33:43.000 --> 33:45.200
 because it's so much less efficient

33:45.200 --> 33:46.320
 than gradient optimization.

33:46.320 --> 33:49.260
 So it has to have a way of estimating gradient.

33:49.260 --> 33:52.780
 Is it possible that some kind of logic based reasoning

33:52.780 --> 33:55.400
 emerges in pockets as a useful,

33:55.400 --> 33:58.100
 like you said, if the brain is an objective function,

33:58.100 --> 34:01.300
 maybe it's a mechanism for creating objective functions.

34:01.300 --> 34:06.300
 It's a mechanism for creating knowledge bases, for example,

34:06.520 --> 34:08.380
 that can then be queried.

34:08.380 --> 34:10.300
 Like maybe it's like an efficient representation

34:10.300 --> 34:12.700
 of knowledge that's learned in a gradient based way

34:12.700 --> 34:13.780
 or something like that.

34:13.780 --> 34:15.980
 Well, so I think there is a lot of different types

34:15.980 --> 34:17.340
 of intelligence.

34:17.340 --> 34:19.700
 So first of all, I think the type of logical reasoning

34:19.700 --> 34:23.780
 that we think about that we are maybe stemming

34:23.780 --> 34:27.740
 from sort of classical AI of the 1970s and 80s.

34:29.080 --> 34:33.020
 I think humans use that relatively rarely

34:33.020 --> 34:34.740
 and are not particularly good at it.

34:34.740 --> 34:37.560
 But we judge each other based on our ability

34:37.560 --> 34:40.620
 to solve those rare problems.

34:40.620 --> 34:41.660
 It's called an IQ test.

34:41.660 --> 34:42.700
 I don't think so.

34:42.700 --> 34:45.260
 Like I'm not very good at chess.

34:45.260 --> 34:47.420
 Yes, I'm judging you this whole time.

34:47.420 --> 34:49.740
 Because, well, we actually.

34:49.740 --> 34:53.500
 With your heritage, I'm sure you're good at chess.

34:53.500 --> 34:55.060
 No, stereotypes.

34:55.060 --> 34:56.700
 Not all stereotypes are true.

34:58.020 --> 34:59.020
 Well, I'm terrible at chess.

34:59.020 --> 35:04.020
 So, but I think perhaps another type of intelligence

35:04.660 --> 35:08.980
 that I have is this ability of sort of building models

35:08.980 --> 35:13.820
 to the world from reasoning obviously,

35:13.820 --> 35:15.980
 but also data.

35:15.980 --> 35:18.900
 And those models generally are more kind of analogical.

35:18.900 --> 35:22.380
 So it's reasoning by simulation,

35:22.380 --> 35:25.120
 and by analogy, where you use one model

35:25.120 --> 35:26.900
 to apply to a new situation.

35:26.900 --> 35:28.500
 Even though you've never seen that situation,

35:28.500 --> 35:31.620
 you can sort of connect it to a situation

35:31.620 --> 35:33.500
 you've encountered before.

35:33.500 --> 35:36.700
 And your reasoning is more akin

35:36.700 --> 35:38.420
 to some sort of internal simulation.

35:38.420 --> 35:41.140
 So you're kind of simulating what's happening

35:41.140 --> 35:42.240
 when you're building, I don't know,

35:42.240 --> 35:44.100
 a box out of wood or something, right?

35:44.100 --> 35:47.460
 You can imagine in advance what would be the result

35:47.460 --> 35:49.660
 of cutting the wood in this particular way.

35:49.660 --> 35:52.900
 Are you going to use screws or nails or whatever?

35:52.900 --> 35:54.180
 When you are interacting with someone,

35:54.180 --> 35:55.780
 you also have a model of that person

35:55.780 --> 35:58.340
 and sort of interact with that person,

35:59.580 --> 36:03.660
 having this model in mind to kind of tell the person

36:03.660 --> 36:05.280
 what you think is useful to them.

36:05.280 --> 36:10.220
 So I think this ability to construct models to the world

36:10.220 --> 36:13.900
 is basically the essence, the essence of intelligence.

36:13.900 --> 36:18.220
 And the ability to use it then to plan actions

36:18.220 --> 36:23.080
 that will fulfill a particular criterion,

36:23.080 --> 36:25.460
 of course, is necessary as well.

36:25.460 --> 36:27.740
 So I'm going to ask you a series of impossible questions

36:27.740 --> 36:30.180
 as we keep asking, as I've been doing.

36:30.180 --> 36:33.460
 So if that's the fundamental sort of dark matter

36:33.460 --> 36:36.580
 of intelligence, this ability to form a background model,

36:36.580 --> 36:41.460
 what's your intuition about how much knowledge is required?

36:41.460 --> 36:43.100
 You know, I think dark matter,

36:43.100 --> 36:45.980
 you could put a percentage on it

36:45.980 --> 36:50.060
 of the composition of the universe

36:50.060 --> 36:51.460
 and how much of it is dark matter,

36:51.460 --> 36:52.640
 how much of it is dark energy,

36:52.640 --> 36:57.640
 how much information do you think is required

36:57.900 --> 36:59.920
 to be a house cat?

36:59.920 --> 37:02.900
 So you have to be able to, when you see a box going in,

37:02.900 --> 37:06.220
 when you see a human compute the most evil action,

37:06.220 --> 37:07.940
 if there's a thing that's near an edge,

37:07.940 --> 37:10.980
 you knock it off, all of that,

37:10.980 --> 37:12.740
 plus the extra stuff you mentioned,

37:12.740 --> 37:15.700
 which is a great self awareness of the physics

37:15.700 --> 37:18.740
 of your own body and the world.

37:18.740 --> 37:21.540
 How much knowledge is required, do you think, to solve it?

37:22.500 --> 37:25.620
 I don't even know how to measure an answer to that question.

37:25.620 --> 37:26.680
 I'm not sure how to measure it,

37:26.680 --> 37:31.140
 but whatever it is, it fits in about 800,000 neurons,

37:32.380 --> 37:33.900
 800 million neurons.

37:33.900 --> 37:36.300
 What's the representation does?

37:36.300 --> 37:38.540
 Everything, all knowledge, everything, right?

37:40.100 --> 37:41.500
 You know, it's less than a billion.

37:41.500 --> 37:44.420
 A dog is 2 billion, but a cat is less than 1 billion.

37:45.500 --> 37:48.140
 And so multiply that by a thousand

37:48.140 --> 37:50.300
 and you get the number of synapses.

37:50.300 --> 37:52.780
 And I think almost all of it is learned

37:52.780 --> 37:55.940
 through this, you know, a sort of self supervised running,

37:55.940 --> 37:58.500
 although, you know, I think a tiny sliver

37:58.500 --> 37:59.900
 is learned through reinforcement running

37:59.900 --> 38:02.220
 and certainly very little through, you know,

38:02.220 --> 38:03.340
 classical supervised running,

38:03.340 --> 38:05.180
 although it's not even clear how supervised running

38:05.180 --> 38:08.120
 actually works in the biological world.

38:09.260 --> 38:12.860
 So I think almost all of it is self supervised running,

38:12.860 --> 38:17.860
 but it's driven by the sort of ingrained objective functions

38:18.180 --> 38:21.400
 that a cat or a human have at the base of their brain,

38:21.400 --> 38:24.880
 which kind of drives their behavior.

38:24.880 --> 38:28.580
 So, you know, nature tells us you're hungry.

38:29.480 --> 38:31.900
 It doesn't tell us how to feed ourselves.

38:31.900 --> 38:33.500
 That's something that the rest of our brain

38:33.500 --> 38:34.820
 has to figure out, right?

38:35.780 --> 38:37.940
 What's interesting is there might be more

38:37.940 --> 38:39.660
 like deeper objective functions

38:39.660 --> 38:41.300
 than allowing the whole thing.

38:41.300 --> 38:44.500
 So hunger may be some kind of,

38:44.500 --> 38:46.140
 now you go to like neurobiology,

38:46.140 --> 38:51.140
 it might be just the brain trying to maintain homeostasis.

38:52.460 --> 38:57.460
 So hunger is just one of the human perceivable symptoms

38:58.020 --> 38:59.380
 of the brain being unhappy

38:59.380 --> 39:01.460
 with the way things are currently.

39:01.460 --> 39:04.140
 It could be just like one really dumb objective function

39:04.140 --> 39:04.980
 at the core.

39:04.980 --> 39:08.460
 But that's how behavior is driven.

39:08.460 --> 39:11.260
 The fact that, you know, or basal ganglia

39:12.360 --> 39:14.820
 drive us to do things that are different

39:14.820 --> 39:18.180
 from say an orangutan or certainly a cat

39:18.180 --> 39:20.060
 is what makes, you know, human nature

39:20.060 --> 39:23.280
 versus orangutan nature versus cat nature.

39:23.280 --> 39:27.100
 So for example, you know, our basal ganglia

39:27.100 --> 39:32.100
 drives us to seek the company of other humans.

39:32.220 --> 39:34.540
 And that's because nature has figured out

39:34.540 --> 39:37.540
 that we need to be social animals for our species to survive.

39:37.540 --> 39:40.320
 And it's true of many primates.

39:41.300 --> 39:42.620
 It's not true of orangutans.

39:42.620 --> 39:44.900
 Orangutans are solitary animals.

39:44.900 --> 39:46.900
 They don't seek the company of others.

39:46.900 --> 39:48.200
 In fact, they avoid them.

39:49.300 --> 39:51.060
 In fact, they scream at them when they come too close

39:51.060 --> 39:52.740
 because they're territorial.

39:52.740 --> 39:55.900
 Because for their survival, you know,

39:55.900 --> 39:58.300
 evolution has figured out that's the best thing.

39:58.300 --> 40:00.040
 I mean, they're occasionally social, of course,

40:00.040 --> 40:03.500
 for, you know, reproduction and stuff like that.

40:03.500 --> 40:05.920
 But they're mostly solitary.

40:05.920 --> 40:09.540
 So all of those behaviors are not part of intelligence.

40:09.540 --> 40:10.380
 You know, people say,

40:10.380 --> 40:11.800
 oh, you're never gonna have intelligent machines

40:11.800 --> 40:13.940
 because, you know, human intelligence is social.

40:13.940 --> 40:16.820
 But then you look at orangutans, you look at octopus.

40:16.820 --> 40:18.800
 Octopus never know their parents.

40:18.800 --> 40:20.500
 They barely interact with any other.

40:20.500 --> 40:23.900
 And they get to be really smart in less than a year,

40:23.900 --> 40:26.040
 in like half a year.

40:26.040 --> 40:27.620
 You know, in a year, they're adults.

40:27.620 --> 40:28.780
 In two years, they're dead.

40:28.780 --> 40:33.620
 So there are things that we think, as humans,

40:33.620 --> 40:35.740
 are intimately linked with intelligence,

40:35.740 --> 40:38.840
 like social interaction, like language.

40:39.760 --> 40:42.860
 We think, I think we give way too much importance

40:42.860 --> 40:46.780
 to language as a substrate of intelligence as humans.

40:46.780 --> 40:49.840
 Because we think our reasoning is so linked with language.

40:49.840 --> 40:53.460
 So to solve the house cat intelligence problem,

40:53.460 --> 40:55.500
 you think you could do it on a desert island.

40:55.500 --> 40:58.460
 You could have, you could just have a cat sitting there

41:00.360 --> 41:03.180
 looking at the waves, at the ocean waves,

41:03.180 --> 41:05.740
 and figure a lot of it out.

41:05.740 --> 41:07.500
 It needs to have sort of, you know,

41:07.500 --> 41:11.540
 the right set of drives to kind of, you know,

41:11.540 --> 41:13.980
 get it to do the thing and learn the appropriate things,

41:13.980 --> 41:17.660
 right, but like for example, you know,

41:17.660 --> 41:22.660
 baby humans are driven to learn to stand up and walk.

41:22.660 --> 41:26.020
 You know, that's kind of, this desire is hardwired.

41:26.020 --> 41:28.540
 How to do it precisely is not, that's learned.

41:28.540 --> 41:31.800
 But the desire to walk, move around and stand up,

41:32.840 --> 41:35.940
 that's sort of probably hardwired.

41:35.940 --> 41:38.940
 But it's very simple to hardwire this kind of stuff.

41:38.940 --> 41:42.780
 Oh, like the desire to, well, that's interesting.

41:42.780 --> 41:44.420
 You're hardwired to want to walk.

41:45.620 --> 41:50.460
 That's not, there's gotta be a deeper need for walking.

41:50.460 --> 41:53.140
 I think it was probably socially imposed by society

41:53.140 --> 41:55.580
 that you need to walk all the other bipedal.

41:55.580 --> 41:58.420
 No, like a lot of simple animals that, you know,

41:58.420 --> 42:01.040
 will probably walk without ever watching

42:01.040 --> 42:03.900
 any other members of the species.

42:03.900 --> 42:06.820
 It seems like a scary thing to have to do

42:06.820 --> 42:09.280
 because you suck at bipedal walking at first.

42:09.280 --> 42:13.820
 It seems crawling is much safer, much more like,

42:13.820 --> 42:15.700
 why are you in a hurry?

42:15.700 --> 42:18.660
 Well, because you have this thing that drives you to do it,

42:18.660 --> 42:23.180
 you know, which is sort of part of the sort of

42:24.220 --> 42:25.060
 human development.

42:25.060 --> 42:26.700
 Is that understood actually what?

42:26.700 --> 42:28.220
 Not entirely, no.

42:28.220 --> 42:29.740
 What's the reason you get on two feet?

42:29.740 --> 42:30.620
 It's really hard.

42:30.620 --> 42:32.780
 Like most animals don't get on two feet.

42:32.780 --> 42:33.980
 Well, they get on four feet.

42:33.980 --> 42:35.740
 You know, many mammals get on four feet.

42:35.740 --> 42:36.760
 Yeah, they do. Very quickly.

42:36.760 --> 42:38.500
 Some of them extremely quickly.

42:38.500 --> 42:41.380
 But I don't, you know, like from the last time

42:41.380 --> 42:42.620
 I've interacted with a table,

42:42.620 --> 42:44.940
 that's much more stable than a thing than two legs.

42:44.940 --> 42:46.420
 It's just a really hard problem.

42:46.420 --> 42:48.620
 Yeah, I mean, birds have figured it out with two feet.

42:48.620 --> 42:52.020
 Well, technically we can go into ontology.

42:52.020 --> 42:54.500
 They have four, I guess they have two feet.

42:54.500 --> 42:55.340
 They have two feet.

42:55.340 --> 42:56.380
 Chickens.

42:56.380 --> 42:58.860
 You know, dinosaurs have two feet, many of them.

42:58.860 --> 42:59.700
 Allegedly.

43:01.560 --> 43:04.340
 I'm just now learning that T. rex was eating grass,

43:04.340 --> 43:05.420
 not other animals.

43:05.420 --> 43:08.020
 T. rex might've been a friendly pet.

43:08.020 --> 43:09.260
 What do you think about,

43:10.320 --> 43:13.500
 I don't know if you looked at the test

43:13.500 --> 43:16.380
 for general intelligence that François Chollet put together.

43:16.380 --> 43:18.000
 I don't know if you got a chance to look

43:18.000 --> 43:19.660
 at that kind of thing.

43:19.660 --> 43:21.860
 What's your intuition about how to solve

43:21.860 --> 43:23.740
 like an IQ type of test?

43:23.740 --> 43:24.580
 I don't know.

43:24.580 --> 43:26.140
 I think it's so outside of my radar screen

43:26.140 --> 43:30.740
 that it's not really relevant, I think, in the short term.

43:30.740 --> 43:33.100
 Well, I guess one way to ask,

43:33.100 --> 43:37.780
 another way, perhaps more closer to what do you work is like,

43:37.780 --> 43:42.740
 how do you solve MNIST with very little example data?

43:42.740 --> 43:43.560
 That's right.

43:43.560 --> 43:44.860
 And that's the answer to this probably

43:44.860 --> 43:45.860
 is self supervised learning.

43:45.860 --> 43:47.300
 Just learn to represent images

43:47.300 --> 43:51.060
 and then learning to recognize handwritten digits

43:51.060 --> 43:53.620
 on top of this will only require a few samples.

43:53.620 --> 43:55.460
 And we observe this in humans, right?

43:55.460 --> 43:58.660
 You show a young child a picture book

43:58.660 --> 44:01.940
 with a couple of pictures of an elephant and that's it.

44:01.940 --> 44:03.900
 The child knows what an elephant is.

44:03.900 --> 44:06.700
 And we see this today with practical systems

44:06.700 --> 44:09.540
 that we train image recognition systems

44:09.540 --> 44:13.660
 with enormous amounts of images,

44:13.660 --> 44:15.740
 either completely self supervised

44:15.740 --> 44:16.980
 or very weakly supervised.

44:16.980 --> 44:20.900
 For example, you can train a neural net

44:20.900 --> 44:24.180
 to predict whatever hashtag people type on Instagram, right?

44:24.180 --> 44:25.780
 Then you can do this with billions of images

44:25.780 --> 44:28.540
 because there's billions per day that are showing up.

44:28.540 --> 44:30.700
 So the amount of training data there

44:30.700 --> 44:32.340
 is essentially unlimited.

44:32.340 --> 44:35.380
 And then you take the output representation,

44:35.380 --> 44:37.380
 a couple of layers down from the outputs

44:37.380 --> 44:40.680
 of what the system learned and feed this as input

44:40.680 --> 44:43.780
 to a classifier for any object in the world that you want

44:43.780 --> 44:44.940
 and it works pretty well.

44:44.940 --> 44:47.620
 So that's transfer learning, okay?

44:47.620 --> 44:50.160
 Or weakly supervised transfer learning.

44:51.340 --> 44:53.460
 People are making very, very fast progress

44:53.460 --> 44:55.300
 using self supervised learning

44:55.300 --> 44:57.380
 for this kind of scenario as well.

44:58.580 --> 45:02.500
 And my guess is that that's gonna be the future.

45:02.500 --> 45:03.660
 For self supervised learning,

45:03.660 --> 45:06.800
 how much cleaning do you think is needed

45:06.800 --> 45:11.800
 for filtering malicious signal or what's a better term?

45:11.800 --> 45:15.760
 But like a lot of people use hashtags on Instagram

45:16.760 --> 45:21.200
 to get like good SEO that doesn't fully represent

45:21.200 --> 45:23.100
 the contents of the image.

45:23.100 --> 45:24.520
 Like they'll put a picture of a cat

45:24.520 --> 45:28.060
 and hashtag it with like science, awesome, fun.

45:28.060 --> 45:31.200
 I don't know all kinds, why would you put science?

45:31.200 --> 45:33.080
 That's not very good SEO.

45:33.080 --> 45:34.960
 The way my colleagues who worked on this project

45:34.960 --> 45:39.960
 at Facebook, now Meta AI, a few years ago dealt with this

45:39.960 --> 45:43.760
 is that they only selected something like 17,000 tags

45:43.760 --> 45:48.100
 that correspond to kind of physical things or situations,

45:48.100 --> 45:50.320
 like that has some visual content.

45:52.320 --> 45:57.120
 So you wouldn't have like hash TBT or anything like that.

45:57.120 --> 46:00.820
 Oh, so they keep a very select set of hashtags

46:00.820 --> 46:01.660
 is what you're saying?

46:01.660 --> 46:02.480
 Yeah.

46:02.480 --> 46:03.320
 Okay.

46:03.320 --> 46:06.080
 But it's still in the order of 10 to 20,000.

46:06.080 --> 46:07.960
 So it's fairly large.

46:07.960 --> 46:09.040
 Okay.

46:09.040 --> 46:11.280
 Can you tell me about data augmentation?

46:11.280 --> 46:14.760
 What the heck is data augmentation and how is it used

46:14.760 --> 46:19.080
 maybe contrast of learning for video?

46:19.080 --> 46:20.880
 What are some cool ideas here?

46:20.880 --> 46:22.120
 Right, so data augmentation.

46:22.120 --> 46:24.520
 I mean, first data augmentation is the idea

46:24.520 --> 46:26.960
 of artificially increasing the size of your training set

46:26.960 --> 46:30.020
 by distorting the images that you have

46:30.020 --> 46:32.360
 in ways that don't change the nature of the image, right?

46:32.360 --> 46:35.520
 So you do MNIST, you can do data augmentation on MNIST

46:35.520 --> 46:37.360
 and people have done this since the 1990s, right?

46:37.360 --> 46:40.880
 You take a MNIST digit and you shift it a little bit

46:40.880 --> 46:44.820
 or you change the size or rotate it, skew it,

46:45.800 --> 46:47.000
 you know, et cetera.

46:47.000 --> 46:48.280
 Add noise.

46:48.280 --> 46:49.520
 Add noise, et cetera.

46:49.520 --> 46:52.440
 And it works better if you train a supervised classifier

46:52.440 --> 46:55.600
 with augmented data, you're gonna get better results.

46:55.600 --> 46:58.640
 Now it's become really interesting

46:58.640 --> 47:00.400
 over the last couple of years

47:00.400 --> 47:04.160
 because a lot of self supervised learning techniques

47:04.160 --> 47:07.980
 to pre train vision systems are based on data augmentation.

47:07.980 --> 47:12.000
 And the basic techniques is originally inspired

47:12.000 --> 47:15.840
 by techniques that I worked on in the early 90s

47:15.840 --> 47:17.720
 and Jeff Hinton worked on also in the early 90s.

47:17.720 --> 47:20.040
 They were sort of parallel work.

47:20.040 --> 47:21.600
 I used to call this Siamese network.

47:21.600 --> 47:24.960
 So basically you take two identical copies

47:24.960 --> 47:27.720
 of the same network, they share the same weights

47:27.720 --> 47:31.760
 and you show two different views of the same object.

47:31.760 --> 47:33.920
 Either those two different views may have been obtained

47:33.920 --> 47:35.440
 by data augmentation

47:35.440 --> 47:37.680
 or maybe it's two different views of the same scene

47:37.680 --> 47:40.280
 from a camera that you moved or at different times

47:40.280 --> 47:41.400
 or something like that, right?

47:41.400 --> 47:44.400
 Or two pictures of the same person, things like that.

47:44.400 --> 47:46.480
 And then you train this neural net,

47:46.480 --> 47:48.420
 those two identical copies of this neural net

47:48.420 --> 47:51.420
 to produce an output representation, a vector

47:52.460 --> 47:56.560
 in such a way that the representation for those two images

47:56.560 --> 47:58.880
 are as close to each other as possible,

47:58.880 --> 48:00.840
 as identical to each other as possible, right?

48:00.840 --> 48:02.040
 Because you want the system

48:02.040 --> 48:06.120
 to basically learn a function that will be invariant,

48:06.120 --> 48:08.200
 that will not change, whose output will not change

48:08.200 --> 48:12.480
 when you transform those inputs in those particular ways,

48:12.480 --> 48:14.080
 right?

48:14.080 --> 48:15.680
 So that's easy to do.

48:15.680 --> 48:17.720
 What's complicated is how do you make sure

48:17.720 --> 48:19.520
 that when you show two images that are different,

48:19.520 --> 48:21.960
 the system will produce different things?

48:21.960 --> 48:26.200
 Because if you don't have a specific provision for this,

48:26.200 --> 48:29.160
 the system will just ignore the inputs when you train it,

48:29.160 --> 48:30.360
 it will end up ignoring the input

48:30.360 --> 48:31.740
 and just produce a constant vector

48:31.740 --> 48:33.680
 that is the same for every input, right?

48:33.680 --> 48:35.200
 That's called a collapse.

48:35.200 --> 48:36.720
 Now, how do you avoid collapse?

48:36.720 --> 48:37.800
 So there's two ideas.

48:38.840 --> 48:41.560
 One idea that I proposed in the early 90s

48:41.560 --> 48:43.120
 with my colleagues at Bell Labs,

48:43.120 --> 48:45.360
 Jane Barmley and a couple other people,

48:46.280 --> 48:48.280
 which we now call contrastive learning,

48:48.280 --> 48:50.020
 which is to have negative examples, right?

48:50.020 --> 48:53.160
 So you have pairs of images that you know are different

48:54.400 --> 48:57.480
 and you show them to the network and those two copies,

48:57.480 --> 48:59.760
 and then you push the two output vectors away

48:59.760 --> 49:02.200
 from each other and it will eventually guarantee

49:02.200 --> 49:04.880
 that things that are semantically similar

49:04.880 --> 49:06.480
 produce similar representations

49:06.480 --> 49:07.320
 and things that are different

49:07.320 --> 49:09.080
 produce different representations.

49:10.280 --> 49:11.440
 We actually came up with this idea

49:11.440 --> 49:14.480
 for a project of doing signature verification.

49:14.480 --> 49:18.400
 So we would collect signatures from,

49:18.400 --> 49:20.160
 like multiple signatures on the same person

49:20.160 --> 49:23.280
 and then train a neural net to produce the same representation

49:23.280 --> 49:27.880
 and then force the system to produce different

49:27.880 --> 49:29.920
 representation for different signatures.

49:31.000 --> 49:33.460
 This was actually, the problem was proposed by people

49:33.460 --> 49:38.240
 from what was a subsidiary of AT&T at the time called NCR.

49:38.240 --> 49:40.360
 And they were interested in storing

49:40.360 --> 49:43.500
 representation of the signature on the 80 bytes

49:43.500 --> 49:46.640
 of the magnetic strip of a credit card.

49:46.640 --> 49:48.800
 So we came up with this idea of having a neural net

49:48.800 --> 49:52.280
 with 80 outputs that we would quantize on bytes

49:52.280 --> 49:53.840
 so that we could encode the signature.

49:53.840 --> 49:55.440
 And that encoding was then used to compare

49:55.440 --> 49:57.080
 whether the signature matches or not.

49:57.080 --> 49:57.920
 That's right.

49:57.920 --> 50:00.640
 So then you would sign, you would run through the neural net

50:00.640 --> 50:02.400
 and then you would compare the output vector

50:02.400 --> 50:03.240
 to whatever is stored on your card.

50:03.240 --> 50:04.640
 Did it actually work?

50:04.640 --> 50:06.680
 It worked, but they ended up not using it.

50:08.940 --> 50:10.120
 Because nobody cares actually.

50:10.120 --> 50:13.800
 I mean, the American financial payment system

50:13.800 --> 50:17.560
 is incredibly lax in that respect compared to Europe.

50:17.560 --> 50:18.960
 Oh, with the signatures?

50:18.960 --> 50:20.520
 What's the purpose of signatures anyway?

50:20.520 --> 50:21.360
 This is very different.

50:21.360 --> 50:23.280
 Nobody looks at them, nobody cares.

50:23.280 --> 50:24.440
 It's, yeah.

50:24.440 --> 50:27.840
 Yeah, no, so that's contrastive learning, right?

50:27.840 --> 50:29.440
 So you need positive and negative pairs.

50:29.440 --> 50:31.760
 And the problem with that is that,

50:31.760 --> 50:34.760
 even though I had the original paper on this,

50:34.760 --> 50:36.800
 I'm actually not very positive about it

50:36.800 --> 50:38.640
 because it doesn't work in high dimension.

50:38.640 --> 50:41.040
 If your representation is high dimensional,

50:41.040 --> 50:44.300
 there's just too many ways for two things to be different.

50:44.300 --> 50:45.960
 And so you would need lots and lots

50:45.960 --> 50:48.260
 and lots of negative pairs.

50:48.260 --> 50:50.800
 So there is a particular implementation of this,

50:50.800 --> 50:52.840
 which is relatively recent from actually

50:52.840 --> 50:56.040
 the Google Toronto group where, you know,

50:56.040 --> 50:58.800
 Jeff Hinton is the senior member there.

50:58.800 --> 51:02.000
 It's called SIMCLR, S I M C L R.

51:02.000 --> 51:03.720
 And it, you know, basically a particular way

51:03.720 --> 51:06.760
 of implementing this idea of contrastive learning,

51:06.760 --> 51:08.600
 the particular objective function.

51:08.600 --> 51:13.160
 Now, what I'm much more enthusiastic about these days

51:13.160 --> 51:14.600
 is non contrastive methods.

51:14.600 --> 51:19.600
 So other ways to guarantee that the representations

51:19.600 --> 51:23.240
 would be different for different inputs.

51:24.200 --> 51:28.320
 And it's actually based on an idea that Jeff Hinton

51:28.320 --> 51:30.360
 proposed in the early nineties with his student

51:30.360 --> 51:31.960
 at the time, Sue Becker.

51:31.960 --> 51:33.440
 And it's based on the idea of maximizing

51:33.440 --> 51:35.000
 the mutual information between the outputs

51:35.000 --> 51:36.200
 of the two systems.

51:36.200 --> 51:37.480
 You only show positive pairs.

51:37.480 --> 51:39.160
 You only show pairs of images that you know

51:39.160 --> 51:41.640
 are somewhat similar.

51:41.640 --> 51:44.200
 And you train the two networks to be informative,

51:44.200 --> 51:48.880
 but also to be as informative of each other as possible.

51:48.880 --> 51:51.400
 So basically one representation has to be predictable

51:51.400 --> 51:53.080
 from the other, essentially.

51:54.520 --> 51:56.400
 And, you know, he proposed that idea,

51:56.400 --> 51:59.440
 had, you know, a couple of papers in the early nineties,

51:59.440 --> 52:02.280
 and then nothing was done about it for decades.

52:02.280 --> 52:04.360
 And I kind of revived this idea together

52:04.360 --> 52:06.240
 with my postdocs at FAIR,

52:07.480 --> 52:08.920
 particularly a postdoc called Stefan Denis,

52:08.920 --> 52:11.800
 who is now a junior professor in Finland

52:11.800 --> 52:13.240
 at University of Aalto.

52:13.240 --> 52:18.240
 We came up with something that we call Barlow Twins.

52:18.240 --> 52:20.520
 And it's a particular way of maximizing

52:20.520 --> 52:24.240
 the information content of a vector,

52:24.240 --> 52:26.960
 you know, using some hypotheses.

52:27.920 --> 52:30.920
 And we have kind of another version of it

52:30.920 --> 52:33.480
 that's more recent now called VICREG, V I C A R E G.

52:33.480 --> 52:35.960
 That means Variance, Invariance, Covariance,

52:35.960 --> 52:36.800
 Regularization.

52:36.800 --> 52:38.840
 And it's the thing I'm the most excited about

52:38.840 --> 52:40.600
 in machine learning in the last 15 years.

52:40.600 --> 52:43.360
 I mean, I'm not, I'm really, really excited about this.

52:43.360 --> 52:46.400
 What kind of data augmentation is useful

52:46.400 --> 52:49.280
 for that noncontrastive learning method?

52:49.280 --> 52:51.680
 Are we talking about, does that not matter that much?

52:51.680 --> 52:55.040
 Or it seems like a very important part of the step.

52:55.040 --> 52:55.880
 Yeah.

52:55.880 --> 52:57.120
 How you generate the images that are similar,

52:57.120 --> 52:58.680
 but sufficiently different.

52:58.680 --> 52:59.520
 Yeah, that's right.

52:59.520 --> 53:01.440
 It's an important step and it's also an annoying step

53:01.440 --> 53:02.840
 because you need to have that knowledge

53:02.840 --> 53:05.840
 of what data augmentation you can do

53:05.840 --> 53:09.320
 that do not change the nature of the object.

53:09.320 --> 53:12.280
 And so the standard scenario,

53:12.280 --> 53:14.520
 which a lot of people working in this area are using

53:14.520 --> 53:18.720
 is you use the type of distortion.

53:18.720 --> 53:21.160
 So basically you do a geometric distortion.

53:21.160 --> 53:23.360
 So one basically just shifts the image a little bit,

53:23.360 --> 53:24.400
 it's called cropping.

53:24.400 --> 53:26.880
 Another one kind of changes the scale a little bit.

53:26.880 --> 53:28.240
 Another one kind of rotates it.

53:28.240 --> 53:30.000
 Another one changes the colors.

53:30.000 --> 53:32.040
 You can do a shift in color balance

53:32.040 --> 53:34.880
 or something like that, saturation.

53:34.880 --> 53:36.240
 Another one sort of blurs it.

53:36.240 --> 53:37.080
 Another one adds noise.

53:37.080 --> 53:40.040
 So you have like a catalog of kind of standard things

53:40.040 --> 53:42.120
 and people try to use the same ones

53:42.120 --> 53:44.960
 for different algorithms so that they can compare.

53:44.960 --> 53:47.200
 But some algorithms, some self supervised algorithm

53:47.200 --> 53:49.600
 actually can deal with much bigger,

53:49.600 --> 53:52.480
 like more aggressive data augmentation and some don't.

53:52.480 --> 53:55.400
 So that kind of makes the whole thing difficult.

53:55.400 --> 53:57.760
 But that's the kind of distortions we're talking about.

53:57.760 --> 54:02.520
 And so you train with those distortions

54:02.520 --> 54:07.400
 and then you chop off the last layer, a couple layers

54:07.400 --> 54:11.480
 of the network and you use the representation

54:11.480 --> 54:12.680
 as input to a classifier.

54:12.680 --> 54:16.680
 You train the classifier on ImageNet, let's say,

54:16.680 --> 54:19.600
 or whatever, and measure the performance.

54:19.600 --> 54:23.520
 And interestingly enough, the methods that are really good

54:23.520 --> 54:25.960
 at eliminating the information that is irrelevant,

54:25.960 --> 54:29.200
 which is the distortions between those images,

54:29.200 --> 54:31.480
 do a good job at eliminating it.

54:31.480 --> 54:36.480
 And as a consequence, you cannot use the representations

54:36.480 --> 54:39.080
 in those systems for things like object detection

54:39.080 --> 54:41.480
 and localization because that information is gone.

54:41.480 --> 54:44.760
 So the type of data augmentation you need to do

54:44.760 --> 54:47.720
 depends on the tasks you want eventually the system

54:47.720 --> 54:50.680
 to solve and the type of data augmentation,

54:50.680 --> 54:52.560
 standard data augmentation that we use today

54:52.560 --> 54:54.720
 are only appropriate for object recognition

54:54.720 --> 54:56.040
 or image classification.

54:56.040 --> 54:57.760
 They're not appropriate for things like.

54:57.760 --> 55:00.800
 Can you help me out understand what wide localizations?

55:00.800 --> 55:03.760
 So you're saying it's just not good at the negative,

55:03.760 --> 55:05.440
 like at classifying the negative,

55:05.440 --> 55:07.920
 so that's why it can't be used for the localization?

55:07.920 --> 55:10.360
 No, it's just that you train the system,

55:10.360 --> 55:13.560
 you give it an image and then you give it the same image

55:13.560 --> 55:17.400
 shifted and scaled and you tell it that's the same image.

55:17.400 --> 55:19.160
 So the system basically is trained

55:19.160 --> 55:22.040
 to eliminate the information about position and size.

55:22.040 --> 55:26.200
 So now you want to use that to figure out

55:26.200 --> 55:27.760
 where an object is and what size it is.

55:27.760 --> 55:30.040
 Like a bounding box, like they'd be able to actually.

55:30.040 --> 55:34.160
 Okay, it can still find the object in the image,

55:34.160 --> 55:35.960
 it's just not very good at finding

55:35.960 --> 55:38.960
 the exact boundaries of that object, interesting.

55:38.960 --> 55:42.040
 Interesting, which that's an interesting

55:42.040 --> 55:43.480
 sort of philosophical question,

55:43.480 --> 55:46.800
 how important is object localization anyway?

55:46.800 --> 55:51.240
 We're like obsessed by measuring image segmentation,

55:51.240 --> 55:53.420
 obsessed by measuring perfectly knowing

55:53.420 --> 55:56.760
 the boundaries of objects when arguably

55:56.760 --> 56:01.760
 that's not that essential to understanding

56:01.840 --> 56:03.800
 what are the contents of the scene.

56:03.800 --> 56:05.880
 On the other hand, I think evolutionarily,

56:05.880 --> 56:08.200
 the first vision systems in animals

56:08.200 --> 56:10.040
 were basically all about localization,

56:10.040 --> 56:12.480
 very little about recognition.

56:12.480 --> 56:15.320
 And in the human brain, you have two separate pathways

56:15.320 --> 56:20.320
 for recognizing the nature of a scene or an object

56:20.880 --> 56:22.320
 and localizing objects.

56:22.320 --> 56:25.200
 So you use the first pathway called eventual pathway

56:25.200 --> 56:28.160
 for telling what you're looking at.

56:29.140 --> 56:30.560
 The other pathway, the dorsal pathway,

56:30.560 --> 56:34.120
 is used for navigation, for grasping, for everything else.

56:34.120 --> 56:36.920
 And basically a lot of the things you need for survival

56:36.920 --> 56:39.740
 are localization and detection.

56:41.880 --> 56:45.080
 Is similarity learning or contrastive learning,

56:45.080 --> 56:46.520
 are these non contrastive methods

56:46.520 --> 56:48.880
 the same as understanding something?

56:48.880 --> 56:50.680
 Just because you know a distorted cat

56:50.680 --> 56:52.600
 is the same as a non distorted cat,

56:52.600 --> 56:56.760
 does that mean you understand what it means to be a cat?

56:56.760 --> 56:57.600
 To some extent.

56:57.600 --> 57:00.120
 I mean, it's a superficial understanding, obviously.

57:00.120 --> 57:02.360
 But what is the ceiling of this method, do you think?

57:02.360 --> 57:05.120
 Is this just one trick on the path

57:05.120 --> 57:07.320
 to doing self supervised learning?

57:07.320 --> 57:10.040
 Can we go really, really far?

57:10.040 --> 57:11.280
 I think we can go really far.

57:11.280 --> 57:16.280
 So if we figure out how to use techniques of that type,

57:16.400 --> 57:19.480
 perhaps very different, but the same nature,

57:19.480 --> 57:23.360
 to train a system from video to do video prediction,

57:23.360 --> 57:28.360
 essentially, I think we'll have a path towards,

57:30.440 --> 57:33.520
 I wouldn't say unlimited, but a path towards some level

57:33.520 --> 57:38.120
 of physical common sense in machines.

57:38.120 --> 57:43.120
 And I also think that that ability to learn

57:44.440 --> 57:47.720
 how the world works from a sort of high throughput channel

57:47.720 --> 57:52.720
 like vision is a necessary step towards

57:53.520 --> 57:55.560
 sort of real artificial intelligence.

57:55.560 --> 57:58.080
 In other words, I believe in grounded intelligence.

57:58.080 --> 57:59.920
 I don't think we can train a machine

57:59.920 --> 58:02.200
 to be intelligent purely from text.

58:02.200 --> 58:04.960
 Because I think the amount of information about the world

58:04.960 --> 58:07.680
 that's contained in text is tiny compared

58:07.680 --> 58:09.960
 to what we need to know.

58:11.600 --> 58:15.320
 So for example, and people have attempted to do this

58:15.320 --> 58:18.920
 for 30 years, the psych project and things like that,

58:18.920 --> 58:21.160
 basically kind of writing down all the facts that are known

58:21.160 --> 58:25.240
 and hoping that some sort of common sense will emerge.

58:25.240 --> 58:27.160
 I think it's basically hopeless.

58:27.160 --> 58:28.320
 But let me take an example.

58:28.320 --> 58:31.280
 You take an object, I describe a situation to you.

58:31.280 --> 58:33.560
 I take an object, I put it on the table

58:33.560 --> 58:34.960
 and I push the table.

58:34.960 --> 58:37.240
 It's completely obvious to you that the object

58:37.240 --> 58:39.240
 will be pushed with the table,

58:39.240 --> 58:40.600
 because it's sitting on it.

58:41.840 --> 58:45.040
 There's no text in the world, I believe, that explains this.

58:45.040 --> 58:49.040
 And so if you train a machine as powerful as it could be,

58:49.040 --> 58:53.920
 your GPT 5000 or whatever it is,

58:53.920 --> 58:55.680
 it's never gonna learn about this.

58:57.040 --> 59:01.040
 That information is just not present in any text.

59:01.040 --> 59:03.280
 Well, the question, like with the psych project,

59:03.280 --> 59:08.000
 the dream I think is to have like 10 million,

59:08.000 --> 59:13.000
 say facts like that, that give you a headstart,

59:13.000 --> 59:15.200
 like a parent guiding you.

59:15.200 --> 59:17.280
 Now, we humans don't need a parent to tell us

59:17.280 --> 59:19.240
 that the table will move, sorry,

59:19.240 --> 59:21.440
 the smartphone will move with the table.

59:21.440 --> 59:25.640
 But we get a lot of guidance in other ways.

59:25.640 --> 59:28.160
 So it's possible that we can give it a quick shortcut.

59:28.160 --> 59:29.200
 What about a cat?

59:29.200 --> 59:30.800
 The cat knows that.

59:30.800 --> 59:33.120
 No, but they evolved, so.

59:33.120 --> 59:34.400
 No, they learn like us.

59:35.840 --> 59:37.080
 Sorry, the physics of stuff?

59:37.080 --> 59:38.480
 Yeah.

59:38.480 --> 59:41.360
 Well, yeah, so you're saying it's,

59:41.360 --> 59:45.080
 so you're putting a lot of intelligence

59:45.080 --> 59:47.120
 onto the nurture side, not the nature.

59:47.120 --> 59:47.960
 Yes.

59:47.960 --> 59:50.000
 We seem to have, you know,

59:50.000 --> 59:53.640
 there's a very inefficient arguably process of evolution

59:53.640 --> 59:56.960
 that got us from bacteria to who we are today.

59:57.840 --> 59:59.800
 Started at the bottom, now we're here.

59:59.800 --> 1:00:04.240
 So the question is how, okay,

1:00:04.240 --> 1:00:06.000
 the question is how fundamental is that,

1:00:06.000 --> 1:00:08.400
 the nature of the whole hardware?

1:00:08.400 --> 1:00:11.680
 And then is there any way to shortcut it

1:00:11.680 --> 1:00:12.520
 if it's fundamental?

1:00:12.520 --> 1:00:14.280
 If it's not, if it's most of intelligence,

1:00:14.280 --> 1:00:15.920
 most of the cool stuff we've been talking about

1:00:15.920 --> 1:00:18.800
 is mostly nurture, mostly trained.

1:00:18.800 --> 1:00:20.680
 We figure it out by observing the world.

1:00:20.680 --> 1:00:24.760
 We can form that big, beautiful, sexy background model

1:00:24.760 --> 1:00:27.240
 that you're talking about just by sitting there.

1:00:28.880 --> 1:00:32.600
 Then, okay, then you need to, then like maybe,

1:00:34.800 --> 1:00:37.840
 it is all supervised learning all the way down.

1:00:37.840 --> 1:00:39.000
 Self supervised learning, say.

1:00:39.000 --> 1:00:41.360
 Whatever it is that makes, you know,

1:00:41.360 --> 1:00:44.080
 human intelligence different from other animals,

1:00:44.080 --> 1:00:46.320
 which, you know, a lot of people think is language

1:00:46.320 --> 1:00:48.720
 and logical reasoning and this kind of stuff.

1:00:48.720 --> 1:00:51.000
 It cannot be that complicated because it only popped up

1:00:51.000 --> 1:00:52.840
 in the last million years.

1:00:52.840 --> 1:00:54.320
 Yeah.

1:00:54.320 --> 1:00:57.840
 And, you know, it only involves, you know,

1:00:57.840 --> 1:00:59.640
 less than 1% of our genome might be,

1:00:59.640 --> 1:01:01.200
 which is the difference between human genome

1:01:01.200 --> 1:01:03.360
 and chimps or whatever.

1:01:03.360 --> 1:01:06.640
 So it can't be that complicated.

1:01:06.640 --> 1:01:08.040
 You know, it can't be that fundamental.

1:01:08.040 --> 1:01:10.880
 I mean, most of the complicated stuff

1:01:10.880 --> 1:01:13.640
 already exists in cats and dogs and, you know,

1:01:13.640 --> 1:01:15.840
 certainly primates, nonhuman primates.

1:01:17.120 --> 1:01:18.640
 Yeah, that little thing with humans

1:01:18.640 --> 1:01:22.480
 might be just something about social interaction

1:01:22.480 --> 1:01:24.000
 and ability to maintain ideas

1:01:24.000 --> 1:01:28.160
 across like a collective of people.

1:01:28.160 --> 1:01:30.840
 It sounds very dramatic and very impressive,

1:01:30.840 --> 1:01:33.400
 but it probably isn't mechanistically speaking.

1:01:33.400 --> 1:01:34.680
 It is, but we're not there yet.

1:01:34.680 --> 1:01:39.480
 Like, you know, we have, I mean, this is number 634,

1:01:39.480 --> 1:01:42.080
 you know, in the list of problems we have to solve.

1:01:43.400 --> 1:01:46.880
 So basic physics of the world is number one.

1:01:46.880 --> 1:01:51.600
 What do you, just a quick tangent on data augmentation.

1:01:51.600 --> 1:01:56.600
 So a lot of it is hard coded versus learned.

1:01:57.920 --> 1:02:00.960
 Do you have any intuition that maybe

1:02:00.960 --> 1:02:03.600
 there could be some weird data augmentation,

1:02:03.600 --> 1:02:06.200
 like generative type of data augmentation,

1:02:06.200 --> 1:02:07.680
 like doing something weird to images,

1:02:07.680 --> 1:02:12.680
 which then improves the similarity learning process?

1:02:13.120 --> 1:02:16.280
 So not just kind of dumb, simple distortions,

1:02:16.280 --> 1:02:18.120
 but by you shaking your head,

1:02:18.120 --> 1:02:20.880
 just saying that even simple distortions are enough.

1:02:20.880 --> 1:02:22.800
 I think, no, I think data augmentation

1:02:22.800 --> 1:02:25.080
 is a temporary necessary evil.

1:02:26.480 --> 1:02:28.880
 So what people are working on now is two things.

1:02:28.880 --> 1:02:32.960
 One is the type of self supervised learning,

1:02:32.960 --> 1:02:35.480
 like trying to translate the type of self supervised learning

1:02:35.480 --> 1:02:38.680
 people use in language, translating these two images,

1:02:38.680 --> 1:02:41.800
 which is basically a denoising autoencoder method, right?

1:02:41.800 --> 1:02:46.800
 So you take an image, you block, you mask some parts of it,

1:02:47.320 --> 1:02:49.520
 and then you train some giant neural net

1:02:49.520 --> 1:02:52.640
 to reconstruct the parts that are missing.

1:02:52.640 --> 1:02:56.200
 And until very recently,

1:02:56.200 --> 1:02:59.160
 there was no working methods for that.

1:02:59.160 --> 1:03:01.600
 All the autoencoder type methods for images

1:03:01.600 --> 1:03:03.720
 weren't producing very good representation,

1:03:03.720 --> 1:03:06.600
 but there's a paper now coming out of the fair group

1:03:06.600 --> 1:03:08.960
 at MNL Park that actually works very well.

1:03:08.960 --> 1:03:12.120
 So that doesn't require data augmentation,

1:03:12.120 --> 1:03:15.000
 that requires only masking, okay.

1:03:15.000 --> 1:03:18.640
 Only masking for images, okay.

1:03:18.640 --> 1:03:20.280
 Right, so you mask part of the image

1:03:20.280 --> 1:03:24.560
 and you train a system, which in this case is a transformer

1:03:24.560 --> 1:03:28.400
 because the transformer represents the image

1:03:28.400 --> 1:03:30.880
 as non overlapping patches,

1:03:30.880 --> 1:03:33.320
 so it's easy to mask patches and things like that.

1:03:33.320 --> 1:03:35.680
 Okay, but then my question transfers to that problem,

1:03:35.680 --> 1:03:40.080
 the masking, like why should the mask be square or rectangle?

1:03:40.080 --> 1:03:41.600
 So it doesn't matter, like, you know,

1:03:41.600 --> 1:03:44.360
 I think we're gonna come up probably in the future

1:03:44.360 --> 1:03:49.360
 with sort of ways to mask that are kind of random,

1:03:50.480 --> 1:03:52.920
 essentially, I mean, they are random already, but.

1:03:52.920 --> 1:03:55.880
 No, no, but like something that's challenging,

1:03:56.800 --> 1:03:59.400
 like optimally challenging.

1:03:59.400 --> 1:04:02.440
 So like, I mean, maybe it's a metaphor that doesn't apply,

1:04:02.440 --> 1:04:06.400
 but you're, it seems like there's a data augmentation

1:04:06.400 --> 1:04:09.880
 or masking, there's an interactive element with it.

1:04:09.880 --> 1:04:12.560
 Like you're almost like playing with an image.

1:04:12.560 --> 1:04:14.720
 And like, it's like the way we play with an image

1:04:14.720 --> 1:04:15.680
 in our minds.

1:04:15.680 --> 1:04:16.680
 No, but it's like dropout.

1:04:16.680 --> 1:04:18.160
 It's like Boston machine training.

1:04:18.160 --> 1:04:23.160
 You, you know, every time you see a percept,

1:04:23.200 --> 1:04:26.840
 you also, you can perturb it in some way.

1:04:26.840 --> 1:04:31.520
 And then the principle of the training procedure

1:04:31.520 --> 1:04:33.600
 is to minimize the difference of the output

1:04:33.600 --> 1:04:36.920
 or the representation between the clean version

1:04:36.920 --> 1:04:40.280
 and the corrupted version, essentially, right?

1:04:40.280 --> 1:04:42.000
 And you can do this in real time, right?

1:04:42.000 --> 1:04:44.240
 So, you know, Boston machine work like this, right?

1:04:44.240 --> 1:04:47.400
 You show a percept, you tell the machine

1:04:47.400 --> 1:04:49.840
 that's a good combination of activities

1:04:49.840 --> 1:04:50.880
 or your input neurons.

1:04:50.880 --> 1:04:55.880
 And then you either let them go their merry way

1:04:56.560 --> 1:04:58.960
 without clamping them to values,

1:04:58.960 --> 1:05:01.120
 or you only do this with a subset.

1:05:01.120 --> 1:05:03.520
 And what you're doing is you're training the system

1:05:03.520 --> 1:05:07.000
 so that the stable state of the entire network

1:05:07.000 --> 1:05:08.920
 is the same regardless of whether it sees

1:05:08.920 --> 1:05:11.520
 the entire input or whether it sees only part of it.

1:05:12.880 --> 1:05:14.360
 You know, denoising autoencoder method

1:05:14.360 --> 1:05:15.880
 is basically the same thing, right?

1:05:15.880 --> 1:05:18.600
 You're training a system to reproduce the input,

1:05:18.600 --> 1:05:20.480
 the complete inputs and filling the input

1:05:20.480 --> 1:05:23.400
 and filling the blanks, regardless of which parts

1:05:23.400 --> 1:05:26.280
 are missing, and that's really the underlying principle.

1:05:26.280 --> 1:05:28.320
 And you could imagine sort of, even in the brain,

1:05:28.320 --> 1:05:30.720
 some sort of neural principle where, you know,

1:05:30.720 --> 1:05:32.800
 neurons kind of oscillate, right?

1:05:32.800 --> 1:05:35.520
 So they take their activity and then temporarily

1:05:35.520 --> 1:05:38.040
 they kind of shut off to, you know,

1:05:38.040 --> 1:05:42.120
 force the rest of the system to basically reconstruct

1:05:42.120 --> 1:05:44.800
 the input without their help, you know?

1:05:44.800 --> 1:05:49.040
 And, I mean, you could imagine, you know,

1:05:49.040 --> 1:05:51.040
 more or less biologically possible processes.

1:05:51.040 --> 1:05:51.880
 Something like that.

1:05:51.880 --> 1:05:54.960
 And I guess with this denoising autoencoder

1:05:54.960 --> 1:05:58.720
 and masking and data augmentation,

1:05:58.720 --> 1:06:01.160
 you don't have to worry about being super efficient.

1:06:01.160 --> 1:06:03.960
 You could just do as much as you want

1:06:03.960 --> 1:06:06.160
 and get better over time.

1:06:06.160 --> 1:06:08.800
 Because I was thinking, like, you might want to be clever

1:06:08.800 --> 1:06:12.000
 about the way you do all these procedures, you know,

1:06:12.000 --> 1:06:16.720
 but that's only, it's somehow costly to do every iteration,

1:06:16.720 --> 1:06:17.960
 but it's not really.

1:06:17.960 --> 1:06:19.280
 Not really.

1:06:19.280 --> 1:06:20.280
 Maybe.

1:06:20.280 --> 1:06:21.480
 And then there is, you know,

1:06:21.480 --> 1:06:24.160
 data augmentation without explicit data augmentation.

1:06:24.160 --> 1:06:25.600
 This data augmentation by weighting,

1:06:25.600 --> 1:06:28.080
 which is, you know, the sort of video prediction.

1:06:29.320 --> 1:06:31.480
 You're observing a video clip,

1:06:31.480 --> 1:06:36.400
 observing the, you know, the continuation of that video clip.

1:06:36.400 --> 1:06:38.040
 You try to learn a representation

1:06:38.040 --> 1:06:40.240
 using dual joint embedding architectures

1:06:40.240 --> 1:06:43.280
 in such a way that the representation of the future clip

1:06:43.280 --> 1:06:45.680
 is easily predictable from the representation

1:06:45.680 --> 1:06:48.600
 of the observed clip.

1:06:48.600 --> 1:06:51.840
 Do you think YouTube has enough raw data

1:06:52.720 --> 1:06:56.400
 from which to learn how to be a cat?

1:06:56.400 --> 1:06:57.760
 I think so.

1:06:57.760 --> 1:07:01.200
 So the amount of data is not the constraint.

1:07:01.200 --> 1:07:04.120
 No, it would require some selection, I think.

1:07:04.120 --> 1:07:05.400
 Some selection?

1:07:05.400 --> 1:07:08.480
 Some selection of, you know, maybe the right type of data.

1:07:08.480 --> 1:07:09.320
 You need some.

1:07:09.320 --> 1:07:11.400
 Don't go down the rabbit hole of just cat videos.

1:07:11.400 --> 1:07:14.600
 You might need to watch some lectures or something.

1:07:14.600 --> 1:07:15.720
 No, you wouldn't.

1:07:15.720 --> 1:07:17.480
 How meta would that be

1:07:17.480 --> 1:07:21.400
 if it like watches lectures about intelligence

1:07:21.400 --> 1:07:22.240
 and then learns,

1:07:22.240 --> 1:07:24.320
 watches your lectures in NYU

1:07:24.320 --> 1:07:26.280
 and learns from that how to be intelligent?

1:07:26.280 --> 1:07:27.800
 I don't think that would be enough.

1:07:30.080 --> 1:07:33.240
 What's your, do you find multimodal learning interesting?

1:07:33.240 --> 1:07:35.080
 We've been talking about visual language,

1:07:35.080 --> 1:07:36.440
 like combining those together,

1:07:36.440 --> 1:07:38.120
 maybe audio, all those kinds of things.

1:07:38.120 --> 1:07:40.400
 There's a lot of things that I find interesting

1:07:40.400 --> 1:07:41.240
 in the short term,

1:07:41.240 --> 1:07:44.080
 but are not addressing the important problem

1:07:44.080 --> 1:07:46.600
 that I think are really kind of the big challenges.

1:07:46.600 --> 1:07:48.920
 So I think, you know, things like multitask learning,

1:07:48.920 --> 1:07:53.920
 continual learning, you know, adversarial issues.

1:07:54.360 --> 1:07:57.000
 I mean, those have great practical interests

1:07:57.000 --> 1:08:00.280
 in the relatively short term, possibly,

1:08:00.280 --> 1:08:01.240
 but I don't think they're fundamental.

1:08:01.240 --> 1:08:02.600
 You know, active learning,

1:08:02.600 --> 1:08:04.360
 even to some extent, reinforcement learning.

1:08:04.360 --> 1:08:07.920
 I think those things will become either obsolete

1:08:07.920 --> 1:08:10.800
 or useless or easy

1:08:10.800 --> 1:08:14.880
 once we figured out how to do self supervised

1:08:14.880 --> 1:08:15.880
 representation learning

1:08:15.880 --> 1:08:19.280
 or learning predictive world models.

1:08:19.280 --> 1:08:21.480
 And so I think that's what, you know,

1:08:21.480 --> 1:08:24.400
 the entire community should be focusing on.

1:08:24.400 --> 1:08:25.400
 At least people who are interested

1:08:25.400 --> 1:08:26.680
 in sort of fundamental questions

1:08:26.680 --> 1:08:28.440
 or, you know, really kind of pushing the envelope

1:08:28.440 --> 1:08:31.440
 of AI towards the next stage.

1:08:31.440 --> 1:08:33.080
 But of course, there's like a huge amount of,

1:08:33.080 --> 1:08:34.360
 you know, very interesting work to do

1:08:34.360 --> 1:08:35.840
 in sort of practical questions

1:08:35.840 --> 1:08:38.000
 that have, you know, short term impact.

1:08:38.000 --> 1:08:41.240
 Well, you know, it's difficult to talk about

1:08:41.240 --> 1:08:42.200
 the temporal scale,

1:08:42.200 --> 1:08:44.240
 because all of human civilization

1:08:44.240 --> 1:08:45.400
 will eventually be destroyed

1:08:45.400 --> 1:08:48.520
 because the sun will die out.

1:08:48.520 --> 1:08:50.280
 And even if Elon Musk is successful

1:08:50.280 --> 1:08:54.520
 in multi planetary colonization across the galaxy,

1:08:54.520 --> 1:08:56.560
 eventually the entirety of it

1:08:56.560 --> 1:08:58.920
 will just become giant black holes.

1:08:58.920 --> 1:09:02.120
 And that's gonna take a while though.

1:09:02.120 --> 1:09:04.800
 So, but what I'm saying is then that logic

1:09:04.800 --> 1:09:07.360
 can be used to say it's all meaningless.

1:09:07.360 --> 1:09:10.920
 I'm saying all that to say that multitask learning

1:09:11.840 --> 1:09:15.400
 might be, you're calling it practical

1:09:15.400 --> 1:09:17.280
 or pragmatic or whatever.

1:09:17.280 --> 1:09:19.440
 That might be the thing that achieves something

1:09:19.440 --> 1:09:21.120
 very akin to intelligence

1:09:22.560 --> 1:09:26.880
 while we're trying to solve the more general problem

1:09:26.880 --> 1:09:29.400
 of self supervised learning of background knowledge.

1:09:29.400 --> 1:09:30.640
 So the reason I bring that up,

1:09:30.640 --> 1:09:33.040
 maybe one way to ask that question.

1:09:33.040 --> 1:09:34.000
 I've been very impressed

1:09:34.000 --> 1:09:36.440
 by what Tesla Autopilot team is doing.

1:09:36.440 --> 1:09:38.320
 I don't know if you've gotten a chance to glance

1:09:38.320 --> 1:09:42.080
 at this particular one example of multitask learning,

1:09:42.080 --> 1:09:44.960
 where they're literally taking the problem,

1:09:44.960 --> 1:09:48.880
 like, I don't know, Charles Darwin studying animals.

1:09:48.880 --> 1:09:51.600
 They're studying the problem of driving

1:09:51.600 --> 1:09:53.320
 and asking, okay, what are all the things

1:09:53.320 --> 1:09:55.000
 you have to perceive?

1:09:55.000 --> 1:09:57.800
 And the way they're solving it is one,

1:09:57.800 --> 1:10:00.400
 there's an ontology where you're bringing that to the table.

1:10:00.400 --> 1:10:02.240
 So you're formulating a bunch of different tasks.

1:10:02.240 --> 1:10:04.240
 It's like over a hundred tasks or something like that

1:10:04.240 --> 1:10:06.040
 that they're involved in driving.

1:10:06.040 --> 1:10:07.720
 And then they're deploying it

1:10:07.720 --> 1:10:10.480
 and then getting data back from people that run into trouble

1:10:10.480 --> 1:10:12.680
 and they're trying to figure out, do we add tasks?

1:10:12.680 --> 1:10:16.040
 Do we, like, we focus on each individual task separately?

1:10:16.040 --> 1:10:18.280
 In fact, I would say,

1:10:18.280 --> 1:10:20.680
 I would classify Andre Karpathy's talk in two ways.

1:10:20.680 --> 1:10:22.360
 So one was about doors

1:10:22.360 --> 1:10:24.720
 and the other one about how much ImageNet sucks.

1:10:24.720 --> 1:10:28.560
 He kept going back and forth on those two topics,

1:10:28.560 --> 1:10:30.000
 which ImageNet sucks,

1:10:30.000 --> 1:10:33.040
 meaning you can't just use a single benchmark.

1:10:33.040 --> 1:10:37.240
 There's so, like, you have to have like a giant suite

1:10:37.240 --> 1:10:39.880
 of benchmarks to understand how well your system actually works.

1:10:39.880 --> 1:10:40.720
 Oh, I agree with him.

1:10:40.720 --> 1:10:43.880
 I mean, he's a very sensible guy.

1:10:43.880 --> 1:10:47.560
 Now, okay, it's very clear that if you're faced

1:10:47.560 --> 1:10:50.480
 with an engineering problem that you need to solve

1:10:50.480 --> 1:10:51.920
 in a relatively short time,

1:10:51.920 --> 1:10:55.880
 particularly if you have Elon Musk breathing down your neck,

1:10:55.880 --> 1:10:58.640
 you're going to have to take shortcuts, right?

1:10:58.640 --> 1:11:02.560
 You might think about the fact that the right thing to do

1:11:02.560 --> 1:11:04.520
 and the longterm solution involves, you know,

1:11:04.520 --> 1:11:06.560
 some fancy self supervised running,

1:11:06.560 --> 1:11:10.240
 but you have, you know, Elon Musk breathing down your neck

1:11:10.240 --> 1:11:13.600
 and, you know, this involves, you know, human lives.

1:11:13.600 --> 1:11:17.320
 And so you have to basically just do

1:11:17.320 --> 1:11:20.560
 the systematic engineering and, you know,

1:11:22.000 --> 1:11:23.280
 fine tuning and refinements

1:11:23.280 --> 1:11:26.360
 and trial and error and all that stuff.

1:11:26.360 --> 1:11:27.400
 There's nothing wrong with that.

1:11:27.400 --> 1:11:28.600
 That's called engineering.

1:11:28.600 --> 1:11:33.600
 That's called, you know, putting technology out in the world.

1:11:35.840 --> 1:11:39.880
 And you have to kind of ironclad it before you do this,

1:11:39.880 --> 1:11:44.520
 you know, so much for, you know,

1:11:44.520 --> 1:11:46.240
 grand ideas and principles.

1:11:48.280 --> 1:11:50.720
 But, you know, I'm placing myself sort of, you know,

1:11:50.720 --> 1:11:54.480
 some, you know, upstream of this, you know,

1:11:54.480 --> 1:11:55.760
 quite a bit upstream of this.

1:11:55.760 --> 1:11:58.240
 You're a Plato, think about platonic forms.

1:11:58.240 --> 1:12:01.320
 You're not platonic because eventually

1:12:01.320 --> 1:12:03.120
 I want that stuff to get used,

1:12:03.120 --> 1:12:06.920
 but it's okay if it takes five or 10 years

1:12:06.920 --> 1:12:09.720
 for the community to realize this is the right thing to do.

1:12:09.720 --> 1:12:11.280
 I've done this before.

1:12:11.280 --> 1:12:13.240
 It's been the case before that, you know,

1:12:13.240 --> 1:12:14.440
 I've made that case.

1:12:14.440 --> 1:12:17.760
 I mean, if you look back in the mid 2000, for example,

1:12:17.760 --> 1:12:19.320
 and you ask yourself the question, okay,

1:12:19.320 --> 1:12:22.080
 I want to recognize cars or faces or whatever,

1:12:24.360 --> 1:12:25.560
 you know, I can use convolutional net.

1:12:25.560 --> 1:12:28.360
 So I can use sort of more conventional

1:12:28.360 --> 1:12:29.880
 kind of computer vision techniques, you know,

1:12:29.880 --> 1:12:33.760
 using interest point detectors or assist density features

1:12:33.760 --> 1:12:35.760
 and, you know, sticking an SVM on top.

1:12:35.760 --> 1:12:37.800
 At that time, the datasets were so small

1:12:37.800 --> 1:12:41.920
 that those methods that use more hand engineering

1:12:41.920 --> 1:12:43.560
 worked better than ConvNets.

1:12:43.560 --> 1:12:45.560
 It was just not enough data for ConvNets

1:12:45.560 --> 1:12:48.880
 and ConvNets were a little slow with the kind of hardware

1:12:48.880 --> 1:12:50.840
 that was available at the time.

1:12:50.840 --> 1:12:53.880
 And there was a sea change when, basically,

1:12:53.880 --> 1:12:56.680
 when, you know, datasets became bigger

1:12:56.680 --> 1:12:58.600
 and GPUs became available.

1:12:58.600 --> 1:13:02.960
 That's what, you know, two of the main factors

1:13:02.960 --> 1:13:05.960
 that basically made people change their mind.

1:13:07.880 --> 1:13:11.880
 And you can look at the history of,

1:13:11.880 --> 1:13:15.560
 like, all sub branches of AI or pattern recognition.

1:13:16.400 --> 1:13:19.800
 And there's a similar trajectory followed by techniques

1:13:19.800 --> 1:13:23.960
 where people start by, you know, engineering the hell out of it.

1:13:25.200 --> 1:13:29.200
 You know, be it optical character recognition,

1:13:29.200 --> 1:13:31.760
 speech recognition, computer vision,

1:13:31.760 --> 1:13:34.280
 like image recognition in general,

1:13:34.280 --> 1:13:37.280
 natural language understanding, like, you know, translation,

1:13:37.280 --> 1:13:38.000
 things like that, right?

1:13:38.000 --> 1:13:41.040
 You start to engineer the hell out of it.

1:13:41.040 --> 1:13:42.680
 You start to acquire all the knowledge,

1:13:42.680 --> 1:13:44.760
 the prior knowledge you know about image formation,

1:13:44.760 --> 1:13:46.600
 about, you know, the shape of characters,

1:13:46.600 --> 1:13:49.560
 about, you know, morphological operations,

1:13:49.560 --> 1:13:52.400
 about, like, feature extraction, Fourier transforms,

1:13:52.400 --> 1:13:54.440
 you know, vernicke moments, you know, whatever, right?

1:13:54.440 --> 1:13:56.280
 People have come up with thousands of ways

1:13:56.280 --> 1:13:57.680
 of representing images

1:13:57.680 --> 1:14:01.600
 so that they could be easily classified afterwards.

1:14:01.600 --> 1:14:03.000
 Same for speech recognition, right?

1:14:03.000 --> 1:14:04.640
 There is, you know, it took decades

1:14:04.640 --> 1:14:06.920
 for people to figure out a good front end

1:14:06.920 --> 1:14:09.680
 to preprocess speech signals

1:14:09.680 --> 1:14:11.120
 so that, you know, all the information

1:14:11.120 --> 1:14:13.400
 about what is being said is preserved,

1:14:13.400 --> 1:14:14.440
 but most of the information

1:14:14.440 --> 1:14:16.920
 about the identity of the speaker is gone.

1:14:16.920 --> 1:14:20.880
 You know, kestrel coefficients or whatever, right?

1:14:20.880 --> 1:14:23.400
 And same for text, right?

1:14:23.400 --> 1:14:26.440
 You do named entity recognition and you parse

1:14:26.440 --> 1:14:31.440
 and you do tagging of the parts of speech

1:14:31.800 --> 1:14:34.480
 and, you know, you do this sort of tree representation

1:14:34.480 --> 1:14:36.480
 of clauses and all that stuff, right?

1:14:36.480 --> 1:14:38.120
 Before you can do anything.

1:14:40.720 --> 1:14:43.520
 So that's how it starts, right?

1:14:43.520 --> 1:14:45.160
 Just engineer the hell out of it.

1:14:45.160 --> 1:14:47.920
 And then you start having data

1:14:47.920 --> 1:14:50.160
 and maybe you have more powerful computers.

1:14:50.160 --> 1:14:52.400
 Maybe you know something about statistical learning.

1:14:52.400 --> 1:14:53.640
 So you start using machine learning

1:14:53.640 --> 1:14:54.840
 and it's usually a small sliver

1:14:54.840 --> 1:14:56.800
 on top of your kind of handcrafted system

1:14:56.800 --> 1:14:59.560
 where, you know, you extract features by hand.

1:14:59.560 --> 1:15:02.280
 Okay, and now, you know, nowadays the standard way

1:15:02.280 --> 1:15:04.320
 of doing this is that you train the entire thing end to end

1:15:04.320 --> 1:15:06.720
 with a deep learning system and it learns its own features

1:15:06.720 --> 1:15:10.920
 and, you know, speech recognition systems nowadays

1:15:10.920 --> 1:15:12.920
 or CR systems are completely end to end.

1:15:12.920 --> 1:15:15.320
 It's, you know, it's some giant neural net

1:15:15.320 --> 1:15:17.920
 that takes raw waveforms

1:15:17.920 --> 1:15:20.440
 and produces a sequence of characters coming out.

1:15:20.440 --> 1:15:22.080
 And it's just a huge neural net, right?

1:15:22.080 --> 1:15:24.000
 There's no, you know, Markov model,

1:15:24.000 --> 1:15:26.360
 there's no language model that is explicit

1:15:26.360 --> 1:15:28.600
 other than, you know, something that's ingrained

1:15:28.600 --> 1:15:30.960
 in the sort of neural language model, if you want.

1:15:30.960 --> 1:15:33.400
 Same for translation, same for all kinds of stuff.

1:15:33.400 --> 1:15:36.440
 So you see this continuous evolution

1:15:36.440 --> 1:15:40.440
 from, you know, less and less hand crafting

1:15:40.440 --> 1:15:43.120
 and more and more learning.

1:15:43.120 --> 1:15:48.120
 And I think, I mean, it's true in biology as well.

1:15:50.680 --> 1:15:52.880
 So, I mean, we might disagree about this,

1:15:52.880 --> 1:15:56.860
 maybe not, this one little piece at the end,

1:15:56.860 --> 1:15:58.360
 you mentioned active learning.

1:15:58.360 --> 1:16:01.440
 It feels like active learning,

1:16:01.440 --> 1:16:02.880
 which is the selection of data

1:16:02.880 --> 1:16:05.600
 and also the interactivity needs to be part

1:16:05.600 --> 1:16:06.800
 of this giant neural network.

1:16:06.800 --> 1:16:08.360
 You cannot just be an observer

1:16:08.360 --> 1:16:09.720
 to do self supervised learning.

1:16:09.720 --> 1:16:12.200
 You have to, well, I don't,

1:16:12.200 --> 1:16:14.560
 self supervised learning is just a word,

1:16:14.560 --> 1:16:16.760
 but I would, whatever this giant stack

1:16:16.760 --> 1:16:19.640
 of a neural network that's automatically learning,

1:16:19.640 --> 1:16:24.640
 it feels, my intuition is that you have to have a system,

1:16:26.520 --> 1:16:30.220
 whether it's a physical robot or a digital robot,

1:16:30.220 --> 1:16:32.360
 that's interacting with the world

1:16:32.360 --> 1:16:35.960
 and doing so in a flawed way and improving over time

1:16:35.960 --> 1:16:40.960
 in order to form the self supervised learning.

1:16:41.520 --> 1:16:44.960
 Well, you can't just give it a giant sea of data.

1:16:44.960 --> 1:16:47.120
 Okay, I agree and I disagree.

1:16:47.120 --> 1:16:52.000
 I agree in the sense that I think, I agree in two ways.

1:16:52.000 --> 1:16:54.240
 The first way I agree is that if you want,

1:16:55.140 --> 1:16:57.480
 and you certainly need a causal model of the world

1:16:57.480 --> 1:16:59.120
 that allows you to predict the consequences

1:16:59.120 --> 1:17:01.280
 of your actions, to train that model,

1:17:01.280 --> 1:17:02.760
 you need to take actions, right?

1:17:02.760 --> 1:17:04.600
 You need to be able to act in a world

1:17:04.600 --> 1:17:07.040
 and see the effect for you to be,

1:17:07.040 --> 1:17:08.560
 to learn causal models of the world.

1:17:08.560 --> 1:17:11.560
 So that's not obvious because you can observe others.

1:17:11.560 --> 1:17:12.400
 You can observe others.

1:17:12.400 --> 1:17:14.720
 And you can infer that they're similar to you

1:17:14.720 --> 1:17:16.000
 and then you can learn from that.

1:17:16.000 --> 1:17:18.400
 Yeah, but then you have to kind of hardwire that part,

1:17:18.400 --> 1:17:19.880
 right, and then, you know, mirror neurons

1:17:19.880 --> 1:17:20.720
 and all that stuff, right?

1:17:20.720 --> 1:17:23.280
 So, and it's not clear to me

1:17:23.280 --> 1:17:24.440
 how you would do this in a machine.

1:17:24.440 --> 1:17:29.440
 So I think the action part would be necessary

1:17:30.240 --> 1:17:32.620
 for having causal models of the world.

1:17:32.620 --> 1:17:36.660
 The second reason it may be necessary,

1:17:36.660 --> 1:17:37.860
 or at least more efficient,

1:17:37.860 --> 1:17:41.700
 is that active learning basically, you know,

1:17:41.700 --> 1:17:44.900
 goes for the jugular of what you don't know, right?

1:17:44.900 --> 1:17:48.020
 Is, you know, obvious areas of uncertainty

1:17:48.020 --> 1:17:52.940
 about your world and about how the world behaves.

1:17:52.940 --> 1:17:56.220
 And you can resolve this uncertainty

1:17:56.220 --> 1:17:58.980
 by systematic exploration of that part

1:17:58.980 --> 1:18:00.300
 that you don't know.

1:18:00.300 --> 1:18:01.700
 And if you know that you don't know,

1:18:01.700 --> 1:18:03.020
 then, you know, it makes you curious.

1:18:03.020 --> 1:18:05.620
 You kind of look into situations that,

1:18:05.620 --> 1:18:09.260
 and, you know, across the animal world,

1:18:09.260 --> 1:18:12.900
 different species have different levels of curiosity,

1:18:12.900 --> 1:18:15.100
 right, depending on how they're built, right?

1:18:15.100 --> 1:18:18.780
 So, you know, cats and rats are incredibly curious,

1:18:18.780 --> 1:18:20.620
 dogs not so much, I mean, less.

1:18:20.620 --> 1:18:22.140
 Yeah, so it could be useful

1:18:22.140 --> 1:18:23.900
 to have that kind of curiosity.

1:18:23.900 --> 1:18:24.740
 So it'd be useful,

1:18:24.740 --> 1:18:26.980
 but curiosity just makes the process faster.

1:18:26.980 --> 1:18:28.780
 It doesn't make the process exist.

1:18:28.780 --> 1:18:33.780
 The, so what process, what learning process is it

1:18:33.820 --> 1:18:37.780
 that active learning makes more efficient?

1:18:37.780 --> 1:18:40.300
 And I'm asking that first question, you know,

1:18:42.300 --> 1:18:43.940
 you know, we haven't answered that question yet.

1:18:43.940 --> 1:18:45.860
 So, you know, I worry about active learning

1:18:45.860 --> 1:18:47.300
 once this question is...

1:18:47.300 --> 1:18:49.940
 So it's the more fundamental question to ask.

1:18:49.940 --> 1:18:53.900
 And if active learning or interaction

1:18:53.900 --> 1:18:56.220
 increases the efficiency of the learning,

1:18:56.220 --> 1:18:59.700
 see, sometimes it becomes very different

1:18:59.700 --> 1:19:03.700
 if the increase is several orders of magnitude, right?

1:19:03.700 --> 1:19:04.540
 Like...

1:19:04.540 --> 1:19:05.380
 That's true.

1:19:05.380 --> 1:19:07.620
 But fundamentally it's still the same thing

1:19:07.620 --> 1:19:10.700
 and building up the intuition about how to,

1:19:10.700 --> 1:19:13.340
 in a self supervised way to construct background models,

1:19:13.340 --> 1:19:18.180
 efficient or inefficient, is the core problem.

1:19:18.180 --> 1:19:20.300
 What do you think about Yoshua Bengio's

1:19:20.300 --> 1:19:22.380
 talking about consciousness

1:19:22.380 --> 1:19:24.060
 and all of these kinds of concepts?

1:19:24.060 --> 1:19:29.060
 Okay, I don't know what consciousness is, but...

1:19:29.780 --> 1:19:31.500
 It's a good opener.

1:19:31.500 --> 1:19:33.100
 And to some extent, a lot of the things

1:19:33.100 --> 1:19:34.860
 that are said about consciousness

1:19:34.860 --> 1:19:38.260
 remind me of the questions people were asking themselves

1:19:38.260 --> 1:19:40.900
 in the 18th century or 17th century

1:19:40.900 --> 1:19:44.620
 when they discovered that, you know, how the eye works

1:19:44.620 --> 1:19:46.620
 and the fact that the image at the back of the eye

1:19:46.620 --> 1:19:49.420
 was upside down, right?

1:19:49.420 --> 1:19:50.260
 Because you have a lens.

1:19:50.260 --> 1:19:54.140
 And so on your retina, the image that forms is an image

1:19:54.140 --> 1:19:55.180
 of the world, but it's upside down.

1:19:55.180 --> 1:19:57.820
 How is it that you see right side up?

1:19:57.820 --> 1:20:00.100
 And, you know, with what we know today in science,

1:20:00.100 --> 1:20:03.500
 you know, we realize this question doesn't make any sense

1:20:03.500 --> 1:20:05.980
 or is kind of ridiculous in some way, right?

1:20:05.980 --> 1:20:07.820
 So I think a lot of what is said about consciousness

1:20:07.820 --> 1:20:08.660
 is of that nature.

1:20:08.660 --> 1:20:10.620
 Now, that said, there is a lot of really smart people

1:20:10.620 --> 1:20:13.460
 that for whom I have a lot of respect

1:20:13.460 --> 1:20:14.700
 who are talking about this topic,

1:20:14.700 --> 1:20:17.900
 people like David Chalmers, who is a colleague of mine at NYU.

1:20:17.900 --> 1:20:22.900
 I have kind of an orthodox folk speculative hypothesis

1:20:28.140 --> 1:20:29.180
 about consciousness.

1:20:29.180 --> 1:20:32.020
 So we're talking about the study of a world model.

1:20:32.020 --> 1:20:35.540
 And I think, you know, our entire prefrontal cortex

1:20:35.540 --> 1:20:39.300
 basically is the engine for a world model.

1:20:40.820 --> 1:20:44.580
 But when we are attending at a particular situation,

1:20:44.580 --> 1:20:46.060
 we're focused on that situation.

1:20:46.060 --> 1:20:48.540
 We basically cannot attend to anything else.

1:20:48.540 --> 1:20:53.540
 And that seems to suggest that we basically have

1:20:53.540 --> 1:20:58.420
 only one world model engine in our prefrontal cortex.

1:20:59.780 --> 1:21:02.620
 That engine is configurable to the situation at hand.

1:21:02.620 --> 1:21:04.660
 So we are building a box out of wood,

1:21:04.660 --> 1:21:09.300
 or we are driving down the highway playing chess.

1:21:09.300 --> 1:21:12.820
 We basically have a single model of the world

1:21:12.820 --> 1:21:15.380
 that we configure into the situation at hand,

1:21:15.380 --> 1:21:18.140
 which is why we can only attend to one task at a time.

1:21:19.220 --> 1:21:21.660
 Now, if there is a task that we do repeatedly,

1:21:22.860 --> 1:21:25.940
 it goes from the sort of deliberate reasoning

1:21:25.940 --> 1:21:27.420
 using model of the world and prediction

1:21:27.420 --> 1:21:29.300
 and perhaps something like model predictive control,

1:21:29.300 --> 1:21:31.380
 which I was talking about earlier,

1:21:31.380 --> 1:21:33.340
 to something that is more subconscious

1:21:33.340 --> 1:21:34.380
 that becomes automatic.

1:21:34.380 --> 1:21:35.940
 So I don't know if you've ever played

1:21:35.940 --> 1:21:37.940
 against a chess grandmaster.

1:21:39.180 --> 1:21:43.820
 I get wiped out in 10 plays, right?

1:21:43.820 --> 1:21:48.700
 And I have to think about my move for like 15 minutes.

1:21:50.140 --> 1:21:52.620
 And the person in front of me, the grandmaster,

1:21:52.620 --> 1:21:55.220
 would just react within seconds, right?

1:21:56.540 --> 1:21:58.580
 He doesn't need to think about it.

1:21:58.580 --> 1:21:59.980
 That's become part of the subconscious

1:21:59.980 --> 1:22:02.620
 because it's basically just pattern recognition

1:22:02.620 --> 1:22:03.460
 at this point.

1:22:04.740 --> 1:22:07.660
 Same, the first few hours you drive a car,

1:22:07.660 --> 1:22:09.660
 you are really attentive, you can't do anything else.

1:22:09.660 --> 1:22:13.460
 And then after 20, 30 hours of practice, 50 hours,

1:22:13.460 --> 1:22:15.700
 the subconscious, you can talk to the person next to you,

1:22:15.700 --> 1:22:17.100
 things like that, right?

1:22:17.100 --> 1:22:19.060
 Unless the situation becomes unpredictable

1:22:19.060 --> 1:22:21.060
 and then you have to stop talking.

1:22:21.060 --> 1:22:23.820
 So that suggests you only have one model in your head.

1:22:24.740 --> 1:22:27.860
 And it might suggest the idea that consciousness

1:22:27.860 --> 1:22:29.780
 basically is the module that configures

1:22:29.780 --> 1:22:31.980
 this world model of yours.

1:22:31.980 --> 1:22:36.540
 You need to have some sort of executive kind of overseer

1:22:36.540 --> 1:22:40.620
 that configures your world model for the situation at hand.

1:22:40.620 --> 1:22:43.780
 And that leads to kind of the really curious concept

1:22:43.780 --> 1:22:46.020
 that consciousness is not a consequence

1:22:46.020 --> 1:22:47.660
 of the power of our minds,

1:22:47.660 --> 1:22:49.940
 but of the limitation of our brains.

1:22:49.940 --> 1:22:52.060
 That because we have only one world model,

1:22:52.060 --> 1:22:53.660
 we have to be conscious.

1:22:53.660 --> 1:22:55.220
 If we had as many world models

1:22:55.220 --> 1:22:58.540
 as situations we encounter,

1:22:58.540 --> 1:23:00.740
 then we could do all of them simultaneously

1:23:00.740 --> 1:23:02.940
 and we wouldn't need this sort of executive control

1:23:02.940 --> 1:23:04.540
 that we call consciousness.

1:23:04.540 --> 1:23:05.380
 Yeah, interesting.

1:23:05.380 --> 1:23:08.940
 And somehow maybe that executive controller,

1:23:08.940 --> 1:23:10.980
 I mean, the hard problem of consciousness,

1:23:10.980 --> 1:23:12.860
 there's some kind of chemicals in biology

1:23:12.860 --> 1:23:15.020
 that's creating a feeling,

1:23:15.020 --> 1:23:17.740
 like it feels to experience some of these things.

1:23:18.780 --> 1:23:22.460
 That's kind of like the hard question is,

1:23:22.460 --> 1:23:24.900
 what the heck is that and why is that useful?

1:23:24.900 --> 1:23:26.180
 Maybe the more pragmatic question,

1:23:26.180 --> 1:23:29.940
 why is it useful to feel like this is really you

1:23:29.940 --> 1:23:33.340
 experiencing this versus just like information

1:23:33.340 --> 1:23:34.380
 being processed?

1:23:34.380 --> 1:23:39.020
 It could be just a very nice side effect

1:23:39.020 --> 1:23:41.820
 of the way we evolved.

1:23:41.820 --> 1:23:46.820
 That's just very useful to feel a sense of ownership

1:23:48.620 --> 1:23:51.180
 to the decisions you make, to the perceptions you make,

1:23:51.180 --> 1:23:53.180
 to the model you're trying to maintain.

1:23:53.180 --> 1:23:56.260
 Like you own this thing and this is the only one you got

1:23:56.260 --> 1:23:58.420
 and if you lose it, it's gonna really suck.

1:23:58.420 --> 1:24:00.620
 And so you should really send the brain

1:24:00.620 --> 1:24:02.300
 some signals about it.

1:24:02.300 --> 1:24:06.860
 So what ideas do you believe might be true

1:24:06.860 --> 1:24:10.100
 that most or at least many people disagree with?

1:24:11.260 --> 1:24:13.740
 Let's say in the space of machine learning.

1:24:13.740 --> 1:24:14.940
 Well, it depends who you talk about,

1:24:14.940 --> 1:24:19.940
 but I think, so certainly there is a bunch of people

1:24:20.100 --> 1:24:21.100
 who are nativists, right?

1:24:21.100 --> 1:24:23.300
 Who think that a lot of the basic things about the world

1:24:23.300 --> 1:24:25.300
 are kind of hardwired in our minds.

1:24:26.420 --> 1:24:28.860
 Things like the world is three dimensional, for example,

1:24:28.860 --> 1:24:30.420
 is that hardwired?

1:24:30.420 --> 1:24:32.660
 Things like object permanence,

1:24:32.660 --> 1:24:35.140
 is this something that we learn

1:24:35.140 --> 1:24:37.500
 before the age of three months or so?

1:24:37.500 --> 1:24:39.340
 Or are we born with it?

1:24:39.340 --> 1:24:42.380
 And there are very wide disagreements

1:24:42.380 --> 1:24:46.580
 among the cognitive scientists for this.

1:24:46.580 --> 1:24:48.980
 I think those things are actually very simple to learn.

1:24:50.580 --> 1:24:54.220
 Is it the case that the oriented edge detectors in V1

1:24:54.220 --> 1:24:56.180
 are learned or are they hardwired?

1:24:56.180 --> 1:24:57.260
 I think they are learned.

1:24:57.260 --> 1:24:58.580
 They might be learned before both

1:24:58.580 --> 1:25:00.620
 because it's really easy to generate signals

1:25:00.620 --> 1:25:03.220
 from the retina that actually will train edge detectors.

1:25:04.620 --> 1:25:06.740
 And again, those are things that can be learned

1:25:06.740 --> 1:25:09.580
 within minutes of opening your eyes, right?

1:25:09.580 --> 1:25:12.660
 I mean, since the 1990s,

1:25:12.660 --> 1:25:15.460
 we have algorithms that can learn oriented edge detectors

1:25:15.460 --> 1:25:16.940
 completely unsupervised

1:25:16.940 --> 1:25:19.060
 with the equivalent of a few minutes of real time.

1:25:19.060 --> 1:25:21.540
 So those things have to be learned.

1:25:22.660 --> 1:25:24.580
 And there's also those MIT experiments

1:25:24.580 --> 1:25:27.820
 where you kind of plug the optical nerve

1:25:27.820 --> 1:25:30.300
 on the auditory cortex of a baby ferret, right?

1:25:30.300 --> 1:25:31.300
 And that auditory cortex

1:25:31.300 --> 1:25:33.420
 becomes a visual cortex essentially.

1:25:33.420 --> 1:25:37.980
 So clearly there's learning taking place there.

1:25:37.980 --> 1:25:41.340
 So I think a lot of what people think are so basic

1:25:41.340 --> 1:25:43.180
 that they need to be hardwired,

1:25:43.180 --> 1:25:44.420
 I think a lot of those things are learned

1:25:44.420 --> 1:25:46.260
 because they are easy to learn.

1:25:46.260 --> 1:25:49.980
 So you put a lot of value in the power of learning.

1:25:49.980 --> 1:25:53.340
 What kind of things do you suspect might not be learned?

1:25:53.340 --> 1:25:56.060
 Is there something that could not be learned?

1:25:56.060 --> 1:25:59.820
 So your intrinsic drives are not learned.

1:25:59.820 --> 1:26:03.460
 There are the things that make humans human

1:26:03.460 --> 1:26:07.460
 or make cats different from dogs, right?

1:26:07.460 --> 1:26:10.060
 It's the basic drives that are kind of hardwired

1:26:10.060 --> 1:26:11.940
 in our basal ganglia.

1:26:13.100 --> 1:26:14.060
 I mean, there are people who are working

1:26:14.060 --> 1:26:16.380
 on this kind of stuff that's called intrinsic motivation

1:26:16.380 --> 1:26:18.220
 in the context of reinforcement learning.

1:26:18.220 --> 1:26:20.100
 So these are objective functions

1:26:20.100 --> 1:26:23.100
 where the reward doesn't come from the external world.

1:26:23.100 --> 1:26:24.660
 It's computed by your own brain.

1:26:24.660 --> 1:26:28.140
 Your own brain computes whether you're happy or not, right?

1:26:28.140 --> 1:26:32.540
 It measures your degree of comfort or in comfort.

1:26:33.460 --> 1:26:36.100
 And because it's your brain computing this,

1:26:36.100 --> 1:26:37.780
 presumably it knows also how to estimate

1:26:37.780 --> 1:26:38.780
 gradients of this, right?

1:26:38.780 --> 1:26:43.780
 So it's easier to learn when your objective is intrinsic.

1:26:47.100 --> 1:26:48.780
 So that has to be hardwired.

1:26:50.100 --> 1:26:53.460
 The critic that makes longterm prediction of the outcome,

1:26:53.460 --> 1:26:56.780
 which is the eventual result of this, that's learned.

1:26:57.860 --> 1:26:59.060
 And perception is learned

1:26:59.060 --> 1:27:01.260
 and your model of the world is learned.

1:27:01.260 --> 1:27:04.260
 But let me take an example of why the critic,

1:27:04.260 --> 1:27:06.860
 I mean, an example of how the critic may be learned, right?

1:27:06.860 --> 1:27:11.220
 If I come to you, I reach across the table

1:27:11.220 --> 1:27:13.380
 and I pinch your arm, right?

1:27:13.380 --> 1:27:15.060
 Complete surprise for you.

1:27:15.060 --> 1:27:16.260
 You would not have expected this from me.

1:27:16.260 --> 1:27:18.100
 I was expecting that the whole time, but yes, right.

1:27:18.100 --> 1:27:20.420
 Let's say for the sake of the story, yes.

1:27:20.420 --> 1:27:24.980
 So, okay, your basal ganglia is gonna light up

1:27:24.980 --> 1:27:26.820
 because it's gonna hurt, right?

1:27:28.500 --> 1:27:31.140
 And now your model of the world includes the fact that

1:27:31.140 --> 1:27:34.820
 I may pinch you if I approach my...

1:27:34.820 --> 1:27:36.220
 Don't trust humans.

1:27:36.220 --> 1:27:37.860
 Right, my hand to your arm.

1:27:37.860 --> 1:27:40.020
 So if I try again, you're gonna recoil.

1:27:40.020 --> 1:27:44.060
 And that's your critic, your predictive,

1:27:44.060 --> 1:27:50.660
 your predictor of your ultimate pain system

1:27:50.660 --> 1:27:52.380
 that predicts that something bad is gonna happen

1:27:52.380 --> 1:27:53.860
 and you recoil to avoid it.

1:27:53.860 --> 1:27:55.260
 So even that can be learned.

1:27:55.260 --> 1:27:56.700
 That is learned, definitely.

1:27:56.700 --> 1:28:00.700
 This is what allows you also to define some goals, right?

1:28:00.700 --> 1:28:04.540
 So the fact that you're a school child,

1:28:04.540 --> 1:28:06.780
 you wake up in the morning and you go to school

1:28:06.780 --> 1:28:12.060
 and it's not because you necessarily like waking up early

1:28:12.060 --> 1:28:12.900
 and going to school,

1:28:12.900 --> 1:28:14.620
 but you know that there is a long term objective

1:28:14.620 --> 1:28:15.820
 you're trying to optimize.

1:28:15.820 --> 1:28:18.540
 So Ernest Becker, I'm not sure if you're familiar with him,

1:28:18.540 --> 1:28:20.900
 the philosopher, he wrote the book Denial of Death

1:28:20.900 --> 1:28:23.420
 and his idea is that one of the core motivations

1:28:23.420 --> 1:28:27.220
 of human beings is our terror of death, our fear of death.

1:28:27.220 --> 1:28:28.900
 That's what makes us unique from cats.

1:28:28.900 --> 1:28:30.500
 Cats are just surviving.

1:28:30.500 --> 1:28:35.500
 They do not have a deep, like a cognizance introspection

1:28:37.540 --> 1:28:41.740
 that over the horizon is the end.

1:28:41.740 --> 1:28:43.060
 And then he says that, I mean,

1:28:43.060 --> 1:28:44.420
 there's a terror management theory

1:28:44.420 --> 1:28:46.260
 that just all these psychological experiments

1:28:46.260 --> 1:28:50.020
 that show basically this idea

1:28:50.020 --> 1:28:54.380
 that all of human civilization, everything we create

1:28:54.380 --> 1:28:58.820
 is kind of trying to forget if even for a brief moment

1:28:58.820 --> 1:29:00.660
 that we're going to die.

1:29:00.660 --> 1:29:03.780
 When do you think humans understand

1:29:03.780 --> 1:29:04.900
 that they're going to die?

1:29:04.900 --> 1:29:07.580
 Is it learned early on also?

1:29:07.580 --> 1:29:11.260
 I don't know at what point.

1:29:11.260 --> 1:29:13.460
 I mean, it's a question like at what point

1:29:13.460 --> 1:29:16.420
 do you realize that what death really is?

1:29:16.420 --> 1:29:18.180
 And I think most people don't actually realize

1:29:18.180 --> 1:29:19.220
 what death is, right?

1:29:19.220 --> 1:29:20.940
 I mean, most people believe that you go to heaven

1:29:20.940 --> 1:29:21.860
 or something, right?

1:29:21.860 --> 1:29:25.580
 So to push back on that, what Ernest Becker says

1:29:25.580 --> 1:29:29.300
 and Sheldon Solomon, all of those folks,

1:29:29.300 --> 1:29:31.620
 and I find those ideas a little bit compelling

1:29:31.620 --> 1:29:34.100
 is that there is moments in life, early in life,

1:29:34.100 --> 1:29:36.540
 a lot of this fun happens early in life

1:29:36.540 --> 1:29:41.540
 when you do deeply experience

1:29:41.620 --> 1:29:43.540
 the terror of this realization.

1:29:43.540 --> 1:29:45.980
 And all the things you think about about religion,

1:29:45.980 --> 1:29:48.420
 all those kinds of things that we kind of think about

1:29:48.420 --> 1:29:50.660
 more like teenage years and later,

1:29:50.660 --> 1:29:52.100
 we're talking about way earlier.

1:29:52.100 --> 1:29:53.220
 No, it was like seven or eight years,

1:29:53.220 --> 1:29:54.060
 something like that, yeah.

1:29:54.060 --> 1:29:59.060
 You realize, holy crap, this is like the mystery,

1:29:59.660 --> 1:30:03.220
 the terror, like it's almost like you're a little prey,

1:30:03.220 --> 1:30:05.340
 a little baby deer sitting in the darkness

1:30:05.340 --> 1:30:08.060
 of the jungle or the woods looking all around you.

1:30:08.060 --> 1:30:09.540
 There's darkness full of terror.

1:30:09.540 --> 1:30:12.140
 I mean, that realization says, okay,

1:30:12.140 --> 1:30:14.460
 I'm gonna go back in the comfort of my mind

1:30:14.460 --> 1:30:16.780
 where there is a deep meaning,

1:30:16.780 --> 1:30:20.420
 where there is maybe like pretend I'm immortal

1:30:20.420 --> 1:30:25.060
 in however way, however kind of idea I can construct

1:30:25.060 --> 1:30:27.180
 to help me understand that I'm immortal.

1:30:27.180 --> 1:30:28.660
 Religion helps with that.

1:30:28.660 --> 1:30:31.440
 You can delude yourself in all kinds of ways,

1:30:31.440 --> 1:30:34.220
 like lose yourself in the busyness of each day,

1:30:34.220 --> 1:30:36.380
 have little goals in mind, all those kinds of things

1:30:36.380 --> 1:30:38.100
 to think that it's gonna go on forever.

1:30:38.100 --> 1:30:40.740
 And you kind of know you're gonna die, yeah,

1:30:40.740 --> 1:30:43.820
 and it's gonna be sad, but you don't really understand

1:30:43.820 --> 1:30:45.140
 that you're going to die.

1:30:45.140 --> 1:30:46.460
 And so that's their idea.

1:30:46.460 --> 1:30:49.940
 And I find that compelling because it does seem

1:30:49.940 --> 1:30:52.820
 to be a core unique aspect of human nature

1:30:52.820 --> 1:30:55.180
 that we're able to think that we're going,

1:30:55.180 --> 1:30:59.540
 we're able to really understand that this life is finite.

1:30:59.540 --> 1:31:00.580
 That seems important.

1:31:00.580 --> 1:31:02.260
 There's a bunch of different things there.

1:31:02.260 --> 1:31:04.300
 So first of all, I don't think there is a qualitative

1:31:04.300 --> 1:31:07.520
 difference between us and cats in the term.

1:31:07.520 --> 1:31:10.180
 I think the difference is that we just have a better

1:31:10.180 --> 1:31:14.740
 long term ability to predict in the long term.

1:31:14.740 --> 1:31:17.380
 And so we have a better understanding of how the world works.

1:31:17.380 --> 1:31:20.180
 So we have better understanding of finiteness of life

1:31:20.180 --> 1:31:21.020
 and things like that.

1:31:21.020 --> 1:31:23.540
 So we have a better planning engine than cats?

1:31:23.540 --> 1:31:24.500
 Yeah.

1:31:24.500 --> 1:31:25.340
 Okay.

1:31:25.340 --> 1:31:28.780
 But what's the motivation for planning that far?

1:31:28.780 --> 1:31:30.540
 Well, I think it's just a side effect of the fact

1:31:30.540 --> 1:31:32.340
 that we have just a better planning engine

1:31:32.340 --> 1:31:34.780
 because it makes us, as I said,

1:31:34.780 --> 1:31:37.420
 the essence of intelligence is the ability to predict.

1:31:37.420 --> 1:31:41.220
 And so the, because we're smarter as a side effect,

1:31:41.220 --> 1:31:43.500
 we also have this ability to kind of make predictions

1:31:43.500 --> 1:31:47.580
 about our own future existence or lack thereof.

1:31:47.580 --> 1:31:48.500
 Okay.

1:31:48.500 --> 1:31:50.540
 You say religion helps with that.

1:31:50.540 --> 1:31:53.000
 I think religion hurts actually.

1:31:53.000 --> 1:31:55.000
 It makes people worry about like,

1:31:55.000 --> 1:31:57.500
 what's going to happen after their death, et cetera.

1:31:57.500 --> 1:32:00.820
 If you believe that, you just don't exist after death.

1:32:00.820 --> 1:32:02.940
 Like, it solves completely the problem, at least.

1:32:02.940 --> 1:32:04.940
 You're saying if you don't believe in God,

1:32:04.940 --> 1:32:07.220
 you don't worry about what happens after death?

1:32:07.220 --> 1:32:08.260
 Yeah.

1:32:08.260 --> 1:32:09.100
 I don't know.

1:32:09.100 --> 1:32:11.900
 You only worry about this life

1:32:11.900 --> 1:32:14.220
 because that's the only one you have.

1:32:14.220 --> 1:32:16.140
 I think it's, well, I don't know.

1:32:16.140 --> 1:32:17.740
 If I were to say what Ernest Becker says,

1:32:17.740 --> 1:32:22.140
 and obviously I agree with him more than not,

1:32:22.140 --> 1:32:26.160
 is you do deeply worry.

1:32:26.160 --> 1:32:27.900
 If you believe there's no God,

1:32:27.900 --> 1:32:31.780
 there's still a deep worry of the mystery of it all.

1:32:31.780 --> 1:32:35.700
 Like, how does that make any sense that it just ends?

1:32:35.700 --> 1:32:39.740
 I don't think we can truly understand that this ride,

1:32:39.740 --> 1:32:41.900
 I mean, so much of our life, the consciousness,

1:32:41.900 --> 1:32:46.220
 the ego is invested in this being.

1:32:46.220 --> 1:32:47.580
 And then...

1:32:47.580 --> 1:32:51.560
 Science keeps bringing humanity down from its pedestal.

1:32:51.560 --> 1:32:54.740
 And that's just another example of it.

1:32:54.740 --> 1:32:57.820
 That's wonderful, but for us individual humans,

1:32:57.820 --> 1:33:00.300
 we don't like to be brought down from a pedestal.

1:33:00.300 --> 1:33:03.580
 You're saying like, but see, you're fine with it because,

1:33:03.580 --> 1:33:06.340
 well, so what Ernest Becker would say is you're fine with it

1:33:06.340 --> 1:33:08.580
 because there's just a more peaceful existence for you,

1:33:08.580 --> 1:33:09.580
 but you're not really fine.

1:33:09.580 --> 1:33:10.820
 You're hiding from it.

1:33:10.820 --> 1:33:12.780
 In fact, some of the people that experience

1:33:12.780 --> 1:33:16.700
 the deepest trauma earlier in life,

1:33:16.700 --> 1:33:19.580
 they often, before they seek extensive therapy,

1:33:19.580 --> 1:33:21.060
 will say that I'm fine.

1:33:21.060 --> 1:33:23.460
 It's like when you talk to people who are truly angry,

1:33:23.460 --> 1:33:25.380
 how are you doing, I'm fine.

1:33:25.380 --> 1:33:27.780
 The question is, what's going on?

1:33:27.780 --> 1:33:29.140
 Now I had a near death experience.

1:33:29.140 --> 1:33:33.580
 I had a very bad motorbike accident when I was 17.

1:33:33.580 --> 1:33:36.920
 So, but that didn't have any impact

1:33:36.920 --> 1:33:40.420
 on my reflection on that topic.

1:33:40.420 --> 1:33:43.100
 So I'm basically just playing a bit of devil's advocate,

1:33:43.100 --> 1:33:45.820
 pushing back on wondering,

1:33:45.820 --> 1:33:47.540
 is it truly possible to accept death?

1:33:47.540 --> 1:33:49.340
 And the flip side, that's more interesting,

1:33:49.340 --> 1:33:53.060
 I think for AI and robotics is how important

1:33:53.060 --> 1:33:57.180
 is it to have this as one of the suite of motivations

1:33:57.180 --> 1:34:02.180
 is to not just avoid falling off the roof

1:34:03.320 --> 1:34:08.320
 or something like that, but ponder the end of the ride.

1:34:10.180 --> 1:34:14.820
 If you listen to the stoics, it's a great motivator.

1:34:14.820 --> 1:34:16.900
 It adds a sense of urgency.

1:34:16.900 --> 1:34:21.420
 So maybe to truly fear death or be cognizant of it

1:34:21.420 --> 1:34:26.420
 might give a deeper meaning and urgency to the moment

1:34:26.460 --> 1:34:28.300
 to live fully.

1:34:30.460 --> 1:34:32.220
 Maybe I don't disagree with that.

1:34:32.220 --> 1:34:34.280
 I mean, I think what motivates me here

1:34:34.280 --> 1:34:38.980
 is knowing more about human nature.

1:34:38.980 --> 1:34:41.760
 I mean, I think human nature and human intelligence

1:34:41.760 --> 1:34:42.600
 is a big mystery.

1:34:42.600 --> 1:34:43.780
 It's a scientific mystery

1:34:45.020 --> 1:34:48.580
 in addition to philosophical and et cetera,

1:34:48.580 --> 1:34:50.700
 but I'm a true believer in science.

1:34:50.700 --> 1:34:55.700
 So, and I do have kind of a belief

1:34:56.180 --> 1:34:59.940
 that for complex systems like the brain and the mind,

1:34:59.940 --> 1:35:04.460
 the way to understand it is to try to reproduce it

1:35:04.460 --> 1:35:07.060
 with artifacts that you build

1:35:07.060 --> 1:35:08.900
 because you know what's essential to it

1:35:08.900 --> 1:35:10.180
 when you try to build it.

1:35:10.180 --> 1:35:12.660
 The same way I've used this analogy before with you,

1:35:12.660 --> 1:35:15.780
 I believe, the same way we only started

1:35:15.780 --> 1:35:18.140
 to understand aerodynamics

1:35:18.140 --> 1:35:19.300
 when we started building airplanes

1:35:19.300 --> 1:35:21.500
 and that helped us understand how birds fly.

1:35:22.380 --> 1:35:25.460
 So I think there's kind of a similar process here

1:35:25.460 --> 1:35:29.660
 where we don't have a full theory of intelligence,

1:35:29.660 --> 1:35:31.760
 but building intelligent artifacts

1:35:31.760 --> 1:35:35.480
 will help us perhaps develop some underlying theory

1:35:35.480 --> 1:35:39.380
 that encompasses not just artificial implements,

1:35:39.380 --> 1:35:43.860
 but also human and biological intelligence in general.

1:35:43.860 --> 1:35:46.080
 So you're an interesting person to ask this question

1:35:46.080 --> 1:35:49.400
 about sort of all kinds of different other

1:35:49.400 --> 1:35:53.100
 intelligent entities or intelligences.

1:35:53.100 --> 1:35:56.300
 What are your thoughts about kind of like the touring

1:35:56.300 --> 1:35:58.000
 or the Chinese room question?

1:35:59.240 --> 1:36:02.920
 If we create an AI system that exhibits

1:36:02.920 --> 1:36:06.360
 a lot of properties of intelligence and consciousness,

1:36:07.520 --> 1:36:10.220
 how comfortable are you thinking of that entity

1:36:10.220 --> 1:36:12.340
 as intelligent or conscious?

1:36:12.340 --> 1:36:14.580
 So you're trying to build now systems

1:36:14.580 --> 1:36:16.420
 that have intelligence and there's metrics

1:36:16.420 --> 1:36:21.300
 about their performance, but that metric is external.

1:36:22.740 --> 1:36:26.420
 So how are you, are you okay calling a thing intelligent

1:36:26.420 --> 1:36:29.020
 or are you going to be like most humans

1:36:29.020 --> 1:36:32.700
 and be once again unhappy to be brought down

1:36:32.700 --> 1:36:34.920
 from a pedestal of consciousness slash intelligence?

1:36:34.920 --> 1:36:39.500
 No, I'll be very happy to understand

1:36:39.500 --> 1:36:44.500
 more about human nature, human mind and human intelligence

1:36:45.540 --> 1:36:47.240
 through the construction of machines

1:36:47.240 --> 1:36:50.600
 that have similar abilities.

1:36:50.600 --> 1:36:54.520
 And if a consequence of this is to bring down humanity

1:36:54.520 --> 1:36:58.020
 one notch down from its already low pedestal,

1:36:58.020 --> 1:36:59.140
 I'm just fine with it.

1:36:59.140 --> 1:37:01.360
 That's just the reality of life.

1:37:01.360 --> 1:37:02.460
 So I'm fine with that.

1:37:02.460 --> 1:37:05.020
 Now you were asking me about things that,

1:37:05.020 --> 1:37:07.940
 opinions I have that a lot of people may disagree with.

1:37:07.940 --> 1:37:12.780
 I think if we think about the design

1:37:12.780 --> 1:37:14.300
 of autonomous intelligence systems,

1:37:14.300 --> 1:37:16.860
 so assuming that we are somewhat successful

1:37:16.860 --> 1:37:20.060
 at some level of getting machines to learn models

1:37:20.060 --> 1:37:22.620
 of the world, predictive models of the world,

1:37:22.620 --> 1:37:25.860
 we build intrinsic motivation objective functions

1:37:25.860 --> 1:37:28.340
 to drive the behavior of that system.

1:37:28.340 --> 1:37:30.100
 The system also has perception modules

1:37:30.100 --> 1:37:32.820
 that allows it to estimate the state of the world

1:37:32.820 --> 1:37:34.640
 and then have some way of figuring out

1:37:34.640 --> 1:37:36.180
 the sequence of actions that,

1:37:36.180 --> 1:37:37.880
 to optimize a particular objective.

1:37:39.300 --> 1:37:42.740
 If it has a critic of the type that I was describing before,

1:37:42.740 --> 1:37:44.600
 the thing that makes you recoil your arm

1:37:44.600 --> 1:37:46.300
 the second time I try to pinch you,

1:37:48.620 --> 1:37:51.700
 intelligent autonomous machine will have emotions.

1:37:51.700 --> 1:37:54.060
 I think emotions are an integral part

1:37:54.060 --> 1:37:56.400
 of autonomous intelligence.

1:37:56.400 --> 1:37:59.020
 If you have an intelligent system

1:37:59.020 --> 1:38:03.160
 that is driven by intrinsic motivation, by objectives,

1:38:03.160 --> 1:38:07.680
 if it has a critic that allows it to predict in advance

1:38:07.680 --> 1:38:11.040
 whether the outcome of a situation is gonna be good or bad,

1:38:11.040 --> 1:38:13.480
 is going to have emotions, it's gonna have fear.

1:38:13.480 --> 1:38:14.320
 Yes.

1:38:14.320 --> 1:38:18.180
 When it predicts that the outcome is gonna be bad

1:38:18.180 --> 1:38:20.720
 and something to avoid is gonna have elation

1:38:20.720 --> 1:38:22.520
 when it predicts it's gonna be good.

1:38:24.280 --> 1:38:27.600
 If it has drives to relate with humans,

1:38:28.680 --> 1:38:30.660
 in some ways the way humans have,

1:38:30.660 --> 1:38:34.460
 it's gonna be social, right?

1:38:34.460 --> 1:38:36.460
 And so it's gonna have emotions

1:38:36.460 --> 1:38:38.620
 about attachment and things of that type.

1:38:38.620 --> 1:38:43.620
 So I think the sort of sci fi thing

1:38:44.700 --> 1:38:46.900
 where you see commander data,

1:38:46.900 --> 1:38:50.100
 like having an emotion chip that you can turn off, right?

1:38:50.100 --> 1:38:51.700
 I think that's ridiculous.

1:38:51.700 --> 1:38:53.380
 So, I mean, here's the difficult

1:38:53.380 --> 1:38:57.820
 philosophical social question.

1:38:57.820 --> 1:39:01.020
 Do you think there will be a time like a civil rights

1:39:01.020 --> 1:39:05.180
 movement for robots where, okay, forget the movement,

1:39:05.180 --> 1:39:07.860
 but a discussion like the Supreme Court

1:39:09.740 --> 1:39:12.880
 that particular kinds of robots,

1:39:12.880 --> 1:39:14.860
 you know, particular kinds of systems

1:39:16.100 --> 1:39:18.300
 deserve the same rights as humans

1:39:18.300 --> 1:39:21.660
 because they can suffer just as humans can,

1:39:22.900 --> 1:39:24.740
 all those kinds of things.

1:39:24.740 --> 1:39:27.340
 Well, perhaps, perhaps not.

1:39:27.340 --> 1:39:29.580
 Like imagine that humans were,

1:39:29.580 --> 1:39:33.740
 that you could, you know, die and be restored.

1:39:33.740 --> 1:39:35.500
 Like, you know, you could be sort of, you know,

1:39:35.500 --> 1:39:37.540
 be 3D reprinted and, you know,

1:39:37.540 --> 1:39:40.740
 your brain could be reconstructed in its finest details.

1:39:40.740 --> 1:39:43.140
 Our ideas of rights will change in that case.

1:39:43.140 --> 1:39:44.560
 If you can always just,

1:39:45.900 --> 1:39:48.220
 there's always a backup you could always restore.

1:39:48.220 --> 1:39:50.260
 Maybe like the importance of murder

1:39:50.260 --> 1:39:51.980
 will go down one notch.

1:39:51.980 --> 1:39:52.820
 That's right.

1:39:52.820 --> 1:39:57.580
 But also your desire to do dangerous things,

1:39:57.580 --> 1:40:02.000
 like, you know, skydiving or, you know,

1:40:03.300 --> 1:40:05.660
 or, you know, race car driving,

1:40:05.660 --> 1:40:07.300
 you know, car racing or that kind of stuff,

1:40:07.300 --> 1:40:09.460
 you know, would probably increase

1:40:09.460 --> 1:40:11.140
 or, you know, aeroplanes, aerobatics

1:40:11.140 --> 1:40:12.380
 or that kind of stuff, right?

1:40:12.380 --> 1:40:14.180
 It would be fine to do a lot of those things

1:40:14.180 --> 1:40:17.500
 or explore, you know, dangerous areas and things like that.

1:40:17.500 --> 1:40:19.220
 It would kind of change your relationship.

1:40:19.220 --> 1:40:22.420
 So now it's very likely that robots would be like that

1:40:22.420 --> 1:40:27.060
 because, you know, they'll be based on perhaps technology

1:40:27.060 --> 1:40:30.140
 that is somewhat similar to today's technology

1:40:30.140 --> 1:40:32.260
 and you can always have a backup.

1:40:32.260 --> 1:40:35.700
 So it's possible, I don't know if you like video games,

1:40:35.700 --> 1:40:39.340
 but there's a game called Diablo and...

1:40:39.340 --> 1:40:41.860
 Oh, my sons are huge fans of this.

1:40:41.860 --> 1:40:42.700
 Yes.

1:40:44.100 --> 1:40:47.060
 In fact, they made a game that's inspired by it.

1:40:47.060 --> 1:40:47.900
 Awesome.

1:40:47.900 --> 1:40:49.260
 Like built a game?

1:40:49.260 --> 1:40:52.660
 My three sons have a game design studio between them, yeah.

1:40:52.660 --> 1:40:53.480
 That's awesome.

1:40:53.480 --> 1:40:54.320
 They came out with a game.

1:40:54.320 --> 1:40:55.160
 They just came out with a game.

1:40:55.160 --> 1:40:56.860
 Last year, no, this was last year,

1:40:56.860 --> 1:40:58.180
 early last year, about a year ago.

1:40:58.180 --> 1:40:59.020
 That's awesome.

1:40:59.020 --> 1:41:02.020
 But so in Diablo, there's something called hardcore mode,

1:41:02.020 --> 1:41:05.480
 which if you die, there's no, you're gone.

1:41:05.480 --> 1:41:06.320
 Right.

1:41:06.320 --> 1:41:07.140
 That's it.

1:41:07.140 --> 1:41:09.580
 And so it's possible with AI systems

1:41:10.620 --> 1:41:13.260
 for them to be able to operate successfully

1:41:13.260 --> 1:41:15.580
 and for us to treat them in a certain way

1:41:15.580 --> 1:41:18.400
 because they have to be integrated in human society,

1:41:18.400 --> 1:41:22.020
 they have to be able to die, no copies allowed.

1:41:22.020 --> 1:41:23.860
 In fact, copying is illegal.

1:41:23.860 --> 1:41:25.260
 It's possible with humans as well,

1:41:25.260 --> 1:41:28.580
 like cloning will be illegal, even when it's possible.

1:41:28.580 --> 1:41:29.960
 But cloning is not copying, right?

1:41:29.960 --> 1:41:33.060
 I mean, you don't reproduce the mind of the person

1:41:33.060 --> 1:41:33.940
 and the experience.

1:41:33.940 --> 1:41:34.760
 Right.

1:41:34.760 --> 1:41:36.420
 It's just a delayed twin, so.

1:41:36.420 --> 1:41:39.060
 But then it's, but we were talking about with computers

1:41:39.060 --> 1:41:40.580
 that you will be able to copy.

1:41:40.580 --> 1:41:41.420
 Right.

1:41:41.420 --> 1:41:42.660
 You will be able to perfectly save,

1:41:42.660 --> 1:41:46.640
 pickle the mind state.

1:41:46.640 --> 1:41:49.660
 And it's possible that that will be illegal

1:41:49.660 --> 1:41:52.320
 because that goes against,

1:41:53.300 --> 1:41:55.980
 that will destroy the motivation of the system.

1:41:55.980 --> 1:42:00.240
 Okay, so let's say you have a domestic robot, okay?

1:42:00.240 --> 1:42:01.380
 Sometime in the future.

1:42:01.380 --> 1:42:02.460
 Yes.

1:42:02.460 --> 1:42:06.100
 And the domestic robot comes to you kind of

1:42:06.100 --> 1:42:08.700
 somewhat pre trained, it can do a bunch of things,

1:42:08.700 --> 1:42:10.580
 but it has a particular personality

1:42:10.580 --> 1:42:12.300
 that makes it slightly different from the other robots

1:42:12.300 --> 1:42:14.220
 because that makes them more interesting.

1:42:14.220 --> 1:42:18.060
 And then because it's lived with you for five years,

1:42:18.060 --> 1:42:21.900
 you've grown some attachment to it and vice versa,

1:42:21.900 --> 1:42:24.380
 and it's learned a lot about you.

1:42:24.380 --> 1:42:25.900
 Or maybe it's not a real household robot.

1:42:25.900 --> 1:42:29.380
 Maybe it's a virtual assistant that lives in your,

1:42:29.380 --> 1:42:32.580
 you know, augmented reality glasses or whatever, right?

1:42:32.580 --> 1:42:35.060
 You know, the horror movie type thing, right?

1:42:36.680 --> 1:42:39.620
 And that system to some extent,

1:42:39.620 --> 1:42:43.900
 the intelligence in that system is a bit like your child

1:42:43.900 --> 1:42:47.100
 or maybe your PhD student in the sense that

1:42:47.100 --> 1:42:50.260
 there's a lot of you in that machine now, right?

1:42:50.260 --> 1:42:53.500
 And so if it were a living thing,

1:42:53.500 --> 1:42:56.560
 you would do this for free if you want, right?

1:42:56.560 --> 1:42:58.400
 If it's your child, your child can, you know,

1:42:58.400 --> 1:43:01.580
 then live his or her own life.

1:43:01.580 --> 1:43:04.020
 And you know, the fact that they learn stuff from you

1:43:04.020 --> 1:43:06.540
 doesn't mean that you have any ownership of it, right?

1:43:06.540 --> 1:43:09.380
 But if it's a robot that you've trained,

1:43:09.380 --> 1:43:13.580
 perhaps you have some intellectual property claim

1:43:13.580 --> 1:43:14.420
 about.

1:43:14.420 --> 1:43:15.240
 Oh, intellectual property.

1:43:15.240 --> 1:43:18.140
 Oh, I thought you meant like a permanence value

1:43:18.140 --> 1:43:20.180
 in the sense that part of you is in.

1:43:20.180 --> 1:43:21.700
 Well, there is permanence value, right?

1:43:21.700 --> 1:43:24.660
 So you would lose a lot if that robot were to be destroyed

1:43:24.660 --> 1:43:26.660
 and you had no backup, you would lose a lot, right?

1:43:26.660 --> 1:43:28.100
 You lose a lot of investment, you know,

1:43:28.100 --> 1:43:31.860
 kind of like, you know, a person dying, you know,

1:43:31.860 --> 1:43:34.300
 that a friend of yours dying

1:43:34.300 --> 1:43:36.580
 or a coworker or something like that.

1:43:38.480 --> 1:43:42.340
 But also you have like intellectual property rights

1:43:42.340 --> 1:43:45.940
 in the sense that that system is fine tuned

1:43:45.940 --> 1:43:47.340
 to your particular existence.

1:43:47.340 --> 1:43:49.860
 So that's now a very unique instantiation

1:43:49.860 --> 1:43:51.980
 of that original background model,

1:43:51.980 --> 1:43:54.260
 whatever it was that arrived.

1:43:54.260 --> 1:43:55.660
 And then there are issues of privacy, right?

1:43:55.660 --> 1:44:00.000
 Because now imagine that that robot has its own kind

1:44:00.000 --> 1:44:02.820
 of volition and decides to work for someone else.

1:44:02.820 --> 1:44:06.020
 Or kind of, you know, thinks life with you

1:44:06.020 --> 1:44:07.880
 is sort of untenable or whatever.

1:44:07.880 --> 1:44:12.780
 Now, all the things that that system learned from you,

1:44:14.760 --> 1:44:16.880
 you know, can you like, you know,

1:44:16.880 --> 1:44:18.160
 delete all the personal information

1:44:18.160 --> 1:44:19.680
 that that system knows about you?

1:44:19.680 --> 1:44:22.200
 I mean, that would be kind of an ethical question.

1:44:22.200 --> 1:44:24.760
 Like, you know, can you erase the mind

1:44:24.760 --> 1:44:29.760
 of a intelligent robot to protect your privacy?

1:44:30.040 --> 1:44:31.580
 You can't do this with humans.

1:44:31.580 --> 1:44:32.680
 You can ask them to shut up,

1:44:32.680 --> 1:44:35.640
 but that you don't have complete power over them.

1:44:35.640 --> 1:44:38.040
 You can't erase humans, yeah, it's the problem

1:44:38.040 --> 1:44:40.120
 with the relationships, you know, if you break up,

1:44:40.120 --> 1:44:42.640
 you can't erase the other human.

1:44:42.640 --> 1:44:44.960
 With robots, I think it will have to be the same thing

1:44:44.960 --> 1:44:49.960
 with robots, that risk, that there has to be some risk

1:44:52.420 --> 1:44:55.120
 to our interactions to truly experience them deeply,

1:44:55.120 --> 1:44:56.140
 it feels like.

1:44:56.140 --> 1:44:59.600
 So you have to be able to lose your robot friend

1:44:59.600 --> 1:45:01.680
 and that robot friend to go tweeting

1:45:01.680 --> 1:45:03.680
 about how much of an asshole you were.

1:45:03.680 --> 1:45:06.160
 But then are you allowed to, you know,

1:45:06.160 --> 1:45:08.760
 murder the robot to protect your private information

1:45:08.760 --> 1:45:09.960
 if the robot decides to leave?

1:45:09.960 --> 1:45:14.520
 I have this intuition that for robots with certain,

1:45:14.520 --> 1:45:16.820
 like, it's almost like a regulation.

1:45:16.820 --> 1:45:19.240
 If you declare your robot to be,

1:45:19.240 --> 1:45:20.960
 let's call it sentient or something like that,

1:45:20.960 --> 1:45:24.180
 like this robot is designed for human interaction,

1:45:24.180 --> 1:45:26.040
 then you're not allowed to murder these robots.

1:45:26.040 --> 1:45:28.160
 It's the same as murdering other humans.

1:45:28.160 --> 1:45:30.280
 Well, but what about you do a backup of the robot

1:45:30.280 --> 1:45:32.600
 that you preserve on a hard drive

1:45:32.600 --> 1:45:33.880
 for the equivalent in the future?

1:45:33.880 --> 1:45:34.720
 That might be illegal.

1:45:34.720 --> 1:45:38.080
 It's like piracy is illegal.

1:45:38.080 --> 1:45:39.800
 No, but it's your own robot, right?

1:45:39.800 --> 1:45:41.640
 But you can't, you don't.

1:45:41.640 --> 1:45:45.040
 But then you can wipe out his brain.

1:45:45.040 --> 1:45:47.440
 So this robot doesn't know anything about you anymore,

1:45:47.440 --> 1:45:50.440
 but you still have, technically it's still in existence

1:45:50.440 --> 1:45:51.700
 because you backed it up.

1:45:51.700 --> 1:45:53.560
 And then there'll be these great speeches

1:45:53.560 --> 1:45:55.480
 at the Supreme Court by saying,

1:45:55.480 --> 1:45:57.840
 oh, sure, you can erase the mind of the robot

1:45:57.840 --> 1:46:00.060
 just like you can erase the mind of a human.

1:46:00.060 --> 1:46:01.100
 We both can suffer.

1:46:01.100 --> 1:46:03.360
 There'll be some epic like Obama type character

1:46:03.360 --> 1:46:05.680
 with a speech that we,

1:46:05.680 --> 1:46:07.980
 like the robots and the humans are the same.

1:46:08.840 --> 1:46:09.880
 We can both suffer.

1:46:09.880 --> 1:46:11.380
 We can both hope.

1:46:11.380 --> 1:46:14.880
 We can both, all of those kinds of things,

1:46:14.880 --> 1:46:17.280
 raise families, all that kind of stuff.

1:46:17.280 --> 1:46:20.140
 It's interesting for these, just like you said,

1:46:20.140 --> 1:46:24.200
 emotion seems to be a fascinatingly powerful aspect

1:46:24.200 --> 1:46:27.360
 of human interaction, human robot interaction.

1:46:27.360 --> 1:46:30.480
 And if they're able to exhibit emotions

1:46:30.480 --> 1:46:31.800
 at the end of the day,

1:46:31.800 --> 1:46:35.920
 that's probably going to have us deeply consider

1:46:35.920 --> 1:46:38.480
 human rights, like what we value in humans,

1:46:38.480 --> 1:46:40.320
 what we value in other animals.

1:46:40.320 --> 1:46:42.120
 That's why robots and AI is great.

1:46:42.120 --> 1:46:44.280
 It makes us ask really good questions.

1:46:44.280 --> 1:46:45.480
 The hard questions, yeah.

1:46:45.480 --> 1:46:49.560
 But you asked about the Chinese room type argument.

1:46:49.560 --> 1:46:50.400
 Is it real?

1:46:50.400 --> 1:46:51.480
 If it looks real.

1:46:51.480 --> 1:46:54.400
 I think the Chinese room argument is a really good one.

1:46:54.400 --> 1:46:55.440
 So.

1:46:55.440 --> 1:46:58.440
 So for people who don't know what Chinese room is,

1:46:58.440 --> 1:47:00.740
 you can, I don't even know how to formulate it well,

1:47:00.740 --> 1:47:04.620
 but basically you can mimic the behavior

1:47:04.620 --> 1:47:06.760
 of an intelligence system by just following

1:47:06.760 --> 1:47:10.680
 a giant algorithm code book that tells you exactly

1:47:10.680 --> 1:47:12.880
 how to respond in exactly each case.

1:47:12.880 --> 1:47:14.700
 But is that really intelligent?

1:47:14.700 --> 1:47:16.600
 It's like a giant lookup table.

1:47:16.600 --> 1:47:18.580
 When this person says this, you answer this.

1:47:18.580 --> 1:47:21.000
 When this person says this, you answer this.

1:47:21.000 --> 1:47:24.320
 And if you understand how that works,

1:47:24.320 --> 1:47:27.360
 you have this giant, nearly infinite lookup table.

1:47:27.360 --> 1:47:28.600
 Is that really intelligence?

1:47:28.600 --> 1:47:31.280
 Cause intelligence seems to be a mechanism

1:47:31.280 --> 1:47:33.440
 that's much more interesting and complex

1:47:33.440 --> 1:47:34.620
 than this lookup table.

1:47:34.620 --> 1:47:35.460
 I don't think so.

1:47:35.460 --> 1:47:38.960
 So the, I mean, the real question comes down to,

1:47:38.960 --> 1:47:42.080
 do you think, you know, you can,

1:47:42.080 --> 1:47:44.320
 you can mechanize intelligence in some way,

1:47:44.320 --> 1:47:47.560
 even if that involves learning?

1:47:47.560 --> 1:47:50.720
 And the answer is, of course, yes, there's no question.

1:47:50.720 --> 1:47:53.400
 There's a second question then, which is,

1:47:53.400 --> 1:47:56.560
 assuming you can reproduce intelligence

1:47:56.560 --> 1:47:59.400
 in sort of different hardware than biological hardware,

1:47:59.400 --> 1:48:04.400
 you know, like computers, can you, you know,

1:48:04.440 --> 1:48:09.440
 match human intelligence in all the domains

1:48:09.600 --> 1:48:11.880
 in which humans are intelligent?

1:48:12.920 --> 1:48:13.920
 Is it possible, right?

1:48:13.920 --> 1:48:17.040
 So that's the hypothesis of strong AI.

1:48:17.040 --> 1:48:20.700
 The answer to this, in my opinion, is an unqualified yes.

1:48:20.700 --> 1:48:22.640
 This will as well happen at some point.

1:48:22.640 --> 1:48:25.300
 There's no question that machines at some point

1:48:25.300 --> 1:48:26.640
 will become more intelligent than humans

1:48:26.640 --> 1:48:28.640
 in all domains where humans are intelligent.

1:48:28.640 --> 1:48:30.200
 This is not for tomorrow.

1:48:30.200 --> 1:48:32.240
 It is going to take a long time,

1:48:32.240 --> 1:48:34.800
 regardless of what, you know,

1:48:34.800 --> 1:48:38.120
 Elon and others have claimed or believed.

1:48:38.120 --> 1:48:42.120
 This is a lot harder than many of those guys think it is.

1:48:43.480 --> 1:48:45.800
 And many of those guys who thought it was simpler than that

1:48:45.800 --> 1:48:47.480
 years, you know, five years ago,

1:48:47.480 --> 1:48:49.920
 now think it's hard because it's been five years

1:48:49.920 --> 1:48:53.460
 and they realize it's going to take a lot longer.

1:48:53.460 --> 1:48:55.200
 That includes a bunch of people at DeepMind, for example.

1:48:55.200 --> 1:48:56.160
 But...

1:48:56.160 --> 1:48:57.000
 Oh, interesting.

1:48:57.000 --> 1:48:59.320
 I haven't actually touched base with the DeepMind folks,

1:48:59.320 --> 1:49:03.280
 but some of it, Elon or Demis Hassabis.

1:49:03.280 --> 1:49:05.800
 I mean, sometimes in your role,

1:49:05.800 --> 1:49:08.780
 you have to kind of create deadlines

1:49:08.780 --> 1:49:10.720
 that are nearer than farther away

1:49:10.720 --> 1:49:12.800
 to kind of create an urgency.

1:49:12.800 --> 1:49:14.600
 Because, you know, you have to believe the impossible

1:49:14.600 --> 1:49:16.200
 is possible in order to accomplish it.

1:49:16.200 --> 1:49:18.520
 And there's, of course, a flip side to that coin,

1:49:18.520 --> 1:49:21.280
 but it's a weird, you can't be too cynical

1:49:21.280 --> 1:49:22.400
 if you want to get something done.

1:49:22.400 --> 1:49:23.360
 Absolutely.

1:49:23.360 --> 1:49:24.280
 I agree with that.

1:49:24.280 --> 1:49:26.920
 But, I mean, you have to inspire people, right?

1:49:26.920 --> 1:49:28.800
 To work on sort of ambitious things.

1:49:31.400 --> 1:49:35.620
 So, you know, it's certainly a lot harder than we believe,

1:49:35.620 --> 1:49:38.200
 but there's no question in my mind that this will happen.

1:49:38.200 --> 1:49:40.300
 And now, you know, people are kind of worried about

1:49:40.300 --> 1:49:42.480
 what does that mean for humans?

1:49:42.480 --> 1:49:45.160
 They are going to be brought down from their pedestal,

1:49:45.160 --> 1:49:47.980
 you know, a bunch of notches with that.

1:49:47.980 --> 1:49:51.740
 And, you know, is that going to be good or bad?

1:49:51.740 --> 1:49:53.480
 I mean, it's just going to give more power, right?

1:49:53.480 --> 1:49:56.200
 It's an amplifier for human intelligence, really.

1:49:56.200 --> 1:49:59.720
 So, speaking of doing cool, ambitious things,

1:49:59.720 --> 1:50:02.920
 FAIR, the Facebook AI research group,

1:50:02.920 --> 1:50:05.520
 has recently celebrated its eighth birthday.

1:50:05.520 --> 1:50:08.640
 Or, maybe you can correct me on that.

1:50:08.640 --> 1:50:12.400
 Looking back, what has been the successes, the failures,

1:50:12.400 --> 1:50:14.440
 the lessons learned from the eight years of FAIR?

1:50:14.440 --> 1:50:16.600
 And maybe you can also give context of

1:50:16.600 --> 1:50:21.320
 where does the newly minted meta AI fit into,

1:50:21.320 --> 1:50:22.640
 how does it relate to FAIR?

1:50:22.640 --> 1:50:23.800
 Right, so let me tell you a little bit

1:50:23.800 --> 1:50:25.600
 about the organization of all this.

1:50:26.760 --> 1:50:30.060
 Yeah, FAIR was created almost exactly eight years ago.

1:50:30.060 --> 1:50:31.240
 It wasn't called FAIR yet.

1:50:31.240 --> 1:50:33.620
 It took that name a few months later.

1:50:34.680 --> 1:50:37.760
 And at the time I joined Facebook,

1:50:37.760 --> 1:50:39.520
 there was a group called the AI group

1:50:39.520 --> 1:50:43.560
 that had about 12 engineers and a few scientists,

1:50:43.560 --> 1:50:45.480
 like, you know, 10 engineers and two scientists

1:50:45.480 --> 1:50:47.080
 or something like that.

1:50:47.080 --> 1:50:50.680
 I ran it for three and a half years as a director,

1:50:50.680 --> 1:50:52.380
 you know, hired the first few scientists

1:50:52.380 --> 1:50:55.040
 and kind of set up the culture and organized it,

1:50:55.040 --> 1:50:57.880
 you know, explained to the Facebook leadership

1:50:57.880 --> 1:51:00.200
 what fundamental research was about

1:51:00.200 --> 1:51:03.640
 and how it can work within industry

1:51:03.640 --> 1:51:05.800
 and how it needs to be open and everything.

1:51:07.240 --> 1:51:12.240
 And I think it's been an unqualified success

1:51:12.360 --> 1:51:16.620
 in the sense that FAIR has simultaneously produced,

1:51:17.800 --> 1:51:19.560
 you know, top level research

1:51:19.560 --> 1:51:21.640
 and advanced the science and the technology,

1:51:21.640 --> 1:51:23.480
 provided tools, open source tools,

1:51:23.480 --> 1:51:25.060
 like PyTorch and many others,

1:51:26.680 --> 1:51:29.880
 but at the same time has had a direct

1:51:29.880 --> 1:51:34.680
 or mostly indirect impact on Facebook at the time,

1:51:34.680 --> 1:51:38.580
 now Meta, in the sense that a lot of systems

1:51:38.580 --> 1:51:43.580
 that Meta is built around now are based

1:51:43.600 --> 1:51:48.360
 on research projects that started at FAIR.

1:51:48.360 --> 1:51:49.640
 And so if you were to take out, you know,

1:51:49.640 --> 1:51:52.840
 deep learning out of Facebook services now

1:51:52.840 --> 1:51:55.140
 and Meta more generally,

1:51:55.140 --> 1:51:57.760
 I mean, the company would literally crumble.

1:51:57.760 --> 1:52:01.480
 I mean, it's completely built around AI these days.

1:52:01.480 --> 1:52:04.000
 And it's really essential to the operations.

1:52:04.000 --> 1:52:06.640
 So what happened after three and a half years

1:52:06.640 --> 1:52:10.200
 is that I changed role, I became chief scientist.

1:52:10.200 --> 1:52:14.880
 So I'm not doing day to day management of FAIR anymore.

1:52:14.880 --> 1:52:17.120
 I'm more of a kind of, you know,

1:52:17.120 --> 1:52:18.880
 think about strategy and things like that.

1:52:18.880 --> 1:52:21.440
 And I carry my, I conduct my own research.

1:52:21.440 --> 1:52:23.320
 I have, you know, my own kind of research group

1:52:23.320 --> 1:52:25.320
 working on self supervised learning and things like this,

1:52:25.320 --> 1:52:28.240
 which I didn't have time to do when I was director.

1:52:28.240 --> 1:52:33.240
 So now FAIR is run by Joel Pinot and Antoine Bord together

1:52:34.720 --> 1:52:36.360
 because FAIR is kind of split in two now.

1:52:36.360 --> 1:52:37.860
 There's something called FAIR Labs,

1:52:37.860 --> 1:52:40.940
 which is sort of bottom up science driven research

1:52:40.940 --> 1:52:43.460
 and FAIR Excel, which is slightly more organized

1:52:43.460 --> 1:52:46.440
 for bigger projects that require a little more

1:52:46.440 --> 1:52:49.040
 kind of focus and more engineering support

1:52:49.040 --> 1:52:49.880
 and things like that.

1:52:49.880 --> 1:52:52.920
 So Joel needs FAIR Lab and Antoine Bord needs FAIR Excel.

1:52:52.920 --> 1:52:54.520
 Where are they located?

1:52:54.520 --> 1:52:56.680
 It's delocalized all over.

1:52:58.000 --> 1:53:02.540
 So there's no question that the leadership of the company

1:53:02.540 --> 1:53:06.560
 believes that this was a very worthwhile investment.

1:53:06.560 --> 1:53:11.560
 And what that means is that it's there for the long run.

1:53:12.840 --> 1:53:13.680
 Right?

1:53:13.680 --> 1:53:17.720
 So if you want to talk in these terms, which I don't like,

1:53:17.720 --> 1:53:19.560
 this is a business model, if you want,

1:53:19.560 --> 1:53:23.680
 where FAIR, despite being a very fundamental research lab

1:53:23.680 --> 1:53:25.320
 brings a lot of value to the company,

1:53:25.320 --> 1:53:27.880
 either mostly indirectly through other groups.

1:53:29.920 --> 1:53:31.600
 Now what happened three and a half years ago

1:53:31.600 --> 1:53:34.640
 when I stepped down was also the creation of Facebook AI,

1:53:34.640 --> 1:53:37.700
 which was basically a larger organization

1:53:37.700 --> 1:53:41.740
 that covers FAIR, so FAIR is included in it,

1:53:41.740 --> 1:53:43.880
 but also has other organizations

1:53:43.880 --> 1:53:47.840
 that are focused on applied research

1:53:47.840 --> 1:53:51.220
 or advanced development of AI technology

1:53:51.220 --> 1:53:54.680
 that is more focused on the products of the company.

1:53:54.680 --> 1:53:56.640
 So less emphasis on fundamental research.

1:53:56.640 --> 1:53:58.220
 Less fundamental, but it's still research.

1:53:58.220 --> 1:53:59.760
 I mean, there's a lot of papers coming out

1:53:59.760 --> 1:54:03.960
 of those organizations and the people are awesome

1:54:03.960 --> 1:54:06.400
 and wonderful to interact with.

1:54:06.400 --> 1:54:10.680
 But it serves as kind of a way

1:54:10.680 --> 1:54:15.680
 to kind of scale up if you want sort of AI technology,

1:54:15.720 --> 1:54:17.600
 which, you know, may be very experimental

1:54:17.600 --> 1:54:20.600
 and sort of lab prototypes into things that are usable.

1:54:20.600 --> 1:54:23.040
 So FAIR is a subset of Meta AI.

1:54:23.040 --> 1:54:24.800
 Is FAIR become like KFC?

1:54:24.800 --> 1:54:26.520
 It'll just keep the F.

1:54:26.520 --> 1:54:29.440
 Nobody cares what the F stands for.

1:54:29.440 --> 1:54:34.440
 We'll know soon enough, probably by the end of 2021.

1:54:35.600 --> 1:54:38.400
 I guess it's not a giant change, Mare, FAIR.

1:54:38.400 --> 1:54:39.520
 Well, Mare doesn't sound too good,

1:54:39.520 --> 1:54:43.560
 but the brand people are kind of deciding on this

1:54:43.560 --> 1:54:45.860
 and they've been hesitating for a while now.

1:54:45.860 --> 1:54:48.480
 And they tell us they're going to come up with an answer

1:54:48.480 --> 1:54:50.440
 as to whether FAIR is going to change name

1:54:50.440 --> 1:54:53.480
 or whether we're going to change just the meaning of the F.

1:54:53.480 --> 1:54:54.300
 That's a good call.

1:54:54.300 --> 1:54:56.160
 I would keep FAIR and change the meaning of the F.

1:54:56.160 --> 1:54:57.600
 That would be my preference.

1:54:57.600 --> 1:55:02.280
 I would turn the F into fundamental AI research.

1:55:02.280 --> 1:55:03.120
 Oh, that's really good.

1:55:03.120 --> 1:55:04.280
 Within Meta AI.

1:55:04.280 --> 1:55:06.720
 So this would be meta FAIR,

1:55:06.720 --> 1:55:08.320
 but people will call it FAIR, right?

1:55:08.320 --> 1:55:09.320
 Yeah, exactly.

1:55:09.320 --> 1:55:10.160
 I like it.

1:55:10.160 --> 1:55:15.160
 And now Meta AI is part of the Reality Lab.

1:55:16.680 --> 1:55:21.680
 So Meta now, the new Facebook is called Meta

1:55:21.760 --> 1:55:26.760
 and it's kind of divided into Facebook, Instagram, WhatsApp

1:55:30.400 --> 1:55:32.920
 and Reality Lab.

1:55:32.920 --> 1:55:37.920
 And Reality Lab is about AR, VR, telepresence,

1:55:37.920 --> 1:55:40.520
 communication technology and stuff like that.

1:55:40.520 --> 1:55:44.200
 It's kind of the, you can think of it as the sort of,

1:55:44.200 --> 1:55:47.920
 a combination of sort of new products

1:55:47.920 --> 1:55:51.960
 and technology part of Meta.

1:55:51.960 --> 1:55:54.240
 Is that where the touch sensing for robots,

1:55:54.240 --> 1:55:56.120
 I saw that you were posting about that.

1:55:56.120 --> 1:55:58.240
 Touch sensing for robot is part of FAIR actually.

1:55:58.240 --> 1:55:59.080
 That's a FAIR project.

1:55:59.080 --> 1:55:59.920
 Oh, it is.

1:55:59.920 --> 1:56:00.740
 Okay, cool.

1:56:00.740 --> 1:56:03.040
 Yeah, this is also the, no, but there is the other way,

1:56:03.040 --> 1:56:05.680
 the haptic glove, right?

1:56:05.680 --> 1:56:07.640
 Yes, that's more Reality Lab.

1:56:07.640 --> 1:56:10.760
 That's Reality Lab research.

1:56:10.760 --> 1:56:11.960
 Reality Lab research.

1:56:11.960 --> 1:56:14.400
 By the way, the touch sensors are super interesting.

1:56:14.400 --> 1:56:16.120
 Like integrating that modality

1:56:16.120 --> 1:56:20.120
 into the whole sensing suite is very interesting.

1:56:20.120 --> 1:56:23.680
 So what do you think about the Metaverse?

1:56:23.680 --> 1:56:27.820
 What do you think about this whole kind of expansion

1:56:27.820 --> 1:56:30.920
 of the view of the role of Facebook and Meta in the world?

1:56:30.920 --> 1:56:32.520
 Well, Metaverse really should be thought of

1:56:32.520 --> 1:56:35.360
 as the next step in the internet, right?

1:56:35.360 --> 1:56:40.360
 Sort of trying to kind of make the experience

1:56:41.760 --> 1:56:46.280
 more compelling of being connected

1:56:46.280 --> 1:56:48.320
 either with other people or with content.

1:56:49.520 --> 1:56:54.000
 And we are evolved and trained to evolve

1:56:54.000 --> 1:56:58.680
 in 3D environments where we can see other people.

1:56:58.680 --> 1:57:01.080
 We can talk to them when we're near them

1:57:01.080 --> 1:57:04.360
 or an other viewer far away can't hear us,

1:57:04.360 --> 1:57:05.200
 things like that, right?

1:57:05.200 --> 1:57:08.080
 So there's a lot of social conventions

1:57:08.080 --> 1:57:10.800
 that exist in the real world that we can try to transpose.

1:57:10.800 --> 1:57:13.260
 Now, what is going to be eventually the,

1:57:15.120 --> 1:57:16.240
 how compelling is it going to be?

1:57:16.240 --> 1:57:18.740
 Like, is it going to be the case

1:57:18.740 --> 1:57:21.300
 that people are going to be willing to do this

1:57:21.300 --> 1:57:24.600
 if they have to wear a huge pair of goggles all day?

1:57:24.600 --> 1:57:25.520
 Maybe not.

1:57:26.400 --> 1:57:27.480
 But then again, if the experience

1:57:27.480 --> 1:57:30.320
 is sufficiently compelling, maybe so.

1:57:30.320 --> 1:57:32.200
 Or if the device that you have to wear

1:57:32.200 --> 1:57:34.560
 is just basically a pair of glasses,

1:57:34.560 --> 1:57:36.920
 and technology makes sufficient progress for that.

1:57:38.400 --> 1:57:41.560
 AR is a much easier concept to grasp

1:57:41.560 --> 1:57:45.000
 that you're going to have augmented reality glasses

1:57:45.000 --> 1:57:48.640
 that basically contain some sort of virtual assistant

1:57:48.640 --> 1:57:50.280
 that can help you in your daily lives.

1:57:50.280 --> 1:57:51.920
 But at the same time with the AR,

1:57:51.920 --> 1:57:53.480
 you have to contend with reality.

1:57:53.480 --> 1:57:55.880
 With VR, you can completely detach yourself from reality.

1:57:55.880 --> 1:57:57.200
 So it gives you freedom.

1:57:57.200 --> 1:58:00.360
 It might be easier to design worlds in VR.

1:58:00.360 --> 1:58:02.900
 Yeah, but you can imagine the metaverse

1:58:02.900 --> 1:58:06.520
 being a mix, right?

1:58:06.520 --> 1:58:09.280
 Or like, you can have objects that exist in the metaverse

1:58:09.280 --> 1:58:11.200
 that pop up on top of the real world,

1:58:11.200 --> 1:58:14.380
 or only exist in virtual reality.

1:58:14.380 --> 1:58:17.080
 Okay, let me ask the hard question.

1:58:17.080 --> 1:58:18.520
 Oh, because all of this was easy so far.

1:58:18.520 --> 1:58:19.400
 This was easy.

1:58:20.680 --> 1:58:24.280
 The Facebook, now Meta, the social network

1:58:24.280 --> 1:58:28.280
 has been painted by the media as a net negative for society,

1:58:28.280 --> 1:58:30.840
 even destructive and evil at times.

1:58:30.840 --> 1:58:34.080
 You've pushed back against this, defending Facebook.

1:58:34.080 --> 1:58:36.560
 Can you explain your defense?

1:58:36.560 --> 1:58:38.640
 Yeah, so the description,

1:58:38.640 --> 1:58:42.620
 the company that is being described in some media

1:58:43.960 --> 1:58:47.360
 is not the company we know when we work inside.

1:58:47.360 --> 1:58:52.080
 And it could be claimed that a lot of employees

1:58:52.080 --> 1:58:54.600
 are uninformed about what really goes on in the company,

1:58:54.600 --> 1:58:56.520
 but I'm a vice president.

1:58:56.520 --> 1:58:58.920
 I mean, I have a pretty good vision of what goes on.

1:58:58.920 --> 1:59:00.200
 I don't know everything, obviously.

1:59:00.200 --> 1:59:01.860
 I'm not involved in everything,

1:59:01.860 --> 1:59:05.320
 but certainly not in decision about content moderation

1:59:05.320 --> 1:59:06.160
 or anything like this,

1:59:06.160 --> 1:59:10.160
 but I have some decent vision of what goes on.

1:59:10.160 --> 1:59:13.660
 And this evil that is being described, I just don't see it.

1:59:13.660 --> 1:59:18.200
 And then I think there is an easy story to buy,

1:59:18.200 --> 1:59:21.760
 which is that all the bad things in the world

1:59:21.760 --> 1:59:25.160
 and the reason your friend believe crazy stuff,

1:59:25.160 --> 1:59:30.160
 there's an easy scapegoat in social media in general,

1:59:32.800 --> 1:59:34.480
 Facebook in particular.

1:59:34.480 --> 1:59:35.720
 But you have to look at the data.

1:59:35.720 --> 1:59:40.080
 Is it the case that Facebook, for example,

1:59:40.080 --> 1:59:41.660
 polarizes people politically?

1:59:42.720 --> 1:59:45.220
 Are there academic studies that show this?

1:59:45.220 --> 1:59:50.220
 Is it the case that teenagers think of themselves less

1:59:50.280 --> 1:59:52.160
 if they use Instagram more?

1:59:52.160 --> 1:59:57.160
 Is it the case that people get more riled up

1:59:57.280 --> 2:00:02.280
 against opposite sides in a debate or political opinion

2:00:02.680 --> 2:00:05.720
 if they are more on Facebook or if they are less?

2:00:05.720 --> 2:00:10.720
 And study after study show that none of this is true.

2:00:10.880 --> 2:00:12.400
 This is independent studies by academic.

2:00:12.400 --> 2:00:14.580
 They're not funded by Facebook or Meta.

2:00:15.880 --> 2:00:18.640
 Study by Stanford, by some of my colleagues at NYU actually

2:00:18.640 --> 2:00:20.140
 with whom I have no connection.

2:00:20.140 --> 2:00:24.980
 There's a study recently, they paid people,

2:00:24.980 --> 2:00:29.940
 I think it was in former Yugoslavia,

2:00:29.940 --> 2:00:31.820
 I'm not exactly sure in what part,

2:00:31.820 --> 2:00:34.380
 but they paid people to not use Facebook for a while

2:00:34.380 --> 2:00:39.380
 in the period before the anniversary

2:00:40.240 --> 2:00:43.540
 of the Srebrenica massacres.

2:00:43.540 --> 2:00:47.800
 So people get riled up, like should we have a celebration?

2:00:47.800 --> 2:00:51.120
 I mean, a memorial kind of celebration for it or not.

2:00:51.120 --> 2:00:52.540
 So they paid a bunch of people

2:00:52.540 --> 2:00:54.920
 to not use Facebook for a few weeks.

2:00:56.260 --> 2:00:59.580
 And it turns out that those people ended up

2:00:59.580 --> 2:01:02.660
 being more polarized than they were at the beginning

2:01:02.660 --> 2:01:05.300
 and the people who were more on Facebook were less polarized.

2:01:06.660 --> 2:01:10.460
 There's a study from Stanford of economists at Stanford

2:01:10.460 --> 2:01:12.660
 that try to identify the causes

2:01:12.660 --> 2:01:16.000
 of increasing polarization in the US.

2:01:16.000 --> 2:01:17.820
 And it's been going on for 40 years

2:01:17.820 --> 2:01:22.540
 before Mark Zuckerberg was born continuously.

2:01:22.540 --> 2:01:25.620
 And so if there is a cause,

2:01:25.620 --> 2:01:27.620
 it's not Facebook or social media.

2:01:27.620 --> 2:01:29.580
 So you could say if social media just accelerated,

2:01:29.580 --> 2:01:33.060
 but no, I mean, it's basically a continuous evolution

2:01:33.060 --> 2:01:35.820
 by some measure of polarization in the US.

2:01:35.820 --> 2:01:37.660
 And then you compare this with other countries

2:01:37.660 --> 2:01:41.460
 like the West half of Germany

2:01:41.460 --> 2:01:44.700
 because you can go 40 years in the East side

2:01:44.700 --> 2:01:47.380
 or Denmark or other countries.

2:01:47.380 --> 2:01:49.460
 And they use Facebook just as much

2:01:49.460 --> 2:01:50.700
 and they're not getting more polarized,

2:01:50.700 --> 2:01:52.040
 they're getting less polarized.

2:01:52.040 --> 2:01:56.060
 So if you want to look for a causal relationship there,

2:01:57.640 --> 2:01:59.840
 you can find a scapegoat, but you can't find a cause.

2:01:59.840 --> 2:02:01.720
 Now, if you want to fix the problem,

2:02:01.720 --> 2:02:03.180
 you have to find the right cause.

2:02:03.180 --> 2:02:07.720
 And what rise me up is that people now are accusing Facebook

2:02:07.720 --> 2:02:09.300
 of bad deeds that are done by others

2:02:09.300 --> 2:02:12.380
 and those others are we're not doing anything about them.

2:02:12.380 --> 2:02:14.820
 And by the way, those others include the owner

2:02:14.820 --> 2:02:15.660
 of the Wall Street Journal

2:02:15.660 --> 2:02:17.700
 in which all of those papers were published.

2:02:17.700 --> 2:02:20.060
 So I should mention that I'm talking to Schrepp,

2:02:20.060 --> 2:02:23.460
 Mike Schrepp on this podcast and also Mark Zuckerberg

2:02:23.460 --> 2:02:26.340
 and probably these are conversations you can have with them

2:02:26.340 --> 2:02:27.620
 because it's very interesting to me,

2:02:27.620 --> 2:02:31.900
 even if Facebook has some measurable negative effect,

2:02:31.900 --> 2:02:33.780
 you can't just consider that in isolation.

2:02:33.780 --> 2:02:35.940
 You have to consider about all the positive ways

2:02:35.940 --> 2:02:36.820
 that it connects us.

2:02:36.820 --> 2:02:38.140
 So like every technology.

2:02:38.140 --> 2:02:39.660
 It connects people, it's a question.

2:02:39.660 --> 2:02:43.880
 You can't just say like there's an increase in division.

2:02:43.880 --> 2:02:46.100
 Yes, probably Google search engine

2:02:46.100 --> 2:02:47.900
 has created increase in division.

2:02:47.900 --> 2:02:49.900
 But you have to consider about how much information

2:02:49.900 --> 2:02:51.140
 are brought to the world.

2:02:51.140 --> 2:02:53.700
 Like I'm sure Wikipedia created more division.

2:02:53.700 --> 2:02:55.340
 If you just look at the division,

2:02:55.340 --> 2:02:57.700
 we have to look at the full context of the world

2:02:57.700 --> 2:02:59.100
 and they didn't make a better world.

2:02:59.100 --> 2:02:59.940
 And you have to.

2:02:59.940 --> 2:03:01.660
 The printing press has created more division, right?

2:03:01.660 --> 2:03:02.500
 Exactly.

2:03:02.500 --> 2:03:06.900
 I mean, so when the printing press was invented,

2:03:06.900 --> 2:03:10.780
 the first books that were printed were things like the Bible

2:03:10.780 --> 2:03:13.780
 and that allowed people to read the Bible by themselves,

2:03:13.780 --> 2:03:17.400
 not get the message uniquely from priests in Europe.

2:03:17.400 --> 2:03:20.340
 And that created the Protestant movement

2:03:20.340 --> 2:03:23.660
 and 200 years of religious persecution and wars.

2:03:23.660 --> 2:03:26.180
 So that's a bad side effect of the printing press.

2:03:26.180 --> 2:03:28.500
 Social networks aren't being nearly as bad

2:03:28.500 --> 2:03:29.320
 as the printing press,

2:03:29.320 --> 2:03:31.940
 but nobody would say the printing press was a bad idea.

2:03:33.520 --> 2:03:35.100
 Yeah, a lot of it is perception

2:03:35.100 --> 2:03:38.420
 and there's a lot of different incentives operating here.

2:03:38.420 --> 2:03:40.020
 Maybe a quick comment,

2:03:40.020 --> 2:03:42.700
 since you're one of the top leaders at Facebook

2:03:42.700 --> 2:03:46.760
 and at Meta, sorry, that's in the tech space,

2:03:46.760 --> 2:03:49.700
 I'm sure Facebook involves a lot of incredible

2:03:49.700 --> 2:03:52.900
 technological challenges that need to be solved.

2:03:52.900 --> 2:03:55.000
 A lot of it probably is in the computer infrastructure,

2:03:55.000 --> 2:03:58.920
 the hardware, I mean, it's just a huge amount.

2:03:58.920 --> 2:04:03.580
 Maybe can you give me context about how much of Shrepp's life

2:04:03.580 --> 2:04:06.240
 is AI and how much of it is low level compute?

2:04:06.240 --> 2:04:09.580
 How much of it is flying all around doing business stuff?

2:04:09.580 --> 2:04:12.000
 And the same with Mark Zuckerberg.

2:04:12.000 --> 2:04:13.740
 They really focus on AI.

2:04:13.740 --> 2:04:18.740
 I mean, certainly in the run up of the creation of FAIR

2:04:19.520 --> 2:04:24.060
 and for at least a year after that, if not more,

2:04:24.060 --> 2:04:26.700
 Mark was very, very much focused on AI

2:04:26.700 --> 2:04:29.700
 and was spending quite a lot of effort on it.

2:04:29.700 --> 2:04:30.780
 And that's his style.

2:04:30.780 --> 2:04:32.060
 When he gets interested in something,

2:04:32.060 --> 2:04:34.100
 he reads everything about it.

2:04:34.100 --> 2:04:36.860
 He read some of my papers, for example, before I joined.

2:04:39.620 --> 2:04:41.860
 And so he learned a lot about it.

2:04:41.860 --> 2:04:43.740
 He said he liked notes.

2:04:43.740 --> 2:04:44.580
 Right.

2:04:46.460 --> 2:04:51.100
 And Shrepp was really into it also.

2:04:51.100 --> 2:04:52.800
 I mean, Shrepp is really kind of,

2:04:54.780 --> 2:04:57.940
 has something I've tried to preserve also

2:04:57.940 --> 2:05:00.180
 despite my not so young age,

2:05:00.180 --> 2:05:03.180
 which is a sense of wonder about science and technology.

2:05:03.180 --> 2:05:05.260
 And he certainly has that.

2:05:06.300 --> 2:05:07.420
 He's also a wonderful person.

2:05:07.420 --> 2:05:10.380
 I mean, in terms of like as a manager,

2:05:10.380 --> 2:05:12.140
 like dealing with people and everything.

2:05:12.140 --> 2:05:13.240
 Mark also, actually.

2:05:14.540 --> 2:05:18.020
 I mean, they're very human people.

2:05:18.020 --> 2:05:20.600
 In the case of Mark, it's shockingly human

2:05:20.600 --> 2:05:23.180
 given his trajectory.

2:05:25.460 --> 2:05:28.100
 I mean, the personality of him that is painted in the press,

2:05:28.100 --> 2:05:29.620
 it's just completely wrong.

2:05:29.620 --> 2:05:30.460
 Yeah.

2:05:30.460 --> 2:05:31.980
 But you have to know how to play the press.

2:05:31.980 --> 2:05:36.220
 So that's, I put some of that responsibility on him too.

2:05:36.220 --> 2:05:40.980
 You have to, it's like, you know,

2:05:40.980 --> 2:05:44.300
 like the director, the conductor of an orchestra,

2:05:44.300 --> 2:05:46.980
 you have to play the press and the public

2:05:46.980 --> 2:05:48.020
 in a certain kind of way

2:05:48.020 --> 2:05:49.740
 where you convey your true self to them.

2:05:49.740 --> 2:05:51.060
 If there's a depth and kindness to it.

2:05:51.060 --> 2:05:51.900
 It's hard.

2:05:51.900 --> 2:05:53.740
 And it's probably not the best at it.

2:05:53.740 --> 2:05:54.620
 So, yeah.

2:05:56.460 --> 2:05:57.700
 You have to learn.

2:05:57.700 --> 2:06:00.460
 And it's sad to see, and I'll talk to him about it,

2:06:00.460 --> 2:06:04.060
 but Shrep is slowly stepping down.

2:06:04.060 --> 2:06:07.500
 It's always sad to see folks sort of be there

2:06:07.500 --> 2:06:09.420
 for a long time and slowly.

2:06:09.420 --> 2:06:11.220
 I guess time is sad.

2:06:11.220 --> 2:06:14.780
 I think he's done the thing he set out to do.

2:06:14.780 --> 2:06:17.560
 And, you know, he's got, you know,

2:06:19.700 --> 2:06:21.420
 family priorities and stuff like that.

2:06:21.420 --> 2:06:26.420
 And I understand, you know, after 13 years or something.

2:06:27.900 --> 2:06:28.900
 It's been a good run.

2:06:28.900 --> 2:06:32.100
 Which in Silicon Valley is basically a lifetime.

2:06:32.100 --> 2:06:32.940
 Yeah.

2:06:32.940 --> 2:06:35.000
 You know, because, you know, it's dog years.

2:06:35.000 --> 2:06:37.600
 So, NeurIPS, the conference just wrapped up.

2:06:38.660 --> 2:06:40.580
 Let me just go back to something else.

2:06:40.580 --> 2:06:42.500
 You posted that a paper you coauthored

2:06:42.500 --> 2:06:44.440
 was rejected from NeurIPS.

2:06:44.440 --> 2:06:47.160
 As you said, proudly, in quotes, rejected.

2:06:48.020 --> 2:06:48.940
 It's a joke.

2:06:48.940 --> 2:06:49.760
 Yeah, I know.

2:06:49.760 --> 2:06:53.260
 So, can you describe this paper?

2:06:53.260 --> 2:06:55.700
 And like, what was the idea in it?

2:06:55.700 --> 2:06:59.060
 And also, maybe this is a good opportunity to ask

2:06:59.060 --> 2:07:01.740
 what are the pros and cons, what works and what doesn't

2:07:01.740 --> 2:07:03.620
 about the review process?

2:07:03.620 --> 2:07:04.980
 Yeah, let me talk about the paper first.

2:07:04.980 --> 2:07:08.260
 I'll talk about the review process afterwards.

2:07:09.220 --> 2:07:10.700
 The paper is called VicReg.

2:07:10.700 --> 2:07:12.540
 So, this is, I mentioned that before.

2:07:12.540 --> 2:07:14.900
 Variance, invariance, covariance, regularization.

2:07:14.900 --> 2:07:18.260
 And it's a technique, a noncontrastive learning technique

2:07:18.260 --> 2:07:21.300
 for what I call joint embedding architecture.

2:07:21.300 --> 2:07:23.380
 So, SiameseNets are an example

2:07:23.380 --> 2:07:24.860
 of joint embedding architecture.

2:07:24.860 --> 2:07:26.580
 So, joint embedding architecture is,

2:07:29.220 --> 2:07:30.600
 let me back up a little bit, right?

2:07:30.600 --> 2:07:33.300
 So, if you want to do self supervised learning,

2:07:33.300 --> 2:07:35.140
 you can do it by prediction.

2:07:36.440 --> 2:07:37.920
 So, let's say you want to train the system

2:07:37.920 --> 2:07:38.760
 to predict video, right?

2:07:38.760 --> 2:07:42.500
 You show it a video clip and you train the system

2:07:42.500 --> 2:07:45.040
 to predict the next, the continuation of that video clip.

2:07:45.040 --> 2:07:47.800
 Now, because you need to handle uncertainty,

2:07:47.800 --> 2:07:51.580
 because there are many continuations that are plausible,

2:07:51.580 --> 2:07:54.020
 you need to have, you need to handle this in some way.

2:07:54.020 --> 2:07:56.660
 You need to have a way for the system

2:07:56.660 --> 2:08:00.620
 to be able to produce multiple predictions.

2:08:00.620 --> 2:08:03.500
 And the way, the only way I know to do this

2:08:03.500 --> 2:08:05.420
 is through what's called a latent variable.

2:08:05.420 --> 2:08:08.780
 So, you have some sort of hidden vector

2:08:08.780 --> 2:08:11.180
 of a variable that you can vary over a set

2:08:11.180 --> 2:08:12.580
 or draw from a distribution.

2:08:12.580 --> 2:08:14.500
 And as you vary this vector over a set,

2:08:14.500 --> 2:08:16.000
 the output, the prediction varies

2:08:16.000 --> 2:08:18.740
 over a set of plausible predictions, okay?

2:08:18.740 --> 2:08:19.580
 So, that's called,

2:08:19.580 --> 2:08:23.240
 I call this a generative latent variable model.

2:08:24.140 --> 2:08:24.980
 Got it.

2:08:24.980 --> 2:08:27.060
 Okay, now there is an alternative to this,

2:08:27.060 --> 2:08:28.700
 to handle uncertainty.

2:08:28.700 --> 2:08:33.380
 And instead of directly predicting the next frames

2:08:33.380 --> 2:08:38.380
 of the clip, you also run those through another neural net.

2:08:41.080 --> 2:08:42.500
 So, you now have two neural nets,

2:08:42.500 --> 2:08:47.500
 one that looks at the initial segment of the video clip,

2:08:48.700 --> 2:08:51.260
 and another one that looks at the continuation

2:08:51.260 --> 2:08:52.460
 during training, right?

2:08:53.560 --> 2:08:56.300
 And what you're trying to do is learn a representation

2:08:57.680 --> 2:09:00.780
 of those two video clips that is maximally informative

2:09:00.780 --> 2:09:03.460
 about the video clips themselves,

2:09:03.460 --> 2:09:07.180
 but is such that you can predict the representation

2:09:07.180 --> 2:09:08.580
 of the second video clip

2:09:08.580 --> 2:09:12.340
 from the representation of the first one easily, okay?

2:09:12.340 --> 2:09:13.580
 And you can sort of formalize this

2:09:13.580 --> 2:09:15.340
 in terms of maximizing mutual information

2:09:15.340 --> 2:09:18.140
 and some stuff like that, but it doesn't matter.

2:09:18.140 --> 2:09:21.140
 What you want is informative representations

2:09:24.540 --> 2:09:27.500
 of the two video clips that are mutually predictable.

2:09:28.460 --> 2:09:30.900
 What that means is that there's a lot of details

2:09:30.900 --> 2:09:33.200
 in the second video clips that are irrelevant.

2:09:36.500 --> 2:09:40.500
 Let's say a video clip consists in a camera panning

2:09:40.500 --> 2:09:43.340
 the scene, there's gonna be a piece of that room

2:09:43.340 --> 2:09:46.180
 that is gonna be revealed, and I can somewhat predict

2:09:46.180 --> 2:09:48.060
 what that room is gonna look like,

2:09:48.060 --> 2:09:50.220
 but I may not be able to predict the details

2:09:50.220 --> 2:09:52.300
 of the texture of the ground

2:09:52.300 --> 2:09:54.500
 and where the tiles are ending and stuff like that, right?

2:09:54.500 --> 2:09:56.360
 So, those are irrelevant details

2:09:56.360 --> 2:09:59.620
 that perhaps my representation will eliminate.

2:09:59.620 --> 2:10:03.680
 And so, what I need is to train this second neural net

2:10:03.680 --> 2:10:08.680
 in such a way that whenever the continuation video clip

2:10:08.680 --> 2:10:12.220
 varies over all the plausible continuations,

2:10:13.600 --> 2:10:15.600
 the representation doesn't change.

2:10:15.600 --> 2:10:16.440
 Got it.

2:10:16.440 --> 2:10:18.100
 So, it's the, yeah, yeah, got it.

2:10:18.100 --> 2:10:20.860
 Over the space of the representations,

2:10:20.860 --> 2:10:21.880
 doing the same kind of thing

2:10:21.880 --> 2:10:24.300
 as you do with similarity learning.

2:10:24.300 --> 2:10:25.680
 Right.

2:10:25.680 --> 2:10:28.840
 So, these are two ways to handle multimodality

2:10:28.840 --> 2:10:29.680
 in a prediction, right?

2:10:29.680 --> 2:10:32.280
 In the first way, you parameterize the prediction

2:10:32.280 --> 2:10:33.480
 with a latent variable,

2:10:33.480 --> 2:10:35.800
 but you predict pixels essentially, right?

2:10:35.800 --> 2:10:38.400
 In the second one, you don't predict pixels,

2:10:38.400 --> 2:10:40.720
 you predict an abstract representation of pixels,

2:10:40.720 --> 2:10:43.480
 and you guarantee that this abstract representation

2:10:43.480 --> 2:10:46.200
 has as much information as possible about the input,

2:10:46.200 --> 2:10:47.080
 but sort of, you know,

2:10:47.080 --> 2:10:49.740
 drops all the stuff that you really can't predict,

2:10:49.740 --> 2:10:50.580
 essentially.

2:10:52.120 --> 2:10:53.880
 I used to be a big fan of the first approach.

2:10:53.880 --> 2:10:55.880
 And in fact, in this paper with Hicham Mishra,

2:10:55.880 --> 2:10:58.400
 this blog post, the Dark Matter Intelligence,

2:10:58.400 --> 2:10:59.760
 I was kind of advocating for this.

2:10:59.760 --> 2:11:01.600
 And in the last year and a half,

2:11:01.600 --> 2:11:02.840
 I've completely changed my mind.

2:11:02.840 --> 2:11:04.640
 I'm now a big fan of the second one.

2:11:04.640 --> 2:11:09.640
 And it's because of a small collection of algorithms

2:11:10.000 --> 2:11:13.680
 that have been proposed over the last year and a half or so,

2:11:13.680 --> 2:11:17.800
 two years, to do this, including vCraig,

2:11:17.800 --> 2:11:19.600
 its predecessor called Barlow Twins,

2:11:19.600 --> 2:11:23.560
 which I mentioned, a method from our friends at DeepMind

2:11:23.560 --> 2:11:28.500
 called BYOL, and there's a bunch of others now

2:11:28.500 --> 2:11:29.600
 that kind of work similarly.

2:11:29.600 --> 2:11:32.600
 So, they're all based on this idea of joint embedding.

2:11:32.600 --> 2:11:34.660
 Some of them have an explicit criterion

2:11:34.660 --> 2:11:36.640
 that is an approximation of mutual information.

2:11:36.640 --> 2:11:39.400
 Some others at BYOL work, but we don't really know why.

2:11:39.400 --> 2:11:41.240
 And there's been like lots of theoretical papers

2:11:41.240 --> 2:11:42.360
 about why BYOL works.

2:11:42.360 --> 2:11:43.940
 No, it's not that, because we take it out

2:11:43.940 --> 2:11:46.040
 and it still works, and blah, blah, blah.

2:11:46.040 --> 2:11:47.800
 I mean, so there's like a big debate,

2:11:47.800 --> 2:11:51.540
 but the important point is that we now have a collection

2:11:51.540 --> 2:11:53.720
 of noncontrastive joint embedding methods,

2:11:53.720 --> 2:11:56.400
 which I think is the best thing since sliced bread.

2:11:56.400 --> 2:11:58.320
 So, I'm super excited about this

2:11:58.320 --> 2:12:01.200
 because I think it's our best shot

2:12:01.200 --> 2:12:02.720
 for techniques that would allow us

2:12:02.720 --> 2:12:06.360
 to kind of build predictive world models.

2:12:06.360 --> 2:12:07.440
 And at the same time,

2:12:07.440 --> 2:12:09.920
 learn hierarchical representations of the world,

2:12:09.920 --> 2:12:11.840
 where what matters about the world is preserved

2:12:11.840 --> 2:12:14.440
 and what is irrelevant is eliminated.

2:12:14.440 --> 2:12:15.880
 And by the way, the representations,

2:12:15.880 --> 2:12:19.200
 the before and after, is in the space

2:12:19.200 --> 2:12:22.320
 in a sequence of images, or is it for single images?

2:12:22.320 --> 2:12:24.600
 It would be either for a single image, for a sequence.

2:12:24.600 --> 2:12:25.660
 It doesn't have to be images.

2:12:25.660 --> 2:12:26.680
 This could be applied to text.

2:12:26.680 --> 2:12:28.560
 This could be applied to just about any signal.

2:12:28.560 --> 2:12:32.960
 I'm looking for methods that are generally applicable

2:12:32.960 --> 2:12:36.200
 that are not specific to one particular modality.

2:12:36.200 --> 2:12:37.640
 It could be audio or whatever.

2:12:37.640 --> 2:12:38.460
 Got it.

2:12:38.460 --> 2:12:40.120
 So, what's the story behind this paper?

2:12:40.120 --> 2:12:43.480
 This paper is describing one such method?

2:12:43.480 --> 2:12:44.480
 It's this vcrack method.

2:12:44.480 --> 2:12:45.720
 So, this is coauthored.

2:12:45.720 --> 2:12:49.280
 The first author is a student called Adrien Barne,

2:12:49.280 --> 2:12:52.680
 who is a resident PhD student at Fair Paris,

2:12:52.680 --> 2:12:55.800
 who is coadvised by me and Jean Ponce,

2:12:55.800 --> 2:12:58.720
 who is a professor at École Normale Supérieure,

2:12:58.720 --> 2:13:00.680
 also a research director at INRIA.

2:13:01.600 --> 2:13:03.600
 So, this is a wonderful program in France

2:13:03.600 --> 2:13:06.640
 where PhD students can basically do their PhD in industry,

2:13:06.640 --> 2:13:08.960
 and that's kind of what's happening here.

2:13:10.440 --> 2:13:15.440
 And this paper is a followup on this Bardo Twin paper

2:13:15.480 --> 2:13:18.360
 by my former postdoc, now Stéphane Denis,

2:13:18.360 --> 2:13:21.560
 with Li Jing and Iorij Bontar

2:13:21.560 --> 2:13:24.720
 and a bunch of other people from Fair.

2:13:24.720 --> 2:13:27.840
 And one of the main criticism from reviewers

2:13:27.840 --> 2:13:31.400
 is that vcrack is not different enough from Bardo Twins.

2:13:31.400 --> 2:13:36.400
 But, you know, my impression is that it's, you know,

2:13:36.720 --> 2:13:39.880
 Bardo Twins with a few bugs fixed, essentially,

2:13:39.880 --> 2:13:43.200
 and in the end, this is what people will use.

2:13:43.200 --> 2:13:44.520
 Right, so.

2:13:44.520 --> 2:13:47.080
 But, you know, I'm used to stuff

2:13:47.080 --> 2:13:49.040
 that I submit being rejected for a while.

2:13:49.040 --> 2:13:51.360
 So, it might be rejected and actually exceptionally well cited

2:13:51.360 --> 2:13:52.280
 because people use it.

2:13:52.280 --> 2:13:54.360
 Well, it's already cited like a bunch of times.

2:13:54.360 --> 2:13:57.600
 So, I mean, the question is then to the deeper question

2:13:57.600 --> 2:14:00.240
 about peer review and conferences.

2:14:00.240 --> 2:14:02.600
 I mean, computer science is a field that's kind of unique

2:14:02.600 --> 2:14:04.960
 that the conference is highly prized.

2:14:04.960 --> 2:14:05.800
 That's one.

2:14:05.800 --> 2:14:06.640
 Right.

2:14:06.640 --> 2:14:09.120
 And it's interesting because the peer review process there

2:14:09.120 --> 2:14:11.080
 is similar, I suppose, to journals,

2:14:11.080 --> 2:14:13.640
 but it's accelerated significantly.

2:14:13.640 --> 2:14:16.560
 Well, not significantly, but it goes fast.

2:14:16.560 --> 2:14:19.760
 And it's a nice way to get stuff out quickly,

2:14:19.760 --> 2:14:20.800
 to peer review it quickly,

2:14:20.800 --> 2:14:22.640
 go to present it quickly to the community.

2:14:22.640 --> 2:14:25.160
 So, not quickly, but quicker.

2:14:25.160 --> 2:14:26.000
 Yeah.

2:14:26.000 --> 2:14:27.840
 But nevertheless, it has many of the same flaws

2:14:27.840 --> 2:14:29.120
 of peer review,

2:14:29.120 --> 2:14:31.520
 because it's a limited number of people look at it.

2:14:31.520 --> 2:14:32.800
 There's bias and the following,

2:14:32.800 --> 2:14:35.600
 like that if you want to do new ideas,

2:14:35.600 --> 2:14:37.080
 you're going to get pushback.

2:14:38.120 --> 2:14:42.120
 There's self interested people that kind of can infer

2:14:42.120 --> 2:14:45.320
 who submitted it and kind of, you know,

2:14:45.320 --> 2:14:47.760
 be cranky about it, all that kind of stuff.

2:14:47.760 --> 2:14:51.040
 Yeah, I mean, there's a lot of social phenomena there.

2:14:51.040 --> 2:14:53.200
 There's one social phenomenon, which is that

2:14:53.200 --> 2:14:56.760
 because the field has been growing exponentially,

2:14:56.760 --> 2:14:58.560
 the vast majority of people in the field

2:14:58.560 --> 2:15:00.000
 are extremely junior.

2:15:00.000 --> 2:15:00.840
 Yeah.

2:15:00.840 --> 2:15:01.920
 So, as a consequence,

2:15:01.920 --> 2:15:04.880
 and that's just a consequence of the field growing, right?

2:15:04.880 --> 2:15:07.840
 So, as the number of, as the size of the field

2:15:07.840 --> 2:15:08.920
 kind of starts saturating,

2:15:08.920 --> 2:15:11.440
 you will have less of that problem

2:15:11.440 --> 2:15:15.360
 of reviewers being very inexperienced.

2:15:15.360 --> 2:15:20.160
 A consequence of this is that, you know, young reviewers,

2:15:20.160 --> 2:15:22.840
 I mean, there's a phenomenon which is that

2:15:22.840 --> 2:15:24.640
 reviewers try to make their life easy

2:15:24.640 --> 2:15:27.440
 and to make their life easy when reviewing a paper

2:15:27.440 --> 2:15:28.280
 is very simple.

2:15:28.280 --> 2:15:29.960
 You just have to find a flaw in the paper, right?

2:15:29.960 --> 2:15:34.480
 So, basically they see the task as finding flaws in papers

2:15:34.480 --> 2:15:36.720
 and most papers have flaws, even the good ones.

2:15:36.720 --> 2:15:38.160
 Yeah.

2:15:38.160 --> 2:15:41.480
 So, it's easy to, you know, to do that.

2:15:41.480 --> 2:15:46.440
 Your job is easier as a reviewer if you just focus on this.

2:15:46.440 --> 2:15:49.640
 But what's important is like,

2:15:49.640 --> 2:15:51.520
 is there a new idea in that paper

2:15:51.520 --> 2:15:54.120
 that is likely to influence?

2:15:54.120 --> 2:15:56.240
 It doesn't matter if the experiments are not that great,

2:15:56.240 --> 2:16:00.680
 if the protocol is, you know, so, so, you know,

2:16:00.680 --> 2:16:01.520
 things like that.

2:16:01.520 --> 2:16:05.040
 As long as there is a worthy idea in it

2:16:05.040 --> 2:16:08.080
 that will influence the way people think about the problem,

2:16:09.200 --> 2:16:11.160
 even if they make it better, you know, eventually,

2:16:11.160 --> 2:16:15.480
 I think that's really what makes a paper useful.

2:16:15.480 --> 2:16:19.520
 And so, this combination of social phenomena

2:16:19.520 --> 2:16:24.200
 creates a disease that has plagued, you know,

2:16:24.200 --> 2:16:26.680
 other fields in the past, like speech recognition,

2:16:26.680 --> 2:16:28.560
 where basically, you know, people chase numbers

2:16:28.560 --> 2:16:33.560
 on benchmarks and it's much easier to get a paper accepted

2:16:34.680 --> 2:16:37.040
 if it brings an incremental improvement

2:16:37.040 --> 2:16:42.040
 on a sort of mainstream well accepted method or problem.

2:16:44.160 --> 2:16:46.040
 And those are, to me, boring papers.

2:16:46.040 --> 2:16:47.880
 I mean, they're not useless, right?

2:16:47.880 --> 2:16:50.560
 Because industry, you know, strives

2:16:50.560 --> 2:16:52.400
 on those kinds of progress,

2:16:52.400 --> 2:16:54.080
 but they're not the ones that I'm interested in,

2:16:54.080 --> 2:16:55.680
 in terms of like new concepts and new ideas.

2:16:55.680 --> 2:16:59.320
 So, papers that are really trying to strike

2:16:59.320 --> 2:17:02.600
 kind of new advances generally don't make it.

2:17:02.600 --> 2:17:04.240
 Now, thankfully we have Archive.

2:17:04.240 --> 2:17:05.320
 Archive, exactly.

2:17:05.320 --> 2:17:08.160
 And then there's open review type of situations

2:17:08.160 --> 2:17:11.680
 where you, and then, I mean, Twitter's a kind of open review.

2:17:11.680 --> 2:17:13.880
 I'm a huge believer that review should be done

2:17:13.880 --> 2:17:15.720
 by thousands of people, not two people.

2:17:15.720 --> 2:17:16.760
 I agree.

2:17:16.760 --> 2:17:19.560
 And so Archive, like do you see a future

2:17:19.560 --> 2:17:21.240
 where a lot of really strong papers,

2:17:21.240 --> 2:17:23.640
 it's already the present, but a growing future

2:17:23.640 --> 2:17:25.320
 where it'll just be Archive

2:17:26.280 --> 2:17:31.280
 and you're presenting an ongoing continuous conference

2:17:31.280 --> 2:17:35.560
 called Twitter slash the internet slash Archive Sanity.

2:17:35.560 --> 2:17:38.040
 Andre just released a new version.

2:17:38.040 --> 2:17:40.920
 So just not, you know, not being so elitist

2:17:40.920 --> 2:17:43.440
 about this particular gating.

2:17:43.440 --> 2:17:44.960
 It's not a question of being elitist or not.

2:17:44.960 --> 2:17:49.960
 It's a question of being basically recommendation

2:17:50.120 --> 2:17:53.400
 and sort of approvals for people who don't see themselves

2:17:53.400 --> 2:17:55.880
 as having the ability to do so by themselves, right?

2:17:55.880 --> 2:17:57.320
 And so it saves time, right?

2:17:57.320 --> 2:18:00.000
 If you rely on other people's opinion

2:18:00.000 --> 2:18:03.760
 and you trust those people or those groups

2:18:03.760 --> 2:18:08.760
 to evaluate a paper for you, that saves you time

2:18:09.960 --> 2:18:12.680
 because, you know, you don't have to like scrutinize

2:18:12.680 --> 2:18:15.200
 the paper as much, you know, is brought to your attention.

2:18:15.200 --> 2:18:16.680
 I mean, it's the whole idea of sort of, you know,

2:18:16.680 --> 2:18:18.760
 collective recommender system, right?

2:18:18.760 --> 2:18:22.360
 So I actually thought about this a lot, you know,

2:18:22.360 --> 2:18:24.200
 about 10, 15 years ago,

2:18:24.200 --> 2:18:27.080
 because there were discussions at NIPS

2:18:27.080 --> 2:18:30.040
 and, you know, and we're about to create iClear

2:18:30.040 --> 2:18:31.200
 with Yoshua Bengio.

2:18:31.200 --> 2:18:34.880
 And so I wrote a document kind of describing

2:18:34.880 --> 2:18:38.040
 a reviewing system, which basically was, you know,

2:18:38.040 --> 2:18:39.720
 you post your paper on some repository,

2:18:39.720 --> 2:18:42.560
 let's say archive or now could be open review.

2:18:42.560 --> 2:18:46.240
 And then you can form a reviewing entity,

2:18:46.240 --> 2:18:48.840
 which is equivalent to a reviewing board, you know,

2:18:48.840 --> 2:18:53.840
 of a journal or program committee of a conference.

2:18:53.960 --> 2:18:55.600
 You have to list the members.

2:18:55.600 --> 2:19:00.000
 And then that group reviewing entity can choose

2:19:00.000 --> 2:19:03.720
 to review a particular paper spontaneously or not.

2:19:03.720 --> 2:19:05.600
 There is no exclusive relationship anymore

2:19:05.600 --> 2:19:09.200
 between a paper and a venue or reviewing entity.

2:19:09.200 --> 2:19:11.240
 Any reviewing entity can review any paper

2:19:12.720 --> 2:19:14.080
 or may choose not to.

2:19:15.000 --> 2:19:16.640
 And then, you know, given evaluation,

2:19:16.640 --> 2:19:17.920
 it's not published, not published,

2:19:17.920 --> 2:19:20.320
 it's just an evaluation and a comment,

2:19:20.320 --> 2:19:23.680
 which would be public, signed by the reviewing entity.

2:19:23.680 --> 2:19:25.880
 And if it's signed by a reviewing entity,

2:19:25.880 --> 2:19:27.760
 you know, it's one of the members of reviewing entity.

2:19:27.760 --> 2:19:30.680
 So if the reviewing entity is, you know,

2:19:30.680 --> 2:19:33.720
 Lex Friedman's, you know, preferred papers, right?

2:19:33.720 --> 2:19:35.640
 You know, it's Lex Friedman writing the review.

2:19:35.640 --> 2:19:40.640
 Yes, so for me, that's a beautiful system, I think.

2:19:40.920 --> 2:19:42.880
 But in addition to that,

2:19:42.880 --> 2:19:45.800
 it feels like there should be a reputation system

2:19:45.800 --> 2:19:47.480
 for the reviewers.

2:19:47.480 --> 2:19:49.040
 For the reviewing entities,

2:19:49.040 --> 2:19:50.280
 not the reviewers individually.

2:19:50.280 --> 2:19:51.720
 The reviewing entities, sure.

2:19:51.720 --> 2:19:53.880
 But even within that, the reviewers too,

2:19:53.880 --> 2:19:57.120
 because there's another thing here.

2:19:57.120 --> 2:19:59.360
 It's not just the reputation,

2:19:59.360 --> 2:20:02.680
 it's an incentive for an individual person to do great.

2:20:02.680 --> 2:20:05.040
 Right now, in the academic setting,

2:20:05.040 --> 2:20:07.880
 the incentive is kind of internal,

2:20:07.880 --> 2:20:09.240
 just wanting to do a good job.

2:20:09.240 --> 2:20:11.240
 But honestly, that's not a strong enough incentive

2:20:11.240 --> 2:20:13.720
 to do a really good job in reading a paper,

2:20:13.720 --> 2:20:16.400
 in finding the beautiful amidst the mistakes and the flaws

2:20:16.400 --> 2:20:17.760
 and all that kind of stuff.

2:20:17.760 --> 2:20:20.760
 Like if you're the person that first discovered

2:20:20.760 --> 2:20:25.120
 a powerful paper, and you get to be proud of that discovery,

2:20:25.120 --> 2:20:27.520
 then that gives a huge incentive to you.

2:20:27.520 --> 2:20:29.280
 That's a big part of my proposal, actually,

2:20:29.280 --> 2:20:31.280
 where I describe that as, you know,

2:20:31.280 --> 2:20:35.280
 if your evaluation of papers is predictive

2:20:35.280 --> 2:20:37.560
 of future success, okay,

2:20:37.560 --> 2:20:40.920
 then your reputation should go up as a reviewing entity.

2:20:42.560 --> 2:20:43.760
 So yeah, exactly.

2:20:43.760 --> 2:20:46.280
 I mean, I even had a master's student

2:20:46.280 --> 2:20:49.560
 who was a master's student in library science

2:20:49.560 --> 2:20:52.480
 and computer science actually kind of work out exactly

2:20:52.480 --> 2:20:55.160
 how that should work with formulas and everything.

2:20:55.160 --> 2:20:56.800
 So in terms of implementation,

2:20:56.800 --> 2:20:58.640
 do you think that's something that's doable?

2:20:58.640 --> 2:20:59.720
 I mean, I've been sort of, you know,

2:20:59.720 --> 2:21:02.080
 talking about this to sort of various people

2:21:02.080 --> 2:21:05.960
 like, you know, Andrew McCallum, who started Open Review.

2:21:05.960 --> 2:21:07.800
 And the reason why we picked Open Review

2:21:07.800 --> 2:21:09.120
 for iClear initially,

2:21:09.120 --> 2:21:11.440
 even though it was very early for them,

2:21:11.440 --> 2:21:14.320
 is because my hope was that iClear,

2:21:14.320 --> 2:21:16.760
 it was eventually going to kind of

2:21:16.760 --> 2:21:18.600
 inaugurate this type of system.

2:21:18.600 --> 2:21:22.240
 So iClear kept the idea of Open Reviews.

2:21:22.240 --> 2:21:23.840
 So where the reviews are, you know,

2:21:23.840 --> 2:21:27.320
 published with a paper, which I think is very useful,

2:21:27.320 --> 2:21:29.800
 but in many ways that's kind of reverted

2:21:29.800 --> 2:21:33.280
 to kind of more of a conventional type conferences

2:21:33.280 --> 2:21:34.120
 for everything else.

2:21:34.120 --> 2:21:37.800
 And that, I mean, I don't run iClear.

2:21:37.800 --> 2:21:41.200
 I'm just the president of the foundation,

2:21:41.200 --> 2:21:44.120
 but you know, people who run it

2:21:44.120 --> 2:21:45.680
 should make decisions about how to run it.

2:21:45.680 --> 2:21:48.560
 And I'm not going to tell them because they are volunteers

2:21:48.560 --> 2:21:50.360
 and I'm really thankful that they do that.

2:21:50.360 --> 2:21:53.040
 So, but I'm saddened by the fact

2:21:53.040 --> 2:21:57.120
 that we're not being innovative enough.

2:21:57.120 --> 2:21:57.960
 Yeah, me too.

2:21:57.960 --> 2:21:59.640
 I hope that changes.

2:21:59.640 --> 2:22:00.480
 Yeah.

2:22:00.480 --> 2:22:02.040
 Cause the communication science broadly,

2:22:02.040 --> 2:22:04.200
 but communication computer science ideas

2:22:05.440 --> 2:22:08.400
 is how you make those ideas have impact, I think.

2:22:08.400 --> 2:22:11.440
 Yeah, and I think, you know, a lot of this is

2:22:11.440 --> 2:22:16.200
 because people have in their mind kind of an objective,

2:22:16.200 --> 2:22:19.120
 which is, you know, fairness for authors

2:22:19.120 --> 2:22:22.600
 and the ability to count points basically

2:22:22.600 --> 2:22:24.880
 and give credits accurately.

2:22:24.880 --> 2:22:28.880
 But that comes at the expense of the progress of science.

2:22:28.880 --> 2:22:29.720
 So to some extent,

2:22:29.720 --> 2:22:32.160
 we're slowing down the progress of science.

2:22:32.160 --> 2:22:34.440
 And are we actually achieving fairness?

2:22:34.440 --> 2:22:35.920
 And we're not achieving fairness.

2:22:35.920 --> 2:22:37.880
 You know, we still have biases.

2:22:37.880 --> 2:22:39.840
 You know, we're doing, you know, a double blind review,

2:22:39.840 --> 2:22:44.360
 but you know, the biases are still there.

2:22:44.360 --> 2:22:46.720
 There are different kinds of biases.

2:22:46.720 --> 2:22:49.360
 You write that the phenomenon of emergence,

2:22:49.360 --> 2:22:51.680
 collective behavior exhibited by a large collection

2:22:51.680 --> 2:22:54.280
 of simple elements in interaction

2:22:54.280 --> 2:22:55.760
 is one of the things that got you

2:22:55.760 --> 2:22:57.760
 into neural nets in the first place.

2:22:57.760 --> 2:22:59.120
 I love cellular automata.

2:22:59.120 --> 2:23:02.000
 I love simple interacting elements

2:23:02.000 --> 2:23:04.040
 and the things that emerge from them.

2:23:04.040 --> 2:23:07.880
 Do you think we understand how complex systems can emerge

2:23:07.880 --> 2:23:11.080
 from such simple components that interact simply?

2:23:11.080 --> 2:23:12.320
 No, we don't.

2:23:12.320 --> 2:23:13.160
 It's a big mystery.

2:23:13.160 --> 2:23:14.480
 Also, it's a mystery for physicists.

2:23:14.480 --> 2:23:16.040
 It's a mystery for biologists.

2:23:17.000 --> 2:23:22.000
 You know, how is it that the universe around us

2:23:22.000 --> 2:23:25.120
 seems to be increasing in complexity and not decreasing?

2:23:25.120 --> 2:23:29.640
 I mean, that is a kind of curious property of physics

2:23:29.640 --> 2:23:32.320
 that despite the second law of thermodynamics,

2:23:32.320 --> 2:23:35.960
 we seem to be, you know, evolution and learning

2:23:35.960 --> 2:23:39.640
 and et cetera seems to be kind of at least locally

2:23:40.640 --> 2:23:44.000
 to increase complexity and not decrease it.

2:23:44.000 --> 2:23:46.520
 So perhaps the ultimate purpose of the universe

2:23:46.520 --> 2:23:49.040
 is to just get more complex.

2:23:49.040 --> 2:23:54.040
 Have these, I mean, small pockets of beautiful complexity.

2:23:55.120 --> 2:23:57.120
 Does that, cellular automata,

2:23:57.120 --> 2:23:59.680
 these kinds of emergence of complex systems

2:23:59.680 --> 2:24:04.120
 give you some intuition or guide your understanding

2:24:04.120 --> 2:24:06.680
 of machine learning systems and neural networks and so on?

2:24:06.680 --> 2:24:09.440
 Or are these, for you right now, disparate concepts?

2:24:09.440 --> 2:24:10.880
 Well, it got me into it.

2:24:10.880 --> 2:24:15.600
 You know, I discovered the existence of the perceptron

2:24:15.600 --> 2:24:19.280
 when I was a college student, you know, by reading a book

2:24:19.280 --> 2:24:21.680
 and it was a debate between Chomsky and Piaget

2:24:21.680 --> 2:24:25.920
 and Seymour Papert from MIT was kind of singing the praise

2:24:25.920 --> 2:24:27.400
 of the perceptron in that book.

2:24:27.400 --> 2:24:29.760
 And I, the first time I heard about the running machine,

2:24:29.760 --> 2:24:31.360
 right, so I started digging the literature

2:24:31.360 --> 2:24:33.560
 and I found those paper, those books,

2:24:33.560 --> 2:24:37.120
 which were basically transcription of workshops

2:24:37.120 --> 2:24:39.880
 or conferences from the fifties and sixties

2:24:39.880 --> 2:24:42.160
 about self organizing systems.

2:24:42.160 --> 2:24:44.560
 So there were, there was a series of conferences

2:24:44.560 --> 2:24:48.160
 on self organizing systems and there's books on this.

2:24:48.160 --> 2:24:50.200
 Some of them are, you can actually get them

2:24:50.200 --> 2:24:53.240
 at the internet archive, you know, the digital version.

2:24:55.120 --> 2:24:58.280
 And there are like fascinating articles in there by,

2:24:58.280 --> 2:25:00.360
 there's a guy whose name has been largely forgotten,

2:25:00.360 --> 2:25:04.520
 Heinz von Förster, he's a German physicist

2:25:04.520 --> 2:25:07.240
 who immigrated to the US and worked

2:25:07.240 --> 2:25:11.320
 on self organizing systems in the fifties.

2:25:11.320 --> 2:25:13.800
 And in the sixties he created at University of Illinois

2:25:13.800 --> 2:25:16.440
 at Urbana Champagne, he created the biological

2:25:16.440 --> 2:25:20.440
 computer laboratory, BCL, which was all about neural nets.

2:25:21.680 --> 2:25:23.440
 Unfortunately, that was kind of towards the end

2:25:23.440 --> 2:25:24.920
 of the popularity of neural nets.

2:25:24.920 --> 2:25:27.760
 So that lab never kind of strived very much,

2:25:27.760 --> 2:25:30.360
 but he wrote a bunch of papers about self organization

2:25:30.360 --> 2:25:33.480
 and about the mystery of self organization.

2:25:33.480 --> 2:25:37.000
 An example he has is you take, imagine you are in space,

2:25:37.000 --> 2:25:38.880
 there's no gravity and you have a big box

2:25:38.880 --> 2:25:42.200
 with magnets in it, okay.

2:25:42.200 --> 2:25:43.920
 You know, kind of rectangular magnets

2:25:43.920 --> 2:25:46.880
 with North Pole on one end, South Pole on the other end.

2:25:46.880 --> 2:25:49.640
 You shake the box gently and the magnets will kind of stick

2:25:49.640 --> 2:25:52.440
 to themselves and probably form like complex structure,

2:25:53.480 --> 2:25:55.280
 you know, spontaneously.

2:25:55.280 --> 2:25:57.120
 You know, that could be an example of self organization,

2:25:57.120 --> 2:25:58.400
 but you know, you have lots of examples,

2:25:58.400 --> 2:26:01.280
 neural nets are an example of self organization too,

2:26:01.280 --> 2:26:03.080
 you know, in many respect.

2:26:03.080 --> 2:26:05.960
 And it's a bit of a mystery, you know,

2:26:05.960 --> 2:26:09.520
 how like what is possible with this, you know,

2:26:09.520 --> 2:26:12.960
 pattern formation in physical systems, in chaotic system

2:26:12.960 --> 2:26:16.120
 and things like that, you know, the emergence of life,

2:26:16.120 --> 2:26:16.960
 you know, things like that.

2:26:16.960 --> 2:26:19.560
 So, you know, how does that happen?

2:26:19.560 --> 2:26:22.600
 So it's a big puzzle for physicists as well.

2:26:22.600 --> 2:26:24.720
 It feels like understanding this,

2:26:24.720 --> 2:26:27.920
 the mathematics of emergence

2:26:27.920 --> 2:26:29.720
 in some constrained situations

2:26:29.720 --> 2:26:32.120
 might help us create intelligence,

2:26:32.120 --> 2:26:36.040
 like help us add a little spice to the systems

2:26:36.040 --> 2:26:40.960
 because you seem to be able to in complex systems

2:26:40.960 --> 2:26:44.600
 with emergence to be able to get a lot from little.

2:26:44.600 --> 2:26:47.000
 And so that seems like a shortcut

2:26:47.000 --> 2:26:51.120
 to get big leaps in performance, but...

2:26:51.120 --> 2:26:55.000
 But there's a missing concept that we don't have.

2:26:55.000 --> 2:26:55.840
 Yeah.

2:26:55.840 --> 2:26:58.440
 And it's something also I've been fascinated by

2:26:58.440 --> 2:27:00.720
 since my undergrad days,

2:27:00.720 --> 2:27:03.880
 and it's how you measure complexity, right?

2:27:03.880 --> 2:27:06.960
 So we don't actually have good ways of measuring,

2:27:06.960 --> 2:27:09.840
 or at least we don't have good ways of interpreting

2:27:09.840 --> 2:27:11.920
 the measures that we have at our disposal.

2:27:11.920 --> 2:27:14.480
 Like how do you measure the complexity of something, right?

2:27:14.480 --> 2:27:15.680
 So there's all those things, you know,

2:27:15.680 --> 2:27:18.560
 like, you know, Kolmogorov, Chaitin, Solomonov complexity

2:27:18.560 --> 2:27:20.920
 of, you know, the length of the shortest program

2:27:20.920 --> 2:27:23.320
 that would generate a bit string can be thought of

2:27:23.320 --> 2:27:25.520
 as the complexity of that bit string, right?

2:27:26.840 --> 2:27:28.200
 I've been fascinated by that concept.

2:27:28.200 --> 2:27:30.160
 The problem with that is that

2:27:30.160 --> 2:27:32.840
 that complexity is defined up to a constant,

2:27:32.840 --> 2:27:33.920
 which can be very large.

2:27:34.920 --> 2:27:35.760
 Right.

2:27:35.760 --> 2:27:37.840
 There are similar concepts that are derived from,

2:27:37.840 --> 2:27:42.280
 you know, Bayesian probability theory,

2:27:42.280 --> 2:27:44.520
 where, you know, the complexity of something

2:27:44.520 --> 2:27:48.360
 is the negative log of its probability, essentially, right?

2:27:48.360 --> 2:27:51.120
 And you have a complete equivalence between the two things.

2:27:51.120 --> 2:27:52.120
 And there you would think, you know,

2:27:52.120 --> 2:27:55.160
 the probability is something that's well defined mathematically,

2:27:55.160 --> 2:27:57.200
 which means complexity is well defined.

2:27:57.200 --> 2:27:58.040
 But it's not true.

2:27:58.040 --> 2:28:01.720
 You need to have a model of the distribution.

2:28:01.720 --> 2:28:02.800
 You may need to have a prior

2:28:02.800 --> 2:28:04.200
 if you're doing Bayesian inference.

2:28:04.200 --> 2:28:05.720
 And the prior plays the same role

2:28:05.720 --> 2:28:07.040
 as the choice of the computer

2:28:07.040 --> 2:28:09.480
 with which you measure Kolmogorov complexity.

2:28:09.480 --> 2:28:12.040
 And so every measure of complexity we have

2:28:12.040 --> 2:28:13.600
 has some arbitrary density,

2:28:15.440 --> 2:28:16.840
 you know, an additive constant,

2:28:16.840 --> 2:28:19.560
 which can be arbitrarily large.

2:28:19.560 --> 2:28:23.360
 And so, you know, how can we come up with a good theory

2:28:23.360 --> 2:28:24.640
 of how things become more complex

2:28:24.640 --> 2:28:26.080
 if we don't have a good measure of complexity?

2:28:26.080 --> 2:28:28.200
 Yeah, which we need for this.

2:28:28.200 --> 2:28:32.240
 One way that people study this in the space of biology,

2:28:32.240 --> 2:28:33.760
 the people that study the origin of life

2:28:33.760 --> 2:28:37.120
 or try to recreate the life in the laboratory.

2:28:37.120 --> 2:28:39.200
 And the more interesting one is the alien one,

2:28:39.200 --> 2:28:41.320
 is when we go to other planets,

2:28:41.320 --> 2:28:43.960
 how do we recognize this life?

2:28:43.960 --> 2:28:46.800
 Because, you know, complexity, we associate complexity,

2:28:46.800 --> 2:28:49.000
 maybe some level of mobility with life.

2:28:50.000 --> 2:28:51.680
 You know, we have to be able to, like,

2:28:51.680 --> 2:28:56.680
 have concrete algorithms for, like,

2:28:57.200 --> 2:29:00.000
 measuring the level of complexity we see

2:29:00.000 --> 2:29:02.760
 in order to know the difference between life and non life.

2:29:02.760 --> 2:29:04.040
 And the problem is that complexity

2:29:04.040 --> 2:29:05.440
 is in the eye of the beholder.

2:29:05.440 --> 2:29:07.480
 So let me give you an example.

2:29:07.480 --> 2:29:12.480
 If I give you an image of the MNIST digits, right,

2:29:13.240 --> 2:29:15.400
 and I flip through MNIST digits,

2:29:15.400 --> 2:29:18.120
 there is obviously some structure to it

2:29:18.120 --> 2:29:20.440
 because local structure, you know,

2:29:20.440 --> 2:29:22.240
 neighboring pixels are correlated

2:29:23.200 --> 2:29:25.440
 across the entire data set.

2:29:25.440 --> 2:29:30.440
 I imagine that I apply a random permutation

2:29:30.440 --> 2:29:33.920
 to all the pixels, a fixed random permutation.

2:29:33.920 --> 2:29:35.360
 Now I show you those images,

2:29:35.360 --> 2:29:38.880
 they will look, you know, really disorganized to you,

2:29:38.880 --> 2:29:40.680
 more complex.

2:29:40.680 --> 2:29:42.880
 In fact, they're not more complex in absolute terms,

2:29:42.880 --> 2:29:45.480
 they're exactly the same as originally, right?

2:29:45.480 --> 2:29:46.960
 And if you knew what the permutation was,

2:29:46.960 --> 2:29:49.440
 you know, you could undo the permutation.

2:29:49.440 --> 2:29:52.360
 Now, imagine I give you special glasses

2:29:52.360 --> 2:29:54.120
 that undo that permutation.

2:29:54.120 --> 2:29:56.160
 Now, all of a sudden, what looked complicated

2:29:56.160 --> 2:29:57.000
 becomes simple.

2:29:57.000 --> 2:29:57.920
 Right.

2:29:57.920 --> 2:30:00.400
 So if you have two, if you have, you know,

2:30:00.400 --> 2:30:03.280
 humans on one end, and then another race of aliens

2:30:03.280 --> 2:30:05.440
 that sees the universe with permutation glasses.

2:30:05.440 --> 2:30:06.600
 Yeah, with the permutation glasses.

2:30:06.600 --> 2:30:09.800
 Okay, what we perceive as simple to them

2:30:09.800 --> 2:30:11.760
 is hardly complicated, it's probably heat.

2:30:11.760 --> 2:30:12.600
 Yeah.

2:30:12.600 --> 2:30:13.440
 Heat, yeah.

2:30:13.440 --> 2:30:15.320
 Okay, and what they perceive as simple to us

2:30:15.320 --> 2:30:18.480
 is random fluctuation, it's heat.

2:30:18.480 --> 2:30:19.320
 Yeah.

2:30:19.320 --> 2:30:22.760
 Yeah, it's truly in the eye of the beholder.

2:30:22.760 --> 2:30:23.600
 Yeah.

2:30:23.600 --> 2:30:24.920
 It depends what kind of glasses you're wearing.

2:30:24.920 --> 2:30:25.760
 Right.

2:30:25.760 --> 2:30:26.840
 It depends what kind of algorithm you're running

2:30:26.840 --> 2:30:28.360
 in your perception system.

2:30:28.360 --> 2:30:31.080
 So I don't think we'll have a theory of intelligence,

2:30:31.080 --> 2:30:34.320
 self organization, evolution, things like this,

2:30:34.320 --> 2:30:38.520
 until we have a good handle on a notion of complexity

2:30:38.520 --> 2:30:40.800
 which we know is in the eye of the beholder.

2:30:42.320 --> 2:30:44.400
 Yeah, it's sad to think that we might not be able

2:30:44.400 --> 2:30:47.600
 to detect or interact with alien species

2:30:47.600 --> 2:30:50.280
 because we're wearing different glasses.

2:30:50.280 --> 2:30:51.440
 Because their notion of locality

2:30:51.440 --> 2:30:52.400
 might be different from ours.

2:30:52.400 --> 2:30:53.240
 Yeah, exactly.

2:30:53.240 --> 2:30:55.200
 This actually connects with fascinating questions

2:30:55.200 --> 2:30:58.120
 in physics at the moment, like modern physics,

2:30:58.120 --> 2:31:00.240
 quantum physics, like, you know, questions about,

2:31:00.240 --> 2:31:02.520
 like, you know, can we recover the information

2:31:02.520 --> 2:31:04.520
 that's lost in a black hole and things like this, right?

2:31:04.520 --> 2:31:07.920
 And that relies on notions of complexity,

2:31:09.360 --> 2:31:11.640
 which, you know, I find this fascinating.

2:31:11.640 --> 2:31:13.360
 Can you describe your personal quest

2:31:13.360 --> 2:31:18.360
 to build an expressive electronic wind instrument, EWI?

2:31:19.760 --> 2:31:20.600
 What is it?

2:31:20.600 --> 2:31:24.000
 What does it take to build it?

2:31:24.000 --> 2:31:25.080
 Well, I'm a tinker.

2:31:25.080 --> 2:31:26.760
 I like building things.

2:31:26.760 --> 2:31:28.960
 I like building things with combinations of electronics

2:31:28.960 --> 2:31:31.040
 and, you know, mechanical stuff.

2:31:32.400 --> 2:31:34.120
 You know, I have a bunch of different hobbies,

2:31:34.120 --> 2:31:37.960
 but, you know, probably my first one was little,

2:31:37.960 --> 2:31:39.800
 was building model airplanes and stuff like that.

2:31:39.800 --> 2:31:41.880
 And I still do that to some extent.

2:31:41.880 --> 2:31:43.800
 But also electronics, I taught myself electronics

2:31:43.800 --> 2:31:45.160
 before I studied it.

2:31:46.240 --> 2:31:48.120
 And the reason I taught myself electronics

2:31:48.120 --> 2:31:49.600
 is because of music.

2:31:49.600 --> 2:31:53.200
 My cousin was an aspiring electronic musician

2:31:53.200 --> 2:31:55.000
 and he had an analog synthesizer.

2:31:55.000 --> 2:31:58.000
 And I was, you know, basically modifying it for him

2:31:58.000 --> 2:32:00.280
 and building sequencers and stuff like that, right, for him.

2:32:00.280 --> 2:32:02.640
 I was in high school when I was doing this.

2:32:02.640 --> 2:32:06.040
 That's the interesting, like, progressive rock, like 80s.

2:32:06.040 --> 2:32:08.000
 Like, what's the greatest band of all time,

2:32:08.000 --> 2:32:09.520
 according to Yann LeCun?

2:32:09.520 --> 2:32:11.080
 Oh, man, there's too many of them.

2:32:11.080 --> 2:32:16.080
 But, you know, it's a combination of, you know,

2:32:16.360 --> 2:32:19.800
 Mahavishnu Orchestra, Weather Report,

2:32:19.800 --> 2:32:24.800
 yes, Genesis, you know, pre Peter Gabriel,

2:32:27.120 --> 2:32:29.120
 Gentle Giant, you know, things like that.

2:32:29.120 --> 2:32:29.960
 Great.

2:32:29.960 --> 2:32:32.280
 Okay, so this love of electronics

2:32:32.280 --> 2:32:34.240
 and this love of music combined together.

2:32:34.240 --> 2:32:36.280
 Right, so I was actually trained to play

2:32:36.280 --> 2:32:41.280
 Baroque and Renaissance music and I played in an orchestra

2:32:42.040 --> 2:32:45.640
 when I was in high school and first years of college.

2:32:45.640 --> 2:32:48.040
 And I played the recorder, crumb horn,

2:32:48.040 --> 2:32:50.200
 a little bit of oboe, you know, things like that.

2:32:50.200 --> 2:32:52.520
 So I'm a wind instrument player.

2:32:52.520 --> 2:32:54.080
 But I always wanted to play improvised music,

2:32:54.080 --> 2:32:56.320
 even though I don't know anything about it.

2:32:56.320 --> 2:32:58.760
 And the only way I figured, you know,

2:32:58.760 --> 2:33:01.080
 short of like learning to play saxophone

2:33:01.080 --> 2:33:03.560
 was to play electronic wind instruments.

2:33:03.560 --> 2:33:05.680
 So they behave, you know, the fingering is similar

2:33:05.680 --> 2:33:07.640
 to a saxophone, but, you know,

2:33:07.640 --> 2:33:09.080
 you have wide variety of sound

2:33:09.080 --> 2:33:11.040
 because you control the synthesizer with it.

2:33:11.040 --> 2:33:13.120
 So I had a bunch of those, you know,

2:33:13.120 --> 2:33:18.120
 going back to the late 80s from either Yamaha or Akai.

2:33:18.880 --> 2:33:22.520
 They're both kind of the main manufacturers of those.

2:33:22.520 --> 2:33:23.720
 So they were classically, you know,

2:33:23.720 --> 2:33:25.520
 going back several decades.

2:33:25.520 --> 2:33:27.680
 But I've never been completely satisfied with them

2:33:27.680 --> 2:33:29.280
 because of lack of expressivity.

2:33:31.120 --> 2:33:32.480
 And, you know, those things, you know,

2:33:32.480 --> 2:33:33.400
 are somewhat expressive.

2:33:33.400 --> 2:33:34.760
 I mean, they measure the breath pressure,

2:33:34.760 --> 2:33:36.520
 they measure the lip pressure.

2:33:36.520 --> 2:33:39.800
 And, you know, you have various parameters.

2:33:39.800 --> 2:33:41.480
 You can vary with fingers,

2:33:41.480 --> 2:33:44.800
 but they're not really as expressive

2:33:44.800 --> 2:33:47.040
 as an acoustic instrument, right?

2:33:47.040 --> 2:33:49.400
 You hear John Coltrane play two notes

2:33:49.400 --> 2:33:50.760
 and you know it's John Coltrane,

2:33:50.760 --> 2:33:53.000
 you know, it's got a unique sound.

2:33:53.000 --> 2:33:54.280
 Or Miles Davis, right?

2:33:54.280 --> 2:33:57.480
 You can hear it's Miles Davis playing the trumpet

2:33:57.480 --> 2:34:02.480
 because the sound reflects their, you know,

2:34:02.480 --> 2:34:05.800
 physiognomy, basically, the shape of the vocal track

2:34:07.600 --> 2:34:09.200
 kind of shapes the sound.

2:34:09.200 --> 2:34:12.320
 So how do you do this with an electronic instrument?

2:34:12.320 --> 2:34:13.920
 And I was, many years ago,

2:34:13.920 --> 2:34:15.640
 I met a guy called David Wessel.

2:34:15.640 --> 2:34:18.240
 He was a professor at Berkeley

2:34:18.240 --> 2:34:23.000
 and created the Center for Music Technology there.

2:34:23.000 --> 2:34:25.600
 And he was interested in that question.

2:34:25.600 --> 2:34:28.120
 And so I kept kind of thinking about this for many years.

2:34:28.120 --> 2:34:31.040
 And finally, because of COVID, you know, I was at home,

2:34:31.040 --> 2:34:32.600
 I was in my workshop.

2:34:32.600 --> 2:34:36.040
 My workshop serves also as my kind of Zoom room

2:34:36.040 --> 2:34:37.360
 and home office.

2:34:37.360 --> 2:34:38.800
 And this is in New Jersey?

2:34:38.800 --> 2:34:39.640
 In New Jersey.

2:34:39.640 --> 2:34:43.600
 And I started really being serious about, you know,

2:34:43.600 --> 2:34:45.800
 building my own iwi instrument.

2:34:45.800 --> 2:34:48.160
 What else is going on in that New Jersey workshop?

2:34:48.160 --> 2:34:50.880
 Is there some crazy stuff you've built,

2:34:50.880 --> 2:34:55.200
 like just, or like left on the workshop floor, left behind?

2:34:55.200 --> 2:34:57.600
 A lot of crazy stuff is, you know,

2:34:57.600 --> 2:35:01.680
 electronics built with microcontrollers of various kinds

2:35:01.680 --> 2:35:04.880
 and, you know, weird flying contraptions.

2:35:06.720 --> 2:35:08.720
 So you still love flying?

2:35:08.720 --> 2:35:09.880
 It's a family disease.

2:35:09.880 --> 2:35:13.520
 My dad got me into it when I was a kid.

2:35:13.520 --> 2:35:16.840
 And he was building model airplanes when he was a kid.

2:35:16.840 --> 2:35:19.800
 And he was a mechanical engineer.

2:35:19.800 --> 2:35:21.200
 He taught himself electronics also.

2:35:21.200 --> 2:35:24.080
 So he built his early radio control systems

2:35:24.080 --> 2:35:27.760
 in the late 60s, early 70s.

2:35:27.760 --> 2:35:29.640
 And so that's what got me into,

2:35:29.640 --> 2:35:31.120
 I mean, he got me into kind of, you know,

2:35:31.120 --> 2:35:33.040
 engineering and science and technology.

2:35:33.040 --> 2:35:36.120
 Do you also have an interest in appreciation of flight

2:35:36.120 --> 2:35:38.320
 in other forms, like with drones, quadroptors,

2:35:38.320 --> 2:35:41.720
 or do you, is it model airplane, the thing that's?

2:35:41.720 --> 2:35:45.240
 You know, before drones were, you know,

2:35:45.240 --> 2:35:49.240
 kind of a consumer product, you know,

2:35:49.240 --> 2:35:50.280
 I built my own, you know,

2:35:50.280 --> 2:35:52.000
 with also building a microcontroller

2:35:52.000 --> 2:35:56.240
 with JavaScripts and accelerometers for stabilization,

2:35:56.240 --> 2:35:57.760
 writing the firmware for it, you know.

2:35:57.760 --> 2:35:59.200
 And then when it became kind of a standard thing

2:35:59.200 --> 2:36:00.320
 you could buy, it was boring, you know,

2:36:00.320 --> 2:36:01.160
 I stopped doing it.

2:36:01.160 --> 2:36:02.440
 It was not fun anymore.

2:36:03.520 --> 2:36:04.720
 Yeah.

2:36:04.720 --> 2:36:06.280
 You were doing it before it was cool.

2:36:06.280 --> 2:36:07.120
 Yeah.

2:36:07.120 --> 2:36:10.080
 What advice would you give to a young person today

2:36:10.080 --> 2:36:11.360
 in high school and college

2:36:11.360 --> 2:36:15.960
 that dreams of doing something big like Yann LeCun,

2:36:15.960 --> 2:36:18.960
 like let's talk in the space of intelligence,

2:36:18.960 --> 2:36:21.000
 dreams of having a chance to solve

2:36:21.000 --> 2:36:23.960
 some fundamental problem in space of intelligence,

2:36:23.960 --> 2:36:26.200
 both for their career and just in life,

2:36:26.200 --> 2:36:28.600
 being somebody who was a part

2:36:28.600 --> 2:36:30.680
 of creating something special?

2:36:30.680 --> 2:36:35.400
 So try to get interested by big questions,

2:36:35.400 --> 2:36:38.680
 things like, you know, what is intelligence?

2:36:38.680 --> 2:36:40.440
 What is the universe made of?

2:36:40.440 --> 2:36:41.680
 What's life all about?

2:36:41.680 --> 2:36:42.520
 Things like that.

2:36:45.040 --> 2:36:47.040
 Like even like crazy big questions,

2:36:47.040 --> 2:36:49.040
 like what's time?

2:36:49.040 --> 2:36:51.040
 Like nobody knows what time is.

2:36:53.160 --> 2:36:58.160
 And then learn basic things,

2:36:58.640 --> 2:37:00.680
 like basic methods, either from math,

2:37:00.680 --> 2:37:02.280
 from physics or from engineering.

2:37:03.280 --> 2:37:05.600
 Things that have a long shelf life.

2:37:05.600 --> 2:37:07.280
 Like if you have a choice between,

2:37:07.280 --> 2:37:10.160
 like, you know, learning, you know,

2:37:10.160 --> 2:37:11.720
 mobile programming on iPhone

2:37:12.600 --> 2:37:14.840
 or quantum mechanics, take quantum mechanics.

2:37:16.880 --> 2:37:18.480
 Because you're gonna learn things

2:37:18.480 --> 2:37:20.120
 that you have no idea exist.

2:37:20.120 --> 2:37:25.120
 And you may not, you may never be a quantum physicist,

2:37:25.320 --> 2:37:26.800
 but you will learn about path integrals.

2:37:26.800 --> 2:37:29.120
 And path integrals are used everywhere.

2:37:29.120 --> 2:37:30.280
 It's the same formula that you use

2:37:30.280 --> 2:37:33.280
 for, you know, Bayesian integration and stuff like that.

2:37:33.280 --> 2:37:37.720
 So the ideas, the little ideas within quantum mechanics,

2:37:37.720 --> 2:37:41.440
 within some of these kind of more solidified fields

2:37:41.440 --> 2:37:42.920
 will have a longer shelf life.

2:37:42.920 --> 2:37:46.920
 You'll somehow use indirectly in your work.

2:37:46.920 --> 2:37:48.640
 Learn classical mechanics, like you'll learn

2:37:48.640 --> 2:37:50.120
 about Lagrangian, for example,

2:37:51.360 --> 2:37:55.000
 which is like a huge, hugely useful concept,

2:37:55.000 --> 2:37:57.320
 you know, for all kinds of different things.

2:37:57.320 --> 2:38:01.680
 Learn statistical physics, because all the math

2:38:01.680 --> 2:38:04.360
 that comes out of, you know, for machine learning

2:38:05.480 --> 2:38:07.280
 basically comes out of, was figured out

2:38:07.280 --> 2:38:09.240
 by statistical physicists in the, you know,

2:38:09.240 --> 2:38:10.960
 late 19th, early 20th century, right?

2:38:10.960 --> 2:38:14.320
 So, and for some of them actually more recently

2:38:14.320 --> 2:38:16.120
 for, by people like Giorgio Parisi,

2:38:16.120 --> 2:38:19.040
 who just got the Nobel prize for the replica method,

2:38:19.040 --> 2:38:23.200
 among other things, it's used for a lot of different things.

2:38:23.200 --> 2:38:25.560
 You know, variational inference,

2:38:25.560 --> 2:38:27.640
 that math comes from statistical physics.

2:38:28.600 --> 2:38:33.600
 So a lot of those kind of, you know, basic courses,

2:38:33.960 --> 2:38:36.240
 you know, if you do electrical engineering,

2:38:36.240 --> 2:38:37.360
 you take signal processing,

2:38:37.360 --> 2:38:39.880
 you'll learn about Fourier transforms.

2:38:39.880 --> 2:38:42.720
 Again, something super useful is at the basis

2:38:42.720 --> 2:38:44.920
 of things like graph neural nets,

2:38:44.920 --> 2:38:49.400
 which is an entirely new sub area of, you know,

2:38:49.400 --> 2:38:50.680
 AI machine learning, deep learning,

2:38:50.680 --> 2:38:52.160
 which I think is super promising

2:38:52.160 --> 2:38:54.360
 for all kinds of applications.

2:38:54.360 --> 2:38:55.240
 Something very promising,

2:38:55.240 --> 2:38:56.680
 if you're more interested in applications,

2:38:56.680 --> 2:38:58.840
 is the applications of AI machine learning

2:38:58.840 --> 2:39:00.480
 and deep learning to science,

2:39:01.520 --> 2:39:05.120
 or to science that can help solve big problems

2:39:05.120 --> 2:39:05.960
 in the world.

2:39:05.960 --> 2:39:09.240
 I have colleagues at Meta, at Fair,

2:39:09.240 --> 2:39:11.240
 who started this project called Open Catalyst,

2:39:11.240 --> 2:39:14.560
 and it's an open project collaborative.

2:39:14.560 --> 2:39:16.640
 And the idea is to use deep learning

2:39:16.640 --> 2:39:21.640
 to help design new chemical compounds or materials

2:39:21.960 --> 2:39:23.800
 that would facilitate the separation

2:39:23.800 --> 2:39:25.840
 of hydrogen from oxygen.

2:39:25.840 --> 2:39:29.080
 If you can efficiently separate oxygen from hydrogen

2:39:29.080 --> 2:39:33.520
 with electricity, you solve climate change.

2:39:33.520 --> 2:39:34.480
 It's as simple as that,

2:39:34.480 --> 2:39:37.640
 because you cover, you know,

2:39:37.640 --> 2:39:39.800
 some random desert with solar panels,

2:39:40.800 --> 2:39:42.560
 and you have them work all day,

2:39:42.560 --> 2:39:43.480
 produce hydrogen,

2:39:43.480 --> 2:39:45.400
 and then you shoot the hydrogen wherever it's needed.

2:39:45.400 --> 2:39:46.840
 You don't need anything else.

2:39:48.560 --> 2:39:53.440
 You know, you have controllable power

2:39:53.440 --> 2:39:55.640
 that can be transported anywhere.

2:39:55.640 --> 2:39:59.040
 So if we have a large scale,

2:39:59.040 --> 2:40:02.160
 efficient energy storage technology,

2:40:02.160 --> 2:40:06.640
 like producing hydrogen, we solve climate change.

2:40:06.640 --> 2:40:08.560
 Here's another way to solve climate change,

2:40:08.560 --> 2:40:10.480
 is figuring out how to make fusion work.

2:40:10.480 --> 2:40:11.520
 Now, the problem with fusion

2:40:11.520 --> 2:40:13.640
 is that you make a super hot plasma,

2:40:13.640 --> 2:40:16.240
 and the plasma is unstable and you can't control it.

2:40:16.240 --> 2:40:17.080
 Maybe with deep learning,

2:40:17.080 --> 2:40:19.120
 you can find controllers that will stabilize plasma

2:40:19.120 --> 2:40:21.640
 and make, you know, practical fusion reactors.

2:40:21.640 --> 2:40:23.080
 I mean, that's very speculative,

2:40:23.080 --> 2:40:24.480
 but, you know, it's worth trying,

2:40:24.480 --> 2:40:28.280
 because, you know, the payoff is huge.

2:40:28.280 --> 2:40:29.880
 There's a group at Google working on this,

2:40:29.880 --> 2:40:31.160
 led by John Platt.

2:40:31.160 --> 2:40:33.920
 So control, convert as many problems

2:40:33.920 --> 2:40:36.800
 in science and physics and biology and chemistry

2:40:36.800 --> 2:40:39.760
 into a learnable problem

2:40:39.760 --> 2:40:41.560
 and see if a machine can learn it.

2:40:41.560 --> 2:40:43.880
 Right, I mean, there's properties of, you know,

2:40:43.880 --> 2:40:46.280
 complex materials that we don't understand

2:40:46.280 --> 2:40:48.520
 from first principle, for example, right?

2:40:48.520 --> 2:40:53.040
 So, you know, if we could design new, you know,

2:40:53.040 --> 2:40:56.400
 new materials, we could make more efficient batteries.

2:40:56.400 --> 2:40:58.800
 You know, we could make maybe faster electronics.

2:40:58.800 --> 2:41:01.920
 We could, I mean, there's a lot of things we can imagine

2:41:01.920 --> 2:41:04.480
 doing, or, you know, lighter materials

2:41:04.480 --> 2:41:06.400
 for cars or airplanes or things like that.

2:41:06.400 --> 2:41:07.600
 Maybe better fuel cells.

2:41:07.600 --> 2:41:09.520
 I mean, there's all kinds of stuff we can imagine.

2:41:09.520 --> 2:41:12.280
 If we had good fuel cells, hydrogen fuel cells,

2:41:12.280 --> 2:41:13.640
 we could use them to power airplanes,

2:41:13.640 --> 2:41:17.240
 and, you know, transportation wouldn't be, or cars,

2:41:17.240 --> 2:41:20.280
 and we wouldn't have emission problem,

2:41:20.280 --> 2:41:24.600
 CO2 emission problems for air transportation anymore.

2:41:24.600 --> 2:41:26.880
 So there's a lot of those things, I think,

2:41:26.880 --> 2:41:29.160
 where AI, you know, can be used.

2:41:30.160 --> 2:41:31.560
 And this is not even talking about

2:41:31.560 --> 2:41:33.520
 all the sort of medicine, biology,

2:41:33.520 --> 2:41:35.680
 and everything like that, right?

2:41:35.680 --> 2:41:37.840
 You know, like, you know, protein folding,

2:41:37.840 --> 2:41:40.040
 you know, figuring out, like, how could you design

2:41:40.040 --> 2:41:41.880
 your proteins that it sticks to another protein

2:41:41.880 --> 2:41:44.040
 at a particular site, because that's how you design drugs

2:41:44.040 --> 2:41:44.880
 in the end.

2:41:46.280 --> 2:41:47.600
 So, you know, deep learning would be useful,

2:41:47.600 --> 2:41:49.280
 although those are kind of, you know,

2:41:49.280 --> 2:41:51.120
 would be sort of enormous progress

2:41:51.120 --> 2:41:53.360
 if we could use it for that.

2:41:53.360 --> 2:41:54.320
 Here's an example.

2:41:54.320 --> 2:41:58.280
 If you take, this is like from recent material physics,

2:41:58.280 --> 2:42:02.200
 you take a monoatomic layer of graphene, right?

2:42:02.200 --> 2:42:04.920
 So it's just carbon on a hexagonal mesh,

2:42:04.920 --> 2:42:09.120
 and you make this single atom thick.

2:42:09.120 --> 2:42:10.360
 You put another one on top,

2:42:10.360 --> 2:42:13.080
 you twist them by some magic number of degrees,

2:42:13.080 --> 2:42:14.800
 three degrees or something.

2:42:14.800 --> 2:42:16.760
 It becomes superconductor.

2:42:16.760 --> 2:42:18.240
 Nobody has any idea why.

2:42:18.240 --> 2:42:19.080
 Okay.

2:42:20.800 --> 2:42:22.480
 I want to know how that was discovered,

2:42:22.480 --> 2:42:23.920
 but that's the kind of thing that machine learning

2:42:23.920 --> 2:42:25.800
 can actually discover, these kinds of things.

2:42:25.800 --> 2:42:28.960
 Maybe not, but there is a hint, perhaps,

2:42:28.960 --> 2:42:31.720
 that with machine learning, we would train a system

2:42:31.720 --> 2:42:34.840
 to basically be a phenomenological model

2:42:34.840 --> 2:42:37.240
 of some complex emergent phenomenon,

2:42:37.240 --> 2:42:40.400
 which, you know, superconductivity is one of those,

2:42:42.400 --> 2:42:44.760
 where, you know, this collective phenomenon

2:42:44.760 --> 2:42:46.920
 is too difficult to describe from first principles

2:42:46.920 --> 2:42:48.800
 with the current, you know,

2:42:48.800 --> 2:42:51.920
 the usual sort of reductionist type method,

2:42:51.920 --> 2:42:54.960
 but we could have deep learning systems

2:42:54.960 --> 2:42:57.680
 that predict the properties of a system

2:42:57.680 --> 2:42:59.880
 from a description of it after being trained

2:42:59.880 --> 2:43:04.880
 with sufficiently many samples.

2:43:04.880 --> 2:43:06.680
 This guy, Pascal Foua, at EPFL,

2:43:06.680 --> 2:43:09.800
 he has a startup company that,

2:43:09.800 --> 2:43:13.440
 where he basically trained a convolutional net,

2:43:13.440 --> 2:43:16.640
 essentially, to predict the aerodynamic properties

2:43:16.640 --> 2:43:19.640
 of solids, and you can generate as much data as you want

2:43:19.640 --> 2:43:21.920
 by just running computational free dynamics, right?

2:43:21.920 --> 2:43:26.920
 So you give, like, a wing, airfoil,

2:43:27.800 --> 2:43:29.800
 or something, shape of some kind,

2:43:29.800 --> 2:43:31.400
 and you run computational free dynamics,

2:43:31.400 --> 2:43:35.120
 you get, as a result, the drag and, you know,

2:43:36.160 --> 2:43:37.480
 lift and all that stuff, right?

2:43:37.480 --> 2:43:40.080
 And you can generate lots of data,

2:43:40.080 --> 2:43:41.840
 train a neural net to make those predictions,

2:43:41.840 --> 2:43:44.120
 and now what you have is a differentiable model

2:43:44.120 --> 2:43:47.000
 of, let's say, drag and lift

2:43:47.000 --> 2:43:48.680
 as a function of the shape of that solid,

2:43:48.680 --> 2:43:49.960
 and so you can do back rate and descent,

2:43:49.960 --> 2:43:51.520
 you can optimize the shape

2:43:51.520 --> 2:43:53.280
 so you get the properties you want.

2:43:54.880 --> 2:43:56.040
 Yeah, that's incredible.

2:43:56.040 --> 2:43:58.280
 That's incredible, and on top of all that,

2:43:58.280 --> 2:44:01.480
 probably you should read a little bit of literature

2:44:01.480 --> 2:44:03.600
 and a little bit of history

2:44:03.600 --> 2:44:06.640
 for inspiration and for wisdom,

2:44:06.640 --> 2:44:08.800
 because after all, all of these technologies

2:44:08.800 --> 2:44:10.280
 will have to work in the human world.

2:44:10.280 --> 2:44:11.120
 Yes.

2:44:11.120 --> 2:44:12.640
 And the human world is complicated.

2:44:12.640 --> 2:44:14.120
 It is, sadly.

2:44:15.080 --> 2:44:18.440
 Jan, this is an amazing conversation.

2:44:18.440 --> 2:44:20.400
 I'm really honored that you would talk with me today.

2:44:20.400 --> 2:44:22.240
 Thank you for all the amazing work you're doing

2:44:22.240 --> 2:44:26.280
 at FAIR, at Meta, and thank you for being so passionate

2:44:26.280 --> 2:44:28.120
 after all these years about everything

2:44:28.120 --> 2:44:29.960
 that's going on, you're a beacon of hope

2:44:29.960 --> 2:44:31.600
 for the machine learning community,

2:44:31.600 --> 2:44:33.200
 and thank you so much for spending

2:44:33.200 --> 2:44:34.480
 your valuable time with me today.

2:44:34.480 --> 2:44:35.320
 That was awesome.

2:44:35.320 --> 2:44:36.280
 Thanks for having me on.

2:44:36.280 --> 2:44:37.840
 That was a pleasure.

2:44:38.800 --> 2:44:41.440
 Thanks for listening to this conversation with Jan Lacune.

2:44:41.440 --> 2:44:42.800
 To support this podcast,

2:44:42.800 --> 2:44:45.720
 please check out our sponsors in the description.

2:44:45.720 --> 2:44:47.840
 And now, let me leave you with some words

2:44:47.840 --> 2:44:49.600
 from Isaac Asimov.

2:44:50.640 --> 2:44:53.760
 Your assumptions are your windows on the world.

2:44:53.760 --> 2:44:56.040
 Scrub them off every once in a while,

2:44:56.040 --> 2:44:58.760
 or the light won't come in.

2:44:58.760 --> 2:45:26.760
 Thank you for listening, and hope to see you next time.

