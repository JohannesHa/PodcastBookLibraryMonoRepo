WEBVTT

00:00.000 --> 00:03.040
 The following is a conversation with David Feroci.

00:03.040 --> 00:05.200
 He led the team that built Watson,

00:05.200 --> 00:07.040
 the IBM question answering system

00:07.040 --> 00:09.080
 that beat the top humans in the world

00:09.080 --> 00:11.160
 at the game of Jeopardy.

00:11.160 --> 00:12.920
 For spending a couple hours with David,

00:12.920 --> 00:14.960
 I saw a genuine passion,

00:14.960 --> 00:17.720
 not only for abstract understanding of intelligence,

00:17.720 --> 00:21.240
 but for engineering it to solve real world problems

00:21.240 --> 00:24.800
 under real world deadlines and resource constraints.

00:24.800 --> 00:26.520
 Where science meets engineering

00:26.520 --> 00:29.940
 is where brilliant, simple ingenuity emerges.

00:29.940 --> 00:32.120
 People who work adjoining it to

00:32.120 --> 00:33.800
 have a lot of wisdom earned

00:33.800 --> 00:36.960
 through failures and eventual success.

00:36.960 --> 00:39.080
 David is also the founder, CEO,

00:39.080 --> 00:41.660
 and chief scientist of Elemental Cognition,

00:41.660 --> 00:44.480
 a company working to engineer AI systems

00:44.480 --> 00:47.420
 that understand the world the way people do.

00:47.420 --> 00:50.260
 This is the Artificial Intelligence Podcast.

00:50.260 --> 00:52.700
 If you enjoy it, subscribe on YouTube,

00:52.700 --> 00:54.440
 give it five stars on iTunes,

00:54.440 --> 00:55.840
 support it on Patreon,

00:55.840 --> 00:57.920
 or simply connect with me on Twitter

00:57.920 --> 01:01.360
 at Lex Friedman, spelled F R I D M A N.

01:01.360 --> 01:05.160
 And now, here's my conversation with David Ferrucci.

01:06.120 --> 01:07.980
 Your undergrad was in biology

01:07.980 --> 01:11.280
 with an eye toward medical school

01:11.280 --> 01:14.320
 before you went on for the PhD in computer science.

01:14.320 --> 01:16.800
 So let me ask you an easy question.

01:16.800 --> 01:20.520
 What is the difference between biological systems

01:20.520 --> 01:22.420
 and computer systems?

01:22.420 --> 01:25.240
 In your, when you sit back,

01:25.240 --> 01:28.600
 look at the stars, and think philosophically.

01:28.600 --> 01:30.840
 I often wonder whether or not

01:30.840 --> 01:32.920
 there is a substantive difference.

01:32.920 --> 01:34.480
 I mean, I think the thing that got me

01:34.480 --> 01:37.220
 into computer science and into artificial intelligence

01:37.220 --> 01:39.840
 was exactly this presupposition

01:39.840 --> 01:44.360
 that if we can get machines to think,

01:44.360 --> 01:45.640
 or I should say this question,

01:45.640 --> 01:47.460
 this philosophical question,

01:47.460 --> 01:50.580
 if we can get machines to think,

01:50.580 --> 01:54.800
 to understand, to process information the way we do,

01:54.800 --> 01:57.960
 so if we can describe a procedure, describe a process,

01:57.960 --> 02:02.480
 even if that process were the intelligence process itself,

02:02.480 --> 02:05.280
 then what would be the difference?

02:05.280 --> 02:07.680
 So from a philosophical standpoint,

02:07.680 --> 02:11.640
 I'm not sure I'm convinced that there is.

02:11.640 --> 02:14.920
 I mean, you can go in the direction of spirituality,

02:14.920 --> 02:16.660
 you can go in the direction of the soul,

02:16.660 --> 02:21.660
 but in terms of what we can experience

02:21.660 --> 02:25.980
 from an intellectual and physical perspective,

02:25.980 --> 02:27.460
 I'm not sure there is.

02:27.460 --> 02:31.080
 Clearly, there are different implementations,

02:31.080 --> 02:33.220
 but if you were to say,

02:33.220 --> 02:36.160
 is a biological information processing system

02:36.160 --> 02:38.420
 fundamentally more capable

02:38.420 --> 02:41.020
 than one we might be able to build out of silicon

02:41.020 --> 02:43.880
 or some other substrate,

02:44.900 --> 02:46.500
 I don't know that there is.

02:46.500 --> 02:50.580
 How distant do you think is the biological implementation?

02:50.580 --> 02:53.820
 So fundamentally, they may have the same capabilities,

02:53.820 --> 02:58.260
 but is it really a far mystery

02:58.260 --> 03:00.700
 where a huge number of breakthroughs are needed

03:00.700 --> 03:02.700
 to be able to understand it,

03:02.700 --> 03:06.260
 or is it something that, for the most part,

03:06.260 --> 03:08.620
 in the important aspects,

03:08.620 --> 03:11.100
 echoes of the same kind of characteristics?

03:11.100 --> 03:12.100
 Yeah, that's interesting.

03:12.100 --> 03:15.580
 I mean, so your question presupposes

03:15.580 --> 03:17.520
 that there's this goal to recreate

03:17.520 --> 03:20.880
 what we perceive as biological intelligence.

03:20.880 --> 03:24.360
 I'm not sure that's the,

03:24.360 --> 03:26.560
 I'm not sure that's how I would state the goal.

03:26.560 --> 03:27.760
 I mean, I think that studying.

03:27.760 --> 03:29.220
 What is the goal?

03:29.220 --> 03:32.180
 Good, so I think there are a few goals.

03:32.180 --> 03:35.720
 I think that understanding the human brain

03:35.720 --> 03:38.520
 and how it works is important

03:38.520 --> 03:43.520
 for us to be able to diagnose and treat issues,

03:43.520 --> 03:47.200
 treat issues for us to understand our own strengths

03:47.200 --> 03:51.040
 and weaknesses, both intellectual,

03:51.040 --> 03:52.440
 psychological, and physical.

03:52.440 --> 03:55.000
 So neuroscience and understanding the brain,

03:55.000 --> 03:59.560
 from that perspective, there's a clear goal there.

03:59.560 --> 04:00.920
 From the perspective of saying,

04:00.920 --> 04:04.840
 I wanna mimic human intelligence,

04:04.840 --> 04:06.440
 that one's a little bit more interesting.

04:06.440 --> 04:10.480
 Human intelligence certainly has a lot of things we envy.

04:10.480 --> 04:12.880
 It's also got a lot of problems, too.

04:12.880 --> 04:16.660
 So I think we're capable of sort of stepping back

04:16.660 --> 04:21.280
 and saying, what do we want out of an intelligence?

04:22.160 --> 04:24.400
 How do we wanna communicate with that intelligence?

04:24.400 --> 04:25.560
 How do we want it to behave?

04:25.560 --> 04:27.440
 How do we want it to perform?

04:27.440 --> 04:30.360
 Now, of course, it's somewhat of an interesting argument

04:30.360 --> 04:32.040
 because I'm sitting here as a human

04:32.040 --> 04:33.940
 with a biological brain,

04:33.940 --> 04:36.420
 and I'm critiquing the strengths and weaknesses

04:36.420 --> 04:38.640
 of human intelligence and saying

04:38.640 --> 04:42.200
 that we have the capacity to step back

04:42.200 --> 04:44.120
 and say, gee, what is intelligence

04:44.120 --> 04:46.000
 and what do we really want out of it?

04:46.000 --> 04:48.520
 And that, in and of itself, suggests that

04:50.040 --> 04:52.080
 human intelligence is something quite enviable,

04:52.080 --> 04:57.080
 that it can introspect that way.

04:58.360 --> 05:00.280
 And the flaws, you mentioned the flaws.

05:00.280 --> 05:01.120
 Humans have flaws.

05:01.120 --> 05:04.720
 Yeah, but I think that flaws that human intelligence has

05:04.720 --> 05:08.420
 is extremely prejudicial and biased

05:08.420 --> 05:10.480
 in the way it draws many inferences.

05:10.480 --> 05:12.040
 Do you think those are, sorry to interrupt,

05:12.040 --> 05:14.360
 do you think those are features or are those bugs?

05:14.360 --> 05:19.360
 Do you think the prejudice, the forgetfulness, the fear,

05:21.520 --> 05:22.860
 what are the flaws?

05:22.860 --> 05:23.700
 List them all.

05:23.700 --> 05:24.520
 What, love?

05:24.520 --> 05:25.560
 Maybe that's a flaw.

05:25.560 --> 05:28.920
 You think those are all things that can be gotten,

05:28.920 --> 05:30.780
 get in the way of intelligence

05:30.780 --> 05:33.420
 or the essential components of intelligence?

05:33.420 --> 05:36.160
 Well, again, if you go back and you define intelligence

05:36.160 --> 05:42.120
 as being able to sort of accurately, precisely, rigorously,

05:42.120 --> 05:43.840
 reason, develop answers,

05:43.840 --> 05:46.640
 and justify those answers in an objective way,

05:46.640 --> 05:49.720
 yeah, then human intelligence has these flaws

05:49.720 --> 05:52.880
 in that it tends to be more influenced

05:52.880 --> 05:55.200
 by some of the things you said.

05:56.520 --> 05:59.740
 And it's largely an inductive process,

05:59.740 --> 06:01.580
 meaning it takes past data,

06:01.580 --> 06:03.560
 uses that to predict the future.

06:03.560 --> 06:06.000
 Very advantageous in some cases,

06:06.000 --> 06:09.280
 but fundamentally biased and prejudicial in other cases

06:09.280 --> 06:11.520
 because it's gonna be strongly influenced by its priors,

06:11.520 --> 06:13.880
 whether they're right or wrong

06:13.880 --> 06:17.420
 from some objective reasoning perspective,

06:17.420 --> 06:20.500
 you're gonna favor them because those are the decisions

06:20.500 --> 06:24.000
 or those are the paths that succeeded in the past.

06:24.000 --> 06:29.000
 And I think that mode of intelligence makes a lot of sense

06:29.240 --> 06:33.280
 for when your primary goal is to act quickly

06:33.280 --> 06:37.200
 and survive and make fast decisions.

06:37.200 --> 06:39.360
 And I think those create problems

06:40.300 --> 06:42.040
 when you wanna think more deeply

06:42.040 --> 06:45.120
 and make more objective and reasoned decisions.

06:45.120 --> 06:48.320
 Of course, humans capable of doing both.

06:48.320 --> 06:51.120
 They do sort of one more naturally than they do the other,

06:51.120 --> 06:53.280
 but they're capable of doing both.

06:53.280 --> 06:54.540
 You're saying they do the one

06:54.540 --> 06:56.480
 that responds quickly more naturally.

06:56.480 --> 06:57.320
 Right.

06:57.320 --> 06:58.440
 Because that's the thing we kind of need

06:58.440 --> 07:02.720
 to not be eaten by the predators in the world.

07:02.720 --> 07:07.720
 For example, but then we've learned to reason through logic,

07:09.560 --> 07:12.980
 we've developed science, we train people to do that.

07:13.960 --> 07:16.960
 I think that's harder for the individual to do.

07:16.960 --> 07:20.960
 I think it requires training and teaching.

07:20.960 --> 07:24.180
 I think we are, the human mind certainly is capable of it,

07:24.180 --> 07:25.280
 but we find it more difficult.

07:25.280 --> 07:27.680
 And then there are other weaknesses, if you will,

07:27.680 --> 07:30.620
 as you mentioned earlier, just memory capacity

07:30.620 --> 07:33.800
 and how many chains of inference

07:33.800 --> 07:37.280
 can you actually go through without like losing your way?

07:37.280 --> 07:40.280
 So just focus and...

07:40.280 --> 07:43.240
 So the way you think about intelligence,

07:43.240 --> 07:45.080
 and we're really sort of floating

07:45.080 --> 07:47.220
 in this philosophical space,

07:47.220 --> 07:50.080
 but I think you're like the perfect person

07:50.080 --> 07:51.200
 to talk about this,

07:52.160 --> 07:55.660
 because we'll get to Jeopardy and beyond.

07:55.660 --> 07:58.700
 That's like one of the most incredible accomplishments

07:58.700 --> 08:00.920
 in AI, in the history of AI,

08:00.920 --> 08:03.400
 but hence the philosophical discussion.

08:03.400 --> 08:06.280
 So let me ask, you've kind of alluded to it,

08:06.280 --> 08:09.400
 but let me ask again, what is intelligence?

08:09.400 --> 08:12.460
 Underlying the discussions we'll have

08:12.460 --> 08:15.520
 with Jeopardy and beyond,

08:15.520 --> 08:17.080
 how do you think about intelligence?

08:17.080 --> 08:19.800
 Is it a sufficiently complicated problem

08:19.800 --> 08:22.440
 being able to reason your way through solving that problem?

08:22.440 --> 08:23.800
 Is that kind of how you think about

08:23.800 --> 08:25.440
 what it means to be intelligent?

08:25.440 --> 08:29.720
 So I think of intelligence primarily two ways.

08:29.720 --> 08:33.320
 One is the ability to predict.

08:33.320 --> 08:35.840
 So in other words, if I have a problem,

08:35.840 --> 08:37.600
 can I predict what's gonna happen next?

08:37.600 --> 08:40.900
 Whether it's to predict the answer of a question

08:40.900 --> 08:43.880
 or to say, look, I'm looking at all the market dynamics

08:43.880 --> 08:46.160
 and I'm gonna tell you what's gonna happen next,

08:46.160 --> 08:49.400
 or you're in a room and somebody walks in

08:49.400 --> 08:51.320
 and you're gonna predict what they're gonna do next

08:51.320 --> 08:53.660
 or what they're gonna say next.

08:53.660 --> 08:55.760
 You're in a highly dynamic environment

08:55.760 --> 08:58.680
 full of uncertainty, be able to predict.

08:58.680 --> 09:01.480
 The more variables, the more complex.

09:01.480 --> 09:04.080
 The more possibilities, the more complex.

09:04.080 --> 09:07.720
 But can I take a small amount of prior data

09:07.720 --> 09:09.880
 and learn the pattern and then predict

09:09.880 --> 09:13.040
 what's gonna happen next accurately and consistently?

09:13.920 --> 09:16.960
 That's certainly a form of intelligence.

09:16.960 --> 09:18.320
 What do you need for that, by the way?

09:18.320 --> 09:21.160
 You need to have an understanding

09:21.160 --> 09:22.880
 of the way the world works

09:22.880 --> 09:26.320
 in order to be able to unroll it into the future, right?

09:26.320 --> 09:28.000
 What do you think is needed to predict?

09:28.000 --> 09:29.440
 Depends what you mean by understanding.

09:29.440 --> 09:32.240
 I need to be able to find that function.

09:32.240 --> 09:35.100
 This is very much what deep learning does,

09:35.100 --> 09:38.960
 machine learning does, is if you give me enough prior data

09:38.960 --> 09:41.940
 and you tell me what the output variable is that matters,

09:41.940 --> 09:44.440
 I'm gonna sit there and be able to predict it.

09:44.440 --> 09:47.280
 And if I can predict it accurately

09:47.280 --> 09:50.280
 so that I can get it right more often than not,

09:50.280 --> 09:52.920
 I'm smart, if I can do that with less data

09:52.920 --> 09:56.020
 and less training time, I'm even smarter.

09:58.000 --> 10:00.620
 If I can figure out what's even worth predicting,

10:01.640 --> 10:03.920
 I'm smarter, meaning I'm figuring out

10:03.920 --> 10:06.400
 what path is gonna get me toward a goal.

10:06.400 --> 10:07.560
 What about picking a goal?

10:07.560 --> 10:08.480
 Sorry, you left again.

10:08.480 --> 10:10.120
 Well, that's interesting about picking a goal,

10:10.120 --> 10:11.080
 sort of an interesting thing.

10:11.080 --> 10:13.240
 I think that's where you bring in

10:13.240 --> 10:15.040
 what are you preprogrammed to do?

10:15.040 --> 10:16.000
 We talk about humans,

10:16.000 --> 10:19.400
 and well, humans are preprogrammed to survive.

10:19.400 --> 10:23.320
 So it's sort of their primary driving goal.

10:23.320 --> 10:24.700
 What do they have to do to do that?

10:24.700 --> 10:27.360
 And that can be very complex, right?

10:27.360 --> 10:31.680
 So it's not just figuring out that you need to run away

10:31.680 --> 10:33.640
 from the ferocious tiger,

10:33.640 --> 10:38.640
 but we survive in a social context as an example.

10:38.720 --> 10:42.320
 So understanding the subtleties of social dynamics

10:42.320 --> 10:45.420
 becomes something that's important for surviving,

10:45.420 --> 10:47.200
 finding a mate, reproducing, right?

10:47.200 --> 10:50.320
 So we're continually challenged with

10:50.320 --> 10:53.760
 complex sets of variables, complex constraints,

10:53.760 --> 10:56.880
 rules, if you will, or patterns.

10:56.880 --> 10:59.320
 And we learn how to find the functions

10:59.320 --> 11:00.680
 and predict the things.

11:00.680 --> 11:03.580
 In other words, represent those patterns efficiently

11:03.580 --> 11:04.920
 and be able to predict what's gonna happen.

11:04.920 --> 11:06.080
 And that's a form of intelligence.

11:06.080 --> 11:11.080
 That doesn't really require anything specific

11:11.400 --> 11:13.400
 other than the ability to find that function

11:13.400 --> 11:15.840
 and predict that right answer.

11:15.840 --> 11:18.440
 That's certainly a form of intelligence.

11:18.440 --> 11:23.280
 But then when we say, well, do we understand each other?

11:23.280 --> 11:28.280
 In other words, would you perceive me as intelligent

11:28.640 --> 11:31.040
 beyond that ability to predict?

11:31.040 --> 11:35.220
 So now I can predict, but I can't really articulate

11:35.220 --> 11:37.840
 how I'm going through that process,

11:37.840 --> 11:41.240
 what my underlying theory is for predicting,

11:41.240 --> 11:43.680
 and I can't get you to understand what I'm doing

11:43.680 --> 11:48.040
 so that you can figure out how to do this yourself

11:48.040 --> 11:50.760
 if you did not have, for example,

11:50.760 --> 11:53.840
 the right pattern matching machinery that I did.

11:53.840 --> 11:55.740
 And now we potentially have this breakdown

11:55.740 --> 11:59.080
 where, in effect, I'm intelligent,

11:59.080 --> 12:02.640
 but I'm sort of an alien intelligence relative to you.

12:02.640 --> 12:05.960
 You're intelligent, but nobody knows about it, or I can't.

12:05.960 --> 12:08.200
 Well, I can see the output.

12:08.200 --> 12:11.680
 So you're saying, let's sort of separate the two things.

12:11.680 --> 12:15.880
 One is you explaining why you were able

12:15.880 --> 12:17.260
 to predict the future,

12:19.160 --> 12:23.360
 and the second is me being able to,

12:23.360 --> 12:25.520
 impressing me that you're intelligent,

12:25.520 --> 12:26.520
 me being able to know

12:26.520 --> 12:28.680
 that you successfully predicted the future.

12:28.680 --> 12:29.600
 Do you think that's?

12:29.600 --> 12:31.360
 Well, it's not impressing you that I'm intelligent.

12:31.360 --> 12:33.640
 In other words, you may be convinced

12:33.640 --> 12:35.960
 that I'm intelligent in some form.

12:35.960 --> 12:37.160
 So how, what would convince?

12:37.160 --> 12:38.880
 Because of my ability to predict.

12:38.880 --> 12:39.720
 So I would look at the metrics.

12:39.720 --> 12:41.120
 When you can't, I'd say, wow.

12:41.120 --> 12:44.960
 You're right more times than I am.

12:44.960 --> 12:46.280
 You're doing something interesting.

12:46.280 --> 12:49.120
 That's a form of intelligence.

12:49.120 --> 12:53.400
 But then what happens is, if I say, how are you doing that?

12:53.400 --> 12:55.280
 And you can't communicate with me,

12:55.280 --> 12:57.720
 and you can't describe that to me,

12:57.720 --> 13:00.700
 now I may label you a savant.

13:00.700 --> 13:03.240
 I may say, well, you're doing something weird,

13:03.240 --> 13:06.360
 and it's just not very interesting to me,

13:06.360 --> 13:09.360
 because you and I can't really communicate.

13:09.360 --> 13:12.360
 And so now, so this is interesting, right?

13:12.360 --> 13:15.120
 Because now this is, you're in this weird place

13:15.120 --> 13:17.480
 where for you to be recognized

13:17.480 --> 13:21.300
 as intelligent the way I'm intelligent,

13:21.300 --> 13:24.280
 then you and I sort of have to be able to communicate.

13:24.280 --> 13:28.520
 And then my, we start to understand each other,

13:28.520 --> 13:33.480
 and then my respect and my appreciation,

13:33.480 --> 13:36.760
 my ability to relate to you starts to change.

13:36.760 --> 13:39.080
 So now you're not an alien intelligence anymore.

13:39.080 --> 13:41.080
 You're a human intelligence now,

13:41.080 --> 13:43.880
 because you and I can communicate.

13:43.880 --> 13:48.120
 And so I think when we look at animals, for example,

13:48.120 --> 13:50.720
 animals can do things we can't quite comprehend,

13:50.720 --> 13:51.800
 we don't quite know how they do them,

13:51.800 --> 13:54.420
 but they can't really communicate with us.

13:54.420 --> 13:58.360
 They can't put what they're going through in our terms.

13:58.360 --> 13:59.720
 And so we think of them as sort of,

13:59.720 --> 14:01.520
 well, they're these alien intelligences,

14:01.520 --> 14:03.600
 and they're not really worth necessarily what we're worth.

14:03.600 --> 14:06.360
 We don't treat them the same way as a result of that.

14:06.360 --> 14:11.360
 But it's hard because who knows what's going on.

14:11.640 --> 14:15.640
 So just a quick elaboration on that,

14:15.640 --> 14:18.760
 the explaining that you're intelligent,

14:18.760 --> 14:22.280
 the explaining the reasoning that went into the prediction

14:23.480 --> 14:27.080
 is not some kind of mathematical proof.

14:27.080 --> 14:28.840
 If we look at humans,

14:28.840 --> 14:32.220
 look at political debates and discourse on Twitter,

14:32.220 --> 14:35.340
 it's mostly just telling stories.

14:35.340 --> 14:38.400
 So your task is, sorry,

14:38.400 --> 14:43.400
 your task is not to tell an accurate depiction

14:43.680 --> 14:48.420
 of how you reason, but to tell a story, real or not,

14:48.420 --> 14:52.000
 that convinces me that there was a mechanism by which you.

14:52.000 --> 14:53.600
 Ultimately, that's what a proof is.

14:53.600 --> 14:56.240
 I mean, even a mathematical proof is that.

14:56.240 --> 14:58.200
 Because ultimately, the other mathematicians

14:58.200 --> 15:00.020
 have to be convinced by your proof.

15:01.600 --> 15:03.000
 Otherwise, in fact, there have been.

15:03.000 --> 15:04.440
 That's the metric for success, yeah.

15:04.440 --> 15:06.080
 There have been several proofs out there

15:06.080 --> 15:08.040
 where mathematicians would study for a long time

15:08.040 --> 15:08.880
 before they were convinced

15:08.880 --> 15:10.920
 that it actually proved anything, right?

15:10.920 --> 15:12.240
 You never know if it proved anything

15:12.240 --> 15:14.880
 until the community of mathematicians decided that it did.

15:14.880 --> 15:18.680
 So I mean, but it's a real thing, right?

15:18.680 --> 15:20.960
 And that's sort of the point, right?

15:20.960 --> 15:24.640
 Is that ultimately, this notion of understanding us,

15:24.640 --> 15:28.240
 understanding something is ultimately a social concept.

15:28.240 --> 15:30.740
 In other words, I have to convince enough people

15:30.740 --> 15:33.760
 that I did this in a reasonable way.

15:33.760 --> 15:36.400
 I did this in a way that other people can understand

15:36.400 --> 15:39.840
 and replicate and that it makes sense to them.

15:39.840 --> 15:44.840
 So human intelligence is bound together in that way.

15:44.840 --> 15:47.460
 We're bound up in that sense.

15:47.460 --> 15:49.560
 We sort of never really get away with it

15:49.560 --> 15:52.600
 until we can sort of convince others

15:52.600 --> 15:55.880
 that our thinking process makes sense.

15:55.880 --> 15:59.120
 Did you think the general question of intelligence

15:59.120 --> 16:01.000
 is then also a social construct?

16:01.000 --> 16:06.000
 So if we ask questions of an artificial intelligence system,

16:06.720 --> 16:08.640
 is this system intelligent?

16:08.640 --> 16:12.640
 The answer will ultimately be a socially constructed.

16:12.640 --> 16:16.040
 I think, so I think I'm making two statements.

16:16.040 --> 16:18.040
 I'm saying we can try to define intelligence

16:18.040 --> 16:23.040
 in this super objective way that says, here's this data.

16:23.120 --> 16:26.760
 I wanna predict this type of thing, learn this function.

16:26.760 --> 16:30.360
 And then if you get it right, often enough,

16:30.360 --> 16:32.080
 we consider you intelligent.

16:32.080 --> 16:34.400
 But that's more like a sub bond.

16:34.400 --> 16:35.720
 I think it is.

16:35.720 --> 16:37.120
 It doesn't mean it's not useful.

16:37.120 --> 16:38.640
 It could be incredibly useful.

16:38.640 --> 16:41.460
 It could be solving a problem we can't otherwise solve

16:41.460 --> 16:44.480
 and can solve it more reliably than we can.

16:44.480 --> 16:46.960
 But then there's this notion of,

16:46.960 --> 16:50.420
 can humans take responsibility

16:50.420 --> 16:53.680
 for the decision that you're making?

16:53.680 --> 16:56.120
 Can we make those decisions ourselves?

16:56.120 --> 16:58.840
 Can we relate to the process that you're going through?

16:58.840 --> 17:01.160
 And now you as an agent,

17:01.160 --> 17:04.520
 whether you're a machine or another human, frankly,

17:04.520 --> 17:08.640
 are now obliged to make me understand

17:08.640 --> 17:10.860
 how it is that you're arriving at that answer

17:10.860 --> 17:13.840
 and allow me, me or obviously a community

17:13.840 --> 17:15.000
 or a judge of people to decide

17:15.000 --> 17:17.280
 whether or not that makes sense.

17:17.280 --> 17:20.200
 And by the way, that happens with the humans as well.

17:20.200 --> 17:22.080
 You're sitting down with your staff, for example,

17:22.080 --> 17:25.520
 and you ask for suggestions about what to do next.

17:26.360 --> 17:28.640
 And someone says, oh, I think you should buy.

17:28.640 --> 17:30.560
 And I actually think you should buy this much

17:30.560 --> 17:33.160
 or whatever or sell or whatever it is.

17:33.160 --> 17:35.720
 Or I think you should launch the product today or tomorrow

17:35.720 --> 17:37.080
 or launch this product versus that product,

17:37.080 --> 17:38.600
 whatever the decision may be.

17:38.600 --> 17:39.840
 And you ask why.

17:39.840 --> 17:40.680
 And the person says,

17:40.680 --> 17:42.800
 I just have a good feeling about it.

17:42.800 --> 17:44.400
 And you're not very satisfied.

17:44.400 --> 17:47.600
 Now, that person could be,

17:47.600 --> 17:50.880
 you might say, well, you've been right before,

17:50.880 --> 17:54.140
 but I'm gonna put the company on the line.

17:54.140 --> 17:56.760
 Can you explain to me why I should believe this?

17:56.760 --> 17:58.000
 Right.

17:58.000 --> 18:00.960
 And that explanation may have nothing to do with the truth.

18:00.960 --> 18:01.800
 You just, the ultimate.

18:01.800 --> 18:03.520
 It's gotta convince the other person.

18:03.520 --> 18:05.280
 Still be wrong, still be wrong.

18:05.280 --> 18:06.320
 She's gotta be convincing.

18:06.320 --> 18:07.840
 But it's ultimately gotta be convincing.

18:07.840 --> 18:10.200
 And that's why I'm saying it's,

18:10.200 --> 18:12.160
 we're bound together, right?

18:12.160 --> 18:14.160
 Our intelligences are bound together in that sense.

18:14.160 --> 18:15.360
 We have to understand each other.

18:15.360 --> 18:18.920
 And if, for example, you're giving me an explanation,

18:18.920 --> 18:21.020
 I mean, this is a very important point, right?

18:21.020 --> 18:23.840
 You're giving me an explanation,

18:23.840 --> 18:28.280
 and I'm not good,

18:29.380 --> 18:33.520
 and then I'm not good at reasoning well,

18:33.520 --> 18:35.280
 and being objective,

18:35.280 --> 18:39.160
 and following logical paths and consistent paths,

18:39.160 --> 18:41.400
 and I'm not good at measuring

18:41.400 --> 18:45.520
 and sort of computing probabilities across those paths.

18:45.520 --> 18:47.200
 What happens is collectively,

18:47.200 --> 18:50.120
 we're not gonna do well.

18:50.120 --> 18:52.240
 How hard is that problem?

18:52.240 --> 18:53.180
 The second one.

18:53.180 --> 18:57.960
 So I think we'll talk quite a bit about the first

18:57.960 --> 19:02.960
 on a specific objective metric benchmark performing well.

19:03.840 --> 19:07.440
 But being able to explain the steps,

19:07.440 --> 19:10.580
 the reasoning, how hard is that problem?

19:10.580 --> 19:11.800
 I think that's very hard.

19:11.800 --> 19:13.300
 I mean, I think that that's,

19:16.040 --> 19:18.160
 well, it's hard for humans.

19:18.160 --> 19:20.960
 The thing that's hard for humans, as you know,

19:20.960 --> 19:22.920
 may not necessarily be hard for computers

19:22.920 --> 19:24.440
 and vice versa.

19:24.440 --> 19:29.440
 So, sorry, so how hard is that problem for computers?

19:31.160 --> 19:32.640
 I think it's hard for computers,

19:32.640 --> 19:34.600
 and the reason why I related to,

19:34.600 --> 19:36.400
 or saying that it's also hard for humans

19:36.400 --> 19:38.320
 is because I think when we step back

19:38.320 --> 19:41.920
 and we say we wanna design computers to do that,

19:43.520 --> 19:46.440
 one of the things we have to recognize

19:46.440 --> 19:50.520
 is we're not sure how to do it well.

19:50.520 --> 19:52.960
 I'm not sure we have a recipe for that.

19:52.960 --> 19:55.320
 And even if you wanted to learn it,

19:55.320 --> 19:58.400
 it's not clear exactly what data we use

19:59.480 --> 20:03.720
 and what judgments we use to learn that well.

20:03.720 --> 20:05.480
 And so what I mean by that is

20:05.480 --> 20:09.500
 if you look at the entire enterprise of science,

20:09.500 --> 20:11.640
 science is supposed to be at about

20:11.640 --> 20:13.680
 objective reason and reason, right?

20:13.680 --> 20:17.680
 So we think about, gee, who's the most intelligent person

20:17.680 --> 20:20.500
 or group of people in the world?

20:20.500 --> 20:24.080
 Do we think about the savants who can close their eyes

20:24.080 --> 20:25.540
 and give you a number?

20:25.540 --> 20:27.680
 We think about the think tanks,

20:27.680 --> 20:29.500
 or the scientists or the philosophers

20:29.500 --> 20:32.680
 who kind of work through the details

20:32.680 --> 20:35.960
 and write the papers and come up with the thoughtful,

20:35.960 --> 20:39.480
 logical proofs and use the scientific method.

20:39.480 --> 20:40.680
 I think it's the latter.

20:42.760 --> 20:45.800
 And my point is that how do you train someone to do that?

20:45.800 --> 20:46.920
 And that's what I mean by it's hard.

20:46.920 --> 20:49.400
 How do you, what's the process of training people

20:49.400 --> 20:50.800
 to do that well?

20:50.800 --> 20:52.400
 That's a hard process.

20:52.400 --> 20:56.020
 We work, as a society, we work pretty hard

20:56.020 --> 20:59.240
 to get other people to understand our thinking

20:59.240 --> 21:02.220
 and to convince them of things.

21:02.220 --> 21:04.040
 Now we could persuade them,

21:04.040 --> 21:05.300
 obviously you talked about this,

21:05.300 --> 21:07.520
 like human flaws or weaknesses,

21:07.520 --> 21:12.160
 we can persuade them through emotional means.

21:12.160 --> 21:16.140
 But to get them to understand and connect to

21:16.140 --> 21:19.960
 and follow a logical argument is difficult.

21:19.960 --> 21:22.440
 We try it, we do it, we do it as scientists,

21:22.440 --> 21:24.200
 we try to do it as journalists,

21:24.200 --> 21:27.280
 we try to do it as even artists in many forms,

21:27.280 --> 21:29.780
 as writers, as teachers.

21:29.780 --> 21:32.940
 We go through a fairly significant training process

21:33.860 --> 21:34.700
 to do that.

21:34.700 --> 21:37.900
 And then we could ask, well, why is that so hard?

21:39.040 --> 21:39.920
 But it's hard.

21:39.920 --> 21:42.960
 And for humans, it takes a lot of work.

21:44.060 --> 21:45.960
 And when we step back and say,

21:45.960 --> 21:49.160
 well, how do we get a machine to do that?

21:49.160 --> 21:50.640
 It's a vexing question.

21:51.840 --> 21:55.240
 How would you begin to try to solve that?

21:55.240 --> 21:57.400
 And maybe just a quick pause,

21:57.400 --> 21:59.840
 because there's an optimistic notion

21:59.840 --> 22:01.040
 in the things you're describing,

22:01.040 --> 22:05.120
 which is being able to explain something through reason.

22:05.980 --> 22:08.660
 But if you look at algorithms that recommend things

22:08.660 --> 22:11.800
 that we'll look at next, whether it's Facebook, Google,

22:11.800 --> 22:16.800
 advertisement based companies, their goal is to convince you

22:18.000 --> 22:22.360
 to buy things based on anything.

22:23.480 --> 22:25.440
 So that could be reason,

22:25.440 --> 22:28.100
 because the best of advertisement is showing you things

22:28.100 --> 22:31.980
 that you really do need and explain why you need it.

22:31.980 --> 22:35.660
 But it could also be through emotional manipulation.

22:37.000 --> 22:41.760
 The algorithm that describes why a certain decision

22:41.760 --> 22:45.760
 was made, how hard is it to do it

22:45.760 --> 22:48.160
 through emotional manipulation?

22:48.160 --> 22:51.920
 And why is that a good or a bad thing?

22:52.760 --> 22:56.840
 So you've kind of focused on reason, logic,

22:56.840 --> 23:01.460
 really showing in a clear way why something is good.

23:02.520 --> 23:05.920
 One, is that even a thing that us humans do?

23:05.920 --> 23:09.920
 And two, how do you think of the difference

23:09.920 --> 23:13.460
 in the reasoning aspect and the emotional manipulation?

23:15.160 --> 23:17.360
 So you call it emotional manipulation,

23:17.360 --> 23:20.220
 but more objectively is essentially saying,

23:20.220 --> 23:22.660
 there are certain features of things

23:22.660 --> 23:24.440
 that seem to attract your attention.

23:24.440 --> 23:26.800
 I mean, it kind of give you more of that stuff.

23:26.800 --> 23:28.280
 Manipulation is a bad word.

23:28.280 --> 23:31.140
 Yeah, I mean, I'm not saying it's good right or wrong.

23:31.140 --> 23:32.960
 It works to get your attention

23:32.960 --> 23:34.440
 and it works to get you to buy stuff.

23:34.440 --> 23:36.960
 And when you think about algorithms that look

23:36.960 --> 23:40.000
 at the patterns of features

23:40.000 --> 23:41.920
 that you seem to be spending your money on

23:41.920 --> 23:43.280
 and say, I'm gonna give you something

23:43.280 --> 23:44.820
 with a similar pattern.

23:44.820 --> 23:46.080
 So I'm gonna learn that function

23:46.080 --> 23:48.200
 because the objective is to get you to click on it

23:48.200 --> 23:50.240
 or get you to buy it or whatever it is.

23:51.120 --> 23:53.400
 I don't know, I mean, it is what it is.

23:53.400 --> 23:55.840
 I mean, that's what the algorithm does.

23:55.840 --> 23:57.440
 You can argue whether it's good or bad.

23:57.440 --> 24:00.440
 It depends what your goal is.

24:00.440 --> 24:02.480
 I guess this seems to be very useful

24:02.480 --> 24:05.280
 for convincing, for telling a story.

24:05.280 --> 24:07.680
 For convincing humans, it's good

24:07.680 --> 24:11.800
 because again, this goes back to what is the human behavior

24:11.800 --> 24:16.800
 like, how does the human brain respond to things?

24:17.000 --> 24:19.360
 I think there's a more optimistic view of that too,

24:19.360 --> 24:22.000
 which is that if you're searching

24:22.000 --> 24:23.120
 for certain kinds of things,

24:23.120 --> 24:26.160
 you've already reasoned that you need them.

24:26.160 --> 24:30.080
 And these algorithms are saying, look, that's up to you

24:30.080 --> 24:32.180
 to reason whether you need something or not.

24:32.180 --> 24:33.400
 That's your job.

24:33.400 --> 24:36.920
 You may have an unhealthy addiction to this stuff

24:36.920 --> 24:41.920
 or you may have a reasoned and thoughtful explanation

24:42.880 --> 24:44.520
 for why it's important to you.

24:44.520 --> 24:47.040
 And the algorithms are saying, hey, that's like, whatever.

24:47.040 --> 24:48.040
 Like, that's your problem.

24:48.040 --> 24:50.580
 All I know is you're buying stuff like that.

24:50.580 --> 24:51.920
 You're interested in stuff like that.

24:51.920 --> 24:53.920
 Could be a bad reason, could be a good reason.

24:53.920 --> 24:55.080
 That's up to you.

24:55.080 --> 24:57.520
 I'm gonna show you more of that stuff.

24:57.520 --> 25:01.640
 And I think that it's not good or bad.

25:01.640 --> 25:03.520
 It's not reasoned or not reasoned.

25:03.520 --> 25:04.880
 The algorithm is doing what it does,

25:04.880 --> 25:06.920
 which is saying, you seem to be interested in this.

25:06.920 --> 25:09.320
 I'm gonna show you more of that stuff.

25:09.320 --> 25:11.200
 And I think we're seeing this not just in buying stuff,

25:11.200 --> 25:12.160
 but even in social media.

25:12.160 --> 25:13.960
 You're reading this kind of stuff.

25:13.960 --> 25:15.740
 I'm not judging on whether it's good or bad.

25:15.740 --> 25:16.920
 I'm not reasoning at all.

25:16.920 --> 25:19.200
 I'm just saying, I'm gonna show you other stuff

25:19.200 --> 25:20.800
 with similar features.

25:20.800 --> 25:22.360
 And like, and that's it.

25:22.360 --> 25:23.840
 And I wash my hands from it and I say,

25:23.840 --> 25:25.940
 that's all that's going on.

25:25.940 --> 25:30.940
 You know, there is, people are so harsh on AI systems.

25:31.900 --> 25:34.940
 So one, the bar of performance is extremely high.

25:34.940 --> 25:39.560
 And yet we also ask them to, in the case of social media,

25:39.560 --> 25:42.940
 to help find the better angels of our nature

25:42.940 --> 25:45.980
 and help make a better society.

25:45.980 --> 25:47.860
 What do you think about the role of AI there?

25:47.860 --> 25:48.860
 So that, I agree with you.

25:48.860 --> 25:51.580
 That's the interesting dichotomy, right?

25:51.580 --> 25:54.160
 Because on one hand, we're sitting there

25:54.160 --> 25:55.880
 and we're sort of doing the easy part,

25:55.880 --> 25:57.980
 which is finding the patterns.

25:57.980 --> 26:01.820
 We're not building, the system's not building a theory

26:01.820 --> 26:04.220
 that is consumable and understandable to other humans

26:04.220 --> 26:06.380
 that can be explained and justified.

26:06.380 --> 26:11.380
 And so on one hand to say, oh, you know, AI is doing this.

26:11.500 --> 26:13.700
 Why isn't doing this other thing?

26:13.700 --> 26:16.300
 Well, this other thing's a lot harder.

26:16.300 --> 26:20.180
 And it's interesting to think about why it's harder.

26:20.180 --> 26:23.980
 And because you're interpreting the data

26:23.980 --> 26:26.300
 in the context of prior models.

26:26.300 --> 26:28.140
 In other words, understandings

26:28.140 --> 26:30.260
 of what's important in the world, what's not important.

26:30.260 --> 26:32.060
 What are all the other abstract features

26:32.060 --> 26:35.420
 that drive our decision making?

26:35.420 --> 26:36.980
 What's sensible, what's not sensible,

26:36.980 --> 26:38.620
 what's good, what's bad, what's moral,

26:38.620 --> 26:40.060
 what's valuable, what isn't?

26:40.060 --> 26:41.160
 Where is that stuff?

26:41.160 --> 26:43.300
 No one's applying the interpretation.

26:43.300 --> 26:46.660
 So when I see you clicking on a bunch of stuff

26:46.660 --> 26:49.780
 and I look at these simple features, the raw features,

26:49.780 --> 26:51.140
 the features that are there in the data,

26:51.140 --> 26:56.140
 like what words are being used or how long the material is

26:57.700 --> 27:00.620
 or other very superficial features,

27:00.620 --> 27:02.500
 what colors are being used in the material.

27:02.500 --> 27:03.880
 Like, I don't know why you're clicking

27:03.880 --> 27:04.980
 on this stuff you're clicking.

27:04.980 --> 27:07.620
 Or if it's products, what the price is

27:07.620 --> 27:09.540
 or what the categories and stuff like that.

27:09.540 --> 27:11.540
 And I just feed you more of the same stuff.

27:11.540 --> 27:13.740
 That's very different than kind of getting in there

27:13.740 --> 27:16.020
 and saying, what does this mean?

27:16.020 --> 27:20.380
 The stuff you're reading, like why are you reading it?

27:21.380 --> 27:23.900
 What assumptions are you bringing to the table?

27:23.900 --> 27:26.380
 Are those assumptions sensible?

27:26.380 --> 27:28.980
 Does the material make any sense?

27:28.980 --> 27:33.980
 Does it lead you to thoughtful, good conclusions?

27:34.080 --> 27:37.420
 Again, there's interpretation and judgment involved

27:37.420 --> 27:42.420
 in that process that isn't really happening in the AI today.

27:42.420 --> 27:47.200
 That's harder because you have to start getting

27:47.200 --> 27:52.040
 at the meaning of the stuff, of the content.

27:52.040 --> 27:55.760
 You have to get at how humans interpret the content

27:55.760 --> 27:58.720
 relative to their value system

27:58.720 --> 28:00.600
 and deeper thought processes.

28:00.600 --> 28:04.520
 So that's what meaning means is not just some kind

28:04.520 --> 28:09.300
 of deep, timeless, semantic thing

28:09.300 --> 28:10.960
 that the statement represents,

28:10.960 --> 28:13.400
 but also how a large number of people

28:13.400 --> 28:15.220
 are likely to interpret.

28:15.220 --> 28:19.120
 So that's again, even meaning is a social construct.

28:19.120 --> 28:22.800
 So you have to try to predict how most people

28:22.800 --> 28:24.520
 would understand this kind of statement.

28:24.520 --> 28:27.300
 Yeah, meaning is often relative,

28:27.300 --> 28:30.400
 but meaning implies that the connections go beneath

28:30.400 --> 28:31.840
 the surface of the artifacts.

28:31.840 --> 28:35.480
 If I show you a painting, it's a bunch of colors on a canvas,

28:35.480 --> 28:37.140
 what does it mean to you?

28:37.140 --> 28:39.400
 And it may mean different things to different people

28:39.400 --> 28:42.240
 because of their different experiences.

28:42.240 --> 28:44.720
 It may mean something even different

28:44.720 --> 28:46.440
 to the artist who painted it.

28:47.440 --> 28:50.720
 As we try to get more rigorous with our communication,

28:50.720 --> 28:53.280
 we try to really nail down that meaning.

28:53.280 --> 28:58.280
 So we go from abstract art to precise mathematics,

28:58.840 --> 29:01.520
 precise engineering drawings and things like that.

29:01.520 --> 29:04.480
 We're really trying to say, I wanna narrow

29:04.480 --> 29:08.300
 that space of possible interpretations

29:08.300 --> 29:10.720
 because the precision of the communication

29:10.720 --> 29:13.400
 ends up becoming more and more important.

29:13.400 --> 29:17.920
 And so that means that I have to specify,

29:17.920 --> 29:21.400
 and I think that's why this becomes really hard,

29:21.400 --> 29:24.200
 because if I'm just showing you an artifact

29:24.200 --> 29:26.000
 and you're looking at it superficially,

29:26.000 --> 29:28.220
 whether it's a bunch of words on a page,

29:28.220 --> 29:31.940
 or whether it's brushstrokes on a canvas

29:31.940 --> 29:33.600
 or pixels on a photograph,

29:33.600 --> 29:35.080
 you can sit there and you can interpret

29:35.080 --> 29:38.880
 lots of different ways at many, many different levels.

29:38.880 --> 29:43.880
 But when I wanna align our understanding of that,

29:45.640 --> 29:48.360
 I have to specify a lot more stuff

29:48.360 --> 29:52.320
 that's actually not directly in the artifact.

29:52.320 --> 29:55.960
 Now I have to say, well, how are you interpreting

29:55.960 --> 29:57.240
 this image and that image?

29:57.240 --> 29:59.360
 And what about the colors and what do they mean to you?

29:59.360 --> 30:02.360
 What perspective are you bringing to the table?

30:02.360 --> 30:05.440
 What are your prior experiences with those artifacts?

30:05.440 --> 30:08.800
 What are your fundamental assumptions and values?

30:08.800 --> 30:10.840
 What is your ability to kind of reason,

30:10.840 --> 30:13.640
 to chain together logical implication

30:13.640 --> 30:14.480
 as you're sitting there and saying,

30:14.480 --> 30:16.520
 well, if this is the case, then I would conclude this.

30:16.520 --> 30:19.120
 And if that's the case, then I would conclude that.

30:19.120 --> 30:22.520
 So your reasoning processes and how they work,

30:22.520 --> 30:25.360
 your prior models and what they are,

30:25.360 --> 30:27.200
 your values and your assumptions,

30:27.200 --> 30:30.640
 all those things now come together into the interpretation.

30:30.640 --> 30:33.820
 Getting in sync of that is hard.

30:34.860 --> 30:37.620
 And yet humans are able to intuit some of that

30:37.620 --> 30:39.580
 without any pre.

30:39.580 --> 30:41.580
 Because they have the shared experience.

30:41.580 --> 30:42.940
 And we're not talking about shared,

30:42.940 --> 30:44.420
 two people having shared experience.

30:44.420 --> 30:45.540
 I mean, as a society.

30:45.540 --> 30:46.540
 That's correct.

30:46.540 --> 30:51.180
 We have the shared experience and we have similar brains.

30:51.180 --> 30:54.060
 So we tend to, in other words,

30:54.060 --> 30:56.460
 part of our shared experiences are shared local experience.

30:56.460 --> 30:57.860
 Like we may live in the same culture,

30:57.860 --> 30:59.060
 we may live in the same society

30:59.060 --> 31:02.020
 and therefore we have similar educations.

31:02.020 --> 31:04.100
 We have some of what we like to call prior models

31:04.100 --> 31:05.860
 about the word prior experiences.

31:05.860 --> 31:07.380
 And we use that as a,

31:07.380 --> 31:10.940
 think of it as a wide collection of interrelated variables

31:10.940 --> 31:12.780
 and they're all bound to similar things.

31:12.780 --> 31:15.060
 And so we take that as our background

31:15.060 --> 31:17.540
 and we start interpreting things similarly.

31:17.540 --> 31:21.860
 But as humans, we have a lot of shared experience.

31:21.860 --> 31:24.980
 We do have similar brains, similar goals,

31:24.980 --> 31:28.060
 similar emotions under similar circumstances.

31:28.060 --> 31:29.020
 Because we're both humans.

31:29.020 --> 31:31.420
 So now one of the early questions you asked,

31:31.420 --> 31:36.420
 how is biological and computer information systems

31:37.020 --> 31:37.980
 fundamentally different?

31:37.980 --> 31:42.980
 Well, one is humans come with a lot of pre programmed stuff.

31:43.840 --> 31:45.940
 A ton of program stuff.

31:45.940 --> 31:47.220
 And they're able to communicate

31:47.220 --> 31:50.340
 because they share that stuff.

31:50.340 --> 31:52.700
 Do you think that shared knowledge,

31:54.100 --> 31:57.580
 if we can maybe escape the hard work question,

31:57.580 --> 31:59.460
 how much is encoded in the hardware?

31:59.460 --> 32:01.260
 Just the shared knowledge in the software,

32:01.260 --> 32:05.340
 the history, the many centuries of wars and so on

32:05.340 --> 32:08.000
 that came to today, that shared knowledge.

32:09.660 --> 32:13.400
 How hard is it to encode?

32:14.340 --> 32:15.860
 Do you have a hope?

32:15.860 --> 32:19.340
 Can you speak to how hard is it to encode that knowledge

32:19.340 --> 32:22.780
 systematically in a way that could be used by a computer?

32:22.780 --> 32:25.100
 So I think it is possible to learn to,

32:25.100 --> 32:27.900
 for a machine to program a machine,

32:27.900 --> 32:31.460
 to acquire that knowledge with a similar foundation.

32:31.460 --> 32:36.120
 In other words, a similar interpretive foundation

32:36.120 --> 32:38.060
 for processing that knowledge.

32:38.060 --> 32:39.100
 What do you mean by that?

32:39.100 --> 32:44.100
 So in other words, we view the world in a particular way.

32:44.540 --> 32:48.820
 So in other words, we have a, if you will,

32:48.820 --> 32:50.120
 as humans, we have a framework

32:50.120 --> 32:52.260
 for interpreting the world around us.

32:52.260 --> 32:55.220
 So we have multiple frameworks for interpreting

32:55.220 --> 32:56.060
 the world around us.

32:56.060 --> 32:59.780
 But if you're interpreting, for example,

32:59.780 --> 33:01.340
 socio political interactions,

33:01.340 --> 33:03.140
 you're thinking about where there's people,

33:03.140 --> 33:05.540
 there's collections and groups of people,

33:05.540 --> 33:08.460
 they have goals, goals largely built around survival

33:08.460 --> 33:10.860
 and quality of life.

33:10.860 --> 33:15.860
 There are fundamental economics around scarcity of resources.

33:16.640 --> 33:19.660
 And when humans come and start interpreting

33:19.660 --> 33:21.860
 a situation like that, because you brought up

33:21.860 --> 33:23.600
 like historical events,

33:23.600 --> 33:25.500
 they start interpreting situations like that.

33:25.500 --> 33:29.500
 They apply a lot of this fundamental framework

33:29.500 --> 33:30.740
 for interpreting that.

33:30.740 --> 33:32.260
 Well, who are the people?

33:32.260 --> 33:33.300
 What were their goals?

33:33.300 --> 33:35.020
 What resources did they have?

33:35.020 --> 33:37.020
 How much power influence did they have over the other?

33:37.020 --> 33:40.540
 Like this fundamental substrate, if you will,

33:40.540 --> 33:42.780
 for interpreting and reasoning about that.

33:43.820 --> 33:46.920
 So I think it is possible to imbue a computer

33:46.920 --> 33:50.660
 with that stuff that humans like take for granted

33:50.660 --> 33:54.020
 when they go and sit down and try to interpret things.

33:54.020 --> 33:58.860
 And then with that foundation, they acquire,

33:58.860 --> 34:00.300
 they start acquiring the details,

34:00.300 --> 34:02.820
 the specifics in a given situation,

34:02.820 --> 34:05.700
 are then able to interpret it with regard to that framework.

34:05.700 --> 34:08.700
 And then given that interpretation, they can do what?

34:08.700 --> 34:10.300
 They can predict.

34:10.300 --> 34:12.220
 But not only can they predict,

34:12.220 --> 34:14.820
 they can predict now with an explanation

34:15.940 --> 34:17.940
 that can be given in those terms,

34:17.940 --> 34:20.200
 in the terms of that underlying framework

34:20.200 --> 34:22.300
 that most humans share.

34:22.300 --> 34:24.620
 Now you could find humans that come and interpret events

34:24.620 --> 34:26.300
 very differently than other humans

34:26.300 --> 34:30.620
 because they're like using a different framework.

34:30.620 --> 34:32.500
 The movie Matrix comes to mind

34:32.500 --> 34:36.420
 where they decided humans were really just batteries,

34:36.420 --> 34:39.940
 and that's how they interpreted the value of humans

34:39.940 --> 34:41.640
 as a source of electrical energy.

34:41.640 --> 34:45.460
 So, but I think that for the most part,

34:45.460 --> 34:50.460
 we have a way of interpreting the events

34:50.780 --> 34:52.260
 or the social events around us

34:52.260 --> 34:54.140
 because we have this shared framework.

34:54.140 --> 34:58.700
 It comes from, again, the fact that we're similar beings

34:58.700 --> 35:01.100
 that have similar goals, similar emotions,

35:01.100 --> 35:02.900
 and we can make sense out of these.

35:02.900 --> 35:05.020
 These frameworks make sense to us.

35:05.020 --> 35:08.060
 So how much knowledge is there, do you think?

35:08.060 --> 35:09.580
 So you said it's possible.

35:09.580 --> 35:12.140
 Well, there's a tremendous amount of detailed knowledge

35:12.140 --> 35:12.980
 in the world.

35:12.980 --> 35:17.580
 You could imagine effectively infinite number

35:17.580 --> 35:20.840
 of unique situations and unique configurations

35:20.840 --> 35:22.100
 of these things.

35:22.100 --> 35:25.100
 But the knowledge that you need,

35:25.100 --> 35:27.600
 what I refer to as like the frameworks,

35:27.600 --> 35:29.580
 for you need for interpreting them, I don't think.

35:29.580 --> 35:31.500
 I think those are finite.

35:31.500 --> 35:35.020
 You think the frameworks are more important

35:35.020 --> 35:36.780
 than the bulk of the knowledge?

35:36.780 --> 35:37.780
 So it's like framing.

35:37.780 --> 35:39.220
 Yeah, because what the frameworks do

35:39.220 --> 35:41.580
 is they give you now the ability to interpret and reason,

35:41.580 --> 35:43.100
 and to interpret and reason,

35:43.100 --> 35:46.780
 to interpret and reason over the specifics

35:46.780 --> 35:49.220
 in ways that other humans would understand.

35:49.220 --> 35:51.240
 What about the specifics?

35:51.240 --> 35:53.980
 You know, you acquire the specifics by reading

35:53.980 --> 35:55.540
 and by talking to other people.

35:55.540 --> 35:57.700
 So I'm mostly actually just even,

35:57.700 --> 36:00.240
 if we can focus on even the beginning,

36:00.240 --> 36:01.500
 the common sense stuff,

36:01.500 --> 36:03.420
 the stuff that doesn't even require reading,

36:03.420 --> 36:06.860
 or it almost requires playing around with the world

36:06.860 --> 36:10.820
 or something, just being able to sort of manipulate objects,

36:10.820 --> 36:13.900
 drink water and so on, all of that.

36:13.900 --> 36:16.140
 Every time we try to do that kind of thing

36:16.140 --> 36:21.060
 in robotics or AI, it seems to be like an onion.

36:21.060 --> 36:23.240
 You seem to realize how much knowledge

36:23.240 --> 36:24.620
 is really required to perform

36:24.620 --> 36:27.060
 even some of these basic tasks.

36:27.060 --> 36:30.340
 Do you have that sense as well?

36:30.340 --> 36:33.820
 And if so, how do we get all those details?

36:33.820 --> 36:35.700
 Are they written down somewhere?

36:35.700 --> 36:39.220
 Do they have to be learned through experience?

36:39.220 --> 36:41.340
 So I think when, like, if you're talking about

36:41.340 --> 36:44.700
 sort of the physics, the basic physics around us,

36:44.700 --> 36:46.580
 for example, acquiring information about,

36:46.580 --> 36:48.140
 acquiring how that works.

36:49.720 --> 36:52.220
 Yeah, I mean, I think there's a combination of things going,

36:52.220 --> 36:54.620
 I think there's a combination of things going on.

36:54.620 --> 36:57.780
 I think there is like fundamental pattern matching,

36:57.780 --> 36:59.660
 like what we were talking about before,

36:59.660 --> 37:01.060
 where you see enough examples,

37:01.060 --> 37:03.840
 enough data about something and you start assuming that.

37:03.840 --> 37:05.480
 And with similar input,

37:05.480 --> 37:07.720
 I'm gonna predict similar outputs.

37:07.720 --> 37:10.100
 You can't necessarily explain it at all.

37:10.100 --> 37:13.600
 You may learn very quickly that when you let something go,

37:14.640 --> 37:16.500
 it falls to the ground.

37:16.500 --> 37:19.760
 But you can't necessarily explain that.

37:19.760 --> 37:22.340
 But that's such a deep idea,

37:22.340 --> 37:25.200
 that if you let something go, like the idea of gravity.

37:26.120 --> 37:27.900
 I mean, people are letting things go

37:27.900 --> 37:29.100
 and counting on them falling

37:29.100 --> 37:30.760
 well before they understood gravity.

37:30.760 --> 37:33.860
 But that seems to be, that's exactly what I mean,

37:33.860 --> 37:36.080
 is before you take a physics class

37:36.080 --> 37:39.540
 or study anything about Newton,

37:39.540 --> 37:42.540
 just the idea that stuff falls to the ground

37:42.540 --> 37:45.300
 and then you'd be able to generalize

37:45.300 --> 37:48.540
 that all kinds of stuff falls to the ground.

37:49.540 --> 37:53.420
 It just seems like a non, without encoding it,

37:53.420 --> 37:55.220
 like hard coding it in,

37:55.220 --> 37:57.420
 it seems like a difficult thing to pick up.

37:57.420 --> 38:01.380
 It seems like you have to have a lot of different knowledge

38:01.380 --> 38:05.340
 to be able to integrate that into the framework,

38:05.340 --> 38:07.700
 sort of into everything else.

38:07.700 --> 38:10.340
 So both know that stuff falls to the ground

38:10.340 --> 38:15.340
 and start to reason about sociopolitical discourse.

38:16.340 --> 38:18.540
 So both, like the very basic

38:18.540 --> 38:22.540
 and the high level reasoning decision making.

38:22.540 --> 38:24.940
 I guess my question is, how hard is this problem?

38:26.420 --> 38:29.060
 And sorry to linger on it because again,

38:29.060 --> 38:31.100
 and we'll get to it for sure,

38:31.100 --> 38:34.340
 as what Watson with Jeopardy did is take on a problem

38:34.340 --> 38:35.500
 that's much more constrained

38:35.500 --> 38:38.260
 but has the same hugeness of scale,

38:38.260 --> 38:40.660
 at least from the outsider's perspective.

38:40.660 --> 38:42.900
 So I'm asking the general life question

38:42.900 --> 38:45.580
 of to be able to be an intelligent being

38:45.580 --> 38:50.580
 and reason in the world about both gravity and politics,

38:50.900 --> 38:52.140
 how hard is that problem?

38:53.900 --> 38:56.180
 So I think it's solvable.

38:59.440 --> 39:00.700
 Okay, now beautiful.

39:00.700 --> 39:04.820
 So what about time travel?

39:04.820 --> 39:08.700
 Okay, I'm just saying the same answer.

39:08.700 --> 39:09.700
 Not as convinced.

39:09.700 --> 39:11.100
 Not as convinced yet, okay.

39:11.100 --> 39:14.260
 No, I think it is solvable.

39:14.260 --> 39:16.500
 I mean, I think that it's a learn,

39:16.500 --> 39:18.440
 first of all, it's about getting machines to learn.

39:18.440 --> 39:21.380
 Learning is fundamental.

39:21.380 --> 39:24.420
 And I think we're already in a place that we understand,

39:24.420 --> 39:28.620
 for example, how machines can learn in various ways.

39:28.620 --> 39:32.460
 Right now, our learning stuff is sort of primitive

39:32.460 --> 39:37.460
 in that we haven't sort of taught machines

39:38.040 --> 39:39.260
 to learn the frameworks.

39:39.260 --> 39:41.160
 We don't communicate our frameworks

39:41.160 --> 39:42.860
 because of how shared they are, in some cases we do,

39:42.860 --> 39:46.380
 but we don't annotate, if you will,

39:46.380 --> 39:48.960
 all the data in the world with the frameworks

39:48.960 --> 39:53.120
 that are inherent or underlying our understanding.

39:53.120 --> 39:56.180
 Instead, we just operate with the data.

39:56.180 --> 39:59.100
 So if we wanna be able to reason over the data

39:59.100 --> 40:02.300
 in similar terms in the common frameworks,

40:02.300 --> 40:03.740
 we need to be able to teach the computer,

40:03.740 --> 40:06.300
 or at least we need to program the computer

40:06.300 --> 40:10.480
 to acquire, to have access to and acquire,

40:10.480 --> 40:12.860
 learn the frameworks as well

40:12.860 --> 40:15.740
 and connect the frameworks to the data.

40:15.740 --> 40:18.420
 I think this can be done.

40:18.420 --> 40:22.980
 I think we can start, I think machine learning,

40:22.980 --> 40:26.100
 for example, with enough examples,

40:26.100 --> 40:28.920
 can start to learn these basic dynamics.

40:28.920 --> 40:32.240
 Will they relate them necessarily to the gravity?

40:32.240 --> 40:37.120
 Not unless they can also acquire those theories as well

40:38.320 --> 40:40.940
 and put the experiential knowledge

40:40.940 --> 40:43.400
 and connect it back to the theoretical knowledge.

40:43.400 --> 40:47.200
 I think if we think in terms of these class of architectures

40:47.200 --> 40:51.020
 that are designed to both learn the specifics,

40:51.020 --> 40:54.220
 find the patterns, but also acquire the frameworks

40:54.220 --> 40:56.340
 and connect the data to the frameworks.

40:56.340 --> 40:59.700
 If we think in terms of robust architectures like this,

40:59.700 --> 41:03.420
 I think there is a path toward getting there.

41:03.420 --> 41:06.220
 In terms of encoding architectures like that,

41:06.220 --> 41:09.200
 do you think systems that are able to do this

41:10.300 --> 41:14.940
 will look like neural networks or representing,

41:14.940 --> 41:18.740
 if you look back to the 80s and 90s with the expert systems,

41:18.740 --> 41:24.540
 they're more like graphs, systems that are based in logic,

41:24.540 --> 41:26.500
 able to contain a large amount of knowledge

41:26.500 --> 41:28.500
 where the challenge was the automated acquisition

41:28.500 --> 41:29.860
 of that knowledge.

41:29.860 --> 41:33.820
 I guess the question is when you collect both the frameworks

41:33.820 --> 41:35.300
 and the knowledge from the data,

41:35.300 --> 41:37.260
 what do you think that thing will look like?

41:37.260 --> 41:39.340
 Yeah, so I mean, I think asking the question,

41:39.340 --> 41:41.260
 they look like neural networks is a bit of a red herring.

41:41.260 --> 41:45.180
 I mean, I think that they will certainly do inductive

41:45.180 --> 41:46.720
 or pattern match based reasoning.

41:46.720 --> 41:49.000
 And I've already experimented with architectures

41:49.000 --> 41:52.700
 that combine both that use machine learning

41:52.700 --> 41:55.340
 and neural networks to learn certain classes of knowledge,

41:55.340 --> 41:57.300
 in other words, to find repeated patterns

41:57.300 --> 42:01.540
 in order for it to make good inductive guesses,

42:01.540 --> 42:05.260
 but then ultimately to try to take those learnings

42:05.260 --> 42:09.540
 and marry them, in other words, connect them to frameworks

42:09.540 --> 42:11.500
 so that it can then reason over that

42:11.500 --> 42:13.660
 in terms other humans understand.

42:13.660 --> 42:16.100
 So for example, at elemental cognition, we do both.

42:16.100 --> 42:19.820
 We have architectures that do both, both those things,

42:19.820 --> 42:21.660
 but also have a learning method

42:21.660 --> 42:24.400
 for acquiring the frameworks themselves and saying,

42:24.400 --> 42:27.280
 look, ultimately, I need to take this data.

42:27.280 --> 42:30.020
 I need to interpret it in the form of these frameworks

42:30.020 --> 42:30.860
 so they can reason over it.

42:30.860 --> 42:33.340
 So there is a fundamental knowledge representation,

42:33.340 --> 42:34.220
 like what you're saying,

42:34.220 --> 42:36.780
 like these graphs of logic, if you will.

42:36.780 --> 42:39.280
 There are also neural networks

42:39.280 --> 42:41.620
 that acquire a certain class of information.

42:43.100 --> 42:45.900
 Then they then align them with these frameworks,

42:45.900 --> 42:47.140
 but there's also a mechanism

42:47.140 --> 42:49.180
 to acquire the frameworks themselves.

42:49.180 --> 42:52.540
 Yeah, so it seems like the idea of frameworks

42:52.540 --> 42:55.380
 requires some kind of collaboration with humans.

42:55.380 --> 42:56.300
 Absolutely.

42:56.300 --> 42:59.340
 So do you think of that collaboration as direct?

42:59.340 --> 43:01.900
 Well, and let's be clear.

43:01.900 --> 43:06.060
 Only for the express purpose that you're designing,

43:06.060 --> 43:09.500
 you're designing an intelligence

43:09.500 --> 43:12.500
 that can ultimately communicate with humans

43:12.500 --> 43:15.940
 in the terms of frameworks that help them understand things.

43:17.060 --> 43:19.380
 So to be really clear,

43:19.380 --> 43:24.340
 you can independently create a machine learning system,

43:24.340 --> 43:28.460
 an intelligence that I might call an alien intelligence

43:28.460 --> 43:31.140
 that does a better job than you with some things,

43:31.140 --> 43:33.500
 but can't explain the framework to you.

43:33.500 --> 43:36.720
 That doesn't mean it might be better than you at the thing.

43:36.720 --> 43:39.500
 It might be that you cannot comprehend the framework

43:39.500 --> 43:42.780
 that it may have created for itself that is inexplicable

43:42.780 --> 43:43.900
 to you.

43:43.900 --> 43:45.260
 That's a reality.

43:45.260 --> 43:48.780
 But you're more interested in a case where you can.

43:48.780 --> 43:51.060
 I am, yeah.

43:51.060 --> 43:54.260
 My sort of approach to AI is because

43:54.260 --> 43:55.900
 I've set the goal for myself.

43:55.900 --> 43:58.860
 I want machines to be able to ultimately communicate,

44:00.320 --> 44:01.160
 understanding with humans.

44:01.160 --> 44:03.460
 I want them to be able to acquire and communicate,

44:03.460 --> 44:04.700
 acquire knowledge from humans

44:04.700 --> 44:06.980
 and communicate knowledge to humans.

44:06.980 --> 44:11.580
 They should be using what inductive

44:11.580 --> 44:13.700
 machine learning techniques are good at,

44:13.700 --> 44:16.780
 which is to observe patterns of data,

44:16.780 --> 44:19.260
 whether it be in language or whether it be in images

44:19.260 --> 44:21.340
 or videos or whatever,

44:23.100 --> 44:25.420
 to acquire these patterns,

44:25.420 --> 44:29.340
 to induce the generalizations from those patterns,

44:29.340 --> 44:31.220
 but then ultimately to work with humans

44:31.220 --> 44:34.640
 to connect them to frameworks, interpretations, if you will,

44:34.640 --> 44:36.700
 that ultimately make sense to humans.

44:36.700 --> 44:38.460
 Of course, the machine is gonna have the strength

44:38.460 --> 44:41.420
 that it has, the richer, longer memory,

44:41.420 --> 44:45.380
 but it has the more rigorous reasoning abilities,

44:45.380 --> 44:47.040
 the deeper reasoning abilities,

44:47.040 --> 44:51.060
 so it'll be an interesting complementary relationship

44:51.060 --> 44:53.180
 between the human and the machine.

44:53.180 --> 44:55.100
 Do you think that ultimately needs explainability

44:55.100 --> 44:55.980
 like a machine?

44:55.980 --> 44:57.860
 So if we look, we study, for example,

44:57.860 --> 45:00.820
 Tesla autopilot a lot, where humans,

45:00.820 --> 45:02.780
 I don't know if you've driven the vehicle,

45:02.780 --> 45:04.360
 are aware of what it is.

45:04.360 --> 45:09.100
 So you're basically the human and machine

45:09.100 --> 45:10.300
 are working together there,

45:10.300 --> 45:12.500
 and the human is responsible for their own life

45:12.500 --> 45:14.220
 to monitor the system,

45:14.220 --> 45:18.380
 and the system fails every few miles,

45:18.380 --> 45:20.500
 and so there's hundreds,

45:20.500 --> 45:23.620
 there's millions of those failures a day,

45:23.620 --> 45:25.780
 and so that's like a moment of interaction.

45:25.780 --> 45:26.620
 Do you see?

45:26.620 --> 45:27.900
 Yeah, that's exactly right.

45:27.900 --> 45:29.900
 That's a moment of interaction

45:29.900 --> 45:34.820
 where the machine has learned some stuff,

45:34.820 --> 45:38.720
 it has a failure, somehow the failure's communicated,

45:38.720 --> 45:41.880
 the human is now filling in the mistake, if you will,

45:41.880 --> 45:43.620
 or maybe correcting or doing something

45:43.620 --> 45:45.860
 that is more successful in that case,

45:45.860 --> 45:47.900
 the computer takes that learning.

45:47.900 --> 45:50.260
 So I believe that the collaboration

45:50.260 --> 45:52.300
 between human and machine,

45:52.300 --> 45:53.900
 I mean, that's sort of a primitive example

45:53.900 --> 45:55.040
 and sort of a more,

45:56.920 --> 45:59.220
 another example is where the machine's literally talking

45:59.220 --> 46:02.740
 to you and saying, look, I'm reading this thing.

46:02.740 --> 46:06.580
 I know that the next word might be this or that,

46:06.580 --> 46:08.900
 but I don't really understand why.

46:08.900 --> 46:09.940
 I have my guess.

46:09.940 --> 46:14.060
 Can you help me understand the framework that supports this

46:14.060 --> 46:16.060
 and then can kind of acquire that,

46:16.060 --> 46:18.140
 take that and reason about it and reuse it

46:18.140 --> 46:20.520
 the next time it's reading to try to understand something,

46:20.520 --> 46:24.760
 not unlike a human student might do.

46:24.760 --> 46:27.480
 I mean, I remember when my daughter was in first grade

46:27.480 --> 46:31.180
 and she had a reading assignment about electricity

46:32.280 --> 46:35.600
 and somewhere in the text it says,

46:35.600 --> 46:38.620
 and electricity is produced by water flowing over turbines

46:38.620 --> 46:39.900
 or something like that.

46:39.900 --> 46:41.240
 And then there's a question that says,

46:41.240 --> 46:43.140
 well, how is electricity created?

46:43.140 --> 46:45.180
 And so my daughter comes to me and says,

46:45.180 --> 46:46.500
 I mean, I could, you know,

46:46.500 --> 46:49.200
 created and produced are kind of synonyms in this case.

46:49.200 --> 46:50.620
 So I can go back to the text

46:50.620 --> 46:53.660
 and I can copy by water flowing over turbines,

46:53.660 --> 46:56.120
 but I have no idea what that means.

46:56.120 --> 46:57.620
 Like I don't know how to interpret

46:57.620 --> 47:00.380
 water flowing over turbines and what electricity even is.

47:00.380 --> 47:04.000
 I mean, I can get the answer right by matching the text,

47:04.000 --> 47:06.140
 but I don't have any framework for understanding

47:06.140 --> 47:07.860
 what this means at all.

47:07.860 --> 47:10.500
 And framework really is, I mean, it's a set of,

47:10.500 --> 47:14.140
 not to be mathematical, but axioms of ideas

47:14.140 --> 47:16.340
 that you bring to the table and interpreting stuff

47:16.340 --> 47:18.380
 and then you build those up somehow.

47:18.380 --> 47:20.460
 You build them up with the expectation

47:20.460 --> 47:23.780
 that there's a shared understanding of what they are.

47:23.780 --> 47:27.800
 Sure, yeah, it's the social, that us humans,

47:28.900 --> 47:32.060
 do you have a sense that humans on earth in general

47:32.060 --> 47:36.500
 share a set of, like how many frameworks are there?

47:36.500 --> 47:38.200
 I mean, it depends on how you bound them, right?

47:38.200 --> 47:39.900
 So in other words, how big or small,

47:39.900 --> 47:41.780
 like their individual scope,

47:42.640 --> 47:44.220
 but there's lots and there are new ones.

47:44.220 --> 47:47.620
 I think the way I think about it is kind of in a layer.

47:47.620 --> 47:50.020
 I think that the architectures are being layered in that.

47:50.020 --> 47:53.560
 There's a small set of primitives.

47:53.560 --> 47:56.260
 They allow you the foundation to build frameworks.

47:56.260 --> 47:58.360
 And then there may be many frameworks,

47:58.360 --> 48:00.580
 but you have the ability to acquire them.

48:00.580 --> 48:03.020
 And then you have the ability to reuse them.

48:03.020 --> 48:04.940
 I mean, one of the most compelling ways

48:04.940 --> 48:07.220
 of thinking about this is a reasoning by analogy,

48:07.220 --> 48:08.180
 where I can say, oh, wow,

48:08.180 --> 48:09.980
 I've learned something very similar.

48:11.340 --> 48:15.240
 I never heard of this game soccer,

48:15.240 --> 48:17.820
 but if it's like basketball in the sense

48:17.820 --> 48:19.580
 that the goal's like the hoop

48:19.580 --> 48:20.980
 and I have to get the ball in the hoop

48:20.980 --> 48:23.500
 and I have guards and I have this and I have that,

48:23.500 --> 48:26.460
 like where are the similarities

48:26.460 --> 48:27.740
 and where are the differences?

48:27.740 --> 48:29.120
 And I have a foundation now

48:29.120 --> 48:31.340
 for interpreting this new information.

48:31.340 --> 48:33.260
 And then the different groups,

48:33.260 --> 48:36.380
 like the millennials will have a framework.

48:36.380 --> 48:41.380
 And then, you know, the Democrats and Republicans.

48:41.660 --> 48:43.820
 Millennials, nobody wants that framework.

48:43.820 --> 48:45.860
 Well, I mean, I think, right,

48:45.860 --> 48:48.100
 I mean, you're talking about political and social ways

48:48.100 --> 48:49.860
 of interpreting the world around them.

48:49.860 --> 48:51.980
 And I think these frameworks are still largely,

48:51.980 --> 48:52.800
 largely similar.

48:52.800 --> 48:54.540
 I think they differ in maybe

48:54.540 --> 48:57.380
 what some fundamental assumptions and values are.

48:57.380 --> 48:59.860
 Now, from a reasoning perspective,

48:59.860 --> 49:01.620
 like the ability to process the framework,

49:01.620 --> 49:04.160
 it might not be that different.

49:04.160 --> 49:06.560
 The implications of different fundamental values

49:06.560 --> 49:09.460
 or fundamental assumptions in those frameworks

49:09.460 --> 49:12.160
 may reach very different conclusions.

49:12.160 --> 49:14.780
 So from a social perspective,

49:14.780 --> 49:16.900
 the conclusions may be very different.

49:16.900 --> 49:18.420
 From an intelligence perspective,

49:18.420 --> 49:21.620
 I just followed where my assumptions took me.

49:21.620 --> 49:23.420
 Yeah, the process itself will look similar.

49:23.420 --> 49:25.580
 But that's a fascinating idea

49:25.580 --> 49:30.580
 that frameworks really help carve

49:30.820 --> 49:33.740
 how a statement will be interpreted.

49:33.740 --> 49:38.740
 I mean, having a Democrat and a Republican framework

49:40.360 --> 49:42.180
 and then read the exact same statement

49:42.180 --> 49:44.200
 and the conclusions that you derive

49:44.200 --> 49:45.460
 will be totally different

49:45.460 --> 49:47.620
 from an AI perspective is fascinating.

49:47.620 --> 49:49.460
 What we would want out of the AI

49:49.460 --> 49:51.140
 is to be able to tell you

49:51.140 --> 49:53.740
 that this perspective, one perspective,

49:53.740 --> 49:55.540
 one set of assumptions is gonna lead you here,

49:55.540 --> 49:58.700
 another set of assumptions is gonna lead you there.

49:58.700 --> 50:01.420
 And in fact, to help people reason and say,

50:01.420 --> 50:05.220
 oh, I see where our differences lie.

50:05.220 --> 50:06.940
 I have this fundamental belief about that.

50:06.940 --> 50:09.200
 I have this fundamental belief about that.

50:09.200 --> 50:10.100
 Yeah, that's quite brilliant.

50:10.100 --> 50:12.620
 From my perspective, NLP,

50:12.620 --> 50:14.140
 there's this idea that there's one way

50:14.140 --> 50:16.100
 to really understand a statement,

50:16.100 --> 50:18.780
 but that probably isn't.

50:18.780 --> 50:20.140
 There's probably an infinite number of ways

50:20.140 --> 50:21.980
 to understand a statement, depending on the question.

50:21.980 --> 50:23.420
 There's lots of different interpretations,

50:23.420 --> 50:28.420
 and the broader the content, the richer it is.

50:31.460 --> 50:35.260
 And so you and I can have very different experiences

50:35.260 --> 50:37.420
 with the same text, obviously.

50:37.420 --> 50:41.300
 And if we're committed to understanding each other,

50:42.300 --> 50:45.260
 we start, and that's the other important point,

50:45.260 --> 50:47.740
 if we're committed to understanding each other,

50:47.740 --> 50:51.860
 we start decomposing and breaking down our interpretation

50:51.860 --> 50:54.020
 to its more and more primitive components

50:54.020 --> 50:55.900
 until we get to that point where we say,

50:55.900 --> 50:58.260
 oh, I see why we disagree.

50:58.260 --> 51:00.500
 And we try to understand how fundamental

51:00.500 --> 51:02.220
 that disagreement really is.

51:02.220 --> 51:04.580
 But that requires a commitment

51:04.580 --> 51:06.540
 to breaking down that interpretation

51:06.540 --> 51:08.940
 in terms of that framework in a logical way.

51:08.940 --> 51:12.780
 Otherwise, and this is why I think of AI

51:12.780 --> 51:16.020
 as really complimenting and helping human intelligence

51:16.020 --> 51:19.860
 to overcome some of its biases and its predisposition

51:19.860 --> 51:24.860
 to be persuaded by more shallow reasoning

51:25.060 --> 51:26.980
 in the sense that we get over this idea,

51:26.980 --> 51:29.980
 well, I'm right because I'm Republican,

51:29.980 --> 51:31.380
 or I'm right because I'm Democratic,

51:31.380 --> 51:33.380
 and someone labeled this as Democratic point of view,

51:33.380 --> 51:35.420
 or it has the following keywords in it.

51:35.420 --> 51:38.500
 And if the machine can help us break that argument down

51:38.500 --> 51:41.660
 and say, wait a second, what do you really think

51:41.660 --> 51:42.500
 about this, right?

51:42.500 --> 51:45.460
 So essentially holding us accountable

51:45.460 --> 51:47.540
 to doing more critical thinking.

51:47.540 --> 51:49.500
 We're gonna have to sit and think about this fast.

51:49.500 --> 51:50.940
 That's, I love that.

51:50.940 --> 51:53.580
 I think that's really empowering use of AI

51:53.580 --> 51:57.180
 for the public discourse is completely disintegrating

51:57.180 --> 52:00.460
 currently as we learn how to do it on social media.

52:00.460 --> 52:01.300
 That's right.

52:02.460 --> 52:05.860
 So one of the greatest accomplishments

52:05.860 --> 52:10.860
 in the history of AI is Watson competing

52:12.140 --> 52:14.700
 in the game of Jeopardy against humans.

52:14.700 --> 52:18.940
 And you were a lead in that, a critical part of that.

52:18.940 --> 52:20.620
 Let's start at the very basics.

52:20.620 --> 52:22.860
 What is the game of Jeopardy?

52:22.860 --> 52:25.860
 The game for us humans, human versus human.

52:25.860 --> 52:30.860
 Right, so it's to take a question and answer it.

52:33.900 --> 52:34.740
 The game of Jeopardy.

52:34.740 --> 52:35.580
 It's just the opposite.

52:35.580 --> 52:38.780
 Actually, well, no, but it's not, right?

52:38.780 --> 52:39.620
 It's really not.

52:39.620 --> 52:41.860
 It's really to get a question and answer,

52:41.860 --> 52:43.940
 but it's what we call a factoid question.

52:43.940 --> 52:46.860
 So this notion of like, it really relates to some fact

52:46.860 --> 52:49.260
 that two people would argue

52:49.260 --> 52:50.580
 whether the facts are true or not.

52:50.580 --> 52:51.580
 In fact, most people wouldn't.

52:51.580 --> 52:53.060
 Jeopardy kind of counts on the idea

52:53.060 --> 52:57.660
 that these statements have factual answers.

52:57.660 --> 53:02.020
 And the idea is to, first of all,

53:02.020 --> 53:03.780
 determine whether or not you know the answer,

53:03.780 --> 53:06.100
 which is sort of an interesting twist.

53:06.100 --> 53:07.860
 So first of all, understand the question.

53:07.860 --> 53:08.860
 You have to understand the question.

53:08.860 --> 53:09.860
 What is it asking?

53:09.860 --> 53:10.740
 And that's a good point

53:10.740 --> 53:14.460
 because the questions are not asked directly, right?

53:14.460 --> 53:15.540
 They're all like,

53:15.540 --> 53:18.340
 the way the questions are asked is nonlinear.

53:18.340 --> 53:20.660
 It's like, it's a little bit witty.

53:20.660 --> 53:22.460
 It's a little bit playful sometimes.

53:22.460 --> 53:25.940
 It's a little bit tricky.

53:25.940 --> 53:30.580
 Yeah, they're asked in exactly numerous witty, tricky ways.

53:30.580 --> 53:32.540
 Exactly what they're asking is not obvious.

53:32.540 --> 53:35.060
 It takes inexperienced humans a while

53:35.060 --> 53:36.900
 to go, what is it even asking?

53:36.900 --> 53:39.620
 And it's sort of an interesting realization that you have

53:39.620 --> 53:40.980
 when somebody says, oh, what's,

53:40.980 --> 53:42.420
 Jeopardy is a question answering show.

53:42.420 --> 53:43.860
 And then he's like, oh, like, I know a lot.

53:43.860 --> 53:45.980
 And then you read it and you're still trying

53:45.980 --> 53:48.300
 to process the question and the champions have answered

53:48.300 --> 53:49.140
 and moved on.

53:49.140 --> 53:51.180
 There are three questions ahead

53:51.180 --> 53:54.060
 by the time you figured out what the question even meant.

53:54.060 --> 53:56.460
 So there's definitely an ability there

53:56.460 --> 53:59.500
 to just parse out what the question even is.

53:59.500 --> 54:00.820
 So that was certainly challenging.

54:00.820 --> 54:02.220
 It's interesting historically though,

54:02.220 --> 54:05.460
 if you look back at the Jeopardy games much earlier,

54:05.460 --> 54:08.140
 you know, early games. Like 60s, 70s, that kind of thing.

54:08.140 --> 54:10.180
 The questions were much more direct.

54:10.180 --> 54:11.300
 They weren't quite like that.

54:11.300 --> 54:13.660
 They got sort of more and more interesting,

54:13.660 --> 54:15.340
 the way they asked them that sort of got more

54:15.340 --> 54:18.340
 and more interesting and subtle and nuanced

54:18.340 --> 54:20.780
 and humorous and witty over time,

54:20.780 --> 54:22.500
 which really required the human

54:22.500 --> 54:24.260
 to kind of make the right connections

54:24.260 --> 54:26.860
 in figuring out what the question was even asking.

54:26.860 --> 54:29.940
 So yeah, you have to figure out the questions even asking.

54:29.940 --> 54:31.700
 Then you have to determine whether

54:31.700 --> 54:33.620
 or not you think you know the answer.

54:34.500 --> 54:37.380
 And because you have to buzz in really quickly,

54:37.380 --> 54:39.820
 you sort of have to make that determination

54:39.820 --> 54:41.220
 as quickly as you possibly can.

54:41.220 --> 54:43.460
 Otherwise you lose the opportunity to buzz in.

54:43.460 --> 54:44.300
 You mean...

54:44.300 --> 54:46.140
 Even before you really know if you know the answer.

54:46.140 --> 54:48.660
 I think a lot of humans will assume,

54:48.660 --> 54:53.020
 they'll process it very superficially.

54:53.020 --> 54:54.940
 In other words, what's the topic?

54:54.940 --> 54:55.980
 What are some keywords?

54:55.980 --> 54:58.660
 And just say, do I know this area or not

54:58.660 --> 55:00.820
 before they actually know the answer?

55:00.820 --> 55:03.220
 Then they'll buzz in and think about it.

55:03.220 --> 55:04.700
 So it's interesting what humans do.

55:04.700 --> 55:06.940
 Now, some people who know all things,

55:06.940 --> 55:08.460
 like Ken Jennings or something,

55:08.460 --> 55:10.420
 or the more recent big Jeopardy player,

55:11.460 --> 55:12.420
 I mean, they'll just buzz in.

55:12.420 --> 55:14.100
 They'll just assume they know all of Jeopardy

55:14.100 --> 55:15.900
 and they'll just buzz in.

55:15.900 --> 55:18.380
 Watson, interestingly, didn't even come close

55:18.380 --> 55:20.140
 to knowing all of Jeopardy, right?

55:20.140 --> 55:20.980
 Watson really...

55:20.980 --> 55:22.700
 Even at the peak, even at its best.

55:22.700 --> 55:24.580
 Yeah, so for example, I mean,

55:24.580 --> 55:25.980
 we had this thing called recall,

55:25.980 --> 55:29.420
 which is like how many of all the Jeopardy questions,

55:29.420 --> 55:34.420
 how many could we even find the right answer for anywhere?

55:34.420 --> 55:38.220
 Like, can we come up with, we had a big body of knowledge,

55:38.220 --> 55:39.780
 something in the order of several terabytes.

55:39.780 --> 55:42.900
 I mean, from a web scale, it was actually very small,

55:42.900 --> 55:44.340
 but from like a book scale,

55:44.340 --> 55:46.260
 we're talking about millions of books, right?

55:46.260 --> 55:48.260
 So the equivalent of millions of books,

55:48.260 --> 55:50.340
 encyclopedias, dictionaries, books,

55:50.340 --> 55:52.260
 it's still a ton of information.

55:52.260 --> 55:55.820
 And I think it was only 85% was the answer

55:55.820 --> 55:57.580
 anywhere to be found.

55:57.580 --> 56:00.340
 So you're already down at that level

56:00.340 --> 56:02.060
 just to get started, right?

56:02.060 --> 56:07.060
 So, and so it was important to get a very quick sense

56:07.900 --> 56:10.060
 of do you think you know the right answer to this question?

56:10.060 --> 56:12.180
 So we had to compute that confidence

56:12.180 --> 56:14.300
 as quickly as we possibly could.

56:14.300 --> 56:16.460
 So in effect, we had to answer it

56:16.460 --> 56:21.460
 and at least spend some time essentially answering it

56:22.020 --> 56:26.660
 and then judging the confidence that our answer was right

56:26.660 --> 56:28.060
 and then deciding whether or not

56:28.060 --> 56:30.020
 we were confident enough to buzz in.

56:30.020 --> 56:31.940
 And that would depend on what else was going on in the game.

56:31.940 --> 56:33.380
 Because there was a risk.

56:33.380 --> 56:35.060
 So like if you're really in a situation

56:35.060 --> 56:38.340
 where I have to take a guess, I have very little to lose,

56:38.340 --> 56:40.220
 then you'll buzz in with less confidence.

56:40.220 --> 56:42.940
 So that was accounted for the financial standings

56:42.940 --> 56:44.300
 of the different competitors.

56:44.300 --> 56:45.420
 Correct.

56:45.420 --> 56:46.620
 How much of the game was left?

56:46.620 --> 56:48.260
 How much time was left?

56:48.260 --> 56:50.740
 Where you were in the standing, things like that.

56:50.740 --> 56:52.860
 How many hundreds of milliseconds

56:52.860 --> 56:53.900
 that we're talking about here?

56:53.900 --> 56:55.980
 Do you have a sense of what is?

56:55.980 --> 56:58.420
 We targeted, yeah, we targeted.

56:58.420 --> 57:01.180
 So, I mean, we targeted answering

57:01.180 --> 57:04.660
 in under three seconds and.

57:04.660 --> 57:05.500
 Buzzing in.

57:05.500 --> 57:09.940
 So the decision to buzz in and then the actual answering

57:09.940 --> 57:10.980
 are those two different stages?

57:10.980 --> 57:12.660
 Yeah, they were two different things.

57:12.660 --> 57:14.540
 In fact, we had multiple stages,

57:14.540 --> 57:17.380
 whereas like we would say, let's estimate our confidence,

57:17.380 --> 57:21.060
 which was sort of a shallow answering process.

57:21.060 --> 57:23.820
 And then ultimately decide to buzz in

57:23.820 --> 57:26.340
 and then we may take another second or something

57:27.420 --> 57:30.900
 to kind of go in there and do that.

57:30.900 --> 57:32.180
 But by and large, we were saying like,

57:32.180 --> 57:33.940
 we can't play the game.

57:33.940 --> 57:37.620
 We can't even compete if we can't on average

57:37.620 --> 57:40.380
 answer these questions in around three seconds or less.

57:40.380 --> 57:41.740
 So you stepped in.

57:41.740 --> 57:45.340
 So there's these three humans playing a game

57:45.340 --> 57:47.980
 and you stepped in with the idea that IBM Watson

57:47.980 --> 57:49.980
 would be one of, replace one of the humans

57:49.980 --> 57:52.020
 and compete against two.

57:52.020 --> 57:56.740
 Can you tell the story of Watson taking on this game?

57:56.740 --> 57:57.580
 Sure.

57:57.580 --> 57:58.700
 It seems exceptionally difficult.

57:58.700 --> 58:03.500
 Yeah, so the story was that it was coming up,

58:03.500 --> 58:06.940
 I think to the 10 year anniversary of Big Blue,

58:06.940 --> 58:08.780
 not Big Blue, Deep Blue.

58:08.780 --> 58:11.940
 IBM wanted to do sort of another kind of really

58:11.940 --> 58:15.260
 fun challenge, public challenge that can bring attention

58:15.260 --> 58:17.180
 to IBM research and the kind of the cool stuff

58:17.180 --> 58:18.180
 that we were doing.

58:19.740 --> 58:23.740
 I had been working in AI at IBM for some time.

58:23.740 --> 58:26.460
 I had a team doing what's called

58:26.460 --> 58:28.620
 open domain factoid question answering,

58:28.620 --> 58:31.020
 which is, we're not gonna tell you what the questions are.

58:31.020 --> 58:33.100
 We're not even gonna tell you what they're about.

58:33.100 --> 58:36.860
 Can you go off and get accurate answers to these questions?

58:36.860 --> 58:41.420
 And it was an area of AI research that I was involved in.

58:41.420 --> 58:44.300
 And so it was a very specific passion of mine.

58:44.300 --> 58:47.100
 Language understanding had always been a passion of mine.

58:47.100 --> 58:49.660
 One sort of narrow slice on whether or not

58:49.660 --> 58:51.020
 you could do anything with language

58:51.020 --> 58:52.900
 was this notion of open domain and meaning

58:52.900 --> 58:54.620
 I could ask anything about anything.

58:54.620 --> 58:57.900
 Factoid meaning it essentially had an answer

58:57.900 --> 59:00.940
 and being able to do that accurately and quickly.

59:00.940 --> 59:02.420
 So that was a research area

59:02.420 --> 59:03.980
 that my team had already been in.

59:03.980 --> 59:06.340
 And so completely independently,

59:06.340 --> 59:09.060
 several IBM executives, like what are we gonna do?

59:09.060 --> 59:11.060
 What's the next cool thing to do?

59:11.060 --> 59:13.900
 And Ken Jennings was on his winning streak.

59:13.900 --> 59:16.660
 This was like, whatever it was, 2004, I think,

59:16.660 --> 59:18.780
 was on his winning streak.

59:18.780 --> 59:20.900
 And someone thought, hey, that would be really cool

59:20.900 --> 59:23.940
 if the computer can play Jeopardy.

59:23.940 --> 59:25.740
 And so this was like in 2004,

59:25.740 --> 59:28.020
 they were shopping this thing around

59:28.020 --> 59:33.020
 and everyone was telling the research execs, no way.

59:33.540 --> 59:35.180
 Like, this is crazy.

59:35.180 --> 59:37.020
 And we had some pretty senior people in the field

59:37.020 --> 59:38.180
 and they're saying, no, this is crazy.

59:38.180 --> 59:40.180
 And it would come across my desk and I was like,

59:40.180 --> 59:43.100
 but that's kind of what I'm really interested in doing.

59:44.700 --> 59:47.460
 But there was such this prevailing sense of this is nuts.

59:47.460 --> 59:49.380
 We're not gonna risk IBM's reputation on this.

59:49.380 --> 59:50.500
 We're just not doing it.

59:50.500 --> 59:53.140
 And this happened in 2004, it happened in 2005.

59:53.140 --> 59:58.140
 At the end of 2006, it was coming around again.

59:59.260 --> 1:00:01.100
 And I was coming off of a,

1:00:01.100 --> 1:00:03.060
 I was doing the open domain question answering stuff,

1:00:03.060 --> 1:00:05.940
 but I was coming off a couple other projects.

1:00:05.940 --> 1:00:08.060
 I had a lot more time to put into this.

1:00:08.060 --> 1:00:10.180
 And I argued that it could be done.

1:00:10.180 --> 1:00:12.740
 And I argue it would be crazy not to do this.

1:00:12.740 --> 1:00:15.820
 Can I, you can be honest at this point.

1:00:15.820 --> 1:00:17.580
 So even though you argued for it,

1:00:17.580 --> 1:00:21.540
 what's the confidence that you had yourself privately

1:00:21.540 --> 1:00:22.740
 that this could be done?

1:00:22.740 --> 1:00:25.620
 Was, we just told the story,

1:00:25.620 --> 1:00:27.740
 how you tell stories to convince others.

1:00:27.740 --> 1:00:28.940
 How confident were you?

1:00:28.940 --> 1:00:32.660
 What was your estimation of the problem at that time?

1:00:32.660 --> 1:00:34.300
 So I thought it was possible.

1:00:34.300 --> 1:00:36.300
 And a lot of people thought it was impossible.

1:00:36.300 --> 1:00:37.860
 I thought it was possible.

1:00:37.860 --> 1:00:39.140
 The reason why I thought it was possible

1:00:39.140 --> 1:00:41.500
 was because I did some brief experimentation.

1:00:41.500 --> 1:00:43.460
 I knew a lot about how we were approaching

1:00:43.460 --> 1:00:45.940
 open domain factoid question answering.

1:00:45.940 --> 1:00:47.620
 I've been doing it for some years.

1:00:47.620 --> 1:00:49.340
 I looked at the Jeopardy stuff.

1:00:49.340 --> 1:00:50.900
 I said, this is gonna be hard

1:00:50.900 --> 1:00:54.180
 for a lot of the points that we mentioned earlier.

1:00:54.180 --> 1:00:55.740
 Hard to interpret the question.

1:00:57.060 --> 1:00:58.940
 Hard to do it quickly enough.

1:00:58.940 --> 1:01:00.500
 Hard to compute an accurate confidence.

1:01:00.500 --> 1:01:03.060
 None of this stuff had been done well enough before.

1:01:03.060 --> 1:01:04.660
 But a lot of the technologies we're building

1:01:04.660 --> 1:01:07.500
 were the kinds of technologies that should work.

1:01:07.500 --> 1:01:10.860
 But more to the point, what was driving me was,

1:01:10.860 --> 1:01:12.820
 I was in IBM research.

1:01:12.820 --> 1:01:14.900
 I was a senior leader in IBM research.

1:01:14.900 --> 1:01:17.140
 And this is the kind of stuff we were supposed to do.

1:01:17.140 --> 1:01:18.660
 In other words, we were basically supposed to.

1:01:18.660 --> 1:01:19.700
 This is the moonshot.

1:01:19.700 --> 1:01:20.540
 This is the.

1:01:20.540 --> 1:01:21.900
 We were supposed to take things and say,

1:01:21.900 --> 1:01:24.060
 this is an active research area.

1:01:24.940 --> 1:01:27.540
 It's our obligation to kind of,

1:01:27.540 --> 1:01:30.100
 if we have the opportunity, to push it to the limits.

1:01:30.100 --> 1:01:31.460
 And if it doesn't work,

1:01:31.460 --> 1:01:34.740
 to understand more deeply why we can't do it.

1:01:34.740 --> 1:01:37.900
 And so I was very committed to that notion saying,

1:01:37.900 --> 1:01:40.060
 folks, this is what we do.

1:01:40.060 --> 1:01:42.140
 It's crazy not to do this.

1:01:42.140 --> 1:01:43.740
 This is an active research area.

1:01:43.740 --> 1:01:44.980
 We've been in this for years.

1:01:44.980 --> 1:01:47.940
 Why wouldn't we take this grand challenge

1:01:47.940 --> 1:01:50.700
 and push it as hard as we can?

1:01:50.700 --> 1:01:53.140
 At the very least, we'd be able to come out and say,

1:01:53.140 --> 1:01:57.060
 here's why this problem is way hard.

1:01:57.060 --> 1:01:58.660
 Here's what we tried and here's how we failed.

1:01:58.660 --> 1:02:03.660
 So I was very driven as a scientist from that perspective.

1:02:03.980 --> 1:02:06.580
 And then I also argued,

1:02:06.580 --> 1:02:08.740
 based on what we did a feasibility study,

1:02:08.740 --> 1:02:10.900
 why I thought it was hard but possible.

1:02:10.900 --> 1:02:14.180
 And I showed examples of where it succeeded,

1:02:14.180 --> 1:02:16.100
 where it failed, why it failed,

1:02:16.100 --> 1:02:18.180
 and sort of a high level architecture approach

1:02:18.180 --> 1:02:19.540
 for why we should do it.

1:02:19.540 --> 1:02:22.260
 But for the most part, at that point,

1:02:22.260 --> 1:02:24.660
 the execs really were just looking for someone crazy enough

1:02:24.660 --> 1:02:27.900
 to say yes, because for several years at that point,

1:02:27.900 --> 1:02:32.260
 everyone had said, no, I'm not willing to risk my reputation

1:02:32.260 --> 1:02:34.820
 and my career on this thing.

1:02:34.820 --> 1:02:36.740
 Clearly you did not have such fears.

1:02:36.740 --> 1:02:37.980
 Okay. I did not.

1:02:37.980 --> 1:02:39.540
 So you dived right in.

1:02:39.540 --> 1:02:42.820
 And yet, for what I understand,

1:02:42.820 --> 1:02:46.300
 it was performing very poorly in the beginning.

1:02:46.300 --> 1:02:49.740
 So what were the initial approaches and why did they fail?

1:02:51.300 --> 1:02:54.820
 Well, there were lots of hard aspects to it.

1:02:54.820 --> 1:02:57.700
 I mean, one of the reasons why prior approaches

1:02:57.700 --> 1:03:02.380
 that we had worked on in the past failed

1:03:02.380 --> 1:03:07.380
 was because the questions were difficult to interpret.

1:03:07.780 --> 1:03:10.100
 Like, what are you even asking for, right?

1:03:10.100 --> 1:03:12.500
 Very often, like if the question was very direct,

1:03:12.500 --> 1:03:16.620
 like what city, or what, even then it could be tricky,

1:03:16.620 --> 1:03:21.620
 but what city or what person,

1:03:21.940 --> 1:03:24.220
 is often when it would name it very clearly,

1:03:24.220 --> 1:03:25.420
 you would know that.

1:03:25.420 --> 1:03:28.100
 And if there were just a small set of them,

1:03:28.100 --> 1:03:31.540
 in other words, we're gonna ask about these five types.

1:03:31.540 --> 1:03:33.580
 Like, it's gonna be an answer,

1:03:33.580 --> 1:03:36.820
 and the answer will be a city in this state

1:03:36.820 --> 1:03:37.780
 or a city in this country.

1:03:37.780 --> 1:03:41.020
 The answer will be a person of this type, right?

1:03:41.020 --> 1:03:42.740
 Like an actor or whatever it is.

1:03:42.740 --> 1:03:44.380
 But it turns out that in Jeopardy,

1:03:44.380 --> 1:03:47.580
 there were like tens of thousands of these things.

1:03:47.580 --> 1:03:49.580
 And it was a very, very long tail,

1:03:50.580 --> 1:03:52.500
 meaning that it just went on and on.

1:03:52.500 --> 1:03:56.900
 And so even if you focused on trying to encode the types

1:03:56.900 --> 1:03:59.820
 at the very top, like there's five that were the most,

1:03:59.820 --> 1:04:01.580
 let's say five of the most frequent,

1:04:01.580 --> 1:04:04.140
 you still cover a very small percentage of the data.

1:04:04.140 --> 1:04:07.140
 So you couldn't take that approach of saying,

1:04:07.140 --> 1:04:09.780
 I'm just going to try to collect facts

1:04:09.780 --> 1:04:12.780
 about these five or 10 types or 20 types

1:04:12.780 --> 1:04:14.380
 or 50 types or whatever.

1:04:14.380 --> 1:04:16.940
 So that was like one of the first things,

1:04:16.940 --> 1:04:18.180
 like what do you do about that?

1:04:18.180 --> 1:04:21.500
 And so we came up with an approach toward that.

1:04:21.500 --> 1:04:23.460
 And the approach looked promising,

1:04:23.460 --> 1:04:25.940
 and we continued to improve our ability

1:04:25.940 --> 1:04:29.500
 to handle that problem throughout the project.

1:04:29.500 --> 1:04:32.420
 The other issue was that right from the outside,

1:04:32.420 --> 1:04:34.580
 I said, we're not going to,

1:04:34.580 --> 1:04:37.620
 I committed to doing this in three to five years.

1:04:37.620 --> 1:04:39.060
 So we did it in four.

1:04:39.060 --> 1:04:40.940
 So I got lucky.

1:04:40.940 --> 1:04:42.380
 But one of the things that that,

1:04:42.380 --> 1:04:45.700
 putting that like stake in the ground was,

1:04:45.700 --> 1:04:47.780
 and I knew how hard the language understanding problem was.

1:04:47.780 --> 1:04:51.620
 I said, we're not going to actually understand language

1:04:51.620 --> 1:04:52.740
 to solve this problem.

1:04:53.900 --> 1:04:57.460
 We are not going to interpret the question

1:04:57.460 --> 1:05:00.180
 and the domain of knowledge that the question refers to

1:05:00.180 --> 1:05:02.420
 and reason over that to answer these questions.

1:05:02.420 --> 1:05:04.140
 Obviously we're not going to be doing that.

1:05:04.140 --> 1:05:05.740
 At the same time,

1:05:05.740 --> 1:05:10.380
 simple search wasn't good enough to confidently answer

1:05:10.380 --> 1:05:13.020
 with a single correct answer.

1:05:13.020 --> 1:05:14.260
 First of all, that's like brilliant.

1:05:14.260 --> 1:05:16.140
 That's such a great mix of innovation

1:05:16.140 --> 1:05:18.620
 and practical engineering three, four, eight.

1:05:18.620 --> 1:05:21.780
 So you're not trying to solve the general NLU problem.

1:05:21.780 --> 1:05:25.260
 You're saying, let's solve this in any way possible.

1:05:25.260 --> 1:05:26.100
 Oh, yeah.

1:05:26.100 --> 1:05:28.020
 No, I was committed to saying, look,

1:05:28.020 --> 1:05:29.660
 we're going to solving the open domain

1:05:29.660 --> 1:05:31.020
 question answering problem.

1:05:31.020 --> 1:05:33.540
 We're using Jeopardy as a driver for that.

1:05:33.540 --> 1:05:34.380
 That's a big benchmark.

1:05:34.380 --> 1:05:36.500
 Good enough, big benchmark, exactly.

1:05:36.500 --> 1:05:38.180
 And now we're.

1:05:38.180 --> 1:05:39.020
 How do we do it?

1:05:39.020 --> 1:05:39.940
 We could just like, whatever,

1:05:39.940 --> 1:05:41.140
 like just figure out what works

1:05:41.140 --> 1:05:42.340
 because I want to be able to go back

1:05:42.340 --> 1:05:44.100
 to the academic science community

1:05:44.100 --> 1:05:45.980
 and say, here's what we tried.

1:05:45.980 --> 1:05:46.820
 Here's what worked.

1:05:46.820 --> 1:05:47.660
 Here's what didn't work.

1:05:47.660 --> 1:05:48.500
 Great.

1:05:48.500 --> 1:05:50.260
 I don't want to go in and say,

1:05:50.260 --> 1:05:51.980
 oh, I only have one technology.

1:05:51.980 --> 1:05:52.820
 I have a hammer.

1:05:52.820 --> 1:05:53.660
 I'm only going to use this.

1:05:53.660 --> 1:05:54.700
 I'm going to do whatever it takes.

1:05:54.700 --> 1:05:56.020
 I'm like, I'm going to think out of the box

1:05:56.020 --> 1:05:57.300
 and do whatever it takes.

1:05:57.300 --> 1:06:00.540
 One, and I also, there was another thing I believed.

1:06:00.540 --> 1:06:04.580
 I believed that the fundamental NLP technologies

1:06:04.580 --> 1:06:08.780
 and machine learning technologies would be adequate.

1:06:08.780 --> 1:06:11.940
 And this was an issue of how do we enhance them?

1:06:11.940 --> 1:06:13.620
 How do we integrate them?

1:06:13.620 --> 1:06:15.300
 How do we advance them?

1:06:15.300 --> 1:06:17.220
 So I had one researcher who came to me

1:06:17.220 --> 1:06:18.620
 who had been working on question answering

1:06:18.620 --> 1:06:20.180
 with me for a very long time,

1:06:21.620 --> 1:06:24.260
 who had said, we're going to need Maxwell's equations

1:06:24.260 --> 1:06:25.660
 for question answering.

1:06:25.660 --> 1:06:28.700
 And I said, if we need some fundamental formula

1:06:28.700 --> 1:06:31.820
 that breaks new ground in how we understand language,

1:06:31.820 --> 1:06:33.060
 we're screwed.

1:06:33.060 --> 1:06:34.380
 We're not going to get there from here.

1:06:34.380 --> 1:06:38.020
 Like I am not counting.

1:06:38.020 --> 1:06:39.660
 My assumption is I'm not counting

1:06:39.660 --> 1:06:42.380
 on some brand new invention.

1:06:42.380 --> 1:06:45.420
 What I'm counting on is the ability

1:06:45.420 --> 1:06:48.100
 to take everything it has done before

1:06:48.100 --> 1:06:51.860
 to figure out an architecture on how to integrate it well

1:06:51.860 --> 1:06:54.300
 and then see where it breaks

1:06:54.300 --> 1:06:57.220
 and make the necessary advances we need to make

1:06:57.220 --> 1:06:58.860
 until this thing works.

1:06:58.860 --> 1:07:00.460
 Push it hard to see where it breaks

1:07:00.460 --> 1:07:01.660
 and then patch it up.

1:07:01.660 --> 1:07:03.220
 I mean, that's how people change the world.

1:07:03.220 --> 1:07:05.980
 I mean, that's the Elon Musk approach to the rockets,

1:07:05.980 --> 1:07:08.780
 SpaceX, that's the Henry Ford and so on.

1:07:08.780 --> 1:07:09.620
 I love it.

1:07:09.620 --> 1:07:11.940
 And I happen to be, in this case, I happen to be right,

1:07:11.940 --> 1:07:14.300
 but like we didn't know.

1:07:14.300 --> 1:07:15.860
 But you kind of have to put a stake in the rest

1:07:15.860 --> 1:07:17.380
 of how you're going to run the project.

1:07:17.380 --> 1:07:20.340
 So yeah, and backtracking to search.

1:07:20.340 --> 1:07:24.660
 So if you were to do, what's the brute force solution?

1:07:24.660 --> 1:07:26.100
 What would you search over?

1:07:26.100 --> 1:07:27.940
 So you have a question,

1:07:27.940 --> 1:07:31.300
 how would you search the possible space of answers?

1:07:31.300 --> 1:07:33.900
 Look, web search has come a long way even since then.

1:07:34.820 --> 1:07:37.820
 But at the time, first of all,

1:07:37.820 --> 1:07:39.260
 I mean, there were a couple of other constraints

1:07:39.260 --> 1:07:40.940
 around the problem, which is interesting.

1:07:40.940 --> 1:07:43.100
 So you couldn't go out to the web.

1:07:43.100 --> 1:07:44.980
 You couldn't search the internet.

1:07:44.980 --> 1:07:47.600
 In other words, the AI experiment was,

1:07:47.600 --> 1:07:50.460
 we want a self contained device.

1:07:50.460 --> 1:07:52.940
 If the device is as big as a room, fine,

1:07:52.940 --> 1:07:53.860
 it's as big as a room,

1:07:53.860 --> 1:07:57.980
 but we want a self contained device.

1:07:57.980 --> 1:07:59.260
 You're not going out to the internet.

1:07:59.260 --> 1:08:01.580
 You don't have a lifeline to anything.

1:08:01.580 --> 1:08:04.280
 So it had to kind of fit in a shoe box, if you will,

1:08:04.280 --> 1:08:06.600
 or at least a size of a few refrigerators,

1:08:06.600 --> 1:08:08.060
 whatever it might be.

1:08:08.060 --> 1:08:10.440
 See, but also you couldn't just get out there.

1:08:10.440 --> 1:08:13.060
 You couldn't go off network, right, to kind of go.

1:08:13.060 --> 1:08:14.920
 So there was that limitation.

1:08:14.920 --> 1:08:19.340
 But then we did, but the basic thing was go do web search.

1:08:19.340 --> 1:08:22.940
 Problem was, even when we went and did a web search,

1:08:22.940 --> 1:08:24.540
 I don't remember exactly the numbers,

1:08:24.540 --> 1:08:27.580
 but somewhere in the order of 65% of the time,

1:08:27.580 --> 1:08:30.300
 the answer would be somewhere, you know,

1:08:30.300 --> 1:08:32.900
 in the top 10 or 20 documents.

1:08:32.900 --> 1:08:36.260
 So first of all, that's not even good enough to play Jeopardy.

1:08:36.260 --> 1:08:38.180
 You know, the words, even if you could pull the,

1:08:38.180 --> 1:08:40.240
 even if you could perfectly pull the answer

1:08:40.240 --> 1:08:42.920
 out of the top 20 documents, top 10 documents,

1:08:42.920 --> 1:08:45.240
 whatever it was, which we didn't know how to do.

1:08:45.240 --> 1:08:47.940
 But even if you could do that, you'd be,

1:08:47.940 --> 1:08:49.140
 and you knew it was right,

1:08:49.140 --> 1:08:50.700
 unless you had enough confidence in it, right?

1:08:50.700 --> 1:08:52.180
 So you'd have to pull out the right answer.

1:08:52.180 --> 1:08:54.820
 You'd have to have confidence it was the right answer.

1:08:54.820 --> 1:08:58.100
 And then you'd have to do that fast enough to now go buzz in

1:08:58.100 --> 1:09:00.300
 and you'd still only get 65% of them right,

1:09:00.300 --> 1:09:02.660
 which doesn't even put you in the winner's circle.

1:09:02.660 --> 1:09:05.060
 Winner's circle, you have to be up over 70

1:09:05.060 --> 1:09:06.060
 and you have to do it really quick

1:09:06.060 --> 1:09:08.020
 and you have to do it really quickly.

1:09:08.020 --> 1:09:10.100
 But now the problem is, well,

1:09:10.100 --> 1:09:12.500
 even if I had somewhere in the top 10 documents,

1:09:12.500 --> 1:09:14.980
 how do I figure out where in the top 10 documents

1:09:14.980 --> 1:09:18.040
 that answer is and how do I compute a confidence

1:09:18.040 --> 1:09:19.740
 of all the possible candidates?

1:09:19.740 --> 1:09:21.820
 So it's not like I go in knowing the right answer

1:09:21.820 --> 1:09:22.660
 and I have to pick it.

1:09:22.660 --> 1:09:23.940
 I don't know the right answer.

1:09:23.940 --> 1:09:25.580
 I have a bunch of documents,

1:09:25.580 --> 1:09:27.100
 somewhere in there is the right answer.

1:09:27.100 --> 1:09:28.700
 How do I, as a machine, go out

1:09:28.700 --> 1:09:30.020
 and figure out which one's right?

1:09:30.020 --> 1:09:31.460
 And then how do I score it?

1:09:32.640 --> 1:09:35.300
 So, and now how do I deal with the fact

1:09:35.300 --> 1:09:37.320
 that I can't actually go out to the web?

1:09:37.320 --> 1:09:40.020
 First of all, if you pause on that, just think about it.

1:09:40.020 --> 1:09:42.160
 If you could go to the web,

1:09:42.160 --> 1:09:44.260
 do you think that problem is solvable

1:09:44.260 --> 1:09:45.540
 if you just pause on it?

1:09:45.540 --> 1:09:48.340
 Just thinking even beyond jeopardy,

1:09:49.220 --> 1:09:51.340
 do you think the problem of reading text

1:09:51.340 --> 1:09:53.660
 defined where the answer is?

1:09:53.660 --> 1:09:56.700
 Well, we solved that in some definition of solves

1:09:56.700 --> 1:09:58.020
 given the jeopardy challenge.

1:09:58.020 --> 1:09:59.020
 How did you do it for jeopardy?

1:09:59.020 --> 1:10:03.260
 So how do you take a body of work in a particular topic

1:10:03.260 --> 1:10:05.940
 and extract the key pieces of information?

1:10:05.940 --> 1:10:09.100
 So now forgetting about the huge volumes

1:10:09.100 --> 1:10:10.060
 that are on the web, right?

1:10:10.060 --> 1:10:11.260
 So now we have to figure out,

1:10:11.260 --> 1:10:12.740
 we did a lot of source research.

1:10:12.740 --> 1:10:15.720
 In other words, what body of knowledge

1:10:15.720 --> 1:10:17.180
 is gonna be small enough,

1:10:17.180 --> 1:10:19.820
 but broad enough to answer jeopardy?

1:10:19.820 --> 1:10:21.920
 And we ultimately did find the body of knowledge

1:10:21.920 --> 1:10:22.760
 that did that.

1:10:22.760 --> 1:10:25.100
 I mean, it included Wikipedia and a bunch of other stuff.

1:10:25.100 --> 1:10:26.700
 So like encyclopedia type of stuff.

1:10:26.700 --> 1:10:27.540
 I don't know if you can speak to it.

1:10:27.540 --> 1:10:28.500
 Encyclopedias, dictionaries,

1:10:28.500 --> 1:10:31.140
 different types of semantic resources,

1:10:31.140 --> 1:10:33.980
 like WordNet and other types of semantic resources like that,

1:10:33.980 --> 1:10:36.060
 as well as like some web crawls.

1:10:36.060 --> 1:10:39.060
 In other words, where we went out and took that content

1:10:39.060 --> 1:10:41.700
 and then expanded it based on producing,

1:10:41.700 --> 1:10:44.620
 statistically producing seeds,

1:10:44.620 --> 1:10:48.740
 using those seeds for other searches and then expanding that.

1:10:48.740 --> 1:10:51.500
 So using these like expansion techniques,

1:10:51.500 --> 1:10:53.580
 we went out and had found enough content

1:10:53.580 --> 1:10:54.620
 and we're like, okay, this is good.

1:10:54.620 --> 1:10:56.980
 And even up until the end,

1:10:56.980 --> 1:10:58.380
 we had a thread of research.

1:10:58.380 --> 1:10:59.780
 It was always trying to figure out

1:10:59.780 --> 1:11:02.220
 what content could we efficiently include.

1:11:02.220 --> 1:11:03.420
 I mean, there's a lot of popular,

1:11:03.420 --> 1:11:05.420
 like what is the church lady?

1:11:05.420 --> 1:11:08.020
 Well, I think was one of the, like what,

1:11:09.660 --> 1:11:12.380
 where do you, I guess that's probably an encyclopedia, so.

1:11:12.380 --> 1:11:13.900
 So that was an encyclopedia,

1:11:13.900 --> 1:11:16.060
 but then we would take that stuff

1:11:16.060 --> 1:11:17.780
 and we would go out and we would expand.

1:11:17.780 --> 1:11:20.140
 In other words, we'd go find other content

1:11:20.140 --> 1:11:23.300
 that wasn't in the core resources and expand it.

1:11:23.300 --> 1:11:26.180
 The amount of content, we grew it by an order of magnitude,

1:11:26.180 --> 1:11:28.580
 but still, again, from a web scale perspective,

1:11:28.580 --> 1:11:30.540
 this is very small amount of content.

1:11:30.540 --> 1:11:31.380
 It's very select.

1:11:31.380 --> 1:11:33.100
 We then took all that content,

1:11:33.100 --> 1:11:35.220
 we preanalyzed the crap out of it,

1:11:35.220 --> 1:11:38.500
 meaning we parsed it,

1:11:38.500 --> 1:11:40.700
 broke it down into all those individual words

1:11:40.700 --> 1:11:42.180
 and then we did semantic,

1:11:42.180 --> 1:11:44.620
 syntactic and semantic parses on it,

1:11:44.620 --> 1:11:46.980
 had computer algorithms that annotated it

1:11:46.980 --> 1:11:51.980
 and we indexed that in a very rich and very fast index.

1:11:53.140 --> 1:11:55.260
 So we have a relatively huge amount of,

1:11:55.260 --> 1:11:57.420
 let's say the equivalent of, for the sake of argument,

1:11:57.420 --> 1:11:58.980
 two to 5 million bucks.

1:11:58.980 --> 1:12:01.820
 We've now analyzed all that, blowing up its size even more

1:12:01.820 --> 1:12:03.620
 because now we have all this metadata

1:12:03.620 --> 1:12:05.660
 and then we richly indexed all of that

1:12:05.660 --> 1:12:08.940
 and by the way, in a giant in memory cache.

1:12:08.940 --> 1:12:11.980
 So Watson did not go to disk.

1:12:11.980 --> 1:12:13.660
 So the infrastructure component there,

1:12:13.660 --> 1:12:15.860
 if you could just speak to it, how tough it,

1:12:15.860 --> 1:12:20.780
 I mean, I know 2000, maybe this is 2008, nine,

1:12:22.900 --> 1:12:24.500
 that's kind of a long time ago.

1:12:25.900 --> 1:12:28.260
 How hard is it to use multiple machines?

1:12:28.260 --> 1:12:29.900
 How hard is the infrastructure component,

1:12:29.900 --> 1:12:31.620
 the hardware component?

1:12:31.620 --> 1:12:33.860
 So we used IBM hardware.

1:12:33.860 --> 1:12:36.100
 We had something like, I forgot exactly,

1:12:36.100 --> 1:12:40.740
 but close to 3000 cores completely connected.

1:12:40.740 --> 1:12:42.780
 So you had a switch where every CPU

1:12:42.780 --> 1:12:43.620
 was connected to every other CPU.

1:12:43.620 --> 1:12:46.100
 And they were sharing memory in some kind of way.

1:12:46.100 --> 1:12:47.980
 Large shared memory, right?

1:12:47.980 --> 1:12:50.740
 And all this data was preanalyzed

1:12:50.740 --> 1:12:54.860
 and put into a very fast indexing structure

1:12:54.860 --> 1:12:58.300
 that was all in memory.

1:12:58.300 --> 1:13:01.380
 And then we took that question,

1:13:02.780 --> 1:13:04.380
 we would analyze the question.

1:13:04.380 --> 1:13:07.180
 So all the content was now preanalyzed.

1:13:07.180 --> 1:13:10.820
 So if I went and tried to find a piece of content,

1:13:10.820 --> 1:13:12.540
 it would come back with all the metadata

1:13:12.540 --> 1:13:14.580
 that we had precomputed.

1:13:14.580 --> 1:13:16.940
 How do you shove that question?

1:13:16.940 --> 1:13:20.820
 How do you connect the big knowledge base

1:13:20.820 --> 1:13:22.660
 with the metadata and that's indexed

1:13:22.660 --> 1:13:26.940
 to the simple little witty confusing question?

1:13:26.940 --> 1:13:27.780
 Right.

1:13:27.780 --> 1:13:31.300
 So therein lies the Watson architecture, right?

1:13:31.300 --> 1:13:32.940
 So we would take the question,

1:13:32.940 --> 1:13:34.700
 we would analyze the question.

1:13:34.700 --> 1:13:37.020
 So which means that we would parse it

1:13:37.020 --> 1:13:38.740
 and interpret it a bunch of different ways.

1:13:38.740 --> 1:13:40.820
 We'd try to figure out what is it asking about?

1:13:40.820 --> 1:13:44.420
 So we had multiple strategies

1:13:44.420 --> 1:13:47.100
 to kind of determine what was it asking for.

1:13:47.100 --> 1:13:49.460
 That might be represented as a simple string,

1:13:49.460 --> 1:13:51.420
 a character string,

1:13:51.420 --> 1:13:53.140
 or something we would connect back

1:13:53.140 --> 1:13:54.820
 to different semantic types

1:13:54.820 --> 1:13:56.100
 that were from existing resources.

1:13:56.100 --> 1:13:57.820
 So anyway, the bottom line is

1:13:57.820 --> 1:14:00.420
 we would do a bunch of analysis in the question.

1:14:00.420 --> 1:14:04.220
 And question analysis had to finish and had to finish fast.

1:14:04.220 --> 1:14:05.340
 So we do the question analysis

1:14:05.340 --> 1:14:07.900
 because then from the question analysis,

1:14:07.900 --> 1:14:09.780
 we would now produce searches.

1:14:09.780 --> 1:14:12.700
 So we would, and we had built

1:14:12.700 --> 1:14:14.260
 using open source search engines,

1:14:14.260 --> 1:14:16.100
 we modified them,

1:14:16.100 --> 1:14:17.940
 but we had a number of different search engines

1:14:17.940 --> 1:14:20.740
 we would use that had different characteristics.

1:14:20.740 --> 1:14:22.540
 We went in there and engineered

1:14:22.540 --> 1:14:24.540
 and modified those search engines,

1:14:24.540 --> 1:14:28.500
 ultimately to now take our question analysis,

1:14:28.500 --> 1:14:29.900
 produce multiple queries

1:14:29.900 --> 1:14:33.300
 based on different interpretations of the question

1:14:33.300 --> 1:14:36.460
 and fire out a whole bunch of searches in parallel.

1:14:36.460 --> 1:14:39.820
 And they would come back with passages.

1:14:39.820 --> 1:14:42.060
 So these are passive search algorithms.

1:14:42.060 --> 1:14:43.700
 They would come back with passages.

1:14:43.700 --> 1:14:47.140
 And so now let's say you had a thousand passages.

1:14:47.140 --> 1:14:50.700
 Now for each passage, you parallelize again.

1:14:50.700 --> 1:14:55.220
 So you went out and you parallelize the search.

1:14:55.220 --> 1:14:56.460
 Each search would now come back

1:14:56.460 --> 1:14:58.580
 with a whole bunch of passages.

1:14:58.580 --> 1:15:00.460
 Maybe you had a total of a thousand

1:15:00.460 --> 1:15:02.220
 or 5,000 whatever passages.

1:15:02.220 --> 1:15:03.900
 For each passage now,

1:15:03.900 --> 1:15:05.220
 you'd go and figure out whether or not

1:15:05.220 --> 1:15:06.620
 there was a candidate,

1:15:06.620 --> 1:15:08.540
 we'd call it candidate answer in there.

1:15:08.540 --> 1:15:11.540
 So you had a whole bunch of other algorithms

1:15:11.540 --> 1:15:13.220
 that would find candidate answers,

1:15:13.220 --> 1:15:15.620
 possible answers to the question.

1:15:15.620 --> 1:15:17.620
 And so you had candidate answer,

1:15:17.620 --> 1:15:19.540
 called candidate answer generators,

1:15:19.540 --> 1:15:20.780
 a whole bunch of those.

1:15:20.780 --> 1:15:23.100
 So for every one of these components,

1:15:23.100 --> 1:15:25.620
 the team was constantly doing research coming up,

1:15:25.620 --> 1:15:28.220
 better ways to generate search queries from the questions,

1:15:28.220 --> 1:15:29.940
 better ways to analyze the question,

1:15:29.940 --> 1:15:31.420
 better ways to generate candidates.

1:15:31.420 --> 1:15:35.380
 And speed, so better is accuracy and speed.

1:15:35.380 --> 1:15:38.100
 Correct, so right, speed and accuracy

1:15:38.100 --> 1:15:40.500
 for the most part were separated.

1:15:40.500 --> 1:15:42.180
 We handle that sort of in separate ways.

1:15:42.180 --> 1:15:45.180
 Like I focus purely on accuracy, end to end accuracy.

1:15:45.180 --> 1:15:46.900
 Are we ultimately getting more questions

1:15:46.900 --> 1:15:48.860
 and producing more accurate confidences?

1:15:48.860 --> 1:15:50.180
 And then a whole nother team

1:15:50.180 --> 1:15:52.420
 that was constantly analyzing the workflow

1:15:52.420 --> 1:15:53.780
 to find the bottlenecks.

1:15:53.780 --> 1:15:55.740
 And then figuring out how to both parallelize

1:15:55.740 --> 1:15:58.060
 and drive the algorithm speed.

1:15:58.060 --> 1:15:59.980
 But anyway, so now think of it like,

1:15:59.980 --> 1:16:01.700
 you have this big fan out now, right?

1:16:01.700 --> 1:16:03.620
 Because you had multiple queries,

1:16:03.620 --> 1:16:06.940
 now you have thousands of candidate answers.

1:16:06.940 --> 1:16:09.980
 For each candidate answer, you're gonna score it.

1:16:09.980 --> 1:16:12.420
 So you're gonna use all the data that built up.

1:16:12.420 --> 1:16:15.460
 You're gonna use the question analysis,

1:16:15.460 --> 1:16:17.580
 you're gonna use how the query was generated,

1:16:17.580 --> 1:16:19.820
 you're gonna use the passage itself,

1:16:19.820 --> 1:16:21.620
 and you're gonna use the candidate answer

1:16:21.620 --> 1:16:25.460
 that was generated, and you're gonna score that.

1:16:25.460 --> 1:16:28.020
 So now we have a group of researchers

1:16:28.020 --> 1:16:30.100
 coming up with scores.

1:16:30.100 --> 1:16:32.300
 There are hundreds of different scores.

1:16:32.300 --> 1:16:34.580
 So now you're getting a fan out of it again

1:16:34.580 --> 1:16:37.380
 from however many candidate answers you have

1:16:37.380 --> 1:16:39.260
 to all the different scores.

1:16:39.260 --> 1:16:41.260
 So if you have 200 different scores

1:16:41.260 --> 1:16:42.420
 and you have a thousand candidates,

1:16:42.420 --> 1:16:45.180
 now you have 200,000 scores.

1:16:45.180 --> 1:16:47.060
 And so now you gotta figure out,

1:16:48.060 --> 1:16:52.340
 how do I now rank these answers

1:16:52.340 --> 1:16:54.460
 based on the scores that came back?

1:16:54.460 --> 1:16:56.300
 And I wanna rank them based on the likelihood

1:16:56.300 --> 1:16:58.620
 that they're a correct answer to the question.

1:16:58.620 --> 1:17:01.380
 So every scorer was its own research project.

1:17:01.380 --> 1:17:02.340
 What do you mean by scorer?

1:17:02.340 --> 1:17:04.060
 So is that the annotation process

1:17:04.060 --> 1:17:07.700
 of basically a human being saying that this answer

1:17:07.700 --> 1:17:09.340
 has a quality of?

1:17:09.340 --> 1:17:10.740
 Think of it, if you wanna think of it,

1:17:10.740 --> 1:17:12.580
 what you're doing, you know,

1:17:12.580 --> 1:17:14.060
 if you wanna think about what a human would be doing,

1:17:14.060 --> 1:17:17.060
 human would be looking at a possible answer,

1:17:17.060 --> 1:17:20.860
 they'd be reading the, you know, Emily Dickinson,

1:17:20.860 --> 1:17:23.540
 they'd be reading the passage in which that occurred,

1:17:23.540 --> 1:17:25.340
 they'd be looking at the question,

1:17:25.340 --> 1:17:28.340
 and they'd be making a decision of how likely it is

1:17:28.340 --> 1:17:32.260
 that Emily Dickinson, given this evidence in this passage,

1:17:32.260 --> 1:17:33.900
 is the right answer to that question.

1:17:33.900 --> 1:17:34.740
 Got it.

1:17:34.740 --> 1:17:36.180
 So that's the annotation task.

1:17:36.180 --> 1:17:37.020
 That's the annotation process.

1:17:37.020 --> 1:17:38.780
 That's the scoring task.

1:17:38.780 --> 1:17:41.260
 But scoring implies zero to one kind of continuous.

1:17:41.260 --> 1:17:42.100
 That's right.

1:17:42.100 --> 1:17:42.940
 You give it a zero to one score.

1:17:42.940 --> 1:17:44.380
 So it's not a binary.

1:17:44.380 --> 1:17:45.980
 No, you give it a score.

1:17:46.860 --> 1:17:48.700
 Give it a zero to, yeah, exactly, zero to one score.

1:17:48.700 --> 1:17:50.500
 But humans give different scores,

1:17:50.500 --> 1:17:52.940
 so you have to somehow normalize and all that kind of stuff

1:17:52.940 --> 1:17:54.260
 that deal with all that complexity.

1:17:54.260 --> 1:17:55.740
 It depends on what your strategy is.

1:17:55.740 --> 1:17:57.060
 We both, we...

1:17:57.060 --> 1:17:58.060
 It could be relative, too.

1:17:58.060 --> 1:17:59.420
 It could be...

1:17:59.420 --> 1:18:01.580
 We actually looked at the raw scores

1:18:01.580 --> 1:18:02.780
 as well as standardized scores,

1:18:02.780 --> 1:18:04.980
 because humans are not involved in this.

1:18:04.980 --> 1:18:05.900
 Humans are not involved.

1:18:05.900 --> 1:18:08.700
 Sorry, so I'm misunderstanding the process here.

1:18:08.700 --> 1:18:10.460
 This is passages.

1:18:10.460 --> 1:18:13.300
 Where is the ground truth coming from?

1:18:13.300 --> 1:18:15.860
 Ground truth is only the answers to the questions.

1:18:16.820 --> 1:18:17.940
 So it's end to end.

1:18:17.940 --> 1:18:19.020
 It's end to end.

1:18:19.020 --> 1:18:22.420
 So I was always driving end to end performance.

1:18:22.420 --> 1:18:25.540
 It's a very interesting, a very interesting

1:18:25.540 --> 1:18:27.900
 engineering approach,

1:18:27.900 --> 1:18:30.140
 and ultimately scientific research approach,

1:18:30.140 --> 1:18:31.300
 always driving end to end.

1:18:31.300 --> 1:18:33.260
 Now, that's not to say

1:18:34.780 --> 1:18:38.620
 we wouldn't make hypotheses

1:18:38.620 --> 1:18:42.180
 that individual component performance

1:18:42.180 --> 1:18:44.460
 was related in some way to end to end performance.

1:18:44.460 --> 1:18:45.300
 Of course we would,

1:18:45.300 --> 1:18:48.940
 because people would have to build individual components.

1:18:48.940 --> 1:18:51.340
 But ultimately, to get your component integrated

1:18:51.340 --> 1:18:53.540
 to the system, you have to show impact

1:18:53.540 --> 1:18:55.980
 on end to end performance, question answering performance.

1:18:55.980 --> 1:18:58.420
 So there's many very smart people working on this,

1:18:58.420 --> 1:19:01.540
 and they're basically trying to sell their ideas

1:19:01.540 --> 1:19:03.460
 as a component that should be part of the system.

1:19:03.460 --> 1:19:04.620
 That's right.

1:19:04.620 --> 1:19:07.340
 And they would do research on their component,

1:19:07.340 --> 1:19:09.780
 and they would say things like,

1:19:09.780 --> 1:19:13.140
 I'm gonna improve this as a candidate generator,

1:19:13.140 --> 1:19:15.860
 or I'm gonna improve this as a question score,

1:19:15.860 --> 1:19:17.780
 or as a passive scorer,

1:19:17.780 --> 1:19:20.700
 I'm gonna improve this, or as a parser,

1:19:20.700 --> 1:19:25.500
 and I can improve it by 2% on its component metric,

1:19:25.500 --> 1:19:27.940
 like a better parse, or a better candidate,

1:19:27.940 --> 1:19:30.260
 or a better type estimation, whatever it is.

1:19:30.260 --> 1:19:31.700
 And then I would say,

1:19:31.700 --> 1:19:33.900
 I need to understand how the improvement

1:19:33.900 --> 1:19:35.340
 on that component metric

1:19:35.340 --> 1:19:37.740
 is gonna affect the end to end performance.

1:19:37.740 --> 1:19:39.460
 If you can't estimate that,

1:19:39.460 --> 1:19:41.860
 and can't do experiments to demonstrate that,

1:19:41.860 --> 1:19:43.420
 it doesn't get in.

1:19:43.420 --> 1:19:47.540
 That's like the best run AI project I've ever heard.

1:19:47.540 --> 1:19:48.380
 That's awesome.

1:19:48.380 --> 1:19:51.780
 Okay, what breakthrough would you say,

1:19:51.780 --> 1:19:54.260
 like, I'm sure there's a lot of day to day breakthroughs,

1:19:54.260 --> 1:19:55.620
 but was there like a breakthrough

1:19:55.620 --> 1:19:57.860
 that really helped improve performance?

1:19:57.860 --> 1:20:00.180
 Like where people began to believe,

1:20:01.140 --> 1:20:02.500
 or is it just a gradual process?

1:20:02.500 --> 1:20:04.500
 Well, I think it was a gradual process,

1:20:04.500 --> 1:20:08.980
 but one of the things that I think gave people confidence

1:20:08.980 --> 1:20:11.620
 that we can get there was that,

1:20:11.620 --> 1:20:16.620
 as we follow this procedure of different ideas,

1:20:16.620 --> 1:20:19.140
 build different components,

1:20:19.140 --> 1:20:20.460
 plug them into the architecture,

1:20:20.460 --> 1:20:23.180
 run the system, see how we do,

1:20:23.180 --> 1:20:24.700
 do the error analysis,

1:20:24.700 --> 1:20:28.140
 start off new research projects to improve things.

1:20:28.140 --> 1:20:31.260
 And the very important idea

1:20:31.260 --> 1:20:34.260
 that the individual component work

1:20:37.420 --> 1:20:40.020
 did not have to deeply understand everything

1:20:40.020 --> 1:20:42.220
 that was going on with every other component.

1:20:42.220 --> 1:20:45.060
 And this is where we leverage machine learning

1:20:45.060 --> 1:20:47.380
 in a very important way.

1:20:47.380 --> 1:20:48.780
 So while individual components

1:20:48.780 --> 1:20:51.260
 could be statistically driven machine learning components,

1:20:51.260 --> 1:20:52.740
 some of them were heuristic,

1:20:52.740 --> 1:20:54.580
 some of them were machine learning components,

1:20:54.580 --> 1:20:58.100
 the system has a whole combined all the scores

1:20:58.100 --> 1:20:59.300
 using machine learning.

1:21:00.540 --> 1:21:02.700
 This was critical because that way

1:21:02.700 --> 1:21:04.340
 you can divide and conquer.

1:21:04.340 --> 1:21:07.500
 So you can say, okay, you work on your candidate generator,

1:21:07.500 --> 1:21:09.740
 or you work on this approach to answer scoring,

1:21:09.740 --> 1:21:11.740
 you work on this approach to type scoring,

1:21:11.740 --> 1:21:14.500
 you work on this approach to passage search

1:21:14.500 --> 1:21:16.300
 or to pass a selection and so forth.

1:21:17.340 --> 1:21:19.580
 But when we just plug it in,

1:21:19.580 --> 1:21:22.020
 and we had enough training data to say,

1:21:22.020 --> 1:21:24.540
 now we can train and figure out

1:21:24.540 --> 1:21:29.300
 how do we weigh all the scores relative to each other

1:21:29.300 --> 1:21:31.900
 based on the predicting the outcome,

1:21:31.900 --> 1:21:33.820
 which is right or wrong on Jeopardy.

1:21:33.820 --> 1:21:36.780
 And we had enough training data to do that.

1:21:36.780 --> 1:21:40.500
 So this enabled people to work independently

1:21:40.500 --> 1:21:43.340
 and to let the machine learning do the integration.

1:21:43.340 --> 1:21:45.100
 Beautiful, so yeah, the machine learning

1:21:45.100 --> 1:21:46.340
 is doing the fusion,

1:21:46.340 --> 1:21:48.980
 and then it's a human orchestrated ensemble

1:21:48.980 --> 1:21:50.380
 of different approaches.

1:21:50.380 --> 1:21:51.940
 That's great.

1:21:53.420 --> 1:21:54.940
 Still impressive that you were able

1:21:54.940 --> 1:21:56.500
 to get it done in a few years.

1:21:57.620 --> 1:22:00.420
 That's not obvious to me that it's doable,

1:22:00.420 --> 1:22:03.340
 if I just put myself in that mindset.

1:22:03.340 --> 1:22:05.860
 But when you look back at the Jeopardy challenge,

1:22:07.820 --> 1:22:10.220
 again, when you're looking up at the stars,

1:22:10.220 --> 1:22:15.220
 what are you most proud of, looking back at those days?

1:22:17.420 --> 1:22:21.540
 I'm most proud of my,

1:22:27.900 --> 1:22:30.860
 my commitment and my team's commitment

1:22:32.260 --> 1:22:33.860
 to be true to the science,

1:22:35.060 --> 1:22:38.020
 to not be afraid to fail.

1:22:38.020 --> 1:22:41.540
 That's beautiful because there's so much pressure,

1:22:41.540 --> 1:22:44.380
 because it is a public event, it is a public show,

1:22:44.380 --> 1:22:46.980
 that you were dedicated to the idea.

1:22:46.980 --> 1:22:47.820
 That's right.

1:22:50.460 --> 1:22:53.140
 Do you think it was a success?

1:22:53.140 --> 1:22:55.340
 In the eyes of the world, it was a success.

1:22:56.620 --> 1:22:59.700
 By your, I'm sure, exceptionally high standards,

1:23:00.860 --> 1:23:03.700
 is there something you regret you would do differently?

1:23:03.700 --> 1:23:05.900
 It was a success.

1:23:05.900 --> 1:23:08.180
 It was a success for our goal.

1:23:08.180 --> 1:23:11.340
 Our goal was to build the most advanced

1:23:11.340 --> 1:23:13.260
 open domain question answering system.

1:23:14.700 --> 1:23:16.420
 We went back to the old problems

1:23:16.420 --> 1:23:17.900
 that we used to try to solve,

1:23:17.900 --> 1:23:21.220
 and we did dramatically better on all of them,

1:23:21.220 --> 1:23:24.140
 as well as we beat Jeopardy.

1:23:24.140 --> 1:23:25.780
 So we won at Jeopardy.

1:23:25.780 --> 1:23:28.460
 So it was a success.

1:23:28.460 --> 1:23:32.540
 It was, I worry that the community

1:23:32.540 --> 1:23:36.100
 or the world would not understand it as a success

1:23:36.100 --> 1:23:38.660
 because it came down to only one game.

1:23:38.660 --> 1:23:40.380
 And I knew statistically speaking,

1:23:40.380 --> 1:23:42.260
 this can be a huge technical success,

1:23:42.260 --> 1:23:43.820
 and we could still lose that one game.

1:23:43.820 --> 1:23:47.220
 And that's a whole nother theme of this, of the journey.

1:23:47.220 --> 1:23:50.260
 But it was a success.

1:23:50.260 --> 1:23:53.620
 It was not a success in natural language understanding,

1:23:53.620 --> 1:23:54.980
 but that was not the goal.

1:23:56.620 --> 1:23:59.820
 Yeah, that was, but I would argue,

1:24:00.700 --> 1:24:02.020
 I understand what you're saying

1:24:02.020 --> 1:24:04.140
 in terms of the science,

1:24:04.140 --> 1:24:07.540
 but I would argue that the inspiration of it, right?

1:24:07.540 --> 1:24:11.180
 The, not a success in terms of solving

1:24:11.180 --> 1:24:12.820
 natural language understanding.

1:24:12.820 --> 1:24:15.420
 There was a success of being an inspiration

1:24:16.300 --> 1:24:17.900
 to future challenges.

1:24:17.900 --> 1:24:18.820
 Absolutely.

1:24:18.820 --> 1:24:21.140
 That drive future efforts.

1:24:21.140 --> 1:24:23.740
 What's the difference between how human being

1:24:23.740 --> 1:24:26.860
 compete in Jeopardy and how Watson does it?

1:24:26.860 --> 1:24:28.740
 That's important in terms of intelligence.

1:24:28.740 --> 1:24:31.380
 Yeah, so that actually came up very early on

1:24:31.380 --> 1:24:32.620
 in the project also.

1:24:32.620 --> 1:24:35.180
 In fact, I had people who wanted to be on the project

1:24:35.180 --> 1:24:39.100
 who were early on, who sort of approached me

1:24:39.100 --> 1:24:40.860
 once I committed to do it,

1:24:42.380 --> 1:24:44.300
 had wanted to think about how humans do it.

1:24:44.300 --> 1:24:47.060
 And they were, from a cognition perspective,

1:24:47.060 --> 1:24:49.900
 like human cognition and how that should play.

1:24:49.900 --> 1:24:52.180
 And I would not take them on the project

1:24:52.180 --> 1:24:55.780
 because another assumption or another stake

1:24:55.780 --> 1:24:57.620
 I put in the ground was,

1:24:57.620 --> 1:25:00.180
 I don't really care how humans do this.

1:25:00.180 --> 1:25:01.540
 At least in the context of this project.

1:25:01.540 --> 1:25:03.900
 I need to build in the context of this project.

1:25:03.900 --> 1:25:06.980
 In NLU and in building an AI that understands

1:25:06.980 --> 1:25:09.660
 how it needs to ultimately communicate with humans,

1:25:09.660 --> 1:25:11.260
 I very much care.

1:25:11.260 --> 1:25:16.260
 So it wasn't that I didn't care in general.

1:25:16.540 --> 1:25:20.780
 In fact, as an AI scientist, I care a lot about that,

1:25:20.780 --> 1:25:22.620
 but I'm also a practical engineer

1:25:22.620 --> 1:25:25.540
 and I committed to getting this thing done

1:25:25.540 --> 1:25:27.500
 and I wasn't gonna get distracted.

1:25:27.500 --> 1:25:30.740
 I had to kind of say, like, if I'm gonna get this done,

1:25:30.740 --> 1:25:31.740
 I'm gonna chart this path.

1:25:31.740 --> 1:25:34.900
 And this path says, we're gonna engineer a machine

1:25:35.980 --> 1:25:37.540
 that's gonna get this thing done.

1:25:37.540 --> 1:25:41.500
 And we know what search and NLP can do.

1:25:41.500 --> 1:25:44.140
 We have to build on that foundation.

1:25:44.140 --> 1:25:46.260
 If I come in and take a different approach

1:25:46.260 --> 1:25:48.060
 and start wondering about how the human mind

1:25:48.060 --> 1:25:49.700
 might or might not do this,

1:25:49.700 --> 1:25:54.380
 I'm not gonna get there from here in the timeframe.

1:25:54.380 --> 1:25:56.620
 I think that's a great way to lead the team.

1:25:56.620 --> 1:25:59.180
 But now that it's done and there's one,

1:25:59.180 --> 1:26:03.540
 when you look back, analyze what's the difference actually.

1:26:03.540 --> 1:26:05.460
 So I was a little bit surprised actually

1:26:05.460 --> 1:26:09.020
 to discover over time, as this would come up

1:26:09.020 --> 1:26:11.140
 from time to time and we'd reflect on it,

1:26:13.300 --> 1:26:14.980
 and talking to Ken Jennings a little bit

1:26:14.980 --> 1:26:16.780
 and hearing Ken Jennings talk about

1:26:16.780 --> 1:26:18.860
 how he answered questions,

1:26:18.860 --> 1:26:21.260
 that it might've been closer to the way humans

1:26:21.260 --> 1:26:24.700
 answer questions than I might've imagined previously.

1:26:24.700 --> 1:26:27.860
 Because humans are probably in the game of Jeopardy!

1:26:27.860 --> 1:26:29.500
 at the level of Ken Jennings,

1:26:29.500 --> 1:26:34.500
 are probably also cheating their way to winning, right?

1:26:35.180 --> 1:26:36.020
 Not cheating, but shallow.

1:26:36.020 --> 1:26:37.180
 Well, they're doing shallow analysis.

1:26:37.180 --> 1:26:39.340
 They're doing the fastest possible.

1:26:39.340 --> 1:26:40.860
 They're doing shallow analysis.

1:26:40.860 --> 1:26:44.820
 So they are very quickly analyzing the question

1:26:44.820 --> 1:26:49.820
 and coming up with some key vectors or cues, if you will.

1:26:49.980 --> 1:26:51.060
 And they're taking those cues

1:26:51.060 --> 1:26:52.540
 and they're very quickly going through

1:26:52.540 --> 1:26:54.900
 like their library of stuff,

1:26:54.900 --> 1:26:57.700
 not deeply reasoning about what's going on.

1:26:57.700 --> 1:27:00.580
 And then sort of like a lots of different,

1:27:00.580 --> 1:27:03.180
 like what we would call these scores,

1:27:03.180 --> 1:27:06.060
 would kind of score that in a very shallow way

1:27:06.060 --> 1:27:08.900
 and then say, oh, boom, you know, that's what it is.

1:27:08.900 --> 1:27:12.420
 And so it's interesting as we reflected on that.

1:27:12.420 --> 1:27:16.060
 So we may be doing something that's not too far off

1:27:16.060 --> 1:27:17.220
 from the way humans do it,

1:27:17.220 --> 1:27:21.420
 but we certainly didn't approach it by saying,

1:27:21.420 --> 1:27:22.660
 how would a human do this?

1:27:22.660 --> 1:27:24.620
 Now in elemental cognition,

1:27:24.620 --> 1:27:27.300
 like the project I'm leading now,

1:27:27.300 --> 1:27:28.740
 we ask those questions all the time

1:27:28.740 --> 1:27:31.660
 because ultimately we're trying to do something

1:27:31.660 --> 1:27:35.060
 that is to make the intelligence of the machine

1:27:35.060 --> 1:27:37.740
 and the intelligence of the human very compatible.

1:27:37.740 --> 1:27:38.660
 Well, compatible in the sense

1:27:38.660 --> 1:27:40.940
 they can communicate with one another

1:27:40.940 --> 1:27:44.540
 and they can reason with this shared understanding.

1:27:44.540 --> 1:27:48.020
 So how they think about things and how they build answers,

1:27:48.020 --> 1:27:49.740
 how they build explanations

1:27:49.740 --> 1:27:52.100
 becomes a very important question to consider.

1:27:52.100 --> 1:27:56.900
 So what's the difference between this open domain,

1:27:56.900 --> 1:28:01.900
 but cold constructed question answering of Jeopardy

1:28:02.340 --> 1:28:07.340
 and more something that requires understanding

1:28:07.380 --> 1:28:10.220
 for shared communication with humans and machines?

1:28:10.220 --> 1:28:13.300
 Yeah, well, this goes back to the interpretation

1:28:13.300 --> 1:28:15.540
 of what we were talking about before.

1:28:15.540 --> 1:28:19.140
 Jeopardy, the system's not trying to interpret the question

1:28:19.140 --> 1:28:22.060
 and it's not interpreting the content it's reusing

1:28:22.060 --> 1:28:23.860
 with regard to any particular framework.

1:28:23.860 --> 1:28:26.900
 I mean, it is parsing it and parsing the content

1:28:26.900 --> 1:28:29.460
 and using grammatical cues and stuff like that.

1:28:29.460 --> 1:28:31.700
 So if you think of grammar as a human framework

1:28:31.700 --> 1:28:33.420
 in some sense, it has that,

1:28:33.420 --> 1:28:36.900
 but when you get into the richer semantic frameworks,

1:28:36.900 --> 1:28:40.060
 what do people, how do they think, what motivates them,

1:28:40.060 --> 1:28:41.620
 what are the events that are occurring

1:28:41.620 --> 1:28:42.580
 and why are they occurring

1:28:42.580 --> 1:28:44.420
 and what causes what else to happen

1:28:44.420 --> 1:28:47.460
 and where are things in time and space?

1:28:47.460 --> 1:28:51.260
 And like when you start thinking about how humans formulate

1:28:51.260 --> 1:28:54.020
 and structure the knowledge that they acquire in their head

1:28:54.020 --> 1:28:55.460
 and wasn't doing any of that.

1:28:57.060 --> 1:29:01.500
 What do you think are the essential challenges

1:29:01.500 --> 1:29:05.860
 of like free flowing communication, free flowing dialogue

1:29:05.860 --> 1:29:09.060
 versus question answering even with the framework

1:29:09.060 --> 1:29:11.260
 of the interpretation dialogue?

1:29:11.260 --> 1:29:12.340
 Yep.

1:29:12.340 --> 1:29:14.980
 Do you see free flowing dialogue

1:29:14.980 --> 1:29:19.980
 as a fundamentally more difficult than question answering

1:29:20.420 --> 1:29:23.580
 even with shared interpretation?

1:29:23.580 --> 1:29:26.660
 So dialogue is important in a number of different ways.

1:29:26.660 --> 1:29:27.500
 I mean, it's a challenge.

1:29:27.500 --> 1:29:30.540
 So first of all, when I think about the machine that,

1:29:30.540 --> 1:29:33.300
 when I think about a machine that understands language

1:29:33.300 --> 1:29:36.780
 and ultimately can reason in an objective way

1:29:36.780 --> 1:29:40.580
 that can take the information that it perceives

1:29:40.580 --> 1:29:42.260
 through language or other means

1:29:42.260 --> 1:29:44.540
 and connect it back to these frameworks,

1:29:44.540 --> 1:29:46.220
 reason and explain itself,

1:29:48.020 --> 1:29:50.700
 that system ultimately needs to be able to talk to humans

1:29:50.700 --> 1:29:52.940
 or it needs to be able to interact with humans.

1:29:52.940 --> 1:29:55.180
 So in some sense it needs to dialogue.

1:29:55.180 --> 1:29:57.140
 That doesn't mean that it,

1:29:58.660 --> 1:30:01.820
 sometimes people talk about dialogue and they think,

1:30:01.820 --> 1:30:04.300
 you know, how do humans talk to like,

1:30:04.300 --> 1:30:07.700
 talk to each other in a casual conversation

1:30:07.700 --> 1:30:09.900
 and you can mimic casual conversations.

1:30:11.780 --> 1:30:14.340
 We're not trying to mimic casual conversations.

1:30:14.340 --> 1:30:17.580
 We're really trying to produce a machine

1:30:17.580 --> 1:30:20.260
 whose goal is to help you think

1:30:20.260 --> 1:30:23.620
 and help you reason about your answers and explain why.

1:30:23.620 --> 1:30:26.580
 So instead of like talking to your friend down the street

1:30:26.580 --> 1:30:28.900
 about having a small talk conversation

1:30:28.900 --> 1:30:30.500
 with your friend down the street,

1:30:30.500 --> 1:30:32.380
 this is more about like you would be communicating

1:30:32.380 --> 1:30:34.060
 to the computer on Star Trek

1:30:34.060 --> 1:30:36.780
 where like, what do you wanna think about?

1:30:36.780 --> 1:30:37.620
 Like, what do you wanna reason about?

1:30:37.620 --> 1:30:38.780
 I'm gonna tell you the information I have.

1:30:38.780 --> 1:30:39.860
 I'm gonna have to summarize it.

1:30:39.860 --> 1:30:41.060
 I'm gonna ask you questions.

1:30:41.060 --> 1:30:42.700
 You're gonna answer those questions.

1:30:42.700 --> 1:30:44.300
 I'm gonna go back and forth with you.

1:30:44.300 --> 1:30:46.620
 I'm gonna figure out what your mental model is.

1:30:46.620 --> 1:30:49.940
 I'm gonna now relate that to the information I have

1:30:49.940 --> 1:30:53.060
 and present it to you in a way that you can understand it

1:30:53.060 --> 1:30:54.940
 and then we could ask followup questions.

1:30:54.940 --> 1:30:58.340
 So it's that type of dialogue that you wanna construct.

1:30:58.340 --> 1:31:02.380
 It's more structured, it's more goal oriented,

1:31:02.380 --> 1:31:04.900
 but it needs to be fluid.

1:31:04.900 --> 1:31:09.300
 In other words, it has to be engaging and fluid.

1:31:09.300 --> 1:31:13.100
 It has to be productive and not distracting.

1:31:13.100 --> 1:31:15.700
 So there has to be a model of,

1:31:15.700 --> 1:31:17.580
 in other words, the machine has to have a model

1:31:17.580 --> 1:31:21.860
 of how humans think through things and discuss them.

1:31:22.660 --> 1:31:26.900
 So basically a productive, rich conversation

1:31:28.700 --> 1:31:30.100
 unlike this podcast.

1:31:32.700 --> 1:31:34.940
 I'd like to think it's more similar to this podcast.

1:31:34.940 --> 1:31:36.100
 I wasn't joking.

1:31:37.020 --> 1:31:39.740
 I'll ask you about humor as well, actually.

1:31:39.740 --> 1:31:43.300
 But what's the hardest part of that?

1:31:43.300 --> 1:31:45.340
 Because it seems we're quite far away

1:31:46.620 --> 1:31:49.820
 as a community from that still to be able to,

1:31:49.820 --> 1:31:53.020
 so one is having a shared understanding.

1:31:53.020 --> 1:31:54.920
 That's, I think, a lot of the stuff you said

1:31:54.920 --> 1:31:57.160
 with frameworks is quite brilliant.

1:31:57.160 --> 1:32:01.500
 But just creating a smooth discourse.

1:32:02.740 --> 1:32:05.300
 It feels clunky right now.

1:32:05.300 --> 1:32:07.620
 Which aspects of this whole problem

1:32:07.620 --> 1:32:10.540
 that you just specified of having

1:32:10.540 --> 1:32:12.900
 a productive conversation is the hardest?

1:32:12.900 --> 1:32:17.700
 And that we're, or maybe any aspect of it

1:32:17.700 --> 1:32:20.780
 you can comment on because it's so shrouded in mystery.

1:32:20.780 --> 1:32:24.280
 So I think to do this you kind of have to be creative

1:32:24.280 --> 1:32:25.900
 in the following sense.

1:32:26.820 --> 1:32:29.820
 If I were to do this as purely a machine learning approach

1:32:29.820 --> 1:32:32.940
 and someone said learn how to have a good,

1:32:32.940 --> 1:32:37.940
 fluent, structured knowledge acquisition conversation,

1:32:38.420 --> 1:32:39.980
 I'd go out and say, okay, I have to collect

1:32:39.980 --> 1:32:42.360
 a bunch of data of people doing that.

1:32:42.360 --> 1:32:47.100
 People reasoning well, having a good, structured

1:32:47.100 --> 1:32:50.320
 conversation that both acquires knowledge efficiently

1:32:50.320 --> 1:32:52.420
 as well as produces answers and explanations

1:32:52.420 --> 1:32:54.600
 as part of the process.

1:32:54.600 --> 1:32:57.340
 And you struggle.

1:32:57.340 --> 1:32:58.180
 I don't know.

1:32:58.180 --> 1:32:59.000
 To collect the data.

1:32:59.000 --> 1:33:00.700
 To collect the data because I don't know

1:33:00.700 --> 1:33:02.640
 how much data is like that.

1:33:02.640 --> 1:33:06.140
 Okay, there's one, there's a humorous commentary

1:33:06.140 --> 1:33:08.500
 on the lack of rational discourse.

1:33:08.500 --> 1:33:12.700
 But also even if it's out there, say it was out there,

1:33:12.700 --> 1:33:16.380
 how do you actually annotate, like how do you collect

1:33:16.380 --> 1:33:17.220
 an accessible example?

1:33:17.220 --> 1:33:19.200
 Right, so I think any problem like this

1:33:19.200 --> 1:33:23.140
 where you don't have enough data to represent

1:33:23.140 --> 1:33:24.740
 the phenomenon you want to learn,

1:33:24.740 --> 1:33:26.740
 in other words you want, if you have enough data

1:33:26.740 --> 1:33:28.540
 you could potentially learn the pattern.

1:33:28.540 --> 1:33:30.340
 In an example like this it's hard to do.

1:33:30.340 --> 1:33:34.420
 This is sort of a human sort of thing to do.

1:33:34.420 --> 1:33:37.020
 What recently came out at IBM was the debater projects

1:33:37.020 --> 1:33:39.460
 and it's interesting, right, because now you do have

1:33:39.460 --> 1:33:42.580
 these structured dialogues, these debate things

1:33:42.580 --> 1:33:44.700
 where they did use machine learning techniques

1:33:44.700 --> 1:33:46.980
 to generate these debates.

1:33:49.220 --> 1:33:52.460
 Dialogues are a little bit tougher in my opinion

1:33:52.460 --> 1:33:56.100
 than generating a structured argument

1:33:56.100 --> 1:33:57.580
 where you have lots of other structured arguments

1:33:57.580 --> 1:33:59.540
 like this, you could potentially annotate that data

1:33:59.540 --> 1:34:00.820
 and you could say this is a good response,

1:34:00.820 --> 1:34:03.060
 this is a bad response in a particular domain.

1:34:03.060 --> 1:34:08.060
 Here I have to be responsive and I have to be opportunistic

1:34:08.900 --> 1:34:11.820
 with regard to what is the human saying.

1:34:11.820 --> 1:34:14.900
 So I'm goal oriented in saying I want to solve the problem,

1:34:14.900 --> 1:34:16.580
 I want to acquire the knowledge necessary,

1:34:16.580 --> 1:34:19.140
 but I also have to be opportunistic and responsive

1:34:19.140 --> 1:34:21.040
 to what the human is saying.

1:34:21.040 --> 1:34:24.060
 So I think that it's not clear that we could just train

1:34:24.060 --> 1:34:28.020
 on the body of data to do this, but we could bootstrap it.

1:34:28.020 --> 1:34:30.540
 In other words, we can be creative and we could say,

1:34:30.540 --> 1:34:34.020
 what do we think the structure of a good dialogue is

1:34:34.020 --> 1:34:35.820
 that does this well?

1:34:35.820 --> 1:34:37.860
 And we can start to create that.

1:34:37.860 --> 1:34:42.100
 If we can create that more programmatically,

1:34:42.100 --> 1:34:44.700
 at least to get this process started

1:34:44.700 --> 1:34:47.980
 and I can create a tool that now engages humans effectively,

1:34:47.980 --> 1:34:51.340
 I could start generating data,

1:34:51.340 --> 1:34:53.020
 I could start the human learning process

1:34:53.020 --> 1:34:55.060
 and I can update my machine,

1:34:55.060 --> 1:34:57.700
 but I could also start the automatic learning process

1:34:57.700 --> 1:34:59.860
 as well, but I have to understand

1:34:59.860 --> 1:35:01.860
 what features to even learn over.

1:35:01.860 --> 1:35:04.740
 So I have to bootstrap the process a little bit first.

1:35:04.740 --> 1:35:07.740
 And that's a creative design task

1:35:07.740 --> 1:35:11.060
 that I could then use as input

1:35:11.060 --> 1:35:13.420
 into a more automatic learning task.

1:35:13.420 --> 1:35:16.740
 So some creativity in bootstrapping.

1:35:16.740 --> 1:35:18.020
 What elements of a conversation

1:35:18.020 --> 1:35:21.140
 do you think you would like to see?

1:35:21.140 --> 1:35:25.620
 So one of the benchmarks for me is humor, right?

1:35:25.620 --> 1:35:27.580
 That seems to be one of the hardest.

1:35:27.580 --> 1:35:30.340
 And to me, the biggest contrast is sort of Watson.

1:35:31.340 --> 1:35:33.380
 So one of the greatest sketches,

1:35:33.380 --> 1:35:35.260
 comedy sketches of all time, right,

1:35:35.260 --> 1:35:37.700
 is the SNL celebrity Jeopardy

1:35:38.580 --> 1:35:42.060
 with Alex Trebek and Sean Connery

1:35:42.060 --> 1:35:44.060
 and Burt Reynolds and so on,

1:35:44.060 --> 1:35:47.900
 with Sean Connery commentating on Alex Trebek's

1:35:47.900 --> 1:35:49.380
 while they're alive.

1:35:49.380 --> 1:35:52.860
 And I think all of them are in the negative pointwise.

1:35:52.860 --> 1:35:55.100
 So they're clearly all losing

1:35:55.100 --> 1:35:56.340
 in terms of the game of Jeopardy,

1:35:56.340 --> 1:35:58.340
 but they're winning in terms of comedy.

1:35:58.340 --> 1:36:03.340
 So what do you think about humor in this whole interaction

1:36:03.780 --> 1:36:06.500
 in the dialogue that's productive?

1:36:06.500 --> 1:36:09.780
 Or even just what humor represents to me

1:36:09.780 --> 1:36:14.780
 is the same idea that you're saying about framework,

1:36:15.420 --> 1:36:16.420
 because humor only exists

1:36:16.420 --> 1:36:18.340
 within a particular human framework.

1:36:18.340 --> 1:36:19.580
 So what do you think about humor?

1:36:19.580 --> 1:36:21.540
 What do you think about things like humor

1:36:21.540 --> 1:36:23.340
 that connect to the kind of creativity

1:36:23.340 --> 1:36:25.100
 you mentioned that's needed?

1:36:25.100 --> 1:36:26.380
 I think there's a couple of things going on there.

1:36:26.380 --> 1:36:29.500
 So I sort of feel like,

1:36:29.500 --> 1:36:31.780
 and I might be too optimistic this way,

1:36:31.780 --> 1:36:34.700
 but I think that there are,

1:36:34.700 --> 1:36:39.020
 we did a little bit about with puns in Jeopardy.

1:36:39.020 --> 1:36:40.700
 We literally sat down and said,

1:36:41.660 --> 1:36:43.180
 how do puns work?

1:36:43.180 --> 1:36:44.820
 And it's like wordplay,

1:36:44.820 --> 1:36:46.140
 and you could formalize these things.

1:36:46.140 --> 1:36:48.260
 So I think there's a lot aspects of humor

1:36:48.260 --> 1:36:50.220
 that you could formalize.

1:36:50.220 --> 1:36:51.620
 You could also learn humor.

1:36:51.620 --> 1:36:53.460
 You could just say, what do people laugh at?

1:36:53.460 --> 1:36:54.860
 And if you have enough, again,

1:36:54.860 --> 1:36:56.860
 if you have enough data to represent the phenomenon,

1:36:56.860 --> 1:36:59.460
 you might be able to weigh the features

1:36:59.460 --> 1:37:01.300
 and figure out what humans find funny

1:37:01.300 --> 1:37:02.700
 and what they don't find funny.

1:37:02.700 --> 1:37:05.140
 The machine might not be able to explain

1:37:05.140 --> 1:37:08.060
 why the human is funny unless we sit back

1:37:08.060 --> 1:37:10.180
 and think about that more formally.

1:37:10.180 --> 1:37:12.420
 I think, again, I think you do a combination of both.

1:37:12.420 --> 1:37:13.900
 And I'm always a big proponent of that.

1:37:13.900 --> 1:37:16.700
 I think robust architectures and approaches

1:37:16.700 --> 1:37:19.620
 are always a little bit combination of us reflecting

1:37:19.620 --> 1:37:22.500
 and being creative about how things are structured,

1:37:22.500 --> 1:37:23.780
 how to formalize them,

1:37:23.780 --> 1:37:26.220
 and then taking advantage of large data and doing learning

1:37:26.220 --> 1:37:29.100
 and figuring out how to combine these two approaches.

1:37:29.100 --> 1:37:31.420
 I think there's another aspect to humor though,

1:37:31.420 --> 1:37:34.340
 which goes to the idea that I feel like I can relate

1:37:34.340 --> 1:37:35.820
 to the person telling the story.

1:37:38.820 --> 1:37:42.140
 And I think that's an interesting theme

1:37:42.140 --> 1:37:43.380
 in the whole AI theme,

1:37:43.380 --> 1:37:47.660
 which is, do I feel differently when I know it's a robot?

1:37:48.460 --> 1:37:52.860
 And when I imagine that the robot is not conscious

1:37:52.860 --> 1:37:54.180
 the way I'm conscious,

1:37:54.180 --> 1:37:56.300
 when I imagine the robot does not actually

1:37:56.300 --> 1:37:58.700
 have the experiences that I experience,

1:37:58.700 --> 1:38:00.980
 do I find it funny?

1:38:00.980 --> 1:38:03.060
 Or do, because it's not as related,

1:38:03.060 --> 1:38:06.540
 I don't imagine that the person's relating it to it

1:38:06.540 --> 1:38:07.860
 the way I relate to it.

1:38:07.860 --> 1:38:11.340
 I think this also, you see this in the arts

1:38:11.340 --> 1:38:14.260
 and in entertainment where,

1:38:14.260 --> 1:38:17.380
 sometimes you have savants who are remarkable at a thing,

1:38:17.380 --> 1:38:19.820
 whether it's sculpture or it's music or whatever,

1:38:19.820 --> 1:38:21.300
 but the people who get the most attention

1:38:21.300 --> 1:38:26.300
 are the people who can evoke a similar emotional response,

1:38:26.660 --> 1:38:30.740
 who can get you to emote, right?

1:38:30.740 --> 1:38:31.940
 About the way they are.

1:38:31.940 --> 1:38:34.460
 In other words, who can basically make the connection

1:38:34.460 --> 1:38:37.020
 from the artifact, from the music or the painting

1:38:37.020 --> 1:38:39.780
 of the sculpture to the emotion

1:38:39.780 --> 1:38:42.380
 and get you to share that emotion with them.

1:38:42.380 --> 1:38:44.700
 And then, and that's when it becomes compelling.

1:38:44.700 --> 1:38:46.980
 So they're communicating at a whole different level.

1:38:46.980 --> 1:38:49.340
 They're just not communicating the artifact.

1:38:49.340 --> 1:38:50.980
 They're communicating their emotional response

1:38:50.980 --> 1:38:51.980
 to the artifact.

1:38:51.980 --> 1:38:53.380
 And then you feel like, oh wow,

1:38:53.380 --> 1:38:55.540
 I can relate to that person, I can connect to that,

1:38:55.540 --> 1:38:57.140
 I can connect to that person.

1:38:57.140 --> 1:39:00.660
 So I think humor has that aspect as well.

1:39:00.660 --> 1:39:04.820
 So the idea that you can connect to that person,

1:39:04.820 --> 1:39:06.380
 person being the critical thing,

1:39:07.260 --> 1:39:12.260
 but we're also able to anthropomorphize objects pretty,

1:39:12.620 --> 1:39:15.180
 robots and AI systems pretty well.

1:39:15.180 --> 1:39:18.740
 So we're almost looking to make them human.

1:39:18.740 --> 1:39:20.820
 So maybe from your experience with Watson,

1:39:20.820 --> 1:39:24.940
 maybe you can comment on, did you consider that as part,

1:39:24.940 --> 1:39:27.020
 well, obviously the problem of jeopardy

1:39:27.020 --> 1:39:30.500
 doesn't require anthropomorphization, but nevertheless.

1:39:30.500 --> 1:39:32.300
 Well, there was some interest in doing that.

1:39:32.300 --> 1:39:35.020
 And that's another thing I didn't want to do

1:39:35.020 --> 1:39:36.220
 because I didn't want to distract

1:39:36.220 --> 1:39:38.740
 from the actual scientific task.

1:39:38.740 --> 1:39:39.620
 But you're absolutely right.

1:39:39.620 --> 1:39:43.380
 I mean, humans do anthropomorphize

1:39:43.380 --> 1:39:45.900
 and without necessarily a lot of work.

1:39:45.900 --> 1:39:47.100
 I mean, you just put some eyes

1:39:47.100 --> 1:39:49.220
 and a couple of eyebrow movements

1:39:49.220 --> 1:39:51.820
 and you're getting humans to react emotionally.

1:39:51.820 --> 1:39:53.540
 And I think you can do that.

1:39:53.540 --> 1:39:56.780
 So I didn't mean to suggest that,

1:39:56.780 --> 1:40:00.620
 that that connection cannot be mimicked.

1:40:00.620 --> 1:40:02.260
 I think that connection can be mimicked

1:40:02.260 --> 1:40:07.260
 and can produce that emotional response.

1:40:07.300 --> 1:40:12.300
 I just wonder though, if you're told what's really going on,

1:40:13.020 --> 1:40:17.180
 if you know that the machine is not conscious,

1:40:17.180 --> 1:40:20.740
 not having the same richness of emotional reactions

1:40:20.740 --> 1:40:21.980
 and understanding that it doesn't really

1:40:21.980 --> 1:40:23.380
 share the understanding,

1:40:23.380 --> 1:40:25.100
 but it's essentially just moving its eyebrow

1:40:25.100 --> 1:40:27.180
 or drooping its eyes or making them bigger,

1:40:27.180 --> 1:40:30.180
 whatever it's doing, just getting the emotional response,

1:40:30.180 --> 1:40:31.580
 will you still feel it?

1:40:31.580 --> 1:40:32.420
 Interesting.

1:40:32.420 --> 1:40:34.380
 I think you probably would for a while.

1:40:34.380 --> 1:40:35.860
 And then when it becomes more important

1:40:35.860 --> 1:40:38.700
 that there's a deeper share of understanding,

1:40:38.700 --> 1:40:40.700
 it may run flat, but I don't know.

1:40:40.700 --> 1:40:45.300
 I'm pretty confident that majority of the world,

1:40:45.300 --> 1:40:47.460
 even if you tell them how it works,

1:40:47.460 --> 1:40:49.100
 well, it will not matter,

1:40:49.100 --> 1:40:54.100
 especially if the machine herself says that she is conscious.

1:40:55.420 --> 1:40:56.260
 That's very possible.

1:40:56.260 --> 1:40:59.420
 So you, the scientist that made the machine is saying

1:41:00.700 --> 1:41:02.860
 that this is how the algorithm works.

1:41:02.860 --> 1:41:04.460
 Everybody will just assume you're lying

1:41:04.460 --> 1:41:06.140
 and that there's a conscious being there.

1:41:06.140 --> 1:41:09.220
 So you're deep into the science fiction genre now,

1:41:09.220 --> 1:41:10.060
 but yeah.

1:41:10.060 --> 1:41:12.020
 I don't think it's, it's actually psychology.

1:41:12.020 --> 1:41:13.780
 I think it's not science fiction.

1:41:13.780 --> 1:41:14.900
 I think it's reality.

1:41:14.900 --> 1:41:16.780
 I think it's a really powerful one

1:41:16.780 --> 1:41:19.980
 that we'll have to be exploring in the next few decades.

1:41:19.980 --> 1:41:20.820
 I agree.

1:41:20.820 --> 1:41:23.540
 It's a very interesting element of intelligence.

1:41:23.540 --> 1:41:25.220
 So what do you think,

1:41:25.220 --> 1:41:28.500
 we've talked about social constructs of intelligences

1:41:28.500 --> 1:41:31.140
 and frameworks and the way humans

1:41:31.140 --> 1:41:33.940
 kind of interpret information.

1:41:33.940 --> 1:41:35.700
 What do you think is a good test of intelligence

1:41:35.700 --> 1:41:36.540
 in your view?

1:41:36.540 --> 1:41:41.300
 So there's the Alan Turing with the Turing test.

1:41:41.300 --> 1:41:44.940
 Watson accomplished something very impressive with Jeopardy.

1:41:44.940 --> 1:41:46.780
 What do you think is a test

1:41:47.820 --> 1:41:49.740
 that would impress the heck out of you

1:41:49.740 --> 1:41:52.980
 that you saw that a computer could do?

1:41:52.980 --> 1:41:57.260
 They would say, this is crossing a kind of threshold

1:41:57.260 --> 1:42:00.660
 that gives me pause in a good way.

1:42:02.620 --> 1:42:06.100
 My expectations for AI are generally high.

1:42:06.100 --> 1:42:07.420
 What does high look like by the way?

1:42:07.420 --> 1:42:10.380
 So not the threshold, test is a threshold.

1:42:10.380 --> 1:42:12.460
 What do you think is the destination?

1:42:12.460 --> 1:42:14.540
 What do you think is the ceiling?

1:42:15.780 --> 1:42:18.460
 I think machines will in many measures

1:42:18.460 --> 1:42:21.660
 will be better than us, will become more effective.

1:42:21.660 --> 1:42:25.140
 In other words, better predictors about a lot of things

1:42:25.140 --> 1:42:28.540
 than ultimately we can do.

1:42:28.540 --> 1:42:30.780
 I think where they're gonna struggle

1:42:30.780 --> 1:42:32.260
 is what we talked about before,

1:42:32.260 --> 1:42:36.540
 which is relating to communicating with

1:42:36.540 --> 1:42:40.580
 and understanding humans in deeper ways.

1:42:40.580 --> 1:42:42.420
 And so I think that's a key point,

1:42:42.420 --> 1:42:44.820
 like we can create the super parrot.

1:42:44.820 --> 1:42:47.660
 What I mean by the super parrot is given enough data,

1:42:47.660 --> 1:42:50.140
 a machine can mimic your emotional response,

1:42:50.140 --> 1:42:52.780
 can even generate language that will sound smart

1:42:52.780 --> 1:42:56.380
 and what someone else might say under similar circumstances.

1:42:57.860 --> 1:42:58.940
 Like I would just pause on that,

1:42:58.940 --> 1:43:01.180
 like that's the super parrot, right?

1:43:01.180 --> 1:43:03.660
 So given similar circumstances,

1:43:03.660 --> 1:43:06.940
 moves its faces in similar ways,

1:43:06.940 --> 1:43:09.420
 changes its tone of voice in similar ways,

1:43:09.420 --> 1:43:12.460
 produces strings of language that would similar

1:43:12.460 --> 1:43:14.300
 that a human might say,

1:43:14.300 --> 1:43:16.740
 not necessarily being able to produce

1:43:16.740 --> 1:43:19.420
 a logical interpretation or understanding

1:43:20.620 --> 1:43:25.260
 that would ultimately satisfy a critical interrogation

1:43:25.260 --> 1:43:26.740
 or a critical understanding.

1:43:27.700 --> 1:43:30.540
 I think you just described me in a nutshell.

1:43:30.540 --> 1:43:34.380
 So I think philosophically speaking,

1:43:34.380 --> 1:43:36.580
 you could argue that that's all we're doing

1:43:36.580 --> 1:43:37.860
 as human beings to work super parrots.

1:43:37.860 --> 1:43:40.300
 So I was gonna say, it's very possible,

1:43:40.300 --> 1:43:42.620
 you know, humans do behave that way too.

1:43:42.620 --> 1:43:45.860
 And so upon deeper probing and deeper interrogation,

1:43:45.860 --> 1:43:48.940
 you may find out that there isn't a shared understanding

1:43:48.940 --> 1:43:50.340
 because I think humans do both.

1:43:50.340 --> 1:43:53.140
 Like humans are statistical language model machines

1:43:54.580 --> 1:43:57.660
 and they are capable reasoners.

1:43:57.660 --> 1:43:59.900
 You know, they're both.

1:43:59.900 --> 1:44:02.900
 And you don't know which is going on, right?

1:44:02.900 --> 1:44:07.900
 So, and I think it's an interesting problem.

1:44:09.140 --> 1:44:11.380
 We talked earlier about like where we are

1:44:11.380 --> 1:44:14.700
 in our social and political landscape.

1:44:14.700 --> 1:44:19.540
 Can you distinguish someone who can string words together

1:44:19.540 --> 1:44:21.820
 and sound like they know what they're talking about

1:44:21.820 --> 1:44:24.020
 from someone who actually does?

1:44:24.020 --> 1:44:25.620
 Can you do that without dialogue,

1:44:25.620 --> 1:44:27.780
 without interrogative or probing dialogue?

1:44:27.780 --> 1:44:31.100
 So it's interesting because humans are really good

1:44:31.100 --> 1:44:34.660
 in their own mind, justifying or explaining what they hear

1:44:34.660 --> 1:44:38.860
 because they project their understanding onto yours.

1:44:38.860 --> 1:44:41.540
 So you could say, you could put together a string of words

1:44:41.540 --> 1:44:44.020
 and someone will sit there and interpret it

1:44:44.020 --> 1:44:46.060
 in a way that's extremely biased

1:44:46.060 --> 1:44:47.140
 to the way they wanna interpret it.

1:44:47.140 --> 1:44:48.460
 They wanna assume that you're an idiot

1:44:48.460 --> 1:44:50.060
 and they'll interpret it one way.

1:44:50.060 --> 1:44:51.380
 They will assume you're a genius

1:44:51.380 --> 1:44:54.380
 and they'll interpret it another way that suits their needs.

1:44:54.380 --> 1:44:56.460
 So this is tricky business.

1:44:56.460 --> 1:44:59.060
 So I think to answer your question,

1:44:59.060 --> 1:45:02.220
 as AI gets better and better, better and better mimic,

1:45:02.220 --> 1:45:03.900
 you recreate the super parrots,

1:45:03.900 --> 1:45:06.580
 we're challenged just as we are with,

1:45:06.580 --> 1:45:08.220
 we're challenged with humans.

1:45:08.220 --> 1:45:10.700
 Do you really know what you're talking about?

1:45:10.700 --> 1:45:14.500
 Do you have a meaningful interpretation,

1:45:14.500 --> 1:45:17.940
 a powerful framework that you could reason over

1:45:17.940 --> 1:45:22.940
 and justify your answers, justify your predictions

1:45:23.420 --> 1:45:25.620
 and your beliefs, why you think they make sense.

1:45:25.620 --> 1:45:28.620
 Can you convince me what the implications are?

1:45:28.620 --> 1:45:33.620
 So can you reason intelligently and make me believe

1:45:34.260 --> 1:45:39.260
 that the implications of your prediction and so forth?

1:45:40.260 --> 1:45:43.060
 So what happens is it becomes reflective.

1:45:44.060 --> 1:45:46.420
 My standard for judging your intelligence

1:45:46.420 --> 1:45:47.700
 depends a lot on mine.

1:45:49.940 --> 1:45:54.380
 But you're saying there should be a large group of people

1:45:54.380 --> 1:45:56.900
 with a certain standard of intelligence

1:45:56.900 --> 1:46:01.640
 that would be convinced by this particular AI system.

1:46:02.580 --> 1:46:03.540
 Then they'll pass.

1:46:03.540 --> 1:46:07.660
 There should be, but I think depending on the content,

1:46:07.660 --> 1:46:09.500
 one of the problems we have there

1:46:09.500 --> 1:46:12.580
 is that if that large community of people

1:46:12.580 --> 1:46:16.620
 are not judging it with regard to a rigorous standard

1:46:16.620 --> 1:46:19.500
 of objective logic and reason, you still have a problem.

1:46:19.500 --> 1:46:23.780
 Like masses of people can be persuaded.

1:46:23.780 --> 1:46:25.020
 The millennials, yeah.

1:46:25.020 --> 1:46:27.660
 To turn their brains off.

1:46:29.020 --> 1:46:29.960
 Right, okay.

1:46:31.980 --> 1:46:32.820
 Sorry.

1:46:32.820 --> 1:46:33.780
 By the way, I have nothing against the millennials.

1:46:33.780 --> 1:46:36.060
 No, I don't, I'm just, just.

1:46:36.060 --> 1:46:40.980
 So you're a part of one of the great benchmarks,

1:46:40.980 --> 1:46:43.280
 challenges of AI history.

1:46:43.280 --> 1:46:47.220
 What do you think about AlphaZero, OpenAI5,

1:46:47.220 --> 1:46:50.740
 AlphaStar accomplishments on video games recently,

1:46:50.740 --> 1:46:55.300
 which are also, I think, at least in the case of Go,

1:46:55.300 --> 1:46:57.180
 with AlphaGo and AlphaZero playing Go,

1:46:57.180 --> 1:46:59.700
 was a monumental accomplishment as well.

1:46:59.700 --> 1:47:01.740
 What are your thoughts about that challenge?

1:47:01.740 --> 1:47:03.460
 I think it was a giant landmark for AI.

1:47:03.460 --> 1:47:04.460
 I think it was phenomenal.

1:47:04.460 --> 1:47:06.020
 I mean, it was one of those other things

1:47:06.020 --> 1:47:08.540
 nobody thought like solving Go was gonna be easy,

1:47:08.540 --> 1:47:10.460
 particularly because it's hard for,

1:47:10.460 --> 1:47:12.700
 particularly hard for humans.

1:47:12.700 --> 1:47:15.540
 Hard for humans to learn, hard for humans to excel at.

1:47:15.540 --> 1:47:20.540
 And so it was another measure, a measure of intelligence.

1:47:21.380 --> 1:47:22.500
 It's very cool.

1:47:22.500 --> 1:47:25.260
 I mean, it's very interesting what they did.

1:47:25.260 --> 1:47:27.940
 And I loved how they solved the data problem,

1:47:27.940 --> 1:47:29.180
 which again, they bootstrapped it

1:47:29.180 --> 1:47:30.420
 and got the machine to play itself,

1:47:30.420 --> 1:47:32.720
 to generate enough data to learn from.

1:47:32.720 --> 1:47:33.860
 I think that was brilliant.

1:47:33.860 --> 1:47:35.660
 I think that was great.

1:47:35.660 --> 1:47:38.900
 And of course, the result speaks for itself.

1:47:38.900 --> 1:47:40.900
 I think it makes us think about,

1:47:40.900 --> 1:47:42.940
 again, it is, okay, what's intelligence?

1:47:42.940 --> 1:47:45.520
 What aspects of intelligence are important?

1:47:45.520 --> 1:47:49.340
 Can the Go machine help me make me a better Go player?

1:47:49.340 --> 1:47:51.660
 Is it an alien intelligence?

1:47:51.660 --> 1:47:53.860
 Am I even capable of,

1:47:53.860 --> 1:47:56.060
 like again, if we put in very simple terms,

1:47:56.060 --> 1:47:59.180
 it found the function, it found the Go function.

1:47:59.180 --> 1:48:00.820
 Can I even comprehend the Go function?

1:48:00.820 --> 1:48:02.260
 Can I talk about the Go function?

1:48:02.260 --> 1:48:03.880
 Can I conceptualize the Go function,

1:48:03.880 --> 1:48:05.500
 like whatever it might be?

1:48:05.500 --> 1:48:08.040
 So one of the interesting ideas of that system

1:48:08.040 --> 1:48:10.060
 is that it plays against itself, right?

1:48:10.060 --> 1:48:12.660
 But there's no human in the loop there.

1:48:12.660 --> 1:48:16.460
 So like you're saying, it could have by itself

1:48:16.460 --> 1:48:18.900
 created an alien intelligence.

1:48:18.900 --> 1:48:19.740
 How?

1:48:19.740 --> 1:48:21.820
 Toward a Go, imagine you're sentencing,

1:48:21.820 --> 1:48:24.700
 you're a judge and you're sentencing people,

1:48:24.700 --> 1:48:26.420
 or you're setting policy,

1:48:26.420 --> 1:48:31.160
 or you're making medical decisions,

1:48:31.160 --> 1:48:33.340
 and you can't explain,

1:48:33.340 --> 1:48:34.880
 you can't get anybody to understand

1:48:34.880 --> 1:48:36.140
 what you're doing or why.

1:48:37.300 --> 1:48:40.700
 So it's an interesting dilemma

1:48:40.700 --> 1:48:43.620
 for the applications of AI.

1:48:43.620 --> 1:48:47.420
 Do we hold AI to this accountability

1:48:47.420 --> 1:48:51.460
 that says humans have to be willing

1:48:51.460 --> 1:48:56.380
 to take responsibility for the decision?

1:48:56.380 --> 1:48:58.780
 In other words, can you explain why you would do the thing?

1:48:58.780 --> 1:49:02.040
 Will you get up and speak to other humans

1:49:02.040 --> 1:49:04.660
 and convince them that this was a smart decision?

1:49:04.660 --> 1:49:07.180
 Is the AI enabling you to do that?

1:49:07.180 --> 1:49:10.220
 Can you get behind the logic that was made there?

1:49:10.220 --> 1:49:13.420
 Do you think, sorry to land on this point,

1:49:13.420 --> 1:49:15.420
 because it's a fascinating one.

1:49:15.420 --> 1:49:17.540
 It's a great goal for AI.

1:49:17.540 --> 1:49:21.460
 Do you think it's achievable in many cases?

1:49:21.460 --> 1:49:23.860
 Or, okay, there's two possible worlds

1:49:23.860 --> 1:49:25.820
 that we have in the future.

1:49:25.820 --> 1:49:28.940
 One is where AI systems do like medical diagnosis

1:49:28.940 --> 1:49:32.420
 or things like that, or drive a car

1:49:32.420 --> 1:49:36.580
 without ever explaining to you why it fails when it does.

1:49:36.580 --> 1:49:40.340
 That's one possible world and we're okay with it.

1:49:40.340 --> 1:49:42.980
 Or the other where we are not okay with it

1:49:42.980 --> 1:49:45.380
 and we really hold back the technology

1:49:45.380 --> 1:49:48.780
 from getting too good before it's able to explain.

1:49:48.780 --> 1:49:50.800
 Which of those worlds are more likely, do you think,

1:49:50.800 --> 1:49:53.500
 and which are concerning to you or not?

1:49:53.500 --> 1:49:56.140
 I think the reality is it's gonna be a mix.

1:49:56.140 --> 1:49:57.460
 I'm not sure I have a problem with that.

1:49:57.460 --> 1:49:59.940
 I mean, I think there are tasks that are perfectly fine

1:49:59.940 --> 1:50:03.980
 with machines show a certain level of performance

1:50:03.980 --> 1:50:07.740
 and that level of performance is already better than humans.

1:50:07.740 --> 1:50:11.260
 So for example, I don't know that I take driverless cars.

1:50:11.260 --> 1:50:14.300
 If driverless cars learn how to be more effective drivers

1:50:14.300 --> 1:50:16.900
 than humans but can't explain what they're doing,

1:50:16.900 --> 1:50:19.020
 but bottom line, statistically speaking,

1:50:19.020 --> 1:50:22.380
 they're 10 times safer than humans,

1:50:22.380 --> 1:50:24.960
 I don't know that I care.

1:50:24.960 --> 1:50:27.540
 I think when we have these edge cases

1:50:27.540 --> 1:50:29.700
 when something bad happens and we wanna decide

1:50:29.700 --> 1:50:32.540
 who's liable for that thing and who made that mistake

1:50:32.540 --> 1:50:33.500
 and what do we do about that?

1:50:33.500 --> 1:50:36.740
 And I think those edge cases are interesting cases.

1:50:36.740 --> 1:50:38.940
 And now do we go to designers of the AI

1:50:38.940 --> 1:50:40.700
 and the AI says, I don't know if that's what it learned

1:50:40.700 --> 1:50:43.620
 to do and it says, well, you didn't train it properly.

1:50:43.620 --> 1:50:46.740
 You were negligent in the training data

1:50:46.740 --> 1:50:47.800
 that you gave that machine.

1:50:47.800 --> 1:50:49.380
 Like, how do we drive down the reliability?

1:50:49.380 --> 1:50:52.080
 So I think those are interesting questions.

1:50:53.180 --> 1:50:55.300
 So the optimization problem there, sorry,

1:50:55.300 --> 1:50:56.900
 is to create an AI system that's able

1:50:56.900 --> 1:50:58.780
 to explain the lawyers away.

1:51:00.100 --> 1:51:01.620
 There you go.

1:51:01.620 --> 1:51:04.040
 I think it's gonna be interesting.

1:51:04.040 --> 1:51:05.820
 I mean, I think this is where technology

1:51:05.820 --> 1:51:09.460
 and social discourse are gonna get like deeply intertwined

1:51:09.460 --> 1:51:12.380
 and how we start thinking about problems, decisions

1:51:12.380 --> 1:51:13.500
 and problems like that.

1:51:13.500 --> 1:51:15.860
 I think in other cases it becomes more obvious

1:51:15.860 --> 1:51:20.260
 where it's like, why did you decide

1:51:20.260 --> 1:51:25.260
 to give that person a longer sentence or deny them parole?

1:51:27.180 --> 1:51:30.580
 Again, policy decisions or why did you pick that treatment?

1:51:30.580 --> 1:51:32.300
 Like that treatment ended up killing that guy.

1:51:32.300 --> 1:51:34.680
 Like, why was that a reasonable choice to make?

1:51:36.940 --> 1:51:40.100
 And people are gonna demand explanations.

1:51:40.100 --> 1:51:41.940
 Now there's a reality though here.

1:51:43.460 --> 1:51:45.960
 And the reality is that it's not,

1:51:45.960 --> 1:51:48.620
 I'm not sure humans are making reasonable choices

1:51:48.620 --> 1:51:49.940
 when they do these things.

1:51:49.940 --> 1:51:54.780
 They are using statistical hunches, biases,

1:51:54.780 --> 1:51:58.520
 or even systematically using statistical averages

1:51:58.520 --> 1:51:59.360
 to make calls.

1:51:59.360 --> 1:52:00.700
 This is what happened to my dad

1:52:00.700 --> 1:52:01.940
 and if you saw the talk I gave about that.

1:52:01.940 --> 1:52:06.940
 But they decided that my father was brain dead.

1:52:07.300 --> 1:52:09.340
 He had went into cardiac arrest

1:52:09.340 --> 1:52:12.420
 and it took a long time for the ambulance to get there

1:52:12.420 --> 1:52:14.580
 and he was not resuscitated right away and so forth.

1:52:14.580 --> 1:52:16.900
 And they came and they told me he was brain dead

1:52:16.900 --> 1:52:17.860
 and why was he brain dead?

1:52:17.860 --> 1:52:19.060
 Because essentially they gave me

1:52:19.060 --> 1:52:22.020
 a purely statistical argument under these conditions

1:52:22.020 --> 1:52:25.340
 with these four features, 98% chance he's brain dead.

1:52:25.340 --> 1:52:28.940
 I said, but can you just tell me not inductively,

1:52:28.940 --> 1:52:30.460
 but deductively go there and tell me

1:52:30.460 --> 1:52:32.820
 his brain's not functioning is the way for you to do that.

1:52:32.820 --> 1:52:35.960
 And the protocol in response was,

1:52:35.960 --> 1:52:37.980
 no, this is how we make this decision.

1:52:37.980 --> 1:52:39.720
 I said, this is inadequate for me.

1:52:39.720 --> 1:52:43.060
 I understand the statistics and I don't know how,

1:52:43.060 --> 1:52:44.740
 there's a 2% chance he's still alive.

1:52:44.740 --> 1:52:46.500
 I just don't know the specifics.

1:52:46.500 --> 1:52:49.380
 I need the specifics of this case

1:52:49.380 --> 1:52:51.420
 and I want the deductive logical argument

1:52:51.420 --> 1:52:53.580
 about why you actually know he's brain dead.

1:52:53.580 --> 1:52:55.980
 So I wouldn't sign the do not resuscitate.

1:52:55.980 --> 1:52:57.820
 And I don't know, it was like they went through

1:52:57.820 --> 1:53:00.020
 lots of procedures, it was a big long story,

1:53:00.020 --> 1:53:02.060
 but the bottom was a fascinating story by the way,

1:53:02.060 --> 1:53:04.340
 but how I reasoned and how the doctors reasoned

1:53:04.340 --> 1:53:05.980
 through this whole process.

1:53:05.980 --> 1:53:07.900
 But I don't know, somewhere around 24 hours later

1:53:07.900 --> 1:53:09.460
 or something, he was sitting up in bed

1:53:09.460 --> 1:53:10.980
 with zero brain damage.

1:53:13.940 --> 1:53:18.020
 I mean, what lessons do you draw from that story,

1:53:18.020 --> 1:53:19.500
 that experience?

1:53:19.500 --> 1:53:22.700
 That the data that's being used

1:53:22.700 --> 1:53:24.100
 to make statistical inferences

1:53:24.100 --> 1:53:26.440
 doesn't adequately reflect the phenomenon.

1:53:26.440 --> 1:53:28.660
 So in other words, you're getting shit wrong,

1:53:28.660 --> 1:53:31.720
 I'm sorry, but you're getting stuff wrong

1:53:31.720 --> 1:53:35.260
 because your model is not robust enough

1:53:35.260 --> 1:53:40.260
 and you might be better off not using statistical inference

1:53:41.320 --> 1:53:43.060
 and statistical averages in certain cases

1:53:43.060 --> 1:53:45.220
 when you know the model's insufficient

1:53:45.220 --> 1:53:48.440
 and that you should be reasoning about the specific case

1:53:48.440 --> 1:53:50.100
 more logically and more deductibly

1:53:51.060 --> 1:53:52.420
 and hold yourself responsible

1:53:52.420 --> 1:53:55.360
 and hold yourself accountable to doing that.

1:53:55.360 --> 1:53:59.380
 And perhaps AI has a role to say the exact thing

1:53:59.380 --> 1:54:02.980
 what you just said, which is perhaps this is a case

1:54:02.980 --> 1:54:05.420
 you should think for yourself,

1:54:05.420 --> 1:54:07.180
 you should reason deductively.

1:54:08.020 --> 1:54:13.020
 Well, so it's hard because it's hard to know that.

1:54:14.780 --> 1:54:17.220
 You'd have to go back and you'd have to have enough data

1:54:17.220 --> 1:54:20.180
 to essentially say, and this goes back to how do we,

1:54:20.180 --> 1:54:22.000
 this goes back to the case of how do we decide

1:54:22.000 --> 1:54:25.380
 whether the AI is good enough to do a particular task

1:54:25.380 --> 1:54:27.280
 and regardless of whether or not

1:54:27.280 --> 1:54:28.620
 it produces an explanation.

1:54:30.980 --> 1:54:34.940
 And what standard do we hold for that?

1:54:34.940 --> 1:54:39.940
 So if you look more broadly, for example,

1:54:42.860 --> 1:54:45.940
 as my father, as a medical case,

1:54:48.180 --> 1:54:50.140
 the medical system ultimately helped him a lot

1:54:50.140 --> 1:54:52.500
 throughout his life, without it,

1:54:52.500 --> 1:54:54.680
 he probably would have died much sooner.

1:54:55.640 --> 1:54:58.900
 So overall, it sort of worked for him

1:54:58.900 --> 1:55:00.760
 in sort of a net, net kind of way.

1:55:02.280 --> 1:55:04.820
 Actually, I don't know that that's fair.

1:55:04.820 --> 1:55:07.660
 But maybe not in that particular case, but overall,

1:55:07.660 --> 1:55:10.580
 like the medical system overall does more good than bad.

1:55:10.580 --> 1:55:12.420
 Yeah, the medical system overall

1:55:12.420 --> 1:55:14.300
 was doing more good than bad.

1:55:14.300 --> 1:55:16.560
 Now, there's another argument that suggests

1:55:16.560 --> 1:55:18.620
 that wasn't the case, but for the sake of argument,

1:55:18.620 --> 1:55:21.060
 let's say like that's, let's say a net positive.

1:55:21.060 --> 1:55:22.380
 And I think you have to sit there and there

1:55:22.380 --> 1:55:24.820
 and take that into consideration.

1:55:24.820 --> 1:55:26.660
 Now you look at a particular use case,

1:55:26.660 --> 1:55:28.900
 like for example, making this decision,

1:55:29.960 --> 1:55:32.300
 have you done enough studies to know

1:55:33.400 --> 1:55:35.660
 how good that prediction really is?

1:55:37.140 --> 1:55:40.060
 And have you done enough studies to compare it,

1:55:40.060 --> 1:55:45.060
 to say, well, what if we dug in in a more direct,

1:55:45.420 --> 1:55:47.980
 let's get the evidence, let's do the deductive thing

1:55:47.980 --> 1:55:49.420
 and not use statistics here,

1:55:49.420 --> 1:55:51.580
 how often would that have done better?

1:55:52.460 --> 1:55:53.700
 So you have to do the studies

1:55:53.700 --> 1:55:56.180
 to know how good the AI actually is.

1:55:56.180 --> 1:55:58.500
 And it's complicated because it depends how fast

1:55:58.500 --> 1:55:59.540
 you have to make the decision.

1:55:59.540 --> 1:56:02.320
 So if you have to make the decision super fast,

1:56:02.320 --> 1:56:03.320
 you have no choice.

1:56:04.620 --> 1:56:06.740
 If you have more time, right?

1:56:06.740 --> 1:56:09.040
 But if you're ready to pull the plug,

1:56:09.040 --> 1:56:11.480
 and this is a lot of the argument that I had with a doctor,

1:56:11.480 --> 1:56:13.220
 I said, what's he gonna do if you do it,

1:56:13.220 --> 1:56:16.340
 what's gonna happen to him in that room if you do it my way?

1:56:16.340 --> 1:56:18.740
 You know, well, he's gonna die anyway.

1:56:18.740 --> 1:56:20.100
 So let's do it my way then.

1:56:20.940 --> 1:56:22.860
 I mean, it raises questions for our society

1:56:22.860 --> 1:56:26.500
 to struggle with, as the case with your father,

1:56:26.500 --> 1:56:28.660
 but also when things like race and gender

1:56:28.660 --> 1:56:31.740
 start coming into play when certain,

1:56:31.740 --> 1:56:35.620
 when judgments are made based on things

1:56:35.620 --> 1:56:39.060
 that are complicated in our society,

1:56:39.060 --> 1:56:40.120
 at least in the discourse.

1:56:40.120 --> 1:56:43.980
 And it starts, you know, I think I'm safe to say

1:56:43.980 --> 1:56:46.300
 that most of the violent crimes committed

1:56:46.300 --> 1:56:51.300
 by males, so if you discriminate based,

1:56:51.300 --> 1:56:53.900
 you know, it's a male versus female saying that

1:56:53.900 --> 1:56:56.380
 if it's a male, more likely to commit the crime.

1:56:56.380 --> 1:57:01.100
 This is one of my very positive and optimistic views

1:57:01.100 --> 1:57:05.540
 of why the study of artificial intelligence,

1:57:05.540 --> 1:57:08.540
 the process of thinking and reasoning logically

1:57:08.540 --> 1:57:10.500
 and statistically, and how to combine them

1:57:10.500 --> 1:57:12.180
 is so important for the discourse today,

1:57:12.180 --> 1:57:17.180
 because it's causing a, regardless of what state AI devices

1:57:17.620 --> 1:57:22.220
 are or not, it's causing this dialogue to happen.

1:57:22.220 --> 1:57:24.820
 This is one of the most important dialogues

1:57:24.820 --> 1:57:28.180
 that in my view, the human species can have right now,

1:57:28.180 --> 1:57:33.180
 which is how to think well, how to reason well,

1:57:33.820 --> 1:57:38.820
 how to understand our own cognitive biases

1:57:39.220 --> 1:57:40.980
 and what to do about them.

1:57:40.980 --> 1:57:43.620
 That has got to be one of the most important things

1:57:43.620 --> 1:57:47.240
 we as a species can be doing, honestly.

1:57:47.240 --> 1:57:51.180
 We are, we've created an incredibly complex society.

1:57:51.180 --> 1:57:56.180
 We've created amazing abilities to amplify noise faster

1:57:56.300 --> 1:57:58.400
 than we can amplify signal.

1:57:59.320 --> 1:58:01.220
 We are challenged.

1:58:01.220 --> 1:58:03.620
 We are deeply, deeply challenged.

1:58:03.620 --> 1:58:06.260
 We have, you know, big segments of the population

1:58:06.260 --> 1:58:08.940
 getting hit with enormous amounts of information.

1:58:08.940 --> 1:58:10.940
 Do they know how to do critical thinking?

1:58:10.940 --> 1:58:14.200
 Do they know how to objectively reason?

1:58:14.200 --> 1:58:16.980
 Do they understand what they are doing,

1:58:16.980 --> 1:58:18.780
 nevermind what their AI is doing?

1:58:19.700 --> 1:58:23.180
 This is such an important dialogue to be having.

1:58:23.180 --> 1:58:26.860
 And, you know, we are fundamentally,

1:58:26.860 --> 1:58:31.420
 our thinking can be and easily becomes fundamentally bias.

1:58:31.420 --> 1:58:34.460
 And there are statistics and we shouldn't blind our,

1:58:34.460 --> 1:58:37.300
 we shouldn't discard statistical inference,

1:58:37.300 --> 1:58:39.020
 but we should understand the nature

1:58:39.020 --> 1:58:40.900
 of statistical inference.

1:58:40.900 --> 1:58:44.020
 As a society, as you know,

1:58:44.020 --> 1:58:48.220
 we decide to reject statistical inference

1:58:48.220 --> 1:58:53.220
 to favor understanding and deciding on the individual.

1:58:55.600 --> 1:58:57.180
 Yes.

1:58:57.180 --> 1:59:00.660
 We consciously make that choice.

1:59:00.660 --> 1:59:03.240
 So even if the statistics said,

1:59:04.140 --> 1:59:08.300
 even if the statistics said males are more likely to have,

1:59:08.300 --> 1:59:09.660
 you know, to be violent criminals,

1:59:09.660 --> 1:59:12.580
 we still take each person as an individual

1:59:12.580 --> 1:59:15.940
 and we treat them based on the logic

1:59:16.820 --> 1:59:20.260
 and the knowledge of that situation.

1:59:20.260 --> 1:59:22.940
 We purposefully and intentionally

1:59:24.100 --> 1:59:27.460
 reject the statistical inference.

1:59:28.320 --> 1:59:31.260
 We do that out of respect for the individual.

1:59:31.260 --> 1:59:32.100
 For the individual.

1:59:32.100 --> 1:59:34.060
 Yeah, and that requires reasoning and thinking.

1:59:34.060 --> 1:59:35.180
 Correct.

1:59:35.180 --> 1:59:37.420
 Looking forward, what grand challenges

1:59:37.420 --> 1:59:38.940
 would you like to see in the future?

1:59:38.940 --> 1:59:43.380
 Because the Jeopardy challenge, you know,

1:59:43.380 --> 1:59:45.140
 captivated the world.

1:59:45.140 --> 1:59:48.060
 AlphaGo, AlphaZero captivated the world.

1:59:48.060 --> 1:59:50.260
 Deep Blue certainly beating Kasparov.

1:59:51.580 --> 1:59:55.700
 Gary's bitterness aside captivated the world.

1:59:55.700 --> 1:59:57.880
 What do you think, do you have ideas

1:59:57.880 --> 2:00:00.900
 for next grand challenges for future challenges of that?

2:00:00.900 --> 2:00:03.280
 You know, look, I mean, I think there are lots

2:00:03.280 --> 2:00:05.800
 of really great ideas for grand challenges.

2:00:05.800 --> 2:00:08.500
 I'm particularly focused on one right now,

2:00:08.500 --> 2:00:11.660
 which is, you know, can you demonstrate

2:00:11.660 --> 2:00:14.980
 that they understand, that they could read and understand,

2:00:14.980 --> 2:00:18.020
 that they can acquire these frameworks

2:00:18.020 --> 2:00:19.420
 and communicate, you know,

2:00:19.420 --> 2:00:21.160
 reason and communicate with humans.

2:00:21.160 --> 2:00:23.380
 So it is kind of like the Turing test,

2:00:23.380 --> 2:00:26.540
 but it's a little bit more demanding than the Turing test.

2:00:26.540 --> 2:00:31.260
 It's not enough to convince me that you might be human

2:00:31.260 --> 2:00:34.920
 because you could, you know, you can parrot a conversation.

2:00:34.920 --> 2:00:38.820
 I think, you know, the standard is a little bit higher,

2:00:38.820 --> 2:00:43.380
 is for example, can you, you know, the standard is higher.

2:00:43.380 --> 2:00:45.540
 And I think one of the challenges

2:00:45.540 --> 2:00:50.540
 of devising this grand challenge is that we're not sure

2:00:51.960 --> 2:00:56.220
 what intelligence is, we're not sure how to determine

2:00:56.220 --> 2:00:59.140
 whether or not two people actually understand each other

2:00:59.140 --> 2:01:02.260
 and in what depth they understand it, you know,

2:01:02.260 --> 2:01:04.340
 to what depth they understand each other.

2:01:04.340 --> 2:01:08.380
 So the challenge becomes something along the lines of,

2:01:08.380 --> 2:01:13.380
 can you satisfy me that we have a shared understanding?

2:01:14.800 --> 2:01:18.380
 So if I were to probe and probe and you probe me,

2:01:18.380 --> 2:01:23.380
 can machines really act like thought partners

2:01:23.420 --> 2:01:27.340
 where they can satisfy me that we have a shared,

2:01:27.340 --> 2:01:29.420
 our understanding is shared enough

2:01:29.420 --> 2:01:33.300
 that we can collaborate and produce answers together

2:01:33.300 --> 2:01:35.460
 and that, you know, they can help me explain

2:01:35.460 --> 2:01:36.740
 and justify those answers.

2:01:36.740 --> 2:01:38.100
 So maybe here's an idea.

2:01:38.100 --> 2:01:43.100
 So we'll have AI system run for president and convince.

2:01:44.500 --> 2:01:46.100
 That's too easy.

2:01:46.100 --> 2:01:46.940
 I'm sorry, go ahead.

2:01:46.940 --> 2:01:49.380
 Well, no, you have to convince the voters

2:01:49.380 --> 2:01:51.580
 that they should vote.

2:01:51.580 --> 2:01:53.780
 So like, I guess what does winning look like?

2:01:53.780 --> 2:01:55.860
 Again, that's why I think this is such a challenge

2:01:55.860 --> 2:01:59.980
 because we go back to the emotional persuasion.

2:01:59.980 --> 2:02:04.980
 We go back to, you know, now we're checking off an aspect

2:02:06.040 --> 2:02:11.040
 of human cognition that is in many ways weak or flawed,

2:02:11.460 --> 2:02:13.940
 right, we're so easily manipulated.

2:02:13.940 --> 2:02:18.940
 Our minds are drawn for often the wrong reasons, right?

2:02:19.620 --> 2:02:21.840
 Not the reasons that ultimately matter to us,

2:02:21.840 --> 2:02:23.980
 but the reasons that can easily persuade us.

2:02:23.980 --> 2:02:28.420
 I think we can be persuaded to believe one thing or another

2:02:28.420 --> 2:02:31.340
 for reasons that ultimately don't serve us well

2:02:31.340 --> 2:02:33.160
 in the longterm.

2:02:33.160 --> 2:02:38.160
 And a good benchmark should not play with those elements

2:02:38.460 --> 2:02:40.780
 of emotional manipulation.

2:02:40.780 --> 2:02:41.620
 I don't think so.

2:02:41.620 --> 2:02:44.340
 And I think that's where we have to set the higher standard

2:02:44.340 --> 2:02:47.100
 for ourselves of what, you know, what does it mean?

2:02:47.100 --> 2:02:48.900
 This goes back to rationality

2:02:48.900 --> 2:02:50.580
 and it goes back to objective thinking.

2:02:50.580 --> 2:02:53.300
 And can you produce, can you acquire information

2:02:53.300 --> 2:02:54.800
 and produce reasoned arguments

2:02:54.800 --> 2:02:56.300
 and to those reasoned arguments

2:02:56.300 --> 2:03:00.140
 pass a certain amount of muster and is it,

2:03:00.140 --> 2:03:02.220
 and can you acquire new knowledge?

2:03:02.220 --> 2:03:06.260
 You know, can you, for example, can you reason,

2:03:06.260 --> 2:03:07.460
 I have acquired new knowledge,

2:03:07.460 --> 2:03:11.220
 can you identify where it's consistent or contradictory

2:03:11.220 --> 2:03:12.860
 with other things you've learned?

2:03:12.860 --> 2:03:14.020
 And can you explain that to me

2:03:14.020 --> 2:03:15.580
 and get me to understand that?

2:03:15.580 --> 2:03:18.540
 So I think another way to think about it perhaps

2:03:18.540 --> 2:03:23.540
 is can a machine teach you, can it help you understand

2:03:31.900 --> 2:03:35.260
 something that you didn't really understand before

2:03:35.260 --> 2:03:39.140
 where it's taking you, so you're not,

2:03:39.140 --> 2:03:41.360
 again, it's almost like can it teach you,

2:03:41.360 --> 2:03:46.360
 can it help you learn and in an arbitrary space

2:03:46.360 --> 2:03:49.000
 so it can open those domain space?

2:03:49.000 --> 2:03:50.440
 So can you tell the machine, and again,

2:03:50.440 --> 2:03:52.840
 this borrows from some science fiction,

2:03:52.840 --> 2:03:55.820
 but can you go off and learn about this topic

2:03:55.820 --> 2:03:58.300
 that I'd like to understand better

2:03:58.300 --> 2:04:00.820
 and then work with me to help me understand it?

2:04:02.000 --> 2:04:03.600
 That's quite brilliant.

2:04:03.600 --> 2:04:06.940
 What, the machine that passes that kind of test,

2:04:06.940 --> 2:04:11.520
 do you think it would need to have self awareness

2:04:11.520 --> 2:04:13.120
 or even consciousness?

2:04:13.120 --> 2:04:16.140
 What do you think about consciousness

2:04:16.140 --> 2:04:21.080
 and the importance of it maybe in relation to having a body,

2:04:21.080 --> 2:04:24.720
 having a presence, an entity?

2:04:24.720 --> 2:04:26.720
 Do you think that's important?

2:04:26.720 --> 2:04:28.740
 You know, people used to ask me if Watson was conscious

2:04:28.740 --> 2:04:32.240
 and I used to say, he's conscious of what exactly?

2:04:32.240 --> 2:04:34.580
 I mean, I think, you know, maybe it depends

2:04:34.580 --> 2:04:36.020
 what it is that you're conscious of.

2:04:36.020 --> 2:04:39.380
 I mean, like, so, you know, did it, if you, you know,

2:04:39.380 --> 2:04:42.400
 it's certainly easy for it to answer questions

2:04:42.400 --> 2:04:44.760
 about, it would be trivial to program it

2:04:44.760 --> 2:04:46.600
 to answer questions about whether or not

2:04:46.600 --> 2:04:47.540
 it was playing Jeopardy.

2:04:47.540 --> 2:04:48.980
 I mean, it could certainly answer questions

2:04:48.980 --> 2:04:51.240
 that would imply that it was aware of things.

2:04:51.240 --> 2:04:52.620
 Exactly, what does it mean to be aware

2:04:52.620 --> 2:04:53.640
 and what does it mean to be conscious of?

2:04:53.640 --> 2:04:54.480
 It's sort of interesting.

2:04:54.480 --> 2:04:57.960
 I mean, I think that we differ from one another

2:04:57.960 --> 2:04:59.800
 based on what we're conscious of.

2:05:01.080 --> 2:05:02.660
 But wait, wait a minute, yes, for sure.

2:05:02.660 --> 2:05:05.320
 There's degrees of consciousness in there, so.

2:05:05.320 --> 2:05:06.920
 Well, and there's just areas.

2:05:06.920 --> 2:05:10.120
 Like, it's not just degrees, what are you aware of?

2:05:10.120 --> 2:05:11.120
 Like, what are you not aware of?

2:05:11.120 --> 2:05:13.440
 But nevertheless, there's a very subjective element

2:05:13.440 --> 2:05:14.700
 to our experience.

2:05:16.000 --> 2:05:18.340
 Let me even not talk about consciousness.

2:05:18.340 --> 2:05:21.560
 Let me talk about another, to me,

2:05:21.560 --> 2:05:25.560
 really interesting topic of mortality, fear of mortality.

2:05:25.560 --> 2:05:29.280
 Watson, as far as I could tell,

2:05:29.280 --> 2:05:30.940
 did not have a fear of death.

2:05:32.160 --> 2:05:33.000
 Certainly not.

2:05:33.000 --> 2:05:35.860
 Most, most humans do.

2:05:36.960 --> 2:05:39.040
 Wasn't conscious of death.

2:05:39.040 --> 2:05:40.000
 It wasn't, yeah.

2:05:40.000 --> 2:05:44.160
 So there's an element of finiteness to our existence

2:05:44.160 --> 2:05:47.240
 that I think, like you mentioned, survival,

2:05:47.240 --> 2:05:49.040
 that adds to the whole thing.

2:05:49.040 --> 2:05:50.860
 I mean, consciousness is tied up with that,

2:05:50.860 --> 2:05:52.880
 that we are a thing.

2:05:52.880 --> 2:05:56.200
 It's a subjective thing that ends.

2:05:56.200 --> 2:05:59.000
 And that seems to add a color and flavor

2:05:59.000 --> 2:06:00.440
 to our motivations in a way

2:06:00.440 --> 2:06:05.440
 that seems to be fundamentally important for intelligence,

2:06:05.960 --> 2:06:07.920
 or at least the kind of human intelligence.

2:06:07.920 --> 2:06:10.200
 Well, I think for generating goals, again,

2:06:10.200 --> 2:06:12.280
 I think you could have,

2:06:12.280 --> 2:06:14.560
 you could have an intelligence capability

2:06:14.560 --> 2:06:18.560
 and a capability to learn, a capability to predict.

2:06:18.560 --> 2:06:20.840
 But I think without,

2:06:22.480 --> 2:06:23.960
 I mean, again, you get fear,

2:06:23.960 --> 2:06:27.040
 but essentially without the goal to survive.

2:06:27.040 --> 2:06:29.120
 So you think you can just encode that

2:06:29.120 --> 2:06:30.600
 without having to really?

2:06:30.600 --> 2:06:31.440
 I think you could encode.

2:06:31.440 --> 2:06:32.880
 I mean, you could create a robot now,

2:06:32.880 --> 2:06:36.060
 and you could say, you know, plug it in,

2:06:36.060 --> 2:06:38.520
 and say, protect your power source, you know,

2:06:38.520 --> 2:06:39.720
 and give it some capabilities,

2:06:39.720 --> 2:06:40.900
 and it'll sit there and operate

2:06:40.900 --> 2:06:42.760
 to try to protect its power source and survive.

2:06:42.760 --> 2:06:44.240
 I mean, so I don't know

2:06:44.240 --> 2:06:46.680
 that that's philosophically a hard thing to demonstrate.

2:06:46.680 --> 2:06:48.960
 It sounds like a fairly easy thing to demonstrate

2:06:48.960 --> 2:06:50.040
 that you can give it that goal.

2:06:50.040 --> 2:06:52.360
 Will it come up with that goal by itself?

2:06:52.360 --> 2:06:54.520
 I think you have to program that goal in.

2:06:54.520 --> 2:06:56.660
 But there's something,

2:06:56.660 --> 2:06:58.580
 because I think, as we touched on,

2:06:58.580 --> 2:07:01.480
 intelligence is kind of like a social construct.

2:07:01.480 --> 2:07:06.480
 The fact that a robot will be protecting its power source

2:07:07.080 --> 2:07:12.080
 would add depth and grounding to its intelligence

2:07:12.960 --> 2:07:15.800
 in terms of us being able to respect it.

2:07:15.800 --> 2:07:18.880
 I mean, ultimately, it boils down to us acknowledging

2:07:18.880 --> 2:07:20.660
 that it's intelligent.

2:07:20.660 --> 2:07:23.480
 And the fact that it can die,

2:07:23.480 --> 2:07:26.120
 I think, is an important part of that.

2:07:26.120 --> 2:07:27.820
 The interesting thing to reflect on

2:07:27.820 --> 2:07:29.520
 is how trivial that would be.

2:07:29.520 --> 2:07:32.080
 And I don't think, if you knew how trivial that was,

2:07:32.080 --> 2:07:35.360
 you would associate that with being intelligence.

2:07:35.360 --> 2:07:37.440
 I mean, I literally put in a statement of code

2:07:37.440 --> 2:07:40.400
 that says you have the following actions you can take.

2:07:40.400 --> 2:07:41.600
 You give it a bunch of actions,

2:07:41.600 --> 2:07:44.000
 like maybe you mount a laser gun on it,

2:07:44.000 --> 2:07:48.920
 or you give it the ability to scream or screech or whatever.

2:07:48.920 --> 2:07:52.680
 And you say, if you see your power source threatened,

2:07:52.680 --> 2:07:53.880
 then you could program that in,

2:07:53.880 --> 2:07:58.040
 and you're gonna take these actions to protect it.

2:07:58.040 --> 2:08:02.200
 You know, you could train it on a bunch of things.

2:08:02.200 --> 2:08:04.080
 So, and now you're gonna look at that and you say,

2:08:04.080 --> 2:08:05.280
 well, you know, that's intelligence,

2:08:05.280 --> 2:08:06.840
 which is protecting its power source?

2:08:06.840 --> 2:08:10.220
 Maybe, but that's, again, this human bias that says,

2:08:10.220 --> 2:08:14.600
 the thing I identify, my intelligence and my conscious,

2:08:14.600 --> 2:08:16.720
 so fundamentally with the desire,

2:08:16.720 --> 2:08:18.680
 or at least the behaviors associated

2:08:18.680 --> 2:08:20.400
 with the desire to survive,

2:08:21.340 --> 2:08:23.860
 that if I see another thing doing that,

2:08:24.720 --> 2:08:27.280
 I'm going to assume it's intelligent.

2:08:27.280 --> 2:08:29.760
 What timeline, year,

2:08:29.760 --> 2:08:33.760
 will society have something that would,

2:08:34.640 --> 2:08:36.000
 that you would be comfortable calling

2:08:36.000 --> 2:08:38.100
 an artificial general intelligence system?

2:08:39.560 --> 2:08:41.080
 Well, what's your intuition?

2:08:41.080 --> 2:08:42.480
 Nobody can predict the future,

2:08:42.480 --> 2:08:46.480
 certainly not the next few months or 20 years away,

2:08:46.480 --> 2:08:47.600
 but what's your intuition?

2:08:47.600 --> 2:08:48.900
 How far away are we?

2:08:50.080 --> 2:08:50.920
 I don't know.

2:08:50.920 --> 2:08:52.120
 It's hard to make these predictions.

2:08:52.120 --> 2:08:54.760
 I mean, I would be guessing,

2:08:54.760 --> 2:08:57.000
 and there's so many different variables,

2:08:57.000 --> 2:08:59.080
 including just how much we want to invest in it

2:08:59.080 --> 2:09:02.100
 and how important we think it is,

2:09:03.480 --> 2:09:06.160
 what kind of investment we're willing to make in it,

2:09:06.160 --> 2:09:08.440
 what kind of talent we end up bringing to the table,

2:09:08.440 --> 2:09:10.160
 the incentive structure, all these things.

2:09:10.160 --> 2:09:15.160
 So I think it is possible to do this sort of thing.

2:09:15.220 --> 2:09:19.120
 I think it's, I think trying to sort of

2:09:20.360 --> 2:09:23.040
 ignore many of the variables and things like that,

2:09:23.040 --> 2:09:25.440
 is it a 10 year thing, is it a 23 year?

2:09:25.440 --> 2:09:27.880
 Probably closer to a 20 year thing, I guess.

2:09:27.880 --> 2:09:29.720
 But not several hundred years.

2:09:29.720 --> 2:09:32.080
 No, I don't think it's several hundred years.

2:09:32.080 --> 2:09:33.660
 I don't think it's several hundred years.

2:09:33.660 --> 2:09:38.660
 But again, so much depends on how committed we are

2:09:38.860 --> 2:09:43.120
 to investing and incentivizing this type of work.

2:09:43.120 --> 2:09:45.200
 And it's sort of interesting.

2:09:45.200 --> 2:09:50.200
 Like, I don't think it's obvious how incentivized we are.

2:09:50.280 --> 2:09:53.160
 I think from a task perspective,

2:09:53.160 --> 2:09:57.880
 if we see business opportunities to take this technique

2:09:57.880 --> 2:09:59.120
 or that technique to solve that problem,

2:09:59.120 --> 2:10:03.240
 I think that's the main driver for many of these things.

2:10:03.240 --> 2:10:05.600
 From a general intelligence,

2:10:05.600 --> 2:10:06.920
 it's kind of an interesting question.

2:10:06.920 --> 2:10:09.360
 Are we really motivated to do that?

2:10:09.360 --> 2:10:12.520
 And like, we just struggled ourselves right now

2:10:12.520 --> 2:10:14.760
 to even define what it is.

2:10:14.760 --> 2:10:16.160
 So it's hard to incentivize

2:10:16.160 --> 2:10:17.260
 when we don't even know what it is

2:10:17.260 --> 2:10:18.800
 we're incentivized to create.

2:10:18.800 --> 2:10:23.280
 And if you said mimic a human intelligence,

2:10:23.280 --> 2:10:25.520
 I just think there are so many challenges

2:10:25.520 --> 2:10:27.720
 with the significance and meaning of that.

2:10:27.720 --> 2:10:29.640
 That there's not a clear directive.

2:10:29.640 --> 2:10:32.280
 There's no clear directive to do precisely that thing.

2:10:32.280 --> 2:10:36.480
 So assistance in a larger and larger number of tasks.

2:10:36.480 --> 2:10:38.080
 So being able to,

2:10:38.080 --> 2:10:41.080
 a system that's particularly able to operate my microwave

2:10:41.080 --> 2:10:42.600
 and making a grilled cheese sandwich.

2:10:42.600 --> 2:10:44.960
 I don't even know how to make one of those.

2:10:44.960 --> 2:10:48.020
 And then the same system will be doing the vacuum cleaning.

2:10:48.020 --> 2:10:51.640
 And then the same system would be teaching

2:10:53.540 --> 2:10:56.280
 my kids that I don't have math.

2:10:56.280 --> 2:11:00.720
 I think that when you get into a general intelligence

2:11:00.720 --> 2:11:04.240
 for learning physical tasks,

2:11:04.240 --> 2:11:06.000
 and again, I wanna go back to your body question

2:11:06.000 --> 2:11:07.260
 because I think your body question was interesting,

2:11:07.260 --> 2:11:11.080
 but you wanna go back to learning the abilities

2:11:11.080 --> 2:11:11.920
 to physical tasks.

2:11:11.920 --> 2:11:14.420
 You might have, we might get,

2:11:14.420 --> 2:11:16.020
 I imagine in that timeframe,

2:11:16.020 --> 2:11:18.440
 we will get better and better at learning these kinds

2:11:18.440 --> 2:11:20.320
 of tasks, whether it's mowing your lawn

2:11:20.320 --> 2:11:22.720
 or driving a car or whatever it is.

2:11:22.720 --> 2:11:24.420
 I think we will get better and better at that

2:11:24.420 --> 2:11:25.840
 where it's learning how to make predictions

2:11:25.840 --> 2:11:27.000
 over large bodies of data.

2:11:27.000 --> 2:11:28.280
 I think we're gonna continue to get better

2:11:28.280 --> 2:11:29.280
 and better at that.

2:11:30.520 --> 2:11:33.520
 And machines will outpace humans

2:11:33.520 --> 2:11:35.560
 in a variety of those things.

2:11:35.560 --> 2:11:40.560
 The underlying mechanisms for doing that may be the same,

2:11:40.760 --> 2:11:43.680
 meaning that maybe these are deep nets,

2:11:43.680 --> 2:11:46.280
 there's infrastructure to train them,

2:11:46.280 --> 2:11:49.840
 reusable components to get them to do different classes

2:11:49.840 --> 2:11:51.920
 of tasks, and we get better and better

2:11:51.920 --> 2:11:53.980
 at building these kinds of machines.

2:11:53.980 --> 2:11:56.600
 You could argue that the general learning infrastructure

2:11:56.600 --> 2:12:01.040
 in there is a form of a general type of intelligence.

2:12:01.040 --> 2:12:04.720
 I think what starts getting harder is this notion of,

2:12:06.400 --> 2:12:09.120
 can we effectively communicate and understand

2:12:09.120 --> 2:12:10.840
 and build that shared understanding?

2:12:10.840 --> 2:12:13.200
 Because of the layers of interpretation that are required

2:12:13.200 --> 2:12:16.320
 to do that, and the need for the machine to be engaged

2:12:16.320 --> 2:12:20.320
 with humans at that level in a continuous basis.

2:12:20.320 --> 2:12:23.480
 So how do you get the machine in the game?

2:12:23.480 --> 2:12:26.600
 How do you get the machine in the intellectual game?

2:12:26.600 --> 2:12:29.120
 Yeah, and to solve AGI,

2:12:29.120 --> 2:12:31.000
 you probably have to solve that problem.

2:12:31.000 --> 2:12:31.920
 You have to get the machine,

2:12:31.920 --> 2:12:33.800
 so it's a little bit of a bootstrapping thing.

2:12:33.800 --> 2:12:38.800
 Can we get the machine engaged in the intellectual game,

2:12:39.160 --> 2:12:42.360
 but in the intellectual dialogue with the humans?

2:12:42.360 --> 2:12:44.840
 Are the humans sufficiently in intellectual dialogue

2:12:44.840 --> 2:12:49.640
 with each other to generate enough data in this context?

2:12:49.640 --> 2:12:51.020
 And how do you bootstrap that?

2:12:51.020 --> 2:12:54.080
 Because every one of those conversations,

2:12:54.080 --> 2:12:55.760
 every one of those conversations,

2:12:55.760 --> 2:12:58.040
 those intelligent interactions,

2:12:58.040 --> 2:12:59.680
 require so much prior knowledge

2:12:59.680 --> 2:13:01.640
 that it's a challenge to bootstrap it.

2:13:01.640 --> 2:13:05.840
 So the question is, and how committed?

2:13:05.840 --> 2:13:08.800
 So I think that's possible, but when I go back to,

2:13:08.800 --> 2:13:10.880
 are we incentivized to do that?

2:13:10.880 --> 2:13:13.160
 I know we're incentivized to do the former.

2:13:13.160 --> 2:13:15.900
 Are we incentivized to do the latter significantly enough?

2:13:15.900 --> 2:13:18.460
 Do people understand what the latter really is well enough?

2:13:18.460 --> 2:13:20.880
 Part of the elemental cognition mission

2:13:20.880 --> 2:13:23.520
 is to try to articulate that better and better

2:13:23.520 --> 2:13:24.560
 through demonstrations

2:13:24.560 --> 2:13:26.960
 and through trying to craft these grand challenges

2:13:26.960 --> 2:13:28.120
 and get people to say, look,

2:13:28.120 --> 2:13:30.440
 this is a class of intelligence.

2:13:30.440 --> 2:13:31.840
 This is a class of AI.

2:13:31.840 --> 2:13:33.420
 Do we want this?

2:13:33.420 --> 2:13:35.800
 What is the potential of this?

2:13:35.800 --> 2:13:37.840
 What's the business potential?

2:13:37.840 --> 2:13:40.120
 What's the societal potential to that?

2:13:40.120 --> 2:13:45.080
 And to build up that incentive system around that.

2:13:45.080 --> 2:13:46.820
 Yeah, I think if people don't understand yet,

2:13:46.820 --> 2:13:47.660
 I think they will.

2:13:47.660 --> 2:13:49.620
 I think there's a huge business potential here.

2:13:49.620 --> 2:13:52.020
 So it's exciting that you're working on it.

2:13:54.000 --> 2:13:54.960
 We kind of skipped over,

2:13:54.960 --> 2:13:59.560
 but I'm a huge fan of physical presence of things.

2:13:59.560 --> 2:14:03.320
 Do you think Watson had a body?

2:14:03.320 --> 2:14:08.320
 Do you think having a body adds to the interactive element

2:14:08.320 --> 2:14:11.640
 between the AI system and a human,

2:14:11.640 --> 2:14:13.460
 or just in general to intelligence?

2:14:14.600 --> 2:14:19.600
 So I think going back to that shared understanding bit,

2:14:19.780 --> 2:14:21.640
 humans are very connected to their bodies.

2:14:21.640 --> 2:14:26.320
 I mean, one of the challenges in getting an AI

2:14:26.320 --> 2:14:29.120
 to kind of be a compatible human intelligence

2:14:29.120 --> 2:14:33.660
 is that our physical bodies are generating a lot of features

2:14:33.660 --> 2:14:37.720
 that make up the input.

2:14:37.720 --> 2:14:40.800
 So in other words, our bodies are the tool

2:14:40.800 --> 2:14:42.720
 we use to affect output,

2:14:42.720 --> 2:14:46.360
 but they also generate a lot of input for our brains.

2:14:46.360 --> 2:14:49.800
 So we generate emotion, we generate all these feelings,

2:14:49.800 --> 2:14:52.720
 we generate all these signals that machines don't have.

2:14:52.720 --> 2:14:54.960
 So machines don't have this as the input data

2:14:56.800 --> 2:14:58.720
 and they don't have the feedback that says,

2:14:58.720 --> 2:15:02.940
 I've gotten this emotion or I've gotten this idea,

2:15:02.940 --> 2:15:04.320
 I now want to process it,

2:15:04.320 --> 2:15:08.960
 and then it then affects me as a physical being,

2:15:08.960 --> 2:15:12.200
 and I can play that out.

2:15:12.200 --> 2:15:14.120
 In other words, I could realize the implications of that,

2:15:14.120 --> 2:15:17.520
 implications again, on my mind body complex,

2:15:17.520 --> 2:15:19.960
 I then process that, and the implications again,

2:15:19.960 --> 2:15:23.620
 our internal features are generated, I learn from them,

2:15:23.620 --> 2:15:26.760
 they have an effect on my mind body complex.

2:15:26.760 --> 2:15:28.900
 So it's interesting when we think,

2:15:28.900 --> 2:15:30.440
 do we want a human intelligence?

2:15:30.440 --> 2:15:33.200
 Well, if we want a human compatible intelligence,

2:15:33.200 --> 2:15:34.320
 probably the best thing to do

2:15:34.320 --> 2:15:36.840
 is to embed it in a human body.

2:15:36.840 --> 2:15:39.960
 Just to clarify, and both concepts are beautiful,

2:15:39.960 --> 2:15:44.960
 is humanoid robots, so robots that look like humans is one,

2:15:45.440 --> 2:15:50.440
 or did you mean actually sort of what Elon Musk

2:15:50.720 --> 2:15:52.940
 was working with Neuralink,

2:15:52.940 --> 2:15:55.840
 really embedding intelligence systems

2:15:55.840 --> 2:15:59.800
 to ride along human bodies?

2:15:59.800 --> 2:16:01.840
 No, I mean riding along is different.

2:16:01.840 --> 2:16:05.840
 I meant like if you want to create an intelligence

2:16:05.840 --> 2:16:08.720
 that is human compatible,

2:16:08.720 --> 2:16:10.840
 meaning that it can learn and develop

2:16:10.840 --> 2:16:13.040
 a shared understanding of the world around it,

2:16:13.040 --> 2:16:15.120
 you have to give it a lot of the same substrate.

2:16:15.120 --> 2:16:18.200
 Part of that substrate is the idea

2:16:18.200 --> 2:16:21.120
 that it generates these kinds of internal features,

2:16:21.120 --> 2:16:24.020
 like sort of emotional stuff, it has similar senses,

2:16:24.020 --> 2:16:25.640
 it has to do a lot of the same things

2:16:25.640 --> 2:16:28.200
 with those same senses, right?

2:16:28.200 --> 2:16:29.760
 So I think if you want that,

2:16:29.760 --> 2:16:32.520
 again, I don't know that you want that.

2:16:32.520 --> 2:16:34.280
 That's not my specific goal,

2:16:34.280 --> 2:16:35.860
 I think that's a fascinating scientific goal,

2:16:35.860 --> 2:16:37.860
 I think it has all kinds of other implications.

2:16:37.860 --> 2:16:39.480
 That's sort of not the goal.

2:16:39.480 --> 2:16:41.600
 I want to create, I think of it

2:16:41.600 --> 2:16:44.120
 as I create intellectual thought partners for humans,

2:16:44.120 --> 2:16:46.300
 so that kind of intelligence.

2:16:47.640 --> 2:16:48.560
 I know there are other companies

2:16:48.560 --> 2:16:50.160
 that are creating physical thought partners,

2:16:50.160 --> 2:16:52.460
 physical partners for humans,

2:16:52.460 --> 2:16:56.420
 but that's kind of not where I'm at.

2:16:56.420 --> 2:17:00.760
 But the important point is that a big part

2:17:00.760 --> 2:17:05.760
 of what we process is that physical experience

2:17:06.240 --> 2:17:08.080
 of the world around us.

2:17:08.080 --> 2:17:10.520
 On the point of thought partners,

2:17:10.520 --> 2:17:13.920
 what role does an emotional connection,

2:17:13.920 --> 2:17:17.820
 or forgive me, love, have to play

2:17:17.820 --> 2:17:19.840
 in that thought partnership?

2:17:19.840 --> 2:17:22.000
 Is that something you're interested in,

2:17:22.000 --> 2:17:26.700
 put another way, sort of having a deep connection,

2:17:26.700 --> 2:17:29.300
 beyond intellectual?

2:17:29.300 --> 2:17:30.200
 With the AI?

2:17:30.200 --> 2:17:32.740
 Yeah, with the AI, between human and AI.

2:17:32.740 --> 2:17:34.440
 Is that something that gets in the way

2:17:34.440 --> 2:17:37.560
 of the rational discourse?

2:17:37.560 --> 2:17:39.240
 Is that something that's useful?

2:17:39.240 --> 2:17:41.920
 I worry about biases, obviously.

2:17:41.920 --> 2:17:44.280
 So in other words, if you develop an emotional relationship

2:17:44.280 --> 2:17:46.640
 with a machine, all of a sudden you start,

2:17:46.640 --> 2:17:48.320
 are more likely to believe what it's saying,

2:17:48.320 --> 2:17:50.240
 even if it doesn't make any sense.

2:17:50.240 --> 2:17:53.640
 So I worry about that.

2:17:53.640 --> 2:17:54.600
 But at the same time,

2:17:54.600 --> 2:17:56.560
 I think the opportunity to use machines

2:17:56.560 --> 2:17:59.440
 to provide human companionship is actually not crazy.

2:17:59.440 --> 2:18:04.060
 And intellectual and social companionship

2:18:04.060 --> 2:18:05.240
 is not a crazy idea.

2:18:06.320 --> 2:18:09.960
 Do you have concerns, as a few people do,

2:18:09.960 --> 2:18:11.760
 Elon Musk, Sam Harris,

2:18:11.760 --> 2:18:14.600
 about long term existential threats of AI,

2:18:15.460 --> 2:18:18.760
 and perhaps short term threats of AI?

2:18:18.760 --> 2:18:19.800
 We talked about bias,

2:18:19.800 --> 2:18:21.120
 we talked about different misuses,

2:18:21.120 --> 2:18:25.680
 but do you have concerns about thought partners,

2:18:25.680 --> 2:18:28.600
 systems that are able to help us make decisions

2:18:28.600 --> 2:18:29.780
 together as humans,

2:18:29.780 --> 2:18:31.960
 somehow having a significant negative impact

2:18:31.960 --> 2:18:33.760
 on society in the long term?

2:18:33.760 --> 2:18:35.340
 I think there are things to worry about.

2:18:35.340 --> 2:18:40.340
 I think giving machines too much leverage is a problem.

2:18:41.500 --> 2:18:43.240
 And what I mean by leverage is,

2:18:44.320 --> 2:18:47.040
 is too much control over things that can hurt us,

2:18:47.040 --> 2:18:50.240
 whether it's socially, psychologically, intellectually,

2:18:50.240 --> 2:18:51.640
 or physically.

2:18:51.640 --> 2:18:53.480
 And if you give the machines too much control,

2:18:53.480 --> 2:18:54.760
 I think that's a concern.

2:18:54.760 --> 2:18:56.320
 You forget about the AI,

2:18:56.320 --> 2:18:58.640
 just once you give them too much control,

2:18:58.640 --> 2:19:02.660
 human bad actors can hack them and produce havoc.

2:19:04.760 --> 2:19:07.000
 So that's a problem.

2:19:07.000 --> 2:19:10.040
 And you'd imagine hackers taking over

2:19:10.040 --> 2:19:11.080
 the driverless car network

2:19:11.080 --> 2:19:15.220
 and creating all kinds of havoc.

2:19:15.220 --> 2:19:19.760
 But you could also imagine given the ease

2:19:19.760 --> 2:19:22.800
 at which humans could be persuaded one way or the other,

2:19:22.800 --> 2:19:25.840
 and now we have algorithms that can easily take control

2:19:25.840 --> 2:19:29.640
 over that and amplify noise

2:19:29.640 --> 2:19:32.000
 and move people one direction or another.

2:19:32.000 --> 2:19:34.140
 I mean, humans do that to other humans all the time.

2:19:34.140 --> 2:19:35.420
 And we have marketing campaigns,

2:19:35.420 --> 2:19:38.220
 we have political campaigns that take advantage

2:19:38.220 --> 2:19:41.960
 of our emotions or our fears.

2:19:41.960 --> 2:19:44.160
 And this is done all the time.

2:19:44.160 --> 2:19:47.760
 But with machines, machines are like giant megaphones.

2:19:47.760 --> 2:19:50.680
 We can amplify this in orders of magnitude

2:19:50.680 --> 2:19:54.840
 and fine tune its control so we can tailor the message.

2:19:54.840 --> 2:19:58.640
 We can now very rapidly and efficiently tailor the message

2:19:58.640 --> 2:20:03.640
 to the audience, taking advantage of their biases

2:20:04.200 --> 2:20:06.640
 and amplifying them and using them to persuade them

2:20:06.640 --> 2:20:10.740
 in one direction or another in ways that are not fair,

2:20:10.740 --> 2:20:13.440
 not logical, not objective, not meaningful.

2:20:13.440 --> 2:20:17.000
 And humans, machines empower that.

2:20:17.000 --> 2:20:18.920
 So that's what I mean by leverage.

2:20:18.920 --> 2:20:22.840
 Like it's not new, but wow, it's powerful

2:20:22.840 --> 2:20:24.400
 because machines can do it more effectively,

2:20:24.400 --> 2:20:27.720
 more quickly and we see that already going on

2:20:27.720 --> 2:20:30.480
 in social media and other places.

2:20:31.720 --> 2:20:33.100
 That's scary.

2:20:33.100 --> 2:20:38.100
 And that's why I go back to saying one of the most important

2:20:38.100 --> 2:20:42.860
 That's why I go back to saying one of the most important

2:20:42.860 --> 2:20:45.420
 public dialogues we could be having

2:20:45.420 --> 2:20:47.980
 is about the nature of intelligence

2:20:47.980 --> 2:20:52.140
 and the nature of inference and logic

2:20:52.140 --> 2:20:56.100
 and reason and rationality and us understanding

2:20:56.100 --> 2:20:59.820
 our own biases, us understanding our own cognitive biases

2:20:59.820 --> 2:21:03.140
 and how they work and then how machines work

2:21:03.140 --> 2:21:06.020
 and how do we use them to compliment basically

2:21:06.020 --> 2:21:09.660
 so that in the end we have a stronger overall system.

2:21:09.660 --> 2:21:11.400
 That's just incredibly important.

2:21:13.020 --> 2:21:15.780
 I don't think most people understand that.

2:21:15.780 --> 2:21:19.780
 So like telling your kids or telling your students,

2:21:20.620 --> 2:21:22.540
 this goes back to the cognition.

2:21:22.540 --> 2:21:24.300
 Here's how your brain works.

2:21:24.300 --> 2:21:28.060
 Here's how easy it is to trick your brain, right?

2:21:28.060 --> 2:21:29.460
 There are fundamental cognitive,

2:21:29.460 --> 2:21:34.060
 you should appreciate the different types of thinking

2:21:34.060 --> 2:21:36.820
 and how they work and what you're prone to

2:21:36.820 --> 2:21:40.580
 and what do you prefer?

2:21:40.580 --> 2:21:42.340
 And under what conditions does this make sense

2:21:42.340 --> 2:21:43.620
 versus does that make sense?

2:21:43.620 --> 2:21:46.340
 And then say, here's what AI can do.

2:21:46.340 --> 2:21:48.620
 Here's how it can make this worse

2:21:48.620 --> 2:21:51.020
 and here's how it can make this better.

2:21:51.020 --> 2:21:52.740
 And then that's where the AI has a role

2:21:52.740 --> 2:21:55.620
 is to reveal that trade off.

2:21:56.620 --> 2:22:00.700
 So if you imagine a system that is able to

2:22:00.700 --> 2:22:05.700
 beyond any definition of the Turing test to the benchmark,

2:22:06.960 --> 2:22:10.240
 really an AGI system as a thought partner

2:22:10.240 --> 2:22:12.980
 that you one day will create,

2:22:14.320 --> 2:22:19.320
 what question, what topic of discussion,

2:22:19.520 --> 2:22:23.960
 if you get to pick one, would you have with that system?

2:22:23.960 --> 2:22:28.240
 What would you ask and you get to find out

2:22:28.240 --> 2:22:30.840
 the truth together?

2:22:33.440 --> 2:22:36.200
 So you threw me a little bit with finding the truth

2:22:36.200 --> 2:22:41.040
 at the end, but because the truth is a whole nother topic.

2:22:41.040 --> 2:22:43.600
 But I think the beauty of it,

2:22:43.600 --> 2:22:46.060
 I think what excites me is the beauty of it is

2:22:46.060 --> 2:22:48.700
 if I really have that system, I don't have to pick.

2:22:48.700 --> 2:22:51.600
 So in other words, I can go to and say,

2:22:51.600 --> 2:22:54.060
 this is what I care about today.

2:22:54.060 --> 2:22:57.180
 And that's what we mean by like this general capability,

2:22:57.180 --> 2:23:00.560
 go out, read this stuff in the next three milliseconds.

2:23:00.560 --> 2:23:02.800
 And I wanna talk to you about it.

2:23:02.800 --> 2:23:05.000
 I wanna draw analogies, I wanna understand

2:23:05.000 --> 2:23:08.080
 how this affects this decision or that decision.

2:23:08.080 --> 2:23:09.200
 What if this were true?

2:23:09.200 --> 2:23:10.720
 What if that were true?

2:23:10.720 --> 2:23:13.200
 What knowledge should I be aware of

2:23:13.200 --> 2:23:16.000
 that could impact my decision?

2:23:16.000 --> 2:23:18.960
 Here's what I'm thinking is the main implication.

2:23:18.960 --> 2:23:21.120
 Can you prove that out?

2:23:21.120 --> 2:23:23.280
 Can you give me the evidence that supports that?

2:23:23.280 --> 2:23:25.600
 Can you give me evidence that supports this other thing?

2:23:25.600 --> 2:23:27.400
 Boy, would that be incredible?

2:23:27.400 --> 2:23:28.560
 Would that be just incredible?

2:23:28.560 --> 2:23:30.400
 Just a long discourse.

2:23:30.400 --> 2:23:33.340
 Just to be part of whether it's a medical diagnosis

2:23:33.340 --> 2:23:35.880
 or whether it's the various treatment options

2:23:35.880 --> 2:23:38.400
 or whether it's a legal case

2:23:38.400 --> 2:23:40.040
 or whether it's a social problem

2:23:40.040 --> 2:23:41.040
 that people are discussing,

2:23:41.040 --> 2:23:43.740
 like be part of the dialogue,

2:23:43.740 --> 2:23:48.740
 one that holds itself and us accountable

2:23:49.520 --> 2:23:51.560
 to reasons and objective dialogue.

2:23:52.760 --> 2:23:54.520
 I get goosebumps talking about it, right?

2:23:54.520 --> 2:23:56.180
 It's like, this is what I want.

2:23:57.440 --> 2:24:01.000
 So when you create it, please come back on the podcast

2:24:01.000 --> 2:24:03.480
 and we can have a discussion together

2:24:03.480 --> 2:24:04.800
 and make it even longer.

2:24:04.800 --> 2:24:07.320
 This is a record for the longest conversation

2:24:07.320 --> 2:24:08.160
 in the world.

2:24:08.160 --> 2:24:09.400
 It was an honor, it was a pleasure, David.

2:24:09.400 --> 2:24:10.240
 Thank you so much for talking to me.

2:24:10.240 --> 2:24:30.240
 Thanks so much, a lot of fun.

