WEBVTT

00:00.000 --> 00:03.000
 The following is a conversation with Russ Tedrick,

00:03.000 --> 00:05.560
 a roboticist and professor at MIT

00:05.560 --> 00:07.880
 and vice president of robotics research

00:07.880 --> 00:11.240
 at Toyota Research Institute or TRI.

00:11.240 --> 00:15.160
 He works on control of robots in interesting,

00:15.160 --> 00:18.000
 complicated, underactuated, stochastic,

00:18.000 --> 00:19.960
 difficult to model situations.

00:19.960 --> 00:22.640
 He's a great teacher and a great person,

00:22.640 --> 00:25.040
 one of my favorites at MIT.

00:25.040 --> 00:28.280
 We'll get into a lot of topics in this conversation

00:28.280 --> 00:32.760
 from his time leading MIT's Delta Robotics Challenge team

00:32.760 --> 00:35.400
 to the awesome fact that he often runs

00:35.400 --> 00:40.400
 close to a marathon a day to and from work barefoot.

00:40.480 --> 00:43.400
 For a world class roboticist interested in elegant,

00:43.400 --> 00:46.920
 efficient control of underactuated dynamical systems

00:46.920 --> 00:50.840
 like the human body, this fact makes Russ

00:50.840 --> 00:53.180
 one of the most fascinating people I know.

00:54.480 --> 00:55.780
 Quick summary of the ads.

00:55.780 --> 00:59.220
 Three sponsors, Magic Spoon Cereal, BetterHelp,

00:59.220 --> 01:00.760
 and ExpressVPN.

01:00.760 --> 01:02.620
 Please consider supporting this podcast

01:02.620 --> 01:05.680
 by going to magicspoon.com slash lex

01:05.680 --> 01:07.960
 and using code lex at checkout,

01:07.960 --> 01:10.480
 going to betterhelp.com slash lex

01:10.480 --> 01:14.640
 and signing up at expressvpn.com slash lexpod.

01:14.640 --> 01:16.480
 Click the links in the description,

01:16.480 --> 01:18.800
 buy the stuff, get the discount.

01:18.800 --> 01:21.800
 It really is the best way to support this podcast.

01:21.800 --> 01:24.000
 If you enjoy this thing, subscribe on YouTube,

01:24.000 --> 01:26.240
 review it with five stars on Apple Podcast,

01:26.240 --> 01:28.280
 support it on Patreon, or connect with me

01:28.280 --> 01:31.280
 on Twitter at lexfreedman.

01:31.280 --> 01:33.640
 As usual, I'll do a few minutes of ads now

01:33.640 --> 01:34.880
 and never any ads in the middle

01:34.880 --> 01:37.880
 that can break the flow of the conversation.

01:37.880 --> 01:40.880
 This episode is supported by Magic Spoon,

01:40.880 --> 01:43.460
 low carb keto friendly cereal.

01:43.460 --> 01:45.800
 I've been on a mix of keto or carnivore diet

01:45.800 --> 01:47.320
 for a very long time now.

01:47.320 --> 01:50.520
 That means eating very little carbs.

01:50.520 --> 01:52.200
 I used to love cereal.

01:52.200 --> 01:54.960
 Obviously, most have crazy amounts of sugar,

01:54.960 --> 01:58.000
 which is terrible for you, so I quit years ago,

01:58.000 --> 02:00.420
 but Magic Spoon is a totally new thing.

02:00.420 --> 02:03.000
 Zero sugar, 11 grams of protein,

02:03.000 --> 02:05.720
 and only three net grams of carbs.

02:05.720 --> 02:07.240
 It tastes delicious.

02:07.240 --> 02:09.660
 It has a bunch of flavors, they're all good,

02:09.660 --> 02:11.200
 but if you know what's good for you,

02:11.200 --> 02:13.940
 you'll go with cocoa, my favorite flavor

02:13.940 --> 02:15.820
 and the flavor of champions.

02:15.820 --> 02:19.460
 Click the magicspoon.com slash lex link in the description,

02:19.460 --> 02:22.160
 use code lex at checkout to get the discount

02:22.160 --> 02:24.400
 and to let them know I sent you.

02:24.400 --> 02:26.680
 So buy all of their cereal.

02:26.680 --> 02:28.640
 It's delicious and good for you.

02:28.640 --> 02:29.640
 You won't regret it.

02:30.560 --> 02:33.160
 This show is also sponsored by BetterHelp,

02:33.160 --> 02:36.040
 spelled H E L P Help.

02:36.040 --> 02:39.440
 Check it out at betterhelp.com slash lex.

02:39.440 --> 02:40.600
 They figure out what you need

02:40.600 --> 02:43.240
 and match you with a licensed professional therapist

02:43.240 --> 02:44.960
 in under 48 hours.

02:44.960 --> 02:47.640
 It's not a crisis line, it's not self help,

02:47.640 --> 02:51.040
 it is professional counseling done securely online.

02:51.040 --> 02:53.720
 As you may know, I'm a bit from the David Goggins line

02:53.720 --> 02:57.080
 of creatures and still have some demons to contend with,

02:57.080 --> 03:01.580
 usually on long runs or all nighters full of self doubt.

03:01.580 --> 03:04.360
 I think suffering is essential for creation,

03:04.360 --> 03:06.040
 but you can suffer beautifully

03:06.040 --> 03:08.200
 in a way that doesn't destroy you.

03:08.200 --> 03:11.540
 For most people, I think a good therapist can help in this.

03:11.540 --> 03:13.400
 So it's at least worth a try.

03:13.400 --> 03:15.620
 Check out the reviews, they're all good.

03:15.620 --> 03:19.220
 It's easy, private, affordable, available worldwide.

03:19.220 --> 03:21.640
 You can communicate by text anytime

03:21.640 --> 03:25.080
 and schedule weekly audio and video sessions.

03:25.080 --> 03:28.500
 Check it out at betterhelp.com slash lex.

03:28.500 --> 03:31.840
 This show is also sponsored by ExpressVPN.

03:31.840 --> 03:34.860
 Get it at expressvpn.com slash lex pod

03:34.860 --> 03:37.680
 to get a discount and to support this podcast.

03:37.680 --> 03:39.680
 Have you ever watched The Office?

03:39.680 --> 03:41.900
 If you have, you probably know it's based

03:41.900 --> 03:45.120
 on a UK series also called The Office.

03:45.120 --> 03:48.080
 Not to stir up trouble, but I personally think

03:48.080 --> 03:50.320
 the British version is actually more brilliant

03:50.320 --> 03:53.120
 than the American one, but both are amazing.

03:53.120 --> 03:56.120
 Anyway, there are actually nine other countries

03:56.120 --> 03:58.400
 with their own version of The Office.

03:58.400 --> 04:01.180
 You can get access to them with no geo restriction

04:01.180 --> 04:03.600
 when you use ExpressVPN.

04:03.600 --> 04:05.560
 It lets you control where you want sites

04:05.560 --> 04:07.340
 to think you're located.

04:07.340 --> 04:10.360
 You can choose from nearly 100 different countries,

04:10.360 --> 04:12.120
 giving you access to content

04:12.120 --> 04:14.020
 that isn't available in your region.

04:14.020 --> 04:19.020
 So again, get it on any device at expressvpn.com slash lex pod

04:19.800 --> 04:22.080
 to get an extra three months free

04:22.080 --> 04:25.000
 and to support this podcast.

04:25.000 --> 04:28.640
 And now here's my conversation with Russ Tedrick.

04:29.560 --> 04:31.480
 What is the most beautiful motion

04:31.480 --> 04:34.420
 of an animal or robot that you've ever seen?

04:36.160 --> 04:38.280
 I think the most beautiful motion of a robot

04:38.280 --> 04:41.120
 has to be the passive dynamic walkers.

04:41.120 --> 04:43.320
 I think there's just something fundamentally beautiful.

04:43.320 --> 04:45.360
 The ones in particular that Steve Collins built

04:45.360 --> 04:50.360
 with Andy Ruina at Cornell, a 3D walking machine.

04:50.520 --> 04:53.720
 So it was not confined to a boom or a plane

04:54.680 --> 04:57.460
 that you put it on top of a small ramp,

04:57.460 --> 05:00.500
 give it a little push, it's powered only by gravity.

05:00.500 --> 05:04.320
 No controllers, no batteries whatsoever.

05:04.320 --> 05:06.160
 It just falls down the ramp.

05:06.160 --> 05:09.520
 And at the time it looked more natural, more graceful,

05:09.520 --> 05:13.460
 more human like than any robot we'd seen to date

05:13.460 --> 05:15.240
 powered only by gravity.

05:15.240 --> 05:16.160
 How does it work?

05:17.160 --> 05:19.480
 Well, okay, the simplest model, it's kind of like a slinky.

05:19.480 --> 05:21.560
 It's like an elaborate slinky.

05:21.560 --> 05:23.840
 One of the simplest models we used to think about it

05:23.840 --> 05:25.360
 is actually a rimless wheel.

05:25.360 --> 05:30.100
 So imagine taking a bicycle wheel, but take the rim off.

05:30.100 --> 05:32.640
 So it's now just got a bunch of spokes.

05:32.640 --> 05:33.720
 If you give that a push,

05:33.720 --> 05:35.840
 it still wants to roll down the ramp,

05:35.840 --> 05:38.180
 but every time its foot, its spoke comes around

05:38.180 --> 05:40.680
 and hits the ground, it loses a little energy.

05:41.880 --> 05:43.280
 Every time it takes a step forward,

05:43.280 --> 05:44.580
 it gains a little energy.

05:45.800 --> 05:48.200
 Those things can come into perfect balance.

05:48.200 --> 05:51.240
 And actually they want to, it's a stable phenomenon.

05:51.240 --> 05:53.720
 If it's going too slow, it'll speed up.

05:53.720 --> 05:55.880
 If it's going too fast, it'll slow down

05:55.880 --> 05:58.180
 and it comes into a stable periodic motion.

05:59.480 --> 06:02.120
 Now you can take that rimless wheel,

06:02.120 --> 06:05.040
 which doesn't look very much like a human walking,

06:05.040 --> 06:08.080
 take all the extra spokes away, put a hinge in the middle.

06:08.080 --> 06:09.720
 Now it's two legs.

06:09.720 --> 06:11.880
 That's called our compass gait walker.

06:11.880 --> 06:13.800
 That can still, you give it a little push,

06:13.800 --> 06:15.520
 it starts falling down a ramp.

06:15.520 --> 06:17.240
 It looks a little bit more like walking.

06:17.240 --> 06:18.360
 At least it's a biped.

06:19.700 --> 06:21.400
 But what Steve and Andy,

06:21.400 --> 06:23.480
 and Tad McGeer started the whole exercise,

06:23.480 --> 06:25.200
 but what Steve and Andy did was they took it

06:25.200 --> 06:27.460
 to this beautiful conclusion

06:28.700 --> 06:32.440
 where they built something that had knees, arms, a torso.

06:32.440 --> 06:36.320
 The arms swung naturally, give it a little push.

06:36.320 --> 06:38.720
 And that looked like a stroll through the park.

06:38.720 --> 06:40.240
 How do you design something like that?

06:40.240 --> 06:42.360
 I mean, is that art or science?

06:42.360 --> 06:43.800
 It's on the boundary.

06:43.800 --> 06:47.640
 I think there's a science to getting close to the solution.

06:47.640 --> 06:49.040
 I think there's certainly art in the way

06:49.040 --> 06:52.000
 that they made a beautiful robot.

06:52.000 --> 06:57.000
 But then the finesse, because they were working

06:57.080 --> 06:58.980
 with a system that wasn't perfectly modeled,

06:58.980 --> 07:01.060
 wasn't perfectly controlled,

07:01.060 --> 07:02.800
 there's all these little tricks

07:02.800 --> 07:05.480
 that you have to tune the suction cups at the knees,

07:05.480 --> 07:07.960
 for instance, so that they stick,

07:07.960 --> 07:09.640
 but then they release at just the right time.

07:09.640 --> 07:12.360
 Or there's all these little tricks of the trade,

07:12.360 --> 07:14.440
 which really are art, but it was a point.

07:14.440 --> 07:16.200
 I mean, it made the point.

07:16.200 --> 07:18.800
 We were, at that time, the walking robot,

07:18.800 --> 07:21.840
 the best walking robot in the world was Honda's Asmo.

07:21.840 --> 07:24.120
 Absolutely marvel of modern engineering.

07:24.120 --> 07:25.240
 Is this 90s?

07:25.240 --> 07:27.440
 This was in 97 when they first released.

07:27.440 --> 07:29.920
 It sort of announced P2, and then it went through.

07:29.920 --> 07:32.360
 It was Asmo by then in 2004.

07:32.360 --> 07:37.360
 And it looks like this very cautious walking,

07:37.840 --> 07:41.320
 like you're walking on hot coals or something like that.

07:41.320 --> 07:43.760
 I think it gets a bad rap.

07:43.760 --> 07:45.340
 Asmo is a beautiful machine.

07:45.340 --> 07:47.000
 It does walk with its knees bent.

07:47.000 --> 07:49.740
 Our Atlas walking had its knees bent.

07:49.740 --> 07:52.340
 But actually, Asmo was pretty fantastic.

07:52.340 --> 07:54.320
 But it wasn't energy efficient.

07:54.320 --> 07:56.660
 Neither was Atlas when we worked on Atlas.

07:58.220 --> 08:00.520
 None of our robots that have been that complicated

08:00.520 --> 08:02.480
 have been very energy efficient.

08:04.040 --> 08:09.040
 But there's a thing that happens when you do control,

08:09.680 --> 08:12.480
 when you try to control a system of that complexity.

08:12.480 --> 08:16.480
 You try to use your motors to basically counteract gravity.

08:17.360 --> 08:20.680
 Take whatever the world's doing to you and push back,

08:20.680 --> 08:23.520
 erase the dynamics of the world,

08:23.520 --> 08:25.040
 and impose the dynamics you want

08:25.040 --> 08:28.220
 because you can make them simple and analyzable,

08:28.220 --> 08:30.760
 mathematically simple.

08:30.760 --> 08:34.400
 And this was a very sort of beautiful example

08:34.400 --> 08:36.380
 that you don't have to do that.

08:36.380 --> 08:37.480
 You can just let go.

08:37.480 --> 08:40.280
 Let physics do most of the work, right?

08:40.280 --> 08:42.200
 And you just have to give it a little bit of energy.

08:42.200 --> 08:43.560
 This one only walked down a ramp.

08:43.560 --> 08:45.340
 It would never walk on the flat.

08:45.340 --> 08:46.180
 To walk on the flat,

08:46.180 --> 08:48.480
 you have to give a little energy at some point.

08:48.480 --> 08:51.960
 But maybe instead of trying to take the forces imparted

08:51.960 --> 08:55.200
 to you by the world and replacing them,

08:55.200 --> 08:58.200
 what we should be doing is letting the world push us around

08:58.200 --> 08:59.360
 and we go with the flow.

08:59.360 --> 09:01.280
 Very zen, very zen robot.

09:01.280 --> 09:03.440
 Yeah, but okay, so that sounds very zen,

09:03.440 --> 09:08.440
 but I can also imagine how many like failed versions

09:10.220 --> 09:11.640
 they had to go through.

09:11.640 --> 09:14.040
 Like how many, like, I would say it's probably,

09:14.040 --> 09:15.320
 would you say it's in the thousands

09:15.320 --> 09:17.920
 that they've had to have the system fall down

09:17.920 --> 09:19.840
 before they figured out how to get it?

09:19.840 --> 09:22.560
 I don't know if it's thousands, but it's a lot.

09:22.560 --> 09:23.560
 It takes some patience.

09:23.560 --> 09:25.040
 There's no question.

09:25.040 --> 09:28.320
 So in that sense, control might help a little bit.

09:28.320 --> 09:32.100
 Oh, I think everybody, even at the time,

09:32.100 --> 09:35.020
 said that the answer is to do with that with control.

09:35.020 --> 09:36.340
 But it was just pointing out

09:36.340 --> 09:39.120
 that maybe the way we're doing control right now

09:39.120 --> 09:41.040
 isn't the way we should.

09:41.040 --> 09:41.880
 Got it.

09:41.880 --> 09:43.800
 So what about on the animal side,

09:43.800 --> 09:46.200
 the ones that figured out how to move efficiently?

09:46.200 --> 09:49.440
 Is there anything you find inspiring or beautiful

09:49.440 --> 09:51.160
 in the movement of any particular animal?

09:51.160 --> 09:51.980
 I do have a favorite example.

09:51.980 --> 09:52.820
 Okay.

09:52.820 --> 09:57.160
 So it sort of goes with the passive walking idea.

09:57.160 --> 10:01.400
 So is there, you know, how energy efficient are animals?

10:01.400 --> 10:03.840
 Okay, there's a great series of experiments

10:03.840 --> 10:07.520
 by George Lauder at Harvard and Mike Tranofilo at MIT.

10:07.520 --> 10:10.640
 They were studying fish swimming in a water tunnel.

10:10.640 --> 10:11.820
 Okay.

10:11.820 --> 10:15.240
 And one of these, the type of fish they were studying

10:15.240 --> 10:17.240
 were these rainbow trout,

10:17.240 --> 10:20.360
 because there was a phenomenon well understood

10:20.360 --> 10:22.180
 that rainbow trout, when they're swimming upstream

10:22.180 --> 10:25.120
 in mating season, they kind of hang out behind the rocks.

10:25.120 --> 10:26.080
 And it looks like, I mean,

10:26.080 --> 10:28.080
 that's tiring work swimming upstream.

10:28.080 --> 10:29.180
 They're hanging out behind the rocks.

10:29.180 --> 10:31.980
 Maybe there's something energetically interesting there.

10:31.980 --> 10:33.400
 So they tried to recreate that.

10:33.400 --> 10:36.440
 They put in this water tunnel, a rock basically,

10:36.440 --> 10:40.560
 a cylinder that had the same sort of vortex street,

10:40.560 --> 10:42.480
 the eddies coming off the back of the rock

10:42.480 --> 10:44.240
 that you would see in a stream.

10:44.240 --> 10:46.080
 And they put a real fish behind this

10:46.080 --> 10:48.000
 and watched how it swims.

10:48.000 --> 10:51.960
 And the amazing thing is that if you watch from above

10:51.960 --> 10:53.800
 what the fish swims when it's not behind a rock,

10:53.800 --> 10:56.120
 it has a particular gate.

10:56.120 --> 10:58.240
 You can identify the fish the same way you look

10:58.240 --> 10:59.840
 at a human walking down the street.

10:59.840 --> 11:02.420
 You sort of have a sense of how a human walks.

11:02.420 --> 11:04.120
 The fish has a characteristic gate.

11:05.360 --> 11:07.920
 You put that fish behind the rock, its gate changes.

11:09.160 --> 11:12.720
 And what they saw was that it was actually resonating

11:12.720 --> 11:15.160
 and kind of surfing between the vortices.

11:16.560 --> 11:20.140
 Now, here was the experiment that really was the clincher.

11:20.140 --> 11:22.160
 Because there was still, it wasn't clear how much of that

11:22.160 --> 11:24.000
 was mechanics of the fish,

11:24.000 --> 11:26.940
 how much of that is control, the brain.

11:26.940 --> 11:28.480
 So the clincher experiment,

11:28.480 --> 11:29.800
 and maybe one of my favorites to date,

11:29.800 --> 11:32.020
 although there are many good experiments.

11:33.700 --> 11:37.060
 They took, this was now a dead fish.

11:38.380 --> 11:40.200
 They took a dead fish.

11:40.200 --> 11:41.640
 They put a string that went,

11:41.640 --> 11:44.160
 that tied the mouth of the fish to the rock

11:44.160 --> 11:47.160
 so it couldn't go back and get caught in the grates.

11:47.160 --> 11:49.180
 And then they asked what would that dead fish do

11:49.180 --> 11:51.160
 when it was hanging out behind the rock?

11:51.160 --> 11:52.920
 And so what you'd expect, it sort of flopped around

11:52.920 --> 11:56.120
 like a dead fish in the vortex wake

11:56.120 --> 11:57.800
 until something sort of amazing happens.

11:57.800 --> 12:02.800
 And this video is worth putting in, right?

12:02.880 --> 12:04.040
 What happens?

12:04.040 --> 12:07.520
 The dead fish basically starts swimming upstream, right?

12:07.520 --> 12:12.160
 It's completely dead, no brain, no motors, no control.

12:12.160 --> 12:14.600
 But it's somehow the mechanics of the fish

12:14.600 --> 12:16.360
 resonate with the vortex street

12:16.360 --> 12:18.280
 and it starts swimming upstream.

12:18.280 --> 12:20.520
 It's one of the best examples ever.

12:20.520 --> 12:23.740
 Who do you give credit for that to?

12:23.740 --> 12:27.980
 Is that just evolution constantly just figuring out

12:27.980 --> 12:30.920
 by killing a lot of generations of animals,

12:30.920 --> 12:33.360
 like the most efficient motion?

12:33.360 --> 12:38.360
 Is that, or maybe the physics of our world completely like,

12:38.660 --> 12:40.920
 is like if evolution applied not only to animals,

12:40.920 --> 12:45.220
 but just the entirety of it somehow drives to efficiency,

12:45.220 --> 12:47.020
 like nature likes efficiency?

12:47.020 --> 12:49.980
 I don't know if that question even makes any sense.

12:49.980 --> 12:51.020
 I understand the question.

12:51.020 --> 12:51.860
 That's reasonable.

12:51.860 --> 12:54.460
 I mean, do they co evolve?

12:54.460 --> 12:55.620
 Yeah, somehow co, yeah.

12:55.620 --> 12:59.020
 Like I don't know if an environment can evolve, but.

13:00.020 --> 13:02.340
 I mean, there are experiments that people do,

13:02.340 --> 13:05.940
 careful experiments that show that animals can adapt

13:05.940 --> 13:08.660
 to unusual situations and recover efficiency.

13:08.660 --> 13:11.100
 So there seems like at least in one direction,

13:11.100 --> 13:12.740
 I think there is reason to believe

13:12.740 --> 13:17.100
 that the animal's motor system and probably its mechanics

13:18.100 --> 13:20.060
 adapt in order to be more efficient.

13:20.060 --> 13:23.140
 But efficiency isn't the only goal, of course.

13:23.140 --> 13:26.220
 Sometimes it's too easy to think about only efficiency,

13:26.220 --> 13:30.540
 but we have to do a lot of other things first, not get eaten.

13:30.540 --> 13:34.140
 And then all other things being equal, try to save energy.

13:34.140 --> 13:36.100
 By the way, let's draw a distinction

13:36.100 --> 13:38.160
 between control and mechanics.

13:38.160 --> 13:40.820
 Like how would you define each?

13:40.820 --> 13:41.720
 Yeah.

13:41.720 --> 13:43.940
 I mean, I think part of the point is that

13:43.940 --> 13:47.860
 we shouldn't draw a line as clearly as we tend to.

13:47.860 --> 13:51.460
 But on a robot, we have motors

13:51.460 --> 13:54.840
 and we have the links of the robot, let's say.

13:54.840 --> 13:56.260
 If the motors are turned off,

13:56.260 --> 13:59.780
 the robot has some passive dynamics, okay?

13:59.780 --> 14:01.380
 Gravity does the work.

14:01.380 --> 14:03.700
 You can put springs, I would call that mechanics, right?

14:03.700 --> 14:04.940
 If we have springs and dampers,

14:04.940 --> 14:08.540
 which our muscles are springs and dampers and tendons.

14:08.540 --> 14:10.440
 But then you have something that's doing active work,

14:10.440 --> 14:13.240
 putting energy in, which are your motors on the robot.

14:13.240 --> 14:16.580
 The controller's job is to send commands to the motor

14:16.580 --> 14:19.960
 that add new energy into the system, right?

14:19.960 --> 14:22.820
 So the mechanics and control interplay somewhere,

14:22.820 --> 14:24.820
 the divide is around, you know,

14:24.820 --> 14:27.560
 did you decide to send some commands to your motor

14:27.560 --> 14:28.980
 or did you just leave the motors off,

14:28.980 --> 14:30.580
 let them do their work?

14:30.580 --> 14:33.900
 Would you say is most of nature

14:35.140 --> 14:39.820
 on the dynamic side or the control side?

14:39.820 --> 14:42.260
 So like, if you look at biological systems,

14:43.580 --> 14:45.100
 we're living in a pandemic now,

14:45.100 --> 14:46.700
 like, do you think a virus is a,

14:47.840 --> 14:50.100
 do you think it's a dynamic system

14:50.100 --> 14:54.100
 or is there a lot of control, intelligence?

14:54.100 --> 14:57.040
 I think it's both, but I think we maybe have underestimated

14:57.040 --> 14:59.700
 how important the dynamics are, right?

15:02.020 --> 15:04.300
 I mean, even our bodies, the mechanics of our bodies,

15:04.300 --> 15:06.140
 certainly with exercise, they evolve.

15:06.140 --> 15:11.060
 But so I actually, I lost a finger in early 2000s

15:11.060 --> 15:14.460
 and it's my fifth metacarpal.

15:14.460 --> 15:16.620
 And it turns out you use that a lot

15:16.620 --> 15:19.340
 in ways you don't expect when you're opening jars,

15:19.340 --> 15:20.620
 even when I'm just walking around,

15:20.620 --> 15:23.220
 if I bump it on something, there's a bone there

15:23.220 --> 15:26.780
 that was used to taking contact.

15:26.780 --> 15:28.820
 My fourth metacarpal wasn't used to taking contact,

15:28.820 --> 15:31.100
 it used to hurt, it still does a little bit.

15:31.100 --> 15:34.180
 But actually my bone has remodeled, right?

15:34.180 --> 15:39.180
 Over a couple of years, the geometry,

15:39.580 --> 15:42.100
 the mechanics of that bone changed

15:42.100 --> 15:44.340
 to address the new circumstances.

15:44.340 --> 15:46.820
 So the idea that somehow it's only our brain

15:46.820 --> 15:48.980
 that's adapting or evolving is not right.

15:50.140 --> 15:52.560
 Maybe sticking on evolution for a bit,

15:52.560 --> 15:56.720
 because it's tended to create some interesting things.

15:56.720 --> 16:01.720
 Bipedal walking, why the heck did evolution give us,

16:01.720 --> 16:05.040
 I think we're, are we the only mammals that walk on two feet?

16:05.040 --> 16:09.040
 No, I mean, there's a bunch of animals that do it a bit.

16:09.040 --> 16:09.880
 A bit.

16:09.880 --> 16:12.280
 I think we are the most successful bipeds.

16:12.280 --> 16:17.280
 I think I read somewhere that the reason

16:17.760 --> 16:22.760
 the evolution made us walk on two feet

16:22.760 --> 16:24.720
 is because there's an advantage

16:24.720 --> 16:27.200
 to being able to carry food back to the tribe

16:27.200 --> 16:28.040
 or something like that.

16:28.040 --> 16:31.960
 So like you can carry, it's kind of this communal,

16:31.960 --> 16:35.080
 cooperative thing, so like to carry stuff back

16:35.080 --> 16:40.080
 to a place of shelter and so on to share with others.

16:40.080 --> 16:44.520
 Do you understand at all the value of walking on two feet

16:44.520 --> 16:48.000
 from both a robotics and a human perspective?

16:48.000 --> 16:50.280
 Yeah, there are some great books written

16:50.280 --> 16:54.560
 about evolution of, walking evolution of the human body.

16:54.560 --> 16:59.560
 I think it's easy though to make bad evolutionary arguments.

17:00.600 --> 17:03.740
 Sure, most of them are probably bad,

17:03.740 --> 17:05.320
 but what else can we do?

17:06.200 --> 17:11.120
 I mean, I think a lot of what dominated our evolution

17:11.120 --> 17:15.080
 probably was not the things that worked well

17:15.080 --> 17:18.560
 sort of in the steady state, you know,

17:18.560 --> 17:22.800
 when things are good, but for instance,

17:22.800 --> 17:25.040
 people talk about what we should eat now

17:25.040 --> 17:28.320
 because our ancestors were meat eaters or whatever.

17:28.320 --> 17:30.240
 Oh yeah, I love that, yeah.

17:30.240 --> 17:32.520
 But probably, you know, the reason

17:32.520 --> 17:37.520
 that one pre Homo sapiens species versus another survived

17:39.640 --> 17:43.440
 was not because of whether they ate well

17:43.440 --> 17:45.300
 when there was lots of food.

17:45.300 --> 17:47.920
 But when the ice age came, you know,

17:47.920 --> 17:50.940
 probably one of them happened to be in the wrong place.

17:50.940 --> 17:54.200
 One of them happened to forage a food that was okay

17:54.200 --> 17:58.240
 even when the glaciers came or something like that, I mean.

17:58.240 --> 18:00.560
 There's a million variables that contributed

18:00.560 --> 18:04.080
 and we can't, and our, actually the amount of information

18:04.080 --> 18:06.680
 we're working with and telling these stories,

18:06.680 --> 18:10.220
 these evolutionary stories is very little.

18:10.220 --> 18:13.080
 So yeah, just like you said, it seems like,

18:13.080 --> 18:15.680
 if you study history, it seems like history turns

18:15.680 --> 18:20.280
 on like these little events that otherwise

18:20.280 --> 18:23.320
 would seem meaningless, but in a grant,

18:23.320 --> 18:27.560
 like when you, in retrospect, were turning points.

18:27.560 --> 18:28.400
 Absolutely.

18:28.400 --> 18:31.280
 And that's probably how like somebody got hit in the head

18:31.280 --> 18:35.160
 with a rock because somebody slept with the wrong person

18:35.160 --> 18:38.500
 back in the cave days and somebody get angry

18:38.500 --> 18:41.920
 and that turned, you know, warring tribes

18:41.920 --> 18:45.360
 combined with the environment, all those millions of things

18:45.360 --> 18:47.680
 and the meat eating, which I get a lot of criticism

18:47.680 --> 18:51.480
 because I don't know what your dietary processes are like,

18:51.480 --> 18:55.040
 but these days I've been eating only meat,

18:55.040 --> 18:59.080
 which is, there's a large community of people who say,

18:59.080 --> 19:01.080
 yeah, probably make evolutionary arguments

19:01.080 --> 19:02.720
 and say you're doing a great job.

19:02.720 --> 19:05.760
 There's probably an even larger community of people,

19:05.760 --> 19:08.520
 including my mom, who says it's deeply unhealthy,

19:08.520 --> 19:10.760
 it's wrong, but I just feel good doing it.

19:10.760 --> 19:12.980
 But you're right, these evolutionary arguments

19:12.980 --> 19:15.420
 can be flawed, but is there anything interesting

19:15.420 --> 19:17.320
 to pull out for?

19:17.320 --> 19:19.360
 There's a great book, by the way,

19:19.360 --> 19:21.280
 well, a series of books by Nicholas Taleb

19:21.280 --> 19:23.840
 about Fooled by Randomness and Black Swan.

19:24.800 --> 19:26.840
 Highly recommend them, but yeah,

19:26.840 --> 19:29.160
 they make the point nicely that probably

19:29.160 --> 19:34.160
 it was a few random events that, yes,

19:34.360 --> 19:37.280
 maybe it was someone getting hit by a rock, as you say.

19:39.520 --> 19:42.700
 That said, do you think, I don't know how to ask this

19:42.700 --> 19:44.080
 question or how to talk about this,

19:44.080 --> 19:45.680
 but there's something elegant and beautiful

19:45.680 --> 19:48.800
 about moving on two feet, obviously biased

19:48.800 --> 19:53.280
 because I'm human, but from a robotics perspective, too,

19:53.280 --> 19:55.440
 you work with robots on two feet,

19:56.440 --> 20:00.120
 is it all useful to build robots that are on two feet

20:00.120 --> 20:01.120
 as opposed to four?

20:01.120 --> 20:02.320
 Is there something useful about it?

20:02.320 --> 20:05.540
 I think the most, I mean, the reason I spent a long time

20:05.540 --> 20:09.000
 working on bipedal walking was because it was hard

20:09.000 --> 20:12.480
 and it challenged control theory in ways

20:12.480 --> 20:13.920
 that I thought were important.

20:13.920 --> 20:18.520
 I wouldn't have ever tried to convince you

20:18.520 --> 20:22.440
 that you should start a company around bipeds

20:22.440 --> 20:24.240
 or something like this.

20:24.240 --> 20:26.120
 There are people that make pretty compelling arguments.

20:26.120 --> 20:28.920
 I think the most compelling one is that the world

20:28.920 --> 20:32.320
 is built for the human form, and if you want a robot

20:32.320 --> 20:34.800
 to work in the world we have today,

20:34.800 --> 20:38.020
 then having a human form is a pretty good way to go.

20:39.680 --> 20:42.560
 There are places that a biped can go that would be hard

20:42.560 --> 20:47.560
 for other form factors to go, even natural places,

20:47.640 --> 20:51.360
 but at some point in the long run,

20:51.360 --> 20:54.220
 we'll be building our environments for our robots, probably,

20:54.220 --> 20:56.480
 and so maybe that argument falls aside.

20:56.480 --> 20:58.760
 So you famously run barefoot.

21:00.640 --> 21:02.120
 Do you still run barefoot?

21:02.120 --> 21:03.080
 I still run barefoot.

21:03.080 --> 21:04.760
 That's so awesome.

21:04.760 --> 21:06.320
 Much to my wife's chagrin.

21:07.800 --> 21:09.320
 Do you want to make an evolutionary argument

21:09.320 --> 21:12.680
 for why running barefoot is advantageous?

21:12.680 --> 21:17.560
 What have you learned about human and robot movement

21:17.560 --> 21:19.840
 in general from running barefoot?

21:21.160 --> 21:23.640
 Human or robot and or?

21:23.640 --> 21:25.640
 Well, you know, it happened the other way, right?

21:25.640 --> 21:27.680
 So I was studying walking robots,

21:27.680 --> 21:31.760
 and there's a great conference called

21:31.760 --> 21:35.320
 the Dynamic Walking Conference where it brings together

21:35.320 --> 21:36.980
 both the biomechanics community

21:36.980 --> 21:39.880
 and the walking robots community.

21:39.880 --> 21:41.660
 And so I had been going to this for years

21:41.660 --> 21:45.080
 and hearing talks by people who study barefoot running

21:45.080 --> 21:46.920
 and other, the mechanics of running.

21:48.080 --> 21:50.280
 So I did eventually read Born to Run.

21:50.280 --> 21:52.820
 Most people read Born to Run in the first, right?

21:54.080 --> 21:55.720
 The other thing I had going for me is actually

21:55.720 --> 21:58.800
 that I wasn't a runner before,

21:58.800 --> 22:01.560
 and I learned to run after I had learned

22:01.560 --> 22:03.640
 about barefoot running, or I mean,

22:03.640 --> 22:05.440
 started running longer distances.

22:05.440 --> 22:07.360
 So I didn't have to unlearn.

22:07.360 --> 22:11.080
 And I'm definitely, I'm a big fan of it for me,

22:11.080 --> 22:12.360
 but I'm not going to,

22:12.360 --> 22:14.600
 I tend to not try to convince other people.

22:14.600 --> 22:17.240
 There's people who run beautifully with shoes on,

22:17.240 --> 22:18.300
 and that's good.

22:20.040 --> 22:21.880
 But here's why it makes sense for me.

22:24.040 --> 22:26.360
 It's all about the longterm game, right?

22:26.360 --> 22:29.440
 So I think it's just too easy to run 10 miles,

22:29.440 --> 22:31.560
 feel pretty good, and then you get home at night

22:31.560 --> 22:33.840
 and you realize my knees hurt.

22:33.840 --> 22:35.480
 I did something wrong, right?

22:37.880 --> 22:39.780
 If you take your shoes off,

22:39.780 --> 22:43.040
 then if you hit hard with your foot at all,

22:44.080 --> 22:45.720
 then it hurts.

22:45.720 --> 22:47.560
 You don't like run 10 miles

22:47.560 --> 22:50.800
 and then realize you've done some damage.

22:50.800 --> 22:52.940
 You have immediate feedback telling you

22:52.940 --> 22:55.420
 that you've done something that's maybe suboptimal,

22:55.420 --> 22:56.520
 and you change your gait.

22:56.520 --> 22:57.720
 I mean, it's even subconscious.

22:57.720 --> 23:00.640
 If I, right now, having run many miles barefoot,

23:00.640 --> 23:03.160
 if I put a shoe on, my gait changes

23:03.160 --> 23:04.960
 in a way that I think is not as good.

23:05.840 --> 23:09.520
 So it makes me land softer.

23:09.520 --> 23:13.160
 And I think my goals for running

23:13.160 --> 23:16.860
 are to do it for as long as I can into old age,

23:16.860 --> 23:19.000
 not to win any races.

23:19.000 --> 23:23.420
 And so for me, this is a way to protect myself.

23:23.420 --> 23:25.680
 Yeah, I think, first of all,

23:25.680 --> 23:29.540
 I've tried running barefoot many years ago,

23:29.540 --> 23:30.480
 probably the other way,

23:30.480 --> 23:33.920
 just reading Born to Run.

23:33.920 --> 23:36.440
 But just to understand,

23:36.440 --> 23:39.520
 because I felt like I couldn't put in the miles

23:39.520 --> 23:40.840
 that I wanted to.

23:40.840 --> 23:44.260
 And it feels like running for me,

23:44.260 --> 23:46.280
 and I think for a lot of people,

23:46.280 --> 23:48.880
 was one of those activities that we do often

23:48.880 --> 23:52.140
 and we never really try to learn to do correctly.

23:53.340 --> 23:55.920
 Like, it's funny, there's so many activities

23:55.920 --> 24:00.280
 we do every day, like brushing our teeth, right?

24:00.280 --> 24:02.360
 I think a lot of us, at least me,

24:02.360 --> 24:04.320
 probably have never deeply studied

24:04.320 --> 24:07.040
 how to properly brush my teeth, right?

24:07.040 --> 24:08.960
 Or wash, as now with the pandemic,

24:08.960 --> 24:10.640
 or how to properly wash our hands.

24:10.640 --> 24:13.800
 We do it every day, but we haven't really studied,

24:13.800 --> 24:15.200
 like, am I doing this correctly?

24:15.200 --> 24:17.120
 But running felt like one of those things,

24:17.120 --> 24:20.220
 it was absurd not to study how to do correctly,

24:20.220 --> 24:23.320
 because it's the source of so much pain and suffering.

24:23.320 --> 24:25.680
 Like, I hate running, but I do it.

24:25.680 --> 24:28.940
 I do it because I hate it, but I feel good afterwards.

24:28.940 --> 24:30.280
 But I think it feels like you need

24:30.280 --> 24:31.440
 to learn how to do it properly.

24:31.440 --> 24:33.540
 So that's where barefoot running came in,

24:33.540 --> 24:35.760
 and then I quickly realized that my gait

24:35.760 --> 24:38.040
 was completely wrong.

24:38.040 --> 24:41.440
 I was taking huge steps,

24:41.440 --> 24:45.840
 and landing hard on the heel, all those elements.

24:45.840 --> 24:47.600
 And so, yeah, from that I actually learned

24:47.600 --> 24:49.540
 to take really small steps, look.

24:50.520 --> 24:52.280
 I already forgot the number,

24:52.280 --> 24:55.600
 but I feel like it was 180 a minute or something like that.

24:55.600 --> 25:00.080
 And I remember I actually just took songs

25:00.080 --> 25:03.360
 that are 180 beats per minute,

25:03.360 --> 25:05.500
 and then like tried to run at that beat,

25:06.520 --> 25:07.660
 and just to teach myself.

25:07.660 --> 25:11.120
 It took a long time, and I feel like after a while,

25:11.120 --> 25:14.320
 you learn to run, you adjust properly,

25:14.320 --> 25:15.960
 without going all the way to barefoot.

25:15.960 --> 25:19.440
 But I feel like barefoot is the legit way to do it.

25:19.440 --> 25:21.640
 I mean, I think a lot of people

25:21.640 --> 25:23.360
 would be really curious about it.

25:23.360 --> 25:25.560
 Can you, if they're interested in trying,

25:25.560 --> 25:27.840
 what would you, how would you recommend

25:27.840 --> 25:30.740
 they start, or try, or explore?

25:30.740 --> 25:31.580
 Slowly.

25:31.580 --> 25:33.720
 That's the biggest thing people do,

25:33.720 --> 25:35.920
 is they are excellent runners,

25:35.920 --> 25:37.620
 and they're used to running long distances,

25:37.620 --> 25:39.240
 or running fast, and they take their shoes off,

25:39.240 --> 25:42.520
 and they hurt themselves instantly trying to do

25:42.520 --> 25:44.280
 something that they were used to doing.

25:44.280 --> 25:46.000
 I think I lucked out in the sense

25:46.000 --> 25:50.200
 that I couldn't run very far when I first started trying.

25:50.200 --> 25:51.840
 And I run with minimal shoes too.

25:51.840 --> 25:54.360
 I mean, I will bring along a pair of,

25:54.360 --> 25:56.320
 actually, like aqua socks or something like this,

25:56.320 --> 25:58.320
 I can just slip on, or running sandals,

25:58.320 --> 26:00.360
 I've tried all of them.

26:00.360 --> 26:02.600
 What's the difference between a minimal shoe

26:02.600 --> 26:03.760
 and nothing at all?

26:03.760 --> 26:07.020
 What's, like, feeling wise, what does it feel like?

26:07.020 --> 26:10.000
 There is a, I mean, I notice my gait changing, right?

26:10.000 --> 26:15.000
 So, I mean, your foot has as many muscles

26:15.080 --> 26:17.600
 and sensors as your hand does, right?

26:17.600 --> 26:19.960
 Sensors, ooh, okay.

26:19.960 --> 26:23.200
 And we do amazing things with our hands.

26:23.200 --> 26:26.000
 And we stick our foot in a big, solid shoe, right?

26:26.000 --> 26:29.640
 So there's, I think, you know, when you're barefoot,

26:29.640 --> 26:33.240
 you're just giving yourself more proprioception.

26:33.240 --> 26:35.720
 And that's why you're more aware of some of the gait flaws

26:35.720 --> 26:37.080
 and stuff like this.

26:37.080 --> 26:39.840
 Now, you have less protection too, so.

26:40.720 --> 26:42.400
 Rocks and stuff.

26:42.400 --> 26:45.160
 I mean, yeah, so I think people who are afraid

26:45.160 --> 26:47.160
 of barefoot running are worried about getting cuts

26:47.160 --> 26:48.460
 or stepping on rocks.

26:49.800 --> 26:51.560
 First of all, even if that was a concern,

26:51.560 --> 26:54.240
 I think those are all, like, very short term.

26:54.240 --> 26:55.420
 You know, if I get a scratch or something,

26:55.420 --> 26:56.520
 it'll heal in a week.

26:56.520 --> 26:58.240
 If I blow out my knees, I'm done running forever.

26:58.240 --> 27:01.720
 So I will trade the short term for the long term anytime.

27:01.720 --> 27:04.760
 But even then, you know, and this, again,

27:04.760 --> 27:07.760
 to my wife's chagrin, your feet get tough, right?

27:07.760 --> 27:11.480
 And, yeah, I can run over almost anything now.

27:13.760 --> 27:17.240
 I mean, what, can you talk about,

27:17.240 --> 27:21.940
 is there, like, is there tips or tricks

27:21.940 --> 27:24.820
 that you have, suggestions about,

27:24.820 --> 27:26.620
 like, if I wanted to try it?

27:26.620 --> 27:29.580
 You know, there is a good book, actually.

27:29.580 --> 27:32.700
 There's probably more good books since I read them.

27:32.700 --> 27:35.680
 But Ken Bob, Barefoot Ken Bob Saxton.

27:37.340 --> 27:38.820
 He's an interesting guy.

27:38.820 --> 27:42.620
 But I think his book captures the right way

27:42.620 --> 27:44.180
 to describe running, barefoot running,

27:44.180 --> 27:48.580
 to somebody better than any other I've seen.

27:48.580 --> 27:52.540
 So you run pretty good distances, and you bike,

27:52.540 --> 27:57.540
 and is there, you know, if we talk about bucket list items,

27:57.820 --> 28:00.220
 is there something crazy on your bucket list,

28:00.220 --> 28:02.780
 athletically, that you hope to do one day?

28:04.620 --> 28:07.180
 I mean, my commute is already a little crazy.

28:07.180 --> 28:09.020
 What are we talking about here?

28:09.020 --> 28:11.420
 What distance are we talking about?

28:11.420 --> 28:14.680
 Well, I live about 12 miles from MIT,

28:14.680 --> 28:16.620
 but you can find lots of different ways to get there.

28:16.620 --> 28:20.540
 So, I mean, I've run there for many years, I've biked there.

28:20.540 --> 28:21.460
 Old ways?

28:21.460 --> 28:23.900
 Yeah, but normally I would try to run in

28:23.900 --> 28:25.980
 and then bike home, bike in, run home.

28:25.980 --> 28:28.140
 But you have run there and back before?

28:28.140 --> 28:28.980
 Sure.

28:28.980 --> 28:29.820
 Barefoot?

28:29.820 --> 28:32.260
 Yeah, or with minimal shoes or whatever that.

28:32.260 --> 28:34.340
 12, 12 times two?

28:34.340 --> 28:35.180
 Yeah.

28:35.180 --> 28:36.020
 Okay.

28:36.020 --> 28:38.500
 It became kind of a game of how can I get to work?

28:38.500 --> 28:41.020
 I've rollerbladed, I've done all kinds of weird stuff,

28:41.020 --> 28:42.700
 but my favorite one these days,

28:42.700 --> 28:45.060
 I've been taking the Charles River to work.

28:45.060 --> 28:50.060
 So, I can put in the rowboat not so far from my house,

28:50.740 --> 28:53.300
 but the Charles River takes a long way to get to MIT,

28:53.300 --> 28:56.380
 so I can spend a long time getting there.

28:56.380 --> 28:59.640
 And it's not about, I don't know, it's just about,

29:01.620 --> 29:02.560
 I've had people ask me,

29:02.560 --> 29:04.460
 how can you justify taking that time?

29:05.820 --> 29:10.140
 But for me, it's just a magical time to think,

29:10.140 --> 29:12.320
 to compress, decompress.

29:13.740 --> 29:16.220
 Especially, I'll wake up, do a lot of work in the morning,

29:16.220 --> 29:19.180
 and then I kind of have to just let that settle

29:19.180 --> 29:20.700
 before I'm ready for all my meetings.

29:20.700 --> 29:23.160
 And then on the way home, it's a great time to sort of

29:23.160 --> 29:24.580
 let that settle.

29:24.580 --> 29:29.220
 You lead a large group of people.

29:31.860 --> 29:33.980
 Is there days where you're like,

29:33.980 --> 29:36.620
 oh shit, I gotta get to work in an hour?

29:36.620 --> 29:41.620
 Like, I mean, is there a tension there?

29:45.420 --> 29:47.940
 And like, if we look at the grand scheme of things,

29:47.940 --> 29:49.500
 just like you said, long term,

29:49.500 --> 29:51.700
 that meeting probably doesn't matter.

29:51.700 --> 29:54.660
 Like, you can always say, I'll just, I'll run

29:54.660 --> 29:57.100
 and let the meeting happen, how it happens.

29:57.100 --> 30:02.100
 Like, what, how do you, that zen, how do you,

30:02.200 --> 30:03.580
 what do you do with that tension

30:03.580 --> 30:05.620
 between the real world saying urgently,

30:05.620 --> 30:08.220
 you need to be there, this is important,

30:08.220 --> 30:10.060
 everything is melting down,

30:10.060 --> 30:11.820
 how are we gonna fix this robot?

30:11.820 --> 30:14.660
 There's this critical meeting,

30:14.660 --> 30:18.020
 and then there's this, the zen beauty of just running,

30:18.020 --> 30:20.420
 the simplicity of it, you along with nature.

30:21.380 --> 30:22.700
 What do you do with that?

30:22.700 --> 30:25.540
 I would say I'm not a fast runner, particularly.

30:25.540 --> 30:27.940
 Probably my fastest splits ever was when

30:27.940 --> 30:29.220
 I had to get to daycare on time

30:29.220 --> 30:30.700
 because they were gonna charge me, you know,

30:30.700 --> 30:33.540
 some dollar per minute that I was late.

30:33.540 --> 30:35.940
 I've run some fast splits to daycare.

30:36.980 --> 30:39.800
 But those times are past now.

30:41.700 --> 30:44.900
 I think work, you can find a work life balance in that way.

30:44.900 --> 30:46.100
 I think you just have to.

30:47.260 --> 30:48.620
 I think I am better at work

30:48.620 --> 30:52.180
 because I take time to think on the way in.

30:52.180 --> 30:54.380
 So I plan my day around it,

30:55.300 --> 31:00.300
 and I rarely feel that those are really at odds.

31:00.300 --> 31:03.380
 So what, the bucket list item.

31:03.380 --> 31:08.380
 If we're talking 12 times two, or approaching a marathon,

31:10.620 --> 31:15.060
 what, have you run an ultra marathon before?

31:15.060 --> 31:16.740
 Do you do races?

31:16.740 --> 31:17.580
 Is there, what's a...

31:17.580 --> 31:18.740
 Not to win.

31:21.620 --> 31:23.720
 I'm not gonna like take a dinghy across the Atlantic

31:23.720 --> 31:24.780
 or something if that's what you want.

31:24.780 --> 31:27.920
 But if someone does and wants to write a book,

31:27.920 --> 31:28.760
 I would totally read it

31:28.760 --> 31:31.140
 because I'm a sucker for that kind of thing.

31:31.140 --> 31:33.420
 No, I do have some fun things that I will try.

31:33.420 --> 31:35.300
 You know, I like to, when I travel,

31:35.300 --> 31:37.020
 I almost always bike to Logan Airport

31:37.020 --> 31:38.740
 and fold up a little folding bike

31:38.740 --> 31:41.040
 and then take it with me and bike to wherever I'm going.

31:41.040 --> 31:42.420
 And it's taken me,

31:42.420 --> 31:44.580
 or I'll take a stand up paddle board these days

31:44.580 --> 31:45.500
 on the airplane,

31:45.500 --> 31:47.100
 and then I'll try to paddle around where I'm going

31:47.100 --> 31:47.940
 or whatever.

31:47.940 --> 31:50.720
 And I've done some crazy things, but...

31:50.720 --> 31:55.140
 But not for the, you know, I now talk,

31:55.140 --> 31:57.500
 I don't know if you know who David Goggins is by any chance.

31:57.500 --> 31:58.460
 Not well, but yeah.

31:58.460 --> 32:00.140
 But I talk to him now every day.

32:00.140 --> 32:05.140
 So he's the person who made me do this stupid challenge.

32:05.940 --> 32:10.160
 So he's insane and he does things for the purpose

32:10.160 --> 32:11.380
 in the best kind of way.

32:11.380 --> 32:16.380
 He does things like for the explicit purpose of suffering.

32:16.980 --> 32:18.420
 Like he picks the thing that,

32:18.420 --> 32:21.500
 like whatever he thinks he can do, he does more.

32:22.940 --> 32:27.300
 So is that, do you have that thing in you or are you...

32:27.300 --> 32:28.940
 I think it's become the opposite.

32:29.820 --> 32:30.660
 It's a...

32:30.660 --> 32:32.300
 So you're like that dynamical system

32:32.300 --> 32:34.420
 that the walker, the efficient...

32:34.420 --> 32:37.880
 Yeah, it's leave no pain, right?

32:38.860 --> 32:40.900
 You should end feeling better than you started.

32:40.900 --> 32:41.720
 Okay.

32:41.720 --> 32:45.940
 But it's mostly, I think, and COVID has tested this

32:45.940 --> 32:47.740
 because I've lost my commute.

32:47.740 --> 32:51.980
 I think I'm perfectly happy walking around town

32:51.980 --> 32:55.220
 with my wife and kids if they could get them to go.

32:55.220 --> 32:57.780
 And it's more about just getting outside

32:57.780 --> 32:59.980
 and getting away from the keyboard for some time

32:59.980 --> 33:01.380
 just to let things compress.

33:02.580 --> 33:04.100
 Let's go into robotics a little bit.

33:04.100 --> 33:06.760
 What to use the most beautiful idea in robotics?

33:07.800 --> 33:09.660
 Whether we're talking about control

33:10.780 --> 33:12.740
 or whether we're talking about optimization

33:12.740 --> 33:16.180
 and the math side of things or the engineering side of things

33:16.180 --> 33:18.160
 or the philosophical side of things.

33:20.380 --> 33:23.540
 I think I've been lucky to experience something

33:23.540 --> 33:27.700
 that not so many roboticists have experienced,

33:27.700 --> 33:30.220
 which is to hang out

33:30.220 --> 33:33.540
 with some really amazing control theorists.

33:34.420 --> 33:39.420
 And the clarity of thought

33:40.700 --> 33:43.140
 that some of the more mathematical control theory

33:43.140 --> 33:47.500
 can bring to even very complex, messy looking problems

33:49.480 --> 33:53.140
 is really, it really had a big impact on me

33:53.140 --> 33:57.900
 and I had a day even just a couple of weeks ago

33:57.900 --> 34:01.020
 where I had spent the day on a Zoom robotics conference

34:01.020 --> 34:04.020
 having great conversations with lots of people.

34:04.020 --> 34:06.780
 Felt really good about the ideas

34:06.780 --> 34:09.500
 that were flowing and the like.

34:09.500 --> 34:12.940
 And then I had a late afternoon meeting

34:12.940 --> 34:15.540
 with one of my favorite control theorists

34:15.540 --> 34:20.540
 and we went from these abstract discussions

34:20.540 --> 34:25.540
 about maybes and what ifs and what a great idea

34:25.540 --> 34:29.140
 to these super precise statements

34:30.100 --> 34:33.660
 about systems that aren't that much more simple

34:33.660 --> 34:38.260
 or abstract than the ones I care about deeply.

34:38.260 --> 34:40.200
 And the contrast of that is,

34:42.540 --> 34:43.780
 I don't know, it really gets me.

34:43.780 --> 34:47.580
 I think people underestimate

34:47.580 --> 34:51.580
 maybe the power of clear thinking.

34:51.580 --> 34:56.580
 And so for instance, deep learning is amazing.

34:58.580 --> 35:00.380
 I use it heavily in our work.

35:00.380 --> 35:02.740
 I think it's changed the world, unquestionable.

35:04.700 --> 35:07.020
 It makes it easy to get things to work

35:07.020 --> 35:08.580
 without thinking as critically about it.

35:08.580 --> 35:11.300
 So I think one of the challenges as an educator

35:11.300 --> 35:14.940
 is to think about how do we make sure people get a taste

35:14.940 --> 35:17.860
 of the more rigorous thinking

35:17.860 --> 35:22.620
 that I think goes along with some different approaches.

35:22.620 --> 35:24.020
 Yeah, so that's really interesting.

35:24.020 --> 35:26.900
 So understanding like the fundamentals,

35:26.900 --> 35:31.900
 the first principles of the problem,

35:31.900 --> 35:33.780
 where in this case it's mechanics,

35:33.780 --> 35:38.780
 like how a thing moves, how a thing behaves,

35:38.780 --> 35:40.420
 like all the forces involved,

35:40.420 --> 35:42.740
 like really getting a deep understanding of that.

35:42.740 --> 35:45.340
 I mean, from physics, the first principle thing

35:45.340 --> 35:49.060
 come from physics, and here it's literally physics.

35:50.100 --> 35:51.940
 Yeah, and this applies, in deep learning,

35:51.940 --> 35:54.980
 this applies to not just, I mean,

35:54.980 --> 35:57.300
 it applies so cleanly in robotics,

35:57.300 --> 36:01.500
 but it also applies to just in any data set.

36:01.500 --> 36:05.100
 I find this true, I mean, driving as well.

36:05.100 --> 36:09.100
 There's a lot of folks in that work on autonomous vehicles

36:09.100 --> 36:14.100
 that work on autonomous vehicles that don't study driving,

36:17.900 --> 36:19.420
 like deeply.

36:20.300 --> 36:23.100
 I might be coming a little bit from the psychology side,

36:23.100 --> 36:28.100
 but I remember I spent a ridiculous number of hours

36:28.380 --> 36:31.940
 at lunch, at this like lawn chair,

36:31.940 --> 36:35.740
 and I would sit somewhere in MIT's campus,

36:35.740 --> 36:37.260
 there's a few interesting intersections,

36:37.260 --> 36:39.380
 and we'd just watch people cross.

36:39.380 --> 36:43.220
 So we were studying pedestrian behavior,

36:43.220 --> 36:46.220
 and I felt like, as we record a lot of video,

36:46.220 --> 36:47.820
 to try, and then there's the computer vision

36:47.820 --> 36:50.860
 extracts their movement, how they move their head, and so on,

36:50.860 --> 36:55.340
 but like every time, I felt like I didn't understand enough.

36:55.340 --> 36:58.620
 I just, I felt like I wasn't understanding

36:58.620 --> 37:01.620
 what, how are people signaling to each other,

37:01.620 --> 37:03.580
 what are they thinking,

37:03.580 --> 37:07.820
 how cognizant are they of their fear of death?

37:07.820 --> 37:11.900
 Like, what's the underlying game theory here?

37:11.900 --> 37:14.140
 What are the incentives?

37:14.140 --> 37:17.860
 And then I finally found a live stream of an intersection

37:17.860 --> 37:20.300
 that's like high def that I just, I would watch

37:20.300 --> 37:21.780
 so I wouldn't have to sit out there.

37:21.780 --> 37:23.580
 But it's interesting, so like, I feel.

37:23.580 --> 37:25.180
 But that's tough, that's a tough example,

37:25.180 --> 37:27.100
 because I mean, the learning.

37:27.100 --> 37:28.780
 Humans are involved.

37:28.780 --> 37:33.460
 Not just because human, but I think the learning mantra

37:33.460 --> 37:35.500
 is that basically the statistics of the data

37:35.500 --> 37:37.940
 will tell me things I need to know, right?

37:37.940 --> 37:41.860
 And, you know, for the example you gave

37:41.860 --> 37:45.420
 of all the nuances of, you know, eye contact,

37:45.420 --> 37:47.620
 or hand gestures, or whatever that are happening

37:47.620 --> 37:48.900
 for these subtle interactions

37:48.900 --> 37:51.140
 between pedestrians and traffic, right?

37:51.140 --> 37:54.460
 Maybe the data will tell that story.

37:54.460 --> 37:59.460
 I maybe even, one level more meta than what you're saying.

38:01.300 --> 38:02.660
 For a particular problem,

38:02.660 --> 38:03.820
 I think it might be the case

38:03.820 --> 38:06.020
 that data should tell us the story.

38:07.220 --> 38:09.420
 But I think there's a rigorous thinking

38:09.420 --> 38:11.700
 that is just an essential skill

38:11.700 --> 38:14.580
 for a mathematician or an engineer

38:14.580 --> 38:18.380
 that I just don't wanna lose it.

38:18.380 --> 38:22.460
 There are certainly super rigorous control,

38:22.460 --> 38:24.940
 or sorry, machine learning people.

38:24.940 --> 38:28.020
 I just think deep learning makes it so easy

38:28.020 --> 38:31.580
 to do some things that our next generation,

38:31.580 --> 38:35.860
 are not immediately rewarded

38:35.860 --> 38:38.540
 for going through some of the more rigorous approaches.

38:38.540 --> 38:40.740
 And then I wonder where that takes us.

38:40.740 --> 38:42.260
 Well, I'm actually optimistic about it.

38:42.260 --> 38:44.860
 I just want to do my part

38:44.860 --> 38:48.020
 to try to steer that rigorous thinking.

38:48.020 --> 38:50.940
 So there's like two questions I wanna ask.

38:50.940 --> 38:55.940
 Do you have sort of a good example of rigorous thinking

38:56.860 --> 39:00.860
 where it's easy to get lazy and not do the rigorous thinking?

39:00.860 --> 39:02.500
 And the other question I have is like,

39:02.500 --> 39:07.500
 do you have advice of how to practice rigorous thinking

39:09.140 --> 39:14.140
 in all the computer science disciplines that we've mentioned?

39:16.380 --> 39:21.380
 Yeah, I mean, there are times where problems

39:21.500 --> 39:24.820
 that can be solved with well known mature methods

39:25.860 --> 39:30.300
 could also be solved with a deep learning approach.

39:30.300 --> 39:35.300
 And there's an argument that you must use learning

39:36.740 --> 39:38.380
 even for the parts we already think we know,

39:38.380 --> 39:39.780
 because if the human has touched it,

39:39.780 --> 39:42.460
 then you've biased the system

39:42.460 --> 39:44.340
 and you've suddenly put a bottleneck in there

39:44.340 --> 39:46.300
 that is your own mental model.

39:46.300 --> 39:49.100
 But something like converting a matrix,

39:49.100 --> 39:50.780
 I think we know how to do that pretty well,

39:50.780 --> 39:52.020
 even if it's a pretty big matrix,

39:52.020 --> 39:53.140
 and we understand that pretty well.

39:53.140 --> 39:55.060
 And you could train a deep network to do it,

39:55.060 --> 39:57.340
 but you shouldn't probably.

39:57.340 --> 40:02.220
 So in that sense, rigorous thinking is understanding

40:02.220 --> 40:07.220
 the scope and the limitations of the methods that we have,

40:07.340 --> 40:10.180
 like how to use the tools of mathematics properly.

40:10.180 --> 40:15.100
 Yeah, I think taking a class on analysis

40:15.100 --> 40:18.620
 is all I'm sort of arguing is to take a chance to stop

40:18.620 --> 40:20.900
 and force yourself to think rigorously

40:20.900 --> 40:25.140
 about even the rational numbers or something.

40:25.140 --> 40:27.740
 It doesn't have to be the end all problem.

40:27.740 --> 40:31.100
 But that exercise of clear thinking,

40:31.100 --> 40:33.420
 I think goes a long way,

40:33.420 --> 40:35.260
 and I just wanna make sure we keep preaching it.

40:35.260 --> 40:36.380
 We don't lose it.

40:36.380 --> 40:39.540
 But do you think when you're doing rigorous thinking

40:39.540 --> 40:43.220
 or maybe trying to write down equations

40:43.220 --> 40:47.980
 or sort of explicitly formally describe a system,

40:47.980 --> 40:51.580
 do you think we naturally simplify things too much?

40:51.580 --> 40:53.500
 Is that a danger you run into?

40:53.500 --> 40:56.180
 Like in order to be able to understand something

40:56.180 --> 40:58.180
 about the system mathematically,

40:58.180 --> 41:01.700
 we make it too much of a toy example.

41:01.700 --> 41:04.460
 But I think that's the good stuff, right?

41:04.460 --> 41:07.060
 That's how you understand the fundamentals?

41:07.060 --> 41:07.900
 I think so.

41:07.900 --> 41:10.380
 I think maybe even that's a key to intelligence

41:10.380 --> 41:12.460
 or something, but I mean, okay,

41:12.460 --> 41:15.100
 what if Newton and Galileo had deep learning?

41:15.100 --> 41:18.340
 And they had done a bunch of experiments

41:18.340 --> 41:20.360
 and they told the world,

41:20.360 --> 41:22.460
 here's your weights of your neural network.

41:22.460 --> 41:24.260
 We've solved the problem.

41:24.260 --> 41:25.380
 Where would we be today?

41:25.380 --> 41:28.420
 I don't think we'd be as far as we are.

41:28.420 --> 41:29.260
 There's something to be said

41:29.260 --> 41:32.540
 about having the simplest explanation for a phenomenon.

41:32.540 --> 41:37.180
 So I don't doubt that we can train neural networks

41:37.180 --> 41:42.180
 to predict even physical F equals MA type equations.

41:46.300 --> 41:51.300
 But I maybe, I want another Newton to come along

41:51.300 --> 41:52.940
 because I think there's more to do

41:52.940 --> 41:56.020
 in terms of coming up with the simple models

41:56.020 --> 41:59.860
 for more complicated tasks.

41:59.860 --> 42:04.240
 Yeah, let's not offend AI systems from 50 years

42:04.240 --> 42:06.340
 from now that are listening to this

42:06.340 --> 42:08.260
 that are probably better at,

42:08.260 --> 42:10.180
 might be better coming up

42:10.180 --> 42:13.080
 with F equals MA equations themselves.

42:13.080 --> 42:16.940
 So sorry, I actually think learning is probably a route

42:16.940 --> 42:21.180
 to achieving this, but the representation matters, right?

42:21.180 --> 42:26.180
 And I think having a function that takes my inputs

42:26.200 --> 42:29.060
 to outputs that is arbitrarily complex

42:29.060 --> 42:30.780
 may not be the end goal.

42:30.780 --> 42:34.140
 I think there's still the most simple

42:34.140 --> 42:36.420
 or parsimonious explanation for the data.

42:37.620 --> 42:39.000
 Simple doesn't mean low dimensional.

42:39.000 --> 42:41.020
 That's one thing I think that we've,

42:41.020 --> 42:41.960
 a lesson that we've learned.

42:41.960 --> 42:46.080
 So a standard way to do model reduction

42:46.080 --> 42:47.860
 or system identification and controls

42:47.860 --> 42:50.460
 is the typical formulation is that you try to find

42:50.460 --> 42:54.220
 the minimal state dimension realization of a system

42:54.220 --> 42:57.760
 that hits some error bounds or something like that.

42:57.760 --> 43:00.340
 And that's maybe not, I think we're learning

43:00.340 --> 43:04.560
 that state dimension is not the right metric.

43:05.980 --> 43:06.820
 Of complexity.

43:06.820 --> 43:07.640
 Of complexity.

43:07.640 --> 43:09.460
 But for me, I think a lot about contact,

43:09.460 --> 43:10.820
 the mechanics of contact,

43:10.820 --> 43:13.660
 if a robot hand is picking up an object or something.

43:14.520 --> 43:17.220
 And when I write down the equations of motion for that,

43:17.220 --> 43:19.100
 they look incredibly complex,

43:19.100 --> 43:23.420
 not because, actually not so much

43:23.420 --> 43:26.660
 because of the dynamics of the hand when it's moving,

43:26.660 --> 43:28.500
 but it's just the interactions

43:28.500 --> 43:30.860
 and when they turn on and off, right?

43:30.860 --> 43:33.300
 So having a high dimensional,

43:33.300 --> 43:36.420
 but simple description of what's happening out here is fine.

43:36.420 --> 43:38.480
 But if when I actually start touching,

43:38.480 --> 43:41.860
 if I write down a different dynamical system

43:41.860 --> 43:45.420
 for every polygon on my robot hand

43:45.420 --> 43:47.300
 and every polygon on the object,

43:47.300 --> 43:49.000
 whether it's in contact or not,

43:49.000 --> 43:51.700
 with all the combinatorics that explodes there,

43:51.700 --> 43:54.460
 then that's too complex.

43:54.460 --> 43:55.800
 So I need to somehow summarize that

43:55.800 --> 44:00.800
 with a more intuitive physics way of thinking.

44:01.460 --> 44:03.500
 And yeah, I'm very optimistic

44:03.500 --> 44:05.700
 that machine learning will get us there.

44:05.700 --> 44:08.220
 First of all, I mean, I'll probably do it

44:08.220 --> 44:09.140
 in the introduction,

44:09.140 --> 44:12.900
 but you're one of the great robotics people at MIT.

44:12.900 --> 44:14.300
 You're a professor at MIT.

44:14.300 --> 44:16.480
 You've teach him a lot of amazing courses.

44:16.480 --> 44:19.180
 You run a large group

44:19.180 --> 44:22.780
 and you have a important history for MIT, I think,

44:22.780 --> 44:26.340
 as being a part of the DARPA Robotics Challenge.

44:26.340 --> 44:28.340
 Can you maybe first say,

44:28.340 --> 44:30.000
 what is the DARPA Robotics Challenge

44:30.000 --> 44:35.000
 and then tell your story around it, your journey with it?

44:36.380 --> 44:37.220
 Yeah, sure.

44:39.260 --> 44:41.060
 So the DARPA Robotics Challenge,

44:41.060 --> 44:44.720
 it came on the tails of the DARPA Grand Challenge

44:44.720 --> 44:45.940
 and DARPA Urban Challenge,

44:45.940 --> 44:48.420
 which were the challenges that brought us,

44:49.660 --> 44:52.740
 put a spotlight on self driving cars.

44:55.400 --> 45:00.400
 Gil Pratt was at DARPA and pitched a new challenge

45:01.360 --> 45:03.520
 that involved disaster response.

45:04.980 --> 45:07.140
 It didn't explicitly require humanoids,

45:07.140 --> 45:09.140
 although humanoids came into the picture.

45:10.220 --> 45:14.740
 This happened shortly after the Fukushima disaster in Japan

45:14.740 --> 45:17.660
 and our challenge was motivated roughly by that

45:17.660 --> 45:21.060
 because that was a case where if we had had robots

45:21.060 --> 45:22.700
 that were ready to be sent in,

45:22.700 --> 45:26.580
 there's a chance that we could have averted disaster.

45:26.580 --> 45:30.620
 And certainly after the, in the disaster response,

45:30.620 --> 45:32.380
 there were times we would have loved

45:32.380 --> 45:33.540
 to have sent robots in.

45:34.740 --> 45:39.220
 So in practice, what we ended up with was a grand challenge,

45:39.220 --> 45:41.180
 a DARPA Robotics Challenge,

45:41.180 --> 45:46.180
 where Boston Dynamics was to make humanoid robots.

45:48.660 --> 45:52.520
 People like me and the amazing team at MIT

45:53.660 --> 45:56.780
 were competing first in a simulation challenge

45:56.780 --> 45:59.460
 to try to be one of the ones that wins the right

45:59.460 --> 46:03.340
 to work on one of the Boston Dynamics humanoids

46:03.340 --> 46:06.620
 in order to compete in the final challenge,

46:06.620 --> 46:08.580
 which was a physical challenge.

46:08.580 --> 46:11.260
 And at that point, it was already, so it was decided

46:11.260 --> 46:13.420
 as humanoid robots early on.

46:13.420 --> 46:15.140
 There were two tracks.

46:15.140 --> 46:16.900
 You could enter as a hardware team

46:16.900 --> 46:18.480
 where you brought your own robot,

46:18.480 --> 46:21.380
 or you could enter through the virtual robotics challenge

46:21.380 --> 46:24.300
 as a software team that would try to win the right

46:24.300 --> 46:25.940
 to use one of the Boston Dynamics robots.

46:25.940 --> 46:27.420
 Sure, called Atlas.

46:27.420 --> 46:28.260
 Atlas.

46:28.260 --> 46:29.080
 Humanoid robots.

46:29.080 --> 46:31.500
 Yeah, it was a 400 pound Marvel,

46:31.500 --> 46:34.460
 but a pretty big, scary looking robot.

46:35.620 --> 46:36.700
 Expensive too.

46:36.700 --> 46:38.260
 Expensive, yeah.

46:38.260 --> 46:42.300
 Okay, so I mean, how did you feel

46:42.300 --> 46:44.780
 at the prospect of this kind of challenge?

46:44.780 --> 46:48.820
 I mean, it seems autonomous vehicles,

46:48.820 --> 46:51.060
 yeah, I guess that sounds hard,

46:51.060 --> 46:53.980
 but not really from a robotics perspective.

46:53.980 --> 46:56.020
 It's like, didn't they do it in the 80s

46:56.020 --> 46:57.820
 is the kind of feeling I would have,

46:58.760 --> 47:00.820
 like when you first look at the problem,

47:00.820 --> 47:04.900
 it's on wheels, but like humanoid robots,

47:04.900 --> 47:07.060
 that sounds really hard.

47:07.060 --> 47:12.060
 So what are your, psychologically speaking,

47:12.860 --> 47:15.780
 what were you feeling, excited, scared?

47:15.780 --> 47:18.020
 Why the heck did you get yourself involved

47:18.020 --> 47:19.660
 in this kind of messy challenge?

47:19.660 --> 47:23.260
 We didn't really know for sure what we were signing up for

47:24.540 --> 47:26.820
 in the sense that you could have something that,

47:26.820 --> 47:30.780
 as it was described in the call for participation,

47:30.780 --> 47:33.900
 that could have put a huge emphasis on the dynamics

47:33.900 --> 47:35.700
 of walking and not falling down

47:35.700 --> 47:37.380
 and walking over rough terrain,

47:37.380 --> 47:38.580
 or the same description,

47:38.580 --> 47:40.780
 because the robot had to go into this disaster area

47:40.780 --> 47:44.580
 and turn valves and pick up a drill,

47:44.580 --> 47:45.780
 it cut the hole through a wall,

47:45.780 --> 47:48.420
 it had to do some interesting things.

47:48.420 --> 47:51.860
 The challenge could have really highlighted perception

47:51.860 --> 47:54.820
 and autonomous planning,

47:54.820 --> 47:59.820
 or it ended up that locomoting over complex terrain

48:01.060 --> 48:03.600
 played a pretty big role in the competition.

48:03.600 --> 48:05.520
 So...

48:05.520 --> 48:08.360
 And the degree of autonomy wasn't clear.

48:08.360 --> 48:09.560
 The degree of autonomy

48:09.560 --> 48:11.920
 was always a central part of the discussion.

48:11.920 --> 48:15.560
 So what wasn't clear was how we would be able,

48:15.560 --> 48:17.520
 how far we'd be able to get with it.

48:17.520 --> 48:21.640
 So the idea was always that you want semi autonomy,

48:21.640 --> 48:24.280
 that you want the robot to have enough compute

48:24.280 --> 48:27.640
 that you can have a degraded network link to a human.

48:27.640 --> 48:30.640
 And so the same way we had degraded networks

48:30.640 --> 48:33.160
 at many natural disasters,

48:33.160 --> 48:34.960
 you'd send your robot in,

48:34.960 --> 48:37.540
 you'd be able to get a few bits back and forth,

48:37.540 --> 48:38.920
 but you don't get to have enough

48:38.920 --> 48:42.080
 potentially to fully operate the robot

48:42.080 --> 48:43.480
 in every joint of the robot.

48:44.600 --> 48:46.160
 So, and then the question was,

48:46.160 --> 48:48.880
 and the gamesmanship of the organizers

48:48.880 --> 48:50.680
 was to figure out what we're capable of,

48:50.680 --> 48:52.600
 push us as far as we could,

48:52.600 --> 48:55.300
 so that it would differentiate the teams

48:55.300 --> 48:57.540
 that put more autonomy on the robot

48:57.540 --> 48:59.400
 and had a few clicks and just said,

48:59.400 --> 49:00.920
 go there, do this, go there, do this,

49:00.920 --> 49:03.400
 versus someone who's picking every footstep

49:03.400 --> 49:05.280
 or something like that.

49:05.280 --> 49:08.920
 So what were some memories,

49:10.760 --> 49:13.620
 painful, triumphant from the experience?

49:13.620 --> 49:15.040
 Like what was that journey?

49:15.040 --> 49:17.680
 Maybe if you can dig in a little deeper,

49:17.680 --> 49:21.120
 maybe even on the technical side, on the team side,

49:21.120 --> 49:23.020
 that whole process of,

49:24.120 --> 49:28.200
 from the early idea stages to actually competing.

49:28.200 --> 49:31.680
 I mean, this was a defining experience for me.

49:31.680 --> 49:33.940
 It came at the right time for me in my career.

49:33.940 --> 49:37.480
 I had gotten tenure before I was due a sabbatical,

49:37.480 --> 49:39.840
 and most people do something relaxing

49:39.840 --> 49:41.920
 and restorative for a sabbatical.

49:41.920 --> 49:44.520
 So you got tenure before this?

49:44.520 --> 49:46.200
 Yeah, yeah, yeah.

49:46.200 --> 49:48.120
 It was a good time for me.

49:48.120 --> 49:50.960
 We had a bunch of algorithms that we were very happy with.

49:50.960 --> 49:52.560
 We wanted to see how far we could push them,

49:52.560 --> 49:54.920
 and this was a chance to really test our mettle

49:54.920 --> 49:56.880
 to do more proper software engineering.

49:56.880 --> 50:01.420
 So the team, we all just worked our butts off.

50:01.420 --> 50:04.880
 We were in that lab almost all the time.

50:07.680 --> 50:09.600
 Okay, so there were some, of course,

50:09.600 --> 50:12.080
 high highs and low lows throughout that.

50:12.080 --> 50:13.720
 Anytime you're not sleeping

50:13.720 --> 50:16.400
 and devoting your life to a 400 pound humanoid.

50:18.320 --> 50:20.720
 I remember actually one funny moment

50:20.720 --> 50:21.940
 where we're all super tired,

50:21.940 --> 50:24.760
 and so Atlas had to walk across cinder blocks.

50:24.760 --> 50:26.520
 That was one of the obstacles.

50:26.520 --> 50:28.240
 And I remember Atlas was powered down

50:28.240 --> 50:31.280
 and hanging limp on its harness,

50:31.280 --> 50:34.000
 and the humans were there picking up

50:34.000 --> 50:35.200
 and laying the brick down

50:35.200 --> 50:36.440
 so that the robot could walk over it.

50:36.440 --> 50:38.240
 And I thought, what is wrong with this?

50:38.240 --> 50:41.560
 We've got a robot just watching us

50:41.560 --> 50:42.500
 do all the manual labor

50:42.500 --> 50:47.040
 so that it can take its little stroll across the train.

50:47.040 --> 50:52.040
 But I mean, even the virtual robotics challenge

50:52.120 --> 50:54.640
 was super nerve wracking and dramatic.

50:54.640 --> 50:59.640
 I remember, so we were using Gazebo as a simulator

51:01.520 --> 51:02.360
 on the cloud,

51:02.360 --> 51:03.920
 and there was all these interesting challenges.

51:03.920 --> 51:08.560
 I think the investment that OSR FC,

51:08.560 --> 51:10.020
 whatever they were called at that time,

51:10.020 --> 51:12.220
 Brian Gerkey's team at Open Source Robotics,

51:14.160 --> 51:16.000
 they were pushing on the capabilities of Gazebo

51:16.000 --> 51:20.380
 in order to scale it to the complexity of these challenges.

51:20.380 --> 51:23.900
 So, you know, up to the virtual competition.

51:23.900 --> 51:26.220
 So the virtual competition was,

51:26.220 --> 51:28.480
 you will sign on at a certain time

51:28.480 --> 51:29.840
 and we'll have a network connection

51:29.840 --> 51:32.080
 to another machine on the cloud

51:32.080 --> 51:34.880
 that is running the simulator of your robot.

51:34.880 --> 51:38.160
 And your controller will run on this computer

51:38.160 --> 51:40.920
 and the physics will run on the other

51:40.920 --> 51:43.060
 and you have to connect.

51:43.060 --> 51:48.060
 Now, the physics, they wanted it to run at real time rates

51:48.140 --> 51:50.740
 because there was an element of human interaction.

51:50.740 --> 51:53.280
 And humans, if you do want to teleop,

51:53.280 --> 51:56.120
 it works way better if it's at frame rate.

51:56.120 --> 51:57.120
 Oh, cool.

51:57.120 --> 51:58.720
 But it was very hard to simulate

51:58.720 --> 52:03.240
 these complex scenes at real time rate.

52:03.240 --> 52:06.520
 So right up to like days before the competition,

52:06.520 --> 52:11.040
 the simulator wasn't quite at real time rate.

52:11.040 --> 52:13.280
 And that was great for me because my controller

52:13.280 --> 52:16.280
 was solving a pretty big optimization problem

52:16.280 --> 52:17.760
 and it wasn't quite at real time rate.

52:17.760 --> 52:18.880
 So I was fine.

52:18.880 --> 52:20.480
 I was keeping up with the simulator.

52:20.480 --> 52:22.880
 We were both running at about 0.7.

52:22.880 --> 52:24.960
 And I remember getting this email.

52:24.960 --> 52:28.440
 And by the way, the perception folks on our team hated

52:28.440 --> 52:31.440
 that they knew that if my controller was too slow,

52:31.440 --> 52:32.520
 the robot was gonna fall down.

52:32.520 --> 52:34.920
 And no matter how good their perception system was,

52:34.920 --> 52:36.940
 if I can't make my controller fast.

52:36.940 --> 52:37.920
 Anyways, we get this email

52:37.920 --> 52:40.480
 like three days before the virtual competition.

52:40.480 --> 52:41.480
 It's for all the marbles.

52:41.480 --> 52:44.920
 We're gonna either get a humanoid robot or we're not.

52:44.920 --> 52:45.740
 And we get an email saying,

52:45.740 --> 52:48.680
 good news, we made the robot, the simulator faster.

52:48.680 --> 52:50.560
 It's now at one point.

52:50.560 --> 52:54.800
 And I was just like, oh man, what are we gonna do here?

52:54.800 --> 52:58.420
 So that came in late at night for me.

52:59.520 --> 53:00.560
 A few days ahead.

53:00.560 --> 53:01.440
 A few days ahead.

53:01.440 --> 53:04.000
 I went over, it happened at Frank Permenter,

53:04.000 --> 53:06.800
 who's a very, very sharp.

53:06.800 --> 53:10.240
 He was a student at the time working on optimization.

53:11.160 --> 53:12.180
 He was still in lab.

53:13.640 --> 53:16.680
 Frank, we need to make the quadratic programming solver

53:16.680 --> 53:18.360
 faster, not like a little faster.

53:18.360 --> 53:22.600
 It's actually, you know, and we wrote a new solver

53:22.600 --> 53:26.600
 for that QP together that night.

53:28.160 --> 53:29.400
 It was terrifying.

53:29.400 --> 53:31.920
 So there's a really hard optimization problem

53:31.920 --> 53:33.520
 that you're constantly solving.

53:34.480 --> 53:36.820
 You didn't make the optimization problem simpler?

53:36.820 --> 53:38.480
 You wrote a new solver?

53:38.480 --> 53:42.840
 So, I mean, your observation is almost spot on.

53:42.840 --> 53:44.520
 What we did was what everybody,

53:44.520 --> 53:45.800
 I mean, people know how to do this,

53:45.800 --> 53:49.240
 but we had not yet done this idea of warm starting.

53:49.240 --> 53:51.320
 So we are solving a big optimization problem

53:51.320 --> 53:52.680
 at every time step.

53:52.680 --> 53:54.280
 But if you're running fast enough,

53:54.280 --> 53:55.680
 the optimization problem you're solving

53:55.680 --> 53:57.920
 on the last time step is pretty similar

53:57.920 --> 54:00.040
 to the optimization you're gonna solve with the next.

54:00.040 --> 54:02.240
 We had course had told our commercial solver

54:02.240 --> 54:05.520
 to use warm starting, but even the interface

54:05.520 --> 54:09.840
 to that commercial solver was causing us these delays.

54:09.840 --> 54:12.740
 So what we did was we basically wrote,

54:12.740 --> 54:15.360
 we called it fast QP at the time.

54:15.360 --> 54:18.480
 We wrote a very lightweight, very fast layer,

54:18.480 --> 54:22.120
 which would basically check if nearby solutions

54:22.120 --> 54:24.240
 to the quadratic program were,

54:24.240 --> 54:26.560
 which were very easily checked,

54:26.560 --> 54:28.000
 could stabilize the robot.

54:28.000 --> 54:30.720
 And if they couldn't, we would fall back to the solver.

54:30.720 --> 54:33.120
 You couldn't really test this well, right?

54:33.120 --> 54:33.960
 Or like?

54:33.960 --> 54:37.360
 I mean, so we always knew that if we fell back to,

54:37.360 --> 54:40.440
 if we, it got to the point where if for some reason

54:40.440 --> 54:42.840
 things slowed down and we fell back to the original solver,

54:42.840 --> 54:46.040
 the robot would actually literally fall down.

54:46.040 --> 54:49.360
 So it was a harrowing sort of edge we were,

54:49.360 --> 54:51.200
 ledge we were sort of on.

54:51.200 --> 54:53.200
 But I mean, it actually,

54:53.200 --> 54:55.840
 like the 400 pound human could come crashing to the ground

54:55.840 --> 54:58.020
 if your solver's not fast enough.

54:58.880 --> 55:01.900
 But you know, we had lots of good experiences.

55:01.900 --> 55:05.600
 So can I ask you a weird question I get

55:06.640 --> 55:09.440
 about idea of hard work?

55:09.440 --> 55:14.320
 So actually people, like students of yours

55:14.320 --> 55:17.040
 that I've interacted with and just,

55:17.040 --> 55:19.400
 and robotics people in general,

55:19.400 --> 55:23.400
 but they have moments,

55:23.400 --> 55:28.360
 at moments have worked harder than most people I know

55:28.360 --> 55:30.600
 in terms of, if you look at different disciplines

55:30.600 --> 55:32.360
 of how hard people work.

55:32.360 --> 55:34.560
 But they're also like the happiest.

55:34.560 --> 55:37.000
 Like, just like, I don't know.

55:37.000 --> 55:39.200
 It's the same thing with like running.

55:39.200 --> 55:41.380
 People that push themselves to like the limit,

55:41.380 --> 55:44.760
 they also seem to be like the most like full of life

55:44.760 --> 55:45.600
 somehow.

55:46.720 --> 55:48.680
 And I get often criticized like,

55:48.680 --> 55:50.420
 you're not getting enough sleep.

55:50.420 --> 55:52.000
 What are you doing to your body?

55:52.000 --> 55:54.680
 Blah, blah, blah, like this kind of stuff.

55:54.680 --> 55:58.040
 And I usually just kind of respond like,

55:58.040 --> 55:59.720
 I'm doing what I love.

55:59.720 --> 56:00.920
 I'm passionate about it.

56:00.920 --> 56:01.760
 I love it.

56:01.760 --> 56:04.800
 I feel like it's, it's invigorating.

56:04.800 --> 56:07.640
 I actually think, I don't think the lack of sleep

56:07.640 --> 56:08.860
 is what hurts you.

56:08.860 --> 56:12.040
 I think what hurts you is stress and lack of doing things

56:12.040 --> 56:13.280
 that you're passionate about.

56:13.280 --> 56:14.920
 But in this world, yeah, I mean,

56:14.920 --> 56:19.920
 can you comment about why the heck robotics people

56:20.720 --> 56:25.720
 are willing to push themselves to that degree?

56:26.200 --> 56:27.680
 Is there value in that?

56:27.680 --> 56:29.400
 And why are they so happy?

56:30.360 --> 56:31.920
 I think, I think you got it right.

56:31.920 --> 56:36.440
 I mean, I think the causality is not that we work hard.

56:36.440 --> 56:38.500
 And I think other disciplines work very hard too,

56:38.500 --> 56:40.300
 but it's, I don't think it's that we work hard

56:40.300 --> 56:43.160
 and therefore we are happy.

56:43.160 --> 56:44.700
 I think we found something

56:44.700 --> 56:46.600
 that we're truly passionate about.

56:48.080 --> 56:49.960
 It makes us very happy.

56:49.960 --> 56:52.280
 And then we get a little involved with it

56:52.280 --> 56:54.600
 and spend a lot of time on it.

56:54.600 --> 56:55.980
 What a luxury to have something

56:55.980 --> 56:58.240
 that you wanna spend all your time on, right?

56:59.140 --> 57:00.800
 We could talk about this for many hours,

57:00.800 --> 57:03.880
 but maybe if we could pick,

57:03.880 --> 57:05.480
 is there something on the technical side

57:05.480 --> 57:08.260
 on the approach that you took that's interesting

57:08.260 --> 57:10.240
 that turned out to be a terrible failure

57:10.240 --> 57:13.800
 or a success that you carry into your work today

57:13.800 --> 57:17.260
 about all the different ideas that were involved

57:17.260 --> 57:22.260
 in making, whether in the simulation or in the real world,

57:23.400 --> 57:25.520
 making this semi autonomous system work?

57:25.520 --> 57:30.520
 I mean, it really did teach me something fundamental

57:30.880 --> 57:33.560
 about what it's gonna take to get robustness

57:33.560 --> 57:35.320
 out of a system of this complexity.

57:35.320 --> 57:37.720
 I would say the DARPA challenge

57:37.720 --> 57:41.040
 really was foundational in my thinking.

57:41.040 --> 57:43.720
 I think the autonomous driving community thinks about this.

57:43.720 --> 57:45.580
 I think lots of people thinking

57:45.580 --> 57:47.080
 about safety critical systems

57:47.080 --> 57:48.920
 that might have machine learning in the loop

57:48.920 --> 57:50.360
 are thinking about these questions.

57:50.360 --> 57:53.340
 For me, the DARPA challenge was the moment

57:53.340 --> 57:57.480
 where I realized we've spent every waking minute

57:57.480 --> 57:58.920
 running this robot.

57:58.920 --> 58:01.440
 And again, for the physical competition,

58:01.440 --> 58:02.540
 days before the competition,

58:02.540 --> 58:04.440
 we saw the robot fall down in a way

58:04.440 --> 58:05.980
 it had never fallen down before.

58:05.980 --> 58:09.260
 I thought, how could we have found that?

58:10.520 --> 58:13.600
 We only have one robot, it's running almost all the time.

58:13.600 --> 58:15.560
 We just didn't have enough hours in the day

58:15.560 --> 58:17.120
 to test that robot.

58:17.120 --> 58:19.380
 Something has to change, right?

58:19.380 --> 58:21.080
 And then I think that, I mean,

58:21.080 --> 58:24.880
 I would say that the team that won was,

58:24.880 --> 58:28.020
 from KAIST, was the team that had two robots

58:28.020 --> 58:30.560
 and was able to do not only incredible engineering,

58:30.560 --> 58:33.240
 just absolutely top rate engineering,

58:33.240 --> 58:36.080
 but also they were able to test at a rate

58:36.080 --> 58:39.600
 and discipline that we didn't keep up with.

58:39.600 --> 58:41.120
 What does testing look like?

58:41.120 --> 58:42.280
 What are we talking about here?

58:42.280 --> 58:45.000
 Like, what's a loop of tests?

58:45.000 --> 58:48.800
 Like from start to finish, what is a loop of testing?

58:48.800 --> 58:51.880
 Yeah, I mean, I think there's a whole philosophy to testing.

58:51.880 --> 58:54.440
 There's the unit tests, and you can do that on a hardware,

58:54.440 --> 58:56.360
 you can do that in a small piece of code.

58:56.360 --> 58:58.280
 You write one function, you should write a test

58:58.280 --> 59:00.620
 that checks that function's input and outputs.

59:00.620 --> 59:02.440
 You should also write an integration test

59:02.440 --> 59:05.320
 at the other extreme of running the whole system together,

59:05.320 --> 59:09.120
 where they try to turn on all of the different functions

59:09.120 --> 59:11.560
 that you think are correct.

59:11.560 --> 59:13.400
 It's much harder to write the specifications

59:13.400 --> 59:14.520
 for a system level test,

59:14.520 --> 59:17.360
 especially if that system is as complicated

59:17.360 --> 59:18.460
 as a humanoid robot.

59:18.460 --> 59:21.040
 But the philosophy is sort of the same.

59:21.040 --> 59:24.160
 On the real robot, it's no different,

59:24.160 --> 59:26.040
 but on a real robot,

59:26.040 --> 59:28.640
 it's impossible to run the same experiment twice.

59:28.640 --> 59:32.480
 So if you see a failure,

59:32.480 --> 59:34.380
 you hope you caught something in the logs

59:34.380 --> 59:35.620
 that tell you what happened,

59:35.620 --> 59:36.920
 but you'd probably never be able to run

59:36.920 --> 59:38.480
 exactly that experiment again.

59:39.400 --> 59:44.400
 And right now, I think our philosophy is just,

59:45.720 --> 59:47.880
 basically Monte Carlo estimation,

59:47.880 --> 59:50.880
 is just run as many experiments as we can,

59:50.880 --> 59:53.080
 maybe try to set up the environment

59:53.080 --> 59:58.080
 to make the things we are worried about happen

59:58.120 --> 59:59.880
 as often as possible.

59:59.880 --> 1:00:02.280
 But really we're relying on somewhat random search

1:00:02.280 --> 1:00:03.180
 in order to test.

1:00:04.220 --> 1:00:05.480
 Maybe that's all we'll ever be able to,

1:00:05.480 --> 1:00:07.320
 but I think, you know,

1:00:07.320 --> 1:00:10.520
 cause there's an argument that the things that'll get you

1:00:10.520 --> 1:00:14.040
 are the things that are really nuanced in the world.

1:00:14.040 --> 1:00:15.700
 And there'd be very hard to, for instance,

1:00:15.700 --> 1:00:16.880
 put back in a simulation.

1:00:16.880 --> 1:00:19.880
 Yeah, I guess the edge cases.

1:00:19.880 --> 1:00:21.840
 What was the hardest thing?

1:00:21.840 --> 1:00:24.680
 Like, so you said walking over rough terrain,

1:00:24.680 --> 1:00:27.120
 like just taking footsteps.

1:00:27.120 --> 1:00:31.360
 I mean, people, it's so dramatic and painful

1:00:31.360 --> 1:00:33.520
 in a certain kind of way to watch these videos

1:00:33.520 --> 1:00:37.600
 from the DRC of robots falling.

1:00:37.600 --> 1:00:38.440
 Yep.

1:00:38.440 --> 1:00:39.440
 It's just so heartbreaking.

1:00:39.440 --> 1:00:40.280
 I don't know.

1:00:40.280 --> 1:00:42.400
 Maybe it's because for me at least,

1:00:42.400 --> 1:00:45.120
 we anthropomorphize the robot.

1:00:45.120 --> 1:00:48.400
 Of course, it's also funny for some reason,

1:00:48.400 --> 1:00:51.920
 like humans falling is funny for, I don't,

1:00:51.920 --> 1:00:53.400
 it's some dark reason.

1:00:53.400 --> 1:00:55.300
 I'm not sure why it is so,

1:00:55.300 --> 1:00:57.880
 but it's also like tragic and painful.

1:00:57.880 --> 1:01:00.380
 And so speaking of which, I mean,

1:01:00.380 --> 1:01:05.000
 what made the robots fall and fail in your view?

1:01:05.000 --> 1:01:06.960
 So I can tell you exactly what happened on our,

1:01:06.960 --> 1:01:08.360
 we, I contributed one of those.

1:01:08.360 --> 1:01:10.960
 Our team contributed one of those spectacular falls.

1:01:10.960 --> 1:01:15.560
 Every one of those falls has a complicated story.

1:01:15.560 --> 1:01:16.920
 I mean, at one time,

1:01:16.920 --> 1:01:19.160
 the power effectively went out on the robot

1:01:20.200 --> 1:01:21.720
 because it had been sitting at the door

1:01:21.720 --> 1:01:24.400
 waiting for a green light to be able to proceed

1:01:24.400 --> 1:01:26.280
 and its batteries, you know,

1:01:26.280 --> 1:01:28.080
 and therefore it just fell backwards

1:01:28.080 --> 1:01:29.280
 and smashed its head against the ground.

1:01:29.280 --> 1:01:30.120
 And it was hilarious,

1:01:30.120 --> 1:01:32.760
 but it wasn't because of bad software, right?

1:01:34.100 --> 1:01:37.120
 But for ours, so the hardest part of the challenge,

1:01:37.120 --> 1:01:40.400
 the hardest task in my view was getting out of the Polaris.

1:01:40.400 --> 1:01:43.760
 It was actually relatively easy to drive the Polaris.

1:01:43.760 --> 1:01:44.600
 Can you tell the story?

1:01:44.600 --> 1:01:45.440
 Sorry to interrupt.

1:01:45.440 --> 1:01:46.920
 The story of the car.

1:01:50.040 --> 1:01:51.240
 People should watch this video.

1:01:51.240 --> 1:01:53.900
 I mean, the thing you've come up with is just brilliant,

1:01:53.900 --> 1:01:55.920
 but anyway, sorry, what's...

1:01:55.920 --> 1:01:56.920
 Yeah, we kind of joke.

1:01:56.920 --> 1:01:59.040
 We call it the big robot, little car problem

1:01:59.040 --> 1:02:03.440
 because somehow the race organizers decided

1:02:03.440 --> 1:02:05.360
 to give us a 400 pound humanoid.

1:02:05.360 --> 1:02:07.480
 And then they also provided the vehicle,

1:02:07.480 --> 1:02:08.640
 which was a little Polaris.

1:02:08.640 --> 1:02:11.760
 And the robot didn't really fit in the car.

1:02:11.760 --> 1:02:14.520
 So you couldn't drive the car with your feet

1:02:14.520 --> 1:02:15.720
 under the steering column.

1:02:15.720 --> 1:02:20.240
 We actually had to straddle the main column of the,

1:02:21.280 --> 1:02:23.580
 and have basically one foot in the passenger seat,

1:02:23.580 --> 1:02:25.280
 one foot in the driver's seat,

1:02:25.280 --> 1:02:27.620
 and then drive with our left hand.

1:02:28.880 --> 1:02:31.300
 But the hard part was we had to then park the car,

1:02:31.300 --> 1:02:33.080
 get out of the car.

1:02:33.080 --> 1:02:34.320
 It didn't have a door, that was okay.

1:02:34.320 --> 1:02:38.720
 But it's just getting up from crouched, from sitting,

1:02:38.720 --> 1:02:41.880
 when you're in this very constrained environment.

1:02:41.880 --> 1:02:44.320
 First of all, I remember after watching those videos,

1:02:44.320 --> 1:02:47.840
 I was much more cognizant of how hard it is for me

1:02:47.840 --> 1:02:49.600
 to get in and out of the car,

1:02:49.600 --> 1:02:51.760
 and out of the car, especially.

1:02:51.760 --> 1:02:54.240
 It's actually a really difficult control problem.

1:02:54.240 --> 1:02:55.480
 Yeah.

1:02:55.480 --> 1:02:58.360
 I'm very cognizant of it when I'm like injured

1:02:58.360 --> 1:02:59.200
 for whatever reason.

1:02:59.200 --> 1:03:00.120
 Oh, that's really hard.

1:03:00.120 --> 1:03:01.440
 Yeah.

1:03:01.440 --> 1:03:03.560
 So how did you approach this problem?

1:03:03.560 --> 1:03:08.160
 So we had, you think of NASA's operations,

1:03:08.160 --> 1:03:09.800
 and they have these checklists,

1:03:09.800 --> 1:03:11.080
 prelaunched checklists and the like.

1:03:11.080 --> 1:03:12.380
 We weren't far off from that.

1:03:12.380 --> 1:03:13.500
 We had this big checklist.

1:03:13.500 --> 1:03:16.320
 And on the first day of the competition,

1:03:16.320 --> 1:03:17.520
 we were running down our checklist.

1:03:17.520 --> 1:03:19.120
 And one of the things we had to do,

1:03:19.120 --> 1:03:21.320
 we had to turn off the controller,

1:03:21.320 --> 1:03:23.320
 the piece of software that was running

1:03:23.320 --> 1:03:25.560
 that would drive the left foot of the robot

1:03:25.560 --> 1:03:28.120
 in order to accelerate on the gas.

1:03:28.120 --> 1:03:30.840
 And then we turned on our balancing controller.

1:03:30.840 --> 1:03:34.280
 And the nerves, jitters of the first day of the competition,

1:03:34.280 --> 1:03:35.660
 someone forgot to check that box

1:03:35.660 --> 1:03:37.560
 and turn that controller off.

1:03:37.560 --> 1:03:40.880
 So we used a lot of motion planning

1:03:40.880 --> 1:03:45.320
 to figure out a sort of configuration of the robot

1:03:45.320 --> 1:03:47.200
 that we could get up and over.

1:03:47.200 --> 1:03:49.440
 We relied heavily on our balancing controller.

1:03:50.320 --> 1:03:53.760
 And basically, when the robot was in one

1:03:53.760 --> 1:03:57.560
 of its most precarious sort of configurations,

1:03:57.560 --> 1:04:00.920
 trying to sneak its big leg out of the side,

1:04:01.800 --> 1:04:05.000
 the other controller that thought it was still driving

1:04:05.000 --> 1:04:06.760
 told its left foot to go like this.

1:04:06.760 --> 1:04:09.620
 And that wasn't good.

1:04:11.000 --> 1:04:13.320
 But it turned disastrous for us

1:04:13.320 --> 1:04:16.980
 because what happened was a little bit of push here.

1:04:16.980 --> 1:04:21.080
 Actually, we have videos of us running into the robot

1:04:21.080 --> 1:04:24.680
 with a 10 foot pole and it kind of will recover.

1:04:24.680 --> 1:04:27.800
 But this is a case where there's no space to recover.

1:04:27.800 --> 1:04:30.180
 So a lot of our secondary balancing mechanisms

1:04:30.180 --> 1:04:32.160
 about like take a step to recover,

1:04:32.160 --> 1:04:33.760
 they were all disabled because we were in the car

1:04:33.760 --> 1:04:35.320
 and there was no place to step.

1:04:35.320 --> 1:04:38.380
 So we were relying on our just lowest level reflexes.

1:04:38.380 --> 1:04:42.200
 And even then, I think just hitting the foot on the seat,

1:04:42.200 --> 1:04:44.960
 on the floor, we probably could have recovered from it.

1:04:44.960 --> 1:04:46.400
 But the thing that was bad that happened

1:04:46.400 --> 1:04:49.440
 is when we did that and we jostled a little bit,

1:04:49.440 --> 1:04:53.720
 the tailbone of our robot was only a little off the seat,

1:04:53.720 --> 1:04:54.600
 it hit the seat.

1:04:55.480 --> 1:04:58.260
 And the other foot came off the ground just a little bit.

1:04:58.260 --> 1:05:02.280
 And nothing in our plans had ever told us what to do

1:05:02.280 --> 1:05:05.120
 if your butt's on the seat and your feet are in the air.

1:05:05.120 --> 1:05:06.040
 Feet in the air.

1:05:06.040 --> 1:05:10.080
 And then the thing is once you get off the script,

1:05:10.080 --> 1:05:11.040
 things can go very wrong

1:05:11.040 --> 1:05:12.760
 because even our state estimation,

1:05:12.760 --> 1:05:15.200
 our system that was trying to collect all the data

1:05:15.200 --> 1:05:16.760
 from the sensors and understand

1:05:16.760 --> 1:05:18.480
 what's happening with the robot,

1:05:18.480 --> 1:05:20.080
 it didn't know about this situation.

1:05:20.080 --> 1:05:22.800
 So it was predicting things that were just wrong.

1:05:22.800 --> 1:05:26.560
 And then we did a violent shake and fell off

1:05:26.560 --> 1:05:29.180
 in our face first out of the robot.

1:05:29.180 --> 1:05:32.520
 But like into the destination.

1:05:32.520 --> 1:05:35.360
 That's true, we fell in, we got our point for egress.

1:05:36.320 --> 1:05:39.280
 But so is there any hope for, that's interesting,

1:05:39.280 --> 1:05:43.280
 is there any hope for Atlas to be able to do something

1:05:43.280 --> 1:05:46.320
 when it's just on its butt and feet in the air?

1:05:46.320 --> 1:05:47.200
 Absolutely.

1:05:47.200 --> 1:05:48.520
 So you can, what do you?

1:05:48.520 --> 1:05:50.920
 No, so that is one of the big challenges.

1:05:50.920 --> 1:05:53.840
 And I think it's still true, you know,

1:05:53.840 --> 1:05:58.840
 Boston Dynamics and Antimal and there's this incredible work

1:05:59.120 --> 1:06:02.000
 on legged robots happening around the world.

1:06:04.540 --> 1:06:07.620
 Most of them still are very good at the case

1:06:07.620 --> 1:06:10.080
 where you're making contact with the world at your feet.

1:06:10.080 --> 1:06:12.200
 And they have typically point feet relatively,

1:06:12.200 --> 1:06:14.480
 they have balls on their feet, for instance.

1:06:14.480 --> 1:06:16.600
 If those robots get in a situation

1:06:16.600 --> 1:06:19.880
 where the elbow hits the wall or something like this,

1:06:19.880 --> 1:06:21.240
 that's a pretty different situation.

1:06:21.240 --> 1:06:24.080
 Now they have layers of mechanisms that will make,

1:06:24.080 --> 1:06:27.680
 I think the more mature solutions have ways

1:06:27.680 --> 1:06:31.240
 in which the controller won't do stupid things.

1:06:31.240 --> 1:06:34.720
 But a human, for instance, is able to leverage

1:06:34.720 --> 1:06:36.760
 incidental contact in order to accomplish a goal.

1:06:36.760 --> 1:06:37.800
 In fact, I might, if you push me,

1:06:37.800 --> 1:06:39.720
 I might actually put my hand out

1:06:39.720 --> 1:06:42.220
 and make a new brand new contact.

1:06:42.220 --> 1:06:44.940
 The feet of the robot are doing this on quadrupeds,

1:06:44.940 --> 1:06:49.120
 but we mostly in robotics are afraid of contact

1:06:49.120 --> 1:06:52.040
 on the rest of our body, which is crazy.

1:06:53.180 --> 1:06:56.040
 There's this whole field of motion planning,

1:06:56.040 --> 1:06:58.040
 collision free motion planning.

1:06:58.040 --> 1:06:59.800
 And we write very complex algorithms

1:06:59.800 --> 1:07:01.640
 so that the robot can dance around

1:07:01.640 --> 1:07:04.100
 and make sure it doesn't touch the world.

1:07:05.840 --> 1:07:07.720
 So people are just afraid of contact

1:07:07.720 --> 1:07:09.880
 because contact the scene is a difficult.

1:07:09.880 --> 1:07:13.380
 It's still a difficult control problem and sensing problem.

1:07:13.380 --> 1:07:18.380
 Now you're a serious person, I'm a little bit of an idiot

1:07:21.180 --> 1:07:24.140
 and I'm going to ask you some dumb questions.

1:07:24.140 --> 1:07:27.140
 So I do martial arts.

1:07:27.140 --> 1:07:30.380
 So like jiu jitsu, I wrestled my whole life.

1:07:30.380 --> 1:07:35.380
 So let me ask the question, like whenever people learn

1:07:35.380 --> 1:07:38.500
 that I do any kind of AI or like I mentioned robots

1:07:38.500 --> 1:07:40.040
 and things like that, they say,

1:07:40.040 --> 1:07:45.020
 when are we going to have robots that can win

1:07:45.020 --> 1:07:49.020
 in a wrestling match or in a fight against a human?

1:07:49.880 --> 1:07:52.160
 So we just mentioned sitting on your butt,

1:07:52.160 --> 1:07:53.940
 if you're in the air, that's a common position.

1:07:53.940 --> 1:07:55.420
 Jiu jitsu, when you're on the ground,

1:07:55.420 --> 1:07:57.600
 you're a down opponent.

1:07:59.100 --> 1:08:03.800
 Like how difficult do you think is the problem?

1:08:03.800 --> 1:08:06.880
 And when will we have a robot that can defeat a human

1:08:06.880 --> 1:08:08.580
 in a wrestling match?

1:08:08.580 --> 1:08:11.100
 And we're talking about a lot, like, I don't know

1:08:11.100 --> 1:08:13.940
 if you're familiar with wrestling, but essentially.

1:08:15.340 --> 1:08:16.180
 Not very.

1:08:16.180 --> 1:08:19.580
 It's basically the art of contact.

1:08:19.580 --> 1:08:24.580
 It's like, it's because you're picking contact points

1:08:24.580 --> 1:08:29.300
 and then using like leverage like to off balance

1:08:29.300 --> 1:08:33.940
 to trick people, like you make them feel

1:08:33.940 --> 1:08:35.620
 like you're doing one thing

1:08:35.620 --> 1:08:38.840
 and then they change their balance

1:08:38.840 --> 1:08:41.620
 and then you switch what you're doing

1:08:41.620 --> 1:08:44.100
 and then results in a throw or whatever.

1:08:44.100 --> 1:08:48.540
 So like, it's basically the art of multiple contacts.

1:08:48.540 --> 1:08:49.380
 So.

1:08:49.380 --> 1:08:50.820
 Awesome, that's a nice description of it.

1:08:50.820 --> 1:08:53.040
 So there's also an opponent in there, right?

1:08:53.040 --> 1:08:54.180
 So if.

1:08:54.180 --> 1:08:55.060
 Very dynamic.

1:08:55.060 --> 1:08:58.520
 Right, if you are wrestling a human

1:08:58.520 --> 1:09:02.900
 and are in a game theoretic situation with a human,

1:09:02.900 --> 1:09:07.900
 that's still hard, but just to speak to the, you know,

1:09:08.220 --> 1:09:11.340
 quickly reasoning about contact part of it, for instance.

1:09:11.340 --> 1:09:13.380
 Yeah, maybe even throwing the game theory out of it,

1:09:13.380 --> 1:09:17.700
 almost like, yeah, almost like a non dynamic opponent.

1:09:17.700 --> 1:09:20.060
 Right, there's reasons to be optimistic,

1:09:20.060 --> 1:09:22.660
 but I think our best understanding of those problems

1:09:22.660 --> 1:09:23.920
 are still pretty hard.

1:09:24.820 --> 1:09:29.860
 I have been increasingly focused on manipulation,

1:09:29.860 --> 1:09:31.720
 partly where that's a case where the contact

1:09:31.720 --> 1:09:33.180
 has to be much more rich.

1:09:35.800 --> 1:09:38.260
 And there are some really impressive examples

1:09:38.260 --> 1:09:41.820
 of deep learning policies, controllers

1:09:41.820 --> 1:09:46.820
 that can appear to do good things through contact.

1:09:47.860 --> 1:09:51.380
 We've even got new examples of, you know,

1:09:51.380 --> 1:09:53.940
 deep learning models of predicting what's gonna happen

1:09:53.940 --> 1:09:56.220
 to objects as they go through contact.

1:09:56.220 --> 1:09:59.780
 But I think the challenge you just offered there

1:09:59.780 --> 1:10:01.500
 still eludes us, right?

1:10:01.500 --> 1:10:03.620
 The ability to make a decision

1:10:03.620 --> 1:10:05.320
 based on those models quickly.

1:10:07.560 --> 1:10:10.140
 You know, I have to think though, it's hard for humans too,

1:10:10.140 --> 1:10:11.380
 when you get that complicated.

1:10:11.380 --> 1:10:16.100
 I think probably you had maybe a slow motion version

1:10:16.100 --> 1:10:17.980
 of where you learned the basic skills

1:10:17.980 --> 1:10:20.700
 and you've probably gotten better at it

1:10:20.700 --> 1:10:24.660
 and there's much more subtle to you.

1:10:24.660 --> 1:10:27.940
 But it might still be hard to actually, you know,

1:10:27.940 --> 1:10:32.140
 really on the fly take a, you know, model of your humanoid

1:10:32.140 --> 1:10:35.260
 and figure out how to plan the optimal sequence.

1:10:35.260 --> 1:10:36.660
 That might be a problem we never solve.

1:10:36.660 --> 1:10:40.360
 Well, the, I mean, one of the most amazing things to me

1:10:40.360 --> 1:10:43.740
 about the, we can talk about martial arts.

1:10:43.740 --> 1:10:45.340
 We could also talk about dancing.

1:10:45.340 --> 1:10:46.740
 Doesn't really matter.

1:10:46.740 --> 1:10:50.540
 Too human, I think it's the most interesting study

1:10:50.540 --> 1:10:51.380
 of contact.

1:10:51.380 --> 1:10:53.040
 It's not even the dynamic element of it.

1:10:53.040 --> 1:10:58.040
 It's the, like when you get good at it, it's so effortless.

1:10:58.740 --> 1:11:00.900
 Like I can just, I'm very cognizant

1:11:00.900 --> 1:11:03.380
 of the entirety of the learning process

1:11:03.380 --> 1:11:07.660
 being essentially like learning how to move my body

1:11:07.660 --> 1:11:12.220
 in a way that I could throw very large weights

1:11:12.220 --> 1:11:17.220
 around effortlessly, like, and I can feel the learning.

1:11:18.500 --> 1:11:21.540
 Like I'm a huge believer in drilling of techniques

1:11:21.540 --> 1:11:23.580
 and you can just like feel your, I don't,

1:11:23.580 --> 1:11:26.780
 you're not feeling, you're feeling, sorry,

1:11:26.780 --> 1:11:29.800
 you're learning it intellectually a little bit,

1:11:29.800 --> 1:11:32.820
 but a lot of it is the body learning it somehow,

1:11:32.820 --> 1:11:36.100
 like instinctually and whatever that learning is,

1:11:36.100 --> 1:11:40.780
 that's really, I'm not even sure if that's equivalent

1:11:40.780 --> 1:11:44.760
 to like a deep learning, learning a controller.

1:11:44.760 --> 1:11:46.820
 I think it's something more,

1:11:46.820 --> 1:11:49.720
 it feels like there's a lot of distributed learning

1:11:49.720 --> 1:11:50.560
 going on.

1:11:50.560 --> 1:11:54.520
 Yeah, I think there's hierarchy and composition

1:11:56.440 --> 1:11:59.920
 probably in the systems that we don't capture very well yet.

1:12:00.840 --> 1:12:02.440
 You have layers of control systems.

1:12:02.440 --> 1:12:03.960
 You have reflexes at the bottom layer

1:12:03.960 --> 1:12:07.440
 and you have a system that's capable

1:12:07.440 --> 1:12:11.320
 of planning a vacation to some distant country,

1:12:11.320 --> 1:12:14.240
 which is probably, you probably don't have a controller,

1:12:14.240 --> 1:12:18.260
 a policy for every possible destination you'll ever pick.

1:12:18.260 --> 1:12:19.100
 Right?

1:12:20.380 --> 1:12:23.460
 But there's something magical in the in between

1:12:23.460 --> 1:12:26.340
 and how do you go from these low level feedback loops

1:12:26.340 --> 1:12:30.020
 to something that feels like a pretty complex

1:12:30.020 --> 1:12:31.060
 set of outcomes.

1:12:32.740 --> 1:12:34.760
 You know, my guess is, I think there's evidence

1:12:34.760 --> 1:12:37.620
 that you can plan at some of these levels, right?

1:12:37.620 --> 1:12:41.740
 So Josh Tenenbaum just showed it in his talk the other day.

1:12:41.740 --> 1:12:43.320
 He's got a game he likes to talk about.

1:12:43.320 --> 1:12:46.700
 I think he calls it the pick three game or something,

1:12:46.700 --> 1:12:50.740
 where he puts a bunch of clutter down in front of a person

1:12:50.740 --> 1:12:52.380
 and he says, okay, pick three objects.

1:12:52.380 --> 1:12:55.700
 And it might be a telephone or a shoe

1:12:55.700 --> 1:12:58.940
 or a Kleenex box or whatever.

1:12:59.880 --> 1:13:01.820
 And apparently you pick three items and then you pick,

1:13:01.820 --> 1:13:04.100
 he says, okay, pick the first one up with your right hand,

1:13:04.100 --> 1:13:06.360
 the second one up with your left hand.

1:13:06.360 --> 1:13:08.860
 Now using those objects, now as tools,

1:13:08.860 --> 1:13:10.100
 pick up the third object.

1:13:11.060 --> 1:13:15.700
 Right, so that's down at the level of physics

1:13:15.700 --> 1:13:17.140
 and mechanics and contact mechanics

1:13:17.140 --> 1:13:21.880
 that I think we do learning or we do have policies for,

1:13:21.880 --> 1:13:24.740
 we do control for, almost feedback,

1:13:24.740 --> 1:13:26.300
 but somehow we're able to still,

1:13:26.300 --> 1:13:28.420
 I mean, I've never picked up a telephone

1:13:28.420 --> 1:13:30.220
 with a shoe and a water bottle before.

1:13:30.220 --> 1:13:33.140
 And somehow, and it takes me a little longer to do that

1:13:33.140 --> 1:13:35.180
 the first time, but most of the time

1:13:35.180 --> 1:13:37.260
 we can sort of figure that out.

1:13:37.260 --> 1:13:41.940
 So yeah, I think the amazing thing is this ability

1:13:41.940 --> 1:13:44.100
 to be flexible with our models,

1:13:44.100 --> 1:13:48.700
 plan when we need to use our well oiled controllers

1:13:48.700 --> 1:13:51.820
 when we don't, when we're in familiar territory.

1:13:53.280 --> 1:13:55.560
 Having models, I think the other thing you just said

1:13:55.560 --> 1:13:58.140
 was something about, I think your awareness

1:13:58.140 --> 1:13:59.860
 of what's happening is even changing

1:13:59.860 --> 1:14:02.380
 as you improve your expertise, right?

1:14:02.380 --> 1:14:04.980
 So maybe you have a very approximate model

1:14:04.980 --> 1:14:06.240
 of the mechanics to begin with.

1:14:06.240 --> 1:14:09.300
 And as you gain expertise,

1:14:09.300 --> 1:14:11.920
 you get a more refined version of that model.

1:14:11.920 --> 1:14:16.920
 You're aware of muscles or balance components

1:14:17.100 --> 1:14:19.700
 that you just weren't even aware of before.

1:14:19.700 --> 1:14:21.740
 So how do you scaffold that?

1:14:21.740 --> 1:14:24.180
 Yeah, plus the fear of injury,

1:14:24.180 --> 1:14:28.780
 the ambition of goals, of excelling,

1:14:28.780 --> 1:14:32.020
 and fear of mortality.

1:14:32.020 --> 1:14:33.340
 Let's see, what else is in there?

1:14:33.340 --> 1:14:38.040
 As the motivations, overinflated ego in the beginning,

1:14:38.040 --> 1:14:42.900
 and then a crash of confidence in the middle.

1:14:42.900 --> 1:14:46.700
 All of those seem to be essential for the learning process.

1:14:46.700 --> 1:14:48.140
 And if all that's good,

1:14:48.140 --> 1:14:50.500
 then you're probably optimizing energy efficiency.

1:14:50.500 --> 1:14:53.080
 Yeah, right, so we have to get that right.

1:14:53.080 --> 1:14:58.080
 So there was this idea that you would have robots

1:14:58.580 --> 1:15:03.580
 play soccer better than human players by 2050.

1:15:03.780 --> 1:15:05.300
 That was the goal.

1:15:05.300 --> 1:15:10.140
 Basically, it was the goal to beat world champion team,

1:15:10.140 --> 1:15:13.340
 to become a world cup, beat like a world cup level team.

1:15:13.340 --> 1:15:15.900
 So are we gonna see that first?

1:15:15.900 --> 1:15:19.580
 Or a robot, if you're familiar,

1:15:19.580 --> 1:15:23.440
 there's an organization called UFC for mixed martial arts.

1:15:23.440 --> 1:15:27.100
 Are we gonna see a world cup championship soccer team

1:15:27.100 --> 1:15:32.100
 that have robots, or a UFC champion mixed martial artist

1:15:32.660 --> 1:15:33.860
 as a robot?

1:15:33.860 --> 1:15:37.140
 I mean, it's very hard to say one thing is harder,

1:15:37.140 --> 1:15:38.580
 some problem is harder than the other.

1:15:38.580 --> 1:15:43.580
 What probably matters is who started the organization that,

1:15:44.980 --> 1:15:47.140
 I mean, I think RoboCup has a pretty serious following,

1:15:47.140 --> 1:15:50.860
 and there is a history now of people playing that game,

1:15:50.860 --> 1:15:53.620
 learning about that game, building robots to play that game,

1:15:53.620 --> 1:15:55.820
 building increasingly more human robots.

1:15:55.820 --> 1:15:57.020
 It's got momentum.

1:15:57.020 --> 1:16:00.900
 So if you want to have mixed martial arts compete,

1:16:00.900 --> 1:16:04.000
 you better start your organization now, right?

1:16:05.460 --> 1:16:07.740
 I think almost independent of which problem

1:16:07.740 --> 1:16:08.660
 is technically harder,

1:16:08.660 --> 1:16:11.400
 because they're both hard and they're both different.

1:16:11.400 --> 1:16:12.240
 That's a good point.

1:16:12.240 --> 1:16:14.700
 I mean, those videos are just hilarious,

1:16:14.700 --> 1:16:17.140
 like especially the humanoid robots

1:16:17.140 --> 1:16:21.260
 trying to play soccer.

1:16:21.260 --> 1:16:23.420
 I mean, they're kind of terrible right now.

1:16:23.420 --> 1:16:26.020
 I mean, I guess there is robo sumo wrestling.

1:16:26.020 --> 1:16:28.740
 There's like the robo one competitions,

1:16:28.740 --> 1:16:31.140
 where they do have these robots that go on the table

1:16:31.140 --> 1:16:32.100
 and basically fight.

1:16:32.100 --> 1:16:33.720
 So maybe I'm wrong, maybe.

1:16:33.720 --> 1:16:37.140
 First of all, do you have a year in mind for RoboCup,

1:16:37.140 --> 1:16:39.100
 just from a robotics perspective?

1:16:39.100 --> 1:16:42.060
 Seems like a super exciting possibility

1:16:42.060 --> 1:16:46.340
 that like in the physical space,

1:16:46.340 --> 1:16:47.620
 this is what's interesting.

1:16:47.620 --> 1:16:50.560
 I think the world is captivated.

1:16:50.560 --> 1:16:52.620
 I think it's really exciting.

1:16:52.620 --> 1:16:56.400
 It inspires just a huge number of people

1:16:56.400 --> 1:17:01.400
 when a machine beats a human at a game

1:17:01.460 --> 1:17:03.460
 that humans are really damn good at.

1:17:03.460 --> 1:17:05.740
 So you're talking about chess and go,

1:17:05.740 --> 1:17:09.820
 but that's in the world of digital.

1:17:09.820 --> 1:17:13.320
 I don't think machines have beat humans

1:17:13.320 --> 1:17:16.020
 at a game in the physical space yet,

1:17:16.020 --> 1:17:17.700
 but that would be just.

1:17:17.700 --> 1:17:20.340
 You have to make the rules very carefully, right?

1:17:20.340 --> 1:17:22.980
 I mean, if Atlas kicked me in the shins, I'm down

1:17:22.980 --> 1:17:25.440
 and game over.

1:17:25.440 --> 1:17:30.440
 So it's very subtle on what's fair.

1:17:31.220 --> 1:17:33.020
 I think the fighting one is a weird one.

1:17:33.020 --> 1:17:35.180
 Yeah, because you're talking about a machine

1:17:35.180 --> 1:17:36.500
 that's much stronger than you.

1:17:36.500 --> 1:17:39.740
 But yeah, in terms of soccer, basketball, all those kinds.

1:17:39.740 --> 1:17:40.580
 Even soccer, right?

1:17:40.580 --> 1:17:43.500
 I mean, as soon as there's contact or whatever,

1:17:43.500 --> 1:17:46.540
 and there are some things that the robot will do better.

1:17:46.540 --> 1:17:51.540
 I think if you really set yourself up to try to see

1:17:51.540 --> 1:17:53.140
 could robots win the game of soccer

1:17:53.140 --> 1:17:56.300
 as the rules were written, the right thing

1:17:56.300 --> 1:17:58.060
 for the robot to do is to play very differently

1:17:58.060 --> 1:17:59.680
 than a human would play.

1:17:59.680 --> 1:18:04.060
 You're not gonna get the perfect soccer player robot.

1:18:04.060 --> 1:18:07.900
 You're gonna get something that exploits the rules,

1:18:07.900 --> 1:18:12.220
 exploits its super actuators, its super low bandwidth

1:18:13.420 --> 1:18:15.340
 feedback loops or whatever, and it's gonna play the game

1:18:15.340 --> 1:18:17.540
 differently than you want it to play.

1:18:17.540 --> 1:18:21.380
 And I bet there's ways, I bet there's loopholes, right?

1:18:21.380 --> 1:18:26.380
 We saw that in the DARPA challenge that it's very hard

1:18:27.060 --> 1:18:29.420
 to write a set of rules that someone can't find

1:18:30.660 --> 1:18:32.860
 a way to exploit.

1:18:32.860 --> 1:18:35.020
 Let me ask another ridiculous question.

1:18:35.020 --> 1:18:37.980
 I think this might be the last ridiculous question,

1:18:37.980 --> 1:18:39.220
 but I doubt it.

1:18:39.220 --> 1:18:44.220
 I aspire to ask as many ridiculous questions

1:18:44.540 --> 1:18:48.060
 of a brilliant MIT professor.

1:18:48.060 --> 1:18:52.440
 Okay, I don't know if you've seen the black mirror.

1:18:53.660 --> 1:18:56.740
 It's funny, I never watched the episode.

1:18:56.740 --> 1:19:00.620
 I know when it happened though, because I gave a talk

1:19:00.620 --> 1:19:05.380
 to some MIT faculty one day on a unassuming Monday

1:19:05.380 --> 1:19:08.500
 or whatever I was telling him about the state of robotics.

1:19:08.500 --> 1:19:10.740
 And I showed some video from Boston Dynamics

1:19:10.740 --> 1:19:13.940
 of the quadruped spot at the time.

1:19:13.940 --> 1:19:15.900
 It was the early version of spot.

1:19:15.900 --> 1:19:19.300
 And there was a look of horror that went across the room.

1:19:19.300 --> 1:19:23.220
 And I said, I've shown videos like this a lot of times,

1:19:23.220 --> 1:19:24.060
 what happened?

1:19:24.060 --> 1:19:26.780
 And it turns out that this video had gone,

1:19:26.780 --> 1:19:28.380
 this black mirror episode had changed

1:19:28.380 --> 1:19:33.180
 the way people watched the videos I was putting out.

1:19:33.180 --> 1:19:34.740
 The way they see these kinds of robots.

1:19:34.740 --> 1:19:37.780
 So I talked to so many people who are just terrified

1:19:37.780 --> 1:19:41.020
 because of that episode probably of these kinds of robots.

1:19:41.020 --> 1:19:44.540
 I almost wanna say that they almost enjoy being terrified.

1:19:44.540 --> 1:19:47.100
 I don't even know what it is about human psychology

1:19:47.100 --> 1:19:49.220
 that kind of imagine doomsday,

1:19:49.220 --> 1:19:52.780
 the destruction of the universe or our society

1:19:52.780 --> 1:19:57.340
 and kind of like enjoy being afraid.

1:19:57.340 --> 1:19:59.300
 I don't wanna simplify it, but it feels like

1:19:59.300 --> 1:20:01.020
 they talk about it so often.

1:20:01.020 --> 1:20:06.020
 It almost, there does seem to be an addictive quality to it.

1:20:06.380 --> 1:20:09.500
 I talked to a guy, a guy named Joe Rogan,

1:20:09.500 --> 1:20:11.580
 who's kind of the flag bearer

1:20:11.580 --> 1:20:14.660
 for being terrified at these robots.

1:20:14.660 --> 1:20:17.340
 Do you have two questions?

1:20:17.340 --> 1:20:18.620
 One, do you have an understanding

1:20:18.620 --> 1:20:21.700
 of why people are afraid of robots?

1:20:21.700 --> 1:20:24.940
 And the second question is in black mirror,

1:20:24.940 --> 1:20:26.380
 just to tell you the episode,

1:20:26.380 --> 1:20:28.180
 I don't even remember it that much anymore,

1:20:28.180 --> 1:20:31.100
 but these robots, I think they can shoot

1:20:31.100 --> 1:20:32.820
 like a pellet or something.

1:20:32.820 --> 1:20:36.540
 They basically have, it's basically a spot with a gun.

1:20:36.540 --> 1:20:41.540
 And how far are we away from having robots

1:20:41.940 --> 1:20:44.100
 that go rogue like that?

1:20:44.100 --> 1:20:48.460
 Basically spot that goes rogue for some reason

1:20:48.460 --> 1:20:49.980
 and somehow finds a gun.

1:20:51.300 --> 1:20:56.300
 Right, so, I mean, I'm not a psychologist.

1:20:56.420 --> 1:20:58.580
 I think, I don't know exactly why

1:20:59.860 --> 1:21:01.700
 people react the way they do.

1:21:01.700 --> 1:21:06.700
 I think we have to be careful about the way robots influence

1:21:06.700 --> 1:21:07.980
 our society and the like.

1:21:07.980 --> 1:21:09.860
 I think that's something, that's a responsibility

1:21:09.860 --> 1:21:12.260
 that roboticists need to embrace.

1:21:13.260 --> 1:21:15.460
 I don't think robots are gonna come after me

1:21:15.460 --> 1:21:18.460
 with a kitchen knife or a pellet gun right away.

1:21:18.460 --> 1:21:21.420
 And I mean, if they were programmed in such a way,

1:21:21.420 --> 1:21:25.940
 but I used to joke with Atlas that all I had to do

1:21:25.940 --> 1:21:28.340
 was run for five minutes and its battery would run out.

1:21:28.340 --> 1:21:30.620
 But actually they've got to be careful

1:21:30.620 --> 1:21:32.460
 and actually they've got a very big battery

1:21:32.460 --> 1:21:33.300
 in there by the end.

1:21:33.300 --> 1:21:34.500
 So it was over an hour.

1:21:37.220 --> 1:21:39.420
 I think the fear is a bit cultural though.

1:21:39.420 --> 1:21:44.420
 Cause I mean, you notice that, like, I think in my age,

1:21:45.140 --> 1:21:48.260
 in the US, we grew up watching Terminator, right?

1:21:48.260 --> 1:21:50.500
 If I had grown up at the same time in Japan,

1:21:50.500 --> 1:21:52.740
 I probably would have been watching Astro Boy.

1:21:52.740 --> 1:21:55.860
 And there's a very different reaction to robots

1:21:55.860 --> 1:21:57.460
 in different countries, right?

1:21:57.460 --> 1:22:02.460
 So I don't know if it's a human innate fear of metal marvels

1:22:02.620 --> 1:22:06.420
 or if it's something that we've done to ourselves

1:22:06.420 --> 1:22:07.460
 with our sci fi.

1:22:09.860 --> 1:22:12.580
 Yeah, the stories we tell ourselves through movies,

1:22:12.580 --> 1:22:16.780
 through just through popular media.

1:22:16.780 --> 1:22:21.100
 But if I were to tell, you know, if you were my therapist

1:22:21.100 --> 1:22:24.900
 and I said, I'm really terrified that we're going

1:22:24.900 --> 1:22:29.300
 to have these robots very soon that will hurt us.

1:22:30.900 --> 1:22:35.600
 Like, how do you approach making me feel better?

1:22:36.620 --> 1:22:39.580
 Like, why shouldn't people be afraid?

1:22:39.580 --> 1:22:41.380
 There's a, I think there's a video

1:22:41.380 --> 1:22:44.500
 that went viral recently.

1:22:44.500 --> 1:22:46.900
 Everything, everything was spot in Boston,

1:22:46.900 --> 1:22:48.380
 which goes viral in general.

1:22:48.380 --> 1:22:50.060
 But usually it's like really cool stuff.

1:22:50.060 --> 1:22:51.420
 Like they're doing flips and stuff

1:22:51.420 --> 1:22:56.140
 or like sad stuff, the Atlas being hit with a broomstick

1:22:56.140 --> 1:22:57.300
 or something like that.

1:22:57.300 --> 1:23:02.300
 But there's a video where I think one of the new productions

1:23:02.420 --> 1:23:04.620
 bought robots, which are awesome.

1:23:04.620 --> 1:23:08.540
 It was like patrolling somewhere in like in some country.

1:23:08.540 --> 1:23:11.920
 And like people immediately were like saying like,

1:23:11.920 --> 1:23:14.580
 this is like the dystopian future,

1:23:14.580 --> 1:23:16.380
 like the surveillance state.

1:23:16.380 --> 1:23:18.940
 For some reason, like you can just have a camera,

1:23:18.940 --> 1:23:23.420
 like something about spot being able to walk on four feet

1:23:23.420 --> 1:23:25.940
 with like really terrified people.

1:23:25.940 --> 1:23:30.940
 So like, what do you say to those people?

1:23:31.060 --> 1:23:33.820
 I think there is a legitimate fear there

1:23:33.820 --> 1:23:36.160
 because so much of our future is uncertain.

1:23:37.840 --> 1:23:40.140
 But at the same time, technically speaking,

1:23:40.140 --> 1:23:41.920
 it seems like we're not there yet.

1:23:41.920 --> 1:23:42.820
 So what do you say?

1:23:42.820 --> 1:23:47.820
 I mean, I think technology is complicated.

1:23:48.580 --> 1:23:49.940
 It can be used in many ways.

1:23:49.940 --> 1:23:53.340
 I think there are purely software attacks

1:23:56.360 --> 1:23:59.000
 that somebody could use to do great damage.

1:23:59.000 --> 1:24:01.480
 Maybe they have already, you know,

1:24:01.480 --> 1:24:06.480
 I think wheeled robots could be used in bad ways too.

1:24:08.340 --> 1:24:09.180
 Drones.

1:24:09.180 --> 1:24:14.180
 Drones, right, I don't think that, let's see.

1:24:16.340 --> 1:24:19.920
 I don't want to be building technology

1:24:19.920 --> 1:24:21.860
 just because I'm compelled to build technology

1:24:21.860 --> 1:24:23.580
 and I don't think about it.

1:24:23.580 --> 1:24:27.740
 But I would consider myself a technological optimist,

1:24:27.740 --> 1:24:32.220
 I guess, in the sense that I think we should continue

1:24:32.220 --> 1:24:37.220
 to create and evolve and our world will change.

1:24:37.220 --> 1:24:40.780
 And if we will introduce new challenges,

1:24:40.780 --> 1:24:42.900
 we'll screw something up maybe,

1:24:42.900 --> 1:24:46.220
 but I think also we'll invent ourselves

1:24:46.220 --> 1:24:49.380
 out of those challenges and life will go on.

1:24:49.380 --> 1:24:51.580
 So it's interesting because you didn't mention

1:24:51.580 --> 1:24:54.540
 like this is technically too hard.

1:24:54.540 --> 1:24:57.380
 I don't think robots are, I think people attribute

1:24:57.380 --> 1:24:59.140
 a robot that looks like an animal

1:24:59.140 --> 1:25:02.140
 as maybe having a level of self awareness

1:25:02.140 --> 1:25:05.460
 or consciousness or something that they don't have yet.

1:25:05.460 --> 1:25:09.380
 Right, so it's not, I think our ability

1:25:09.380 --> 1:25:12.780
 to anthropomorphize those robots is probably,

1:25:13.700 --> 1:25:16.540
 we're assuming that they have a level of intelligence

1:25:16.540 --> 1:25:17.940
 that they don't yet have.

1:25:17.940 --> 1:25:20.060
 And that might be part of the fear.

1:25:20.060 --> 1:25:22.260
 So in that sense, it's too hard.

1:25:22.260 --> 1:25:25.540
 But, you know, there are many scary things in the world.

1:25:25.540 --> 1:25:29.860
 Right, so I think we're right to ask those questions.

1:25:29.860 --> 1:25:33.600
 We're right to think about the implications of our work.

1:25:33.600 --> 1:25:38.600
 Right, in the short term as we're working on it for sure,

1:25:39.720 --> 1:25:43.840
 is there something long term that scares you

1:25:43.840 --> 1:25:47.680
 about our future with AI and robots?

1:25:47.680 --> 1:25:52.400
 A lot of folks from Elon Musk to Sam Harris

1:25:52.400 --> 1:25:56.860
 to a lot of folks talk about the existential threats

1:25:56.860 --> 1:25:58.880
 about artificial intelligence.

1:25:58.880 --> 1:26:03.680
 Oftentimes, robots kind of inspire that the most

1:26:03.680 --> 1:26:05.840
 because of the anthropomorphism.

1:26:05.840 --> 1:26:07.400
 Do you have any fears?

1:26:07.400 --> 1:26:09.000
 It's an important question.

1:26:12.120 --> 1:26:14.920
 I actually, I think I like Rod Brooks answer

1:26:14.920 --> 1:26:17.080
 maybe the best on this, I think.

1:26:17.080 --> 1:26:19.320
 And it's not the only answer he's given over the years,

1:26:19.320 --> 1:26:24.320
 but maybe one of my favorites is he says,

1:26:24.360 --> 1:26:25.920
 it's not gonna be, he's got a book,

1:26:25.920 --> 1:26:29.960
 Flesh and Machines, I believe, it's not gonna be

1:26:29.960 --> 1:26:31.880
 the robots versus the people,

1:26:31.880 --> 1:26:34.240
 we're all gonna be robot people.

1:26:34.240 --> 1:26:38.000
 Because, you know, we already have smartphones,

1:26:38.000 --> 1:26:41.120
 some of us have serious technology implanted

1:26:41.120 --> 1:26:43.780
 in our bodies already, whether we have a hearing aid

1:26:43.780 --> 1:26:46.360
 or a pacemaker or anything like this,

1:26:47.800 --> 1:26:50.880
 people with amputations might have prosthetics.

1:26:50.880 --> 1:26:55.880
 And that's a trend I think that is likely to continue.

1:26:57.340 --> 1:27:01.420
 I mean, this is now wild speculation.

1:27:01.420 --> 1:27:05.500
 But I mean, when do we get to cognitive implants

1:27:05.500 --> 1:27:06.620
 and the like, and.

1:27:06.620 --> 1:27:09.500
 Yeah, with neural link, brain computer interfaces,

1:27:09.500 --> 1:27:10.340
 that's interesting.

1:27:10.340 --> 1:27:12.620
 So there's a dance between humans and robots

1:27:12.620 --> 1:27:17.220
 that's going to be, it's going to be impossible

1:27:17.220 --> 1:27:22.220
 to be scared of the other out there, the robot,

1:27:23.380 --> 1:27:26.060
 because the robot will be part of us, essentially.

1:27:26.060 --> 1:27:30.180
 It'd be so intricately sort of part of our society that.

1:27:30.180 --> 1:27:33.060
 Yeah, and it might not even be implanted part of us,

1:27:33.060 --> 1:27:37.220
 but just, it's so much a part of our, yeah, our society.

1:27:37.220 --> 1:27:39.380
 So in that sense, the smartphone is already the robot

1:27:39.380 --> 1:27:41.660
 we should be afraid of, yeah.

1:27:41.660 --> 1:27:45.460
 I mean, yeah, and all the usual fears arise

1:27:45.460 --> 1:27:50.460
 of the misinformation, the manipulation,

1:27:51.860 --> 1:27:53.500
 all those kinds of things that,

1:27:56.180 --> 1:27:57.860
 the problems are all the same.

1:27:57.860 --> 1:28:00.700
 They're human problems, essentially, it feels like.

1:28:00.700 --> 1:28:03.420
 Yeah, I mean, I think the way we interact

1:28:03.420 --> 1:28:07.420
 with each other online is changing the value we put on,

1:28:07.420 --> 1:28:08.940
 you know, personal interaction.

1:28:08.940 --> 1:28:11.260
 And that's a crazy big change that's going to happen

1:28:11.260 --> 1:28:13.080
 and rip through our, has already been ripping

1:28:13.080 --> 1:28:14.200
 through our society, right?

1:28:14.200 --> 1:28:18.060
 And that has implications that are massive.

1:28:18.060 --> 1:28:19.300
 I don't know if they should be scared of it

1:28:19.300 --> 1:28:24.300
 or go with the flow, but I don't see, you know,

1:28:24.700 --> 1:28:26.500
 some battle lines between humans and robots

1:28:26.500 --> 1:28:29.580
 being the first thing to worry about.

1:28:29.580 --> 1:28:33.340
 I mean, I do want to just, as a kind of comment,

1:28:33.340 --> 1:28:35.460
 maybe you can comment about your just feelings

1:28:35.460 --> 1:28:38.660
 about Boston Dynamics in general, but you know,

1:28:38.660 --> 1:28:40.300
 I love science, I love engineering,

1:28:40.300 --> 1:28:42.540
 I think there's so many beautiful ideas in it.

1:28:42.540 --> 1:28:45.300
 And when I look at Boston Dynamics

1:28:45.300 --> 1:28:47.620
 or legged robots in general,

1:28:47.620 --> 1:28:52.620
 I think they inspire people, curiosity and feelings

1:28:54.620 --> 1:28:57.460
 in general, excitement about engineering

1:28:57.460 --> 1:29:00.620
 more than almost anything else in popular culture.

1:29:00.620 --> 1:29:03.660
 And I think that's such an exciting,

1:29:03.660 --> 1:29:06.820
 like responsibility and possibility for robotics.

1:29:06.820 --> 1:29:10.460
 And Boston Dynamics is riding that wave pretty damn well.

1:29:10.460 --> 1:29:13.980
 Like they found it, they've discovered that hunger

1:29:13.980 --> 1:29:17.540
 and curiosity in the people and they're doing magic with it.

1:29:17.540 --> 1:29:19.820
 I don't care if the, I mean, I guess is that their company,

1:29:19.820 --> 1:29:21.340
 they have to make money, right?

1:29:21.340 --> 1:29:24.300
 But they're already doing incredible work

1:29:24.300 --> 1:29:26.940
 and inspiring the world about technology.

1:29:26.940 --> 1:29:30.700
 I mean, do you have thoughts about Boston Dynamics

1:29:30.700 --> 1:29:34.620
 and maybe others, your own work in robotics

1:29:34.620 --> 1:29:36.600
 and inspiring the world in that way?

1:29:36.600 --> 1:29:40.240
 I completely agree, I think Boston Dynamics

1:29:40.240 --> 1:29:42.640
 is absolutely awesome.

1:29:42.640 --> 1:29:46.160
 I think I show my kids those videos, you know,

1:29:46.160 --> 1:29:48.640
 and the best thing that happens is sometimes

1:29:48.640 --> 1:29:50.740
 they've already seen them, you know, right?

1:29:50.740 --> 1:29:55.360
 I think, I just think it's a pinnacle of success

1:29:55.360 --> 1:29:58.760
 in robotics that is just one of the best things

1:29:58.760 --> 1:30:01.660
 that's happened, absolutely completely agree.

1:30:01.660 --> 1:30:06.220
 One of the heartbreaking things to me is how many

1:30:06.220 --> 1:30:11.220
 robotics companies fail, how hard it is to make money

1:30:11.300 --> 1:30:13.100
 with a robotics company.

1:30:13.100 --> 1:30:17.220
 Like iRobot like went through hell just to arrive

1:30:17.220 --> 1:30:19.740
 at a Roomba to figure out one product.

1:30:19.740 --> 1:30:23.900
 And then there's so many home robotics companies

1:30:23.900 --> 1:30:31.900
 like Jibo and Anki, Anki, the cutest toy that's a great robot

1:30:32.720 --> 1:30:36.320
 I thought went down, I'm forgetting a bunch of them,

1:30:36.320 --> 1:30:37.980
 but a bunch of robotics companies fail,

1:30:37.980 --> 1:30:40.620
 Rod's company, Rethink Robotics.

1:30:42.340 --> 1:30:47.260
 Like, do you have anything hopeful to say

1:30:47.260 --> 1:30:50.340
 about the possibility of making money with robots?

1:30:50.340 --> 1:30:54.220
 Oh, I think you can't just look at the failures.

1:30:54.220 --> 1:30:55.940
 I mean, Boston Dynamics is a success.

1:30:55.940 --> 1:30:58.500
 There's lots of companies that are still doing amazingly

1:30:58.500 --> 1:31:01.140
 good work in robotics.

1:31:01.140 --> 1:31:05.360
 I mean, this is the capitalist ecology or something, right?

1:31:05.360 --> 1:31:07.700
 I think you have many companies, you have many startups

1:31:07.700 --> 1:31:11.380
 and they push each other forward and many of them fail

1:31:11.380 --> 1:31:13.820
 and some of them get through and that's sort of

1:31:13.820 --> 1:31:17.040
 the natural way of those things.

1:31:17.040 --> 1:31:20.460
 I don't know that is robotics really that much worse.

1:31:20.460 --> 1:31:22.300
 I feel the pain that you feel too.

1:31:22.300 --> 1:31:26.480
 Every time I read one of these, sometimes it's friends

1:31:26.480 --> 1:31:31.480
 and I definitely wish it went better or went differently.

1:31:33.580 --> 1:31:38.340
 But I think it's healthy and good to have bursts of ideas,

1:31:38.340 --> 1:31:41.880
 bursts of activities, ideas, if they are really aggressive,

1:31:41.880 --> 1:31:43.280
 they should fail sometimes.

1:31:45.180 --> 1:31:46.940
 Certainly that's the research mantra, right?

1:31:46.940 --> 1:31:50.780
 If you're succeeding at every problem you attempt,

1:31:50.780 --> 1:31:53.380
 then you're not choosing aggressively enough.

1:31:53.380 --> 1:31:55.980
 Is it exciting to you, the new spot?

1:31:55.980 --> 1:31:57.620
 Oh, it's so good.

1:31:57.620 --> 1:32:00.140
 When are you getting them as a pet or it?

1:32:00.140 --> 1:32:03.220
 Yeah, I mean, I have to dig up 75K right now.

1:32:03.220 --> 1:32:05.740
 I mean, it's so cool that there's a price tag,

1:32:05.740 --> 1:32:08.620
 you can go and then actually buy it.

1:32:08.620 --> 1:32:11.500
 I have a Skydio R1, love it.

1:32:11.500 --> 1:32:16.500
 So no, I would absolutely be a customer.

1:32:18.580 --> 1:32:20.060
 I wonder what your kids would think about it.

1:32:20.060 --> 1:32:25.060
 I actually, Zach from Boston Dynamics would let my kid drive

1:32:25.660 --> 1:32:27.140
 in one of their demos one time.

1:32:27.140 --> 1:32:31.100
 And that was just so good, so good.

1:32:31.100 --> 1:32:34.220
 And again, I'll forever be grateful for that.

1:32:34.220 --> 1:32:37.260
 And there's something magical about the anthropomorphization

1:32:37.260 --> 1:32:42.260
 of that arm, it adds another level of human connection.

1:32:42.580 --> 1:32:47.480
 I'm not sure we understand from a control aspect,

1:32:47.480 --> 1:32:49.500
 the value of anthropomorphization.

1:32:51.540 --> 1:32:53.980
 I think that's an understudied

1:32:53.980 --> 1:32:57.060
 and under understood engineering problem.

1:32:57.060 --> 1:33:00.160
 There's been a, like psychologists have been studying it.

1:33:00.160 --> 1:33:02.860
 I think it's part like manipulating our mind

1:33:02.860 --> 1:33:06.740
 to believe things is a valuable engineering.

1:33:06.740 --> 1:33:08.820
 Like this is another degree of freedom

1:33:08.820 --> 1:33:09.820
 that can be controlled.

1:33:09.820 --> 1:33:11.380
 I like that, yeah, I think that's right.

1:33:11.380 --> 1:33:16.020
 I think there's something that humans seem to do

1:33:16.020 --> 1:33:19.000
 or maybe my dangerous introspection is,

1:33:20.340 --> 1:33:23.820
 I think we are able to make very simple models

1:33:23.820 --> 1:33:27.780
 that assume a lot about the world very quickly.

1:33:27.780 --> 1:33:31.220
 And then it takes us a lot more time, like you're wrestling.

1:33:31.220 --> 1:33:33.080
 You probably thought you knew what you were doing

1:33:33.080 --> 1:33:35.340
 with wrestling and you were fairly functional

1:33:35.340 --> 1:33:36.900
 as a complete wrestler.

1:33:36.900 --> 1:33:39.340
 And then you slowly got more expertise.

1:33:39.340 --> 1:33:44.340
 So maybe it's natural that our first level of defense

1:33:45.740 --> 1:33:48.040
 against seeing a new robot is to think of it

1:33:48.040 --> 1:33:52.420
 in our existing models of how humans and animals behave.

1:33:52.420 --> 1:33:55.060
 And it's just, as you spend more time with it,

1:33:55.060 --> 1:33:56.980
 then you'll develop more sophisticated models

1:33:56.980 --> 1:33:59.420
 that will appreciate the differences.

1:34:00.340 --> 1:34:01.620
 Exactly.

1:34:01.620 --> 1:34:05.700
 Can you say what does it take to control a robot?

1:34:05.700 --> 1:34:08.580
 Like what is the control problem of a robot?

1:34:08.580 --> 1:34:10.980
 And in general, what is a robot in your view?

1:34:10.980 --> 1:34:13.900
 Like how do you think of this system?

1:34:15.020 --> 1:34:16.020
 What is a robot?

1:34:16.020 --> 1:34:17.580
 What is a robot?

1:34:17.580 --> 1:34:18.400
 I think robotics.

1:34:18.400 --> 1:34:20.020
 I told you ridiculous questions.

1:34:20.020 --> 1:34:21.500
 No, no, it's good.

1:34:21.500 --> 1:34:22.980
 I mean, there's standard definitions

1:34:22.980 --> 1:34:27.460
 of combining computation with some ability

1:34:27.460 --> 1:34:29.060
 to do mechanical work.

1:34:29.060 --> 1:34:30.980
 I think that gets us pretty close.

1:34:30.980 --> 1:34:34.180
 But I think robotics has this problem

1:34:34.180 --> 1:34:37.200
 that once things really work,

1:34:37.200 --> 1:34:38.920
 we don't call them robots anymore.

1:34:38.920 --> 1:34:42.920
 Like my dishwasher at home is pretty sophisticated,

1:34:44.100 --> 1:34:45.600
 beautiful mechanisms.

1:34:45.600 --> 1:34:46.940
 There's actually a pretty good computer,

1:34:46.940 --> 1:34:49.580
 probably a couple of chips in there doing amazing things.

1:34:49.580 --> 1:34:51.620
 We don't think of that as a robot anymore,

1:34:51.620 --> 1:34:52.460
 which isn't fair.

1:34:52.460 --> 1:34:53.940
 Because then what roughly it means

1:34:53.940 --> 1:34:58.340
 that robotics always has to solve the next problem

1:34:58.340 --> 1:35:00.580
 and doesn't get to celebrate its past successes.

1:35:00.580 --> 1:35:04.740
 I mean, even factory room floor robots

1:35:05.660 --> 1:35:06.860
 are super successful.

1:35:06.860 --> 1:35:08.260
 They're amazing.

1:35:08.260 --> 1:35:09.500
 But that's not the ones,

1:35:09.500 --> 1:35:10.880
 I mean, people think of them as robots,

1:35:10.880 --> 1:35:11.720
 but they don't,

1:35:11.720 --> 1:35:14.500
 if you ask what are the successes of robotics,

1:35:14.500 --> 1:35:17.860
 somehow it doesn't come to your mind immediately.

1:35:17.860 --> 1:35:20.560
 So the definition of robot is a system

1:35:20.560 --> 1:35:23.500
 with some level of automation that fails frequently.

1:35:23.500 --> 1:35:28.420
 Something like, it's the computation plus mechanical work

1:35:28.420 --> 1:35:30.540
 and an unsolved problem.

1:35:30.540 --> 1:35:32.300
 It's an unsolved problem, yeah.

1:35:32.300 --> 1:35:37.020
 So from a perspective of control and mechanics,

1:35:37.020 --> 1:35:39.840
 dynamics, what is a robot?

1:35:40.700 --> 1:35:42.380
 So there are many different types of robots.

1:35:42.380 --> 1:35:47.380
 The control that you need for a Jibo robot,

1:35:47.620 --> 1:35:50.620
 you know, some robot that's sitting on your countertop

1:35:50.620 --> 1:35:53.580
 and interacting with you, but not touching you,

1:35:53.580 --> 1:35:55.820
 for instance, is very different than what you need

1:35:55.820 --> 1:35:59.460
 for an autonomous car or an autonomous drone.

1:35:59.460 --> 1:36:01.020
 It's very different than what you need for a robot

1:36:01.020 --> 1:36:04.740
 that's gonna walk or pick things up with its hands, right?

1:36:04.740 --> 1:36:09.140
 My passion has always been for the places

1:36:09.140 --> 1:36:10.540
 where you're interacting more,

1:36:10.540 --> 1:36:13.700
 you're doing more dynamic interactions with the world.

1:36:13.700 --> 1:36:17.760
 So walking, now manipulation.

1:36:18.740 --> 1:36:21.700
 And the control problems there are beautiful.

1:36:21.700 --> 1:36:25.940
 I think contact is one thing that differentiates them

1:36:25.940 --> 1:36:29.240
 from many of the control problems we've solved classically,

1:36:29.240 --> 1:36:32.780
 right, like modern control grew up stabilizing fighter jets

1:36:32.780 --> 1:36:34.060
 that were passively unstable,

1:36:34.060 --> 1:36:37.020
 and there's like amazing success stories from control

1:36:37.020 --> 1:36:38.080
 all over the place.

1:36:39.140 --> 1:36:41.340
 Power grid, I mean, there's all kinds of,

1:36:41.340 --> 1:36:44.640
 it's everywhere that we don't even realize,

1:36:44.640 --> 1:36:47.540
 just like AI is now.

1:36:47.540 --> 1:36:51.500
 So you mentioned contact, like what's contact?

1:36:51.500 --> 1:36:54.980
 So an airplane is an extremely complex system

1:36:54.980 --> 1:36:57.380
 or a spacecraft landing or whatever,

1:36:57.380 --> 1:36:59.340
 but at least it has the luxury

1:36:59.340 --> 1:37:03.640
 of things change relatively continuously.

1:37:03.640 --> 1:37:04.940
 That's an oversimplification.

1:37:04.940 --> 1:37:07.060
 But if I make a small change

1:37:07.060 --> 1:37:10.140
 in the command I send to my actuator,

1:37:10.140 --> 1:37:12.680
 then the path that the robot will take

1:37:12.680 --> 1:37:15.860
 tends to change only by a small amount.

1:37:16.820 --> 1:37:18.860
 And there's a feedback mechanism here.

1:37:18.860 --> 1:37:19.700
 That's what we're talking about.

1:37:19.700 --> 1:37:20.980
 And there's a feedback mechanism.

1:37:20.980 --> 1:37:23.780
 And thinking about this as locally,

1:37:23.780 --> 1:37:25.820
 like a linear system, for instance,

1:37:25.820 --> 1:37:29.220
 I can use more linear algebra tools

1:37:29.220 --> 1:37:31.340
 to study systems like that,

1:37:31.340 --> 1:37:35.540
 generalizations of linear algebra to these smooth systems.

1:37:36.400 --> 1:37:37.380
 What is contact?

1:37:37.380 --> 1:37:41.540
 The robot has something very discontinuous

1:37:41.540 --> 1:37:43.620
 that happens when it makes or breaks,

1:37:43.620 --> 1:37:45.420
 when it starts touching the world.

1:37:45.420 --> 1:37:48.080
 And even the way it touches or the order of contacts

1:37:48.080 --> 1:37:53.080
 can change the outcome in potentially unpredictable ways.

1:37:53.080 --> 1:37:55.880
 Not unpredictable, but complex ways.

1:37:56.880 --> 1:37:58.680
 I do think there's a little bit of,

1:38:01.440 --> 1:38:04.580
 a lot of people will say that contact is hard in robotics,

1:38:04.580 --> 1:38:05.640
 even to simulate.

1:38:06.360 --> 1:38:08.720
 And I think there's a little bit of a,

1:38:08.720 --> 1:38:09.640
 there's truth to that,

1:38:09.640 --> 1:38:12.020
 but maybe a misunderstanding around that.

1:38:13.560 --> 1:38:18.560
 So what is limiting is that when we think about our robots

1:38:19.600 --> 1:38:21.400
 and we write our simulators,

1:38:21.400 --> 1:38:24.480
 we often make an assumption that objects are rigid.

1:38:26.000 --> 1:38:30.720
 And when it comes down, that their mass moves all,

1:38:30.720 --> 1:38:33.800
 stays in a constant position relative to each other itself.

1:38:37.080 --> 1:38:39.360
 And that leads to some paradoxes

1:38:39.360 --> 1:38:40.560
 when you go to try to talk about

1:38:40.560 --> 1:38:43.200
 rigid body mechanics and contact.

1:38:43.200 --> 1:38:48.200
 And so for instance, if I have a three legged stool

1:38:48.200 --> 1:38:51.840
 with just imagine it comes to a point at the leg.

1:38:51.840 --> 1:38:54.400
 So it's only touching the world at a point.

1:38:54.400 --> 1:38:56.920
 If I draw my physics,

1:38:56.920 --> 1:39:00.280
 my high school physics diagram of the system,

1:39:00.280 --> 1:39:01.600
 then there's a couple of things

1:39:01.600 --> 1:39:03.800
 that I'm given by elementary physics.

1:39:03.800 --> 1:39:06.320
 I know if the system, if the table is at rest,

1:39:06.320 --> 1:39:08.480
 if it's not moving, zero velocities,

1:39:09.520 --> 1:39:11.120
 that means that the normal force,

1:39:11.120 --> 1:39:13.280
 all the forces are in balance.

1:39:13.280 --> 1:39:16.400
 So the force of gravity is being countered

1:39:16.400 --> 1:39:20.080
 by the forces that the ground is pushing on my table legs.

1:39:21.240 --> 1:39:23.880
 I also know since it's not rotating

1:39:23.880 --> 1:39:25.800
 that the moments have to balance.

1:39:25.800 --> 1:39:29.560
 And since it's a three dimensional table,

1:39:29.560 --> 1:39:31.120
 it could fall in any direction.

1:39:31.120 --> 1:39:33.040
 It actually tells me uniquely

1:39:33.040 --> 1:39:35.360
 what those three normal forces have to be.

1:39:37.080 --> 1:39:39.600
 If I have four legs on my table,

1:39:39.600 --> 1:39:43.280
 four legged table and they were perfectly machined

1:39:43.280 --> 1:39:45.360
 to be exactly the right same height

1:39:45.360 --> 1:39:48.040
 and they're set down and the table's not moving,

1:39:48.040 --> 1:39:51.960
 then the basic conservation laws don't tell me,

1:39:51.960 --> 1:39:54.040
 there are many solutions for the forces

1:39:54.040 --> 1:39:56.600
 that the ground could be putting on my legs

1:39:56.600 --> 1:39:59.040
 that would still result in the table not moving.

1:40:00.200 --> 1:40:03.920
 Now, the reason that seems fine, I could just pick one.

1:40:03.920 --> 1:40:06.720
 But it gets funny now because if you think about friction,

1:40:07.840 --> 1:40:11.000
 what we think about with friction is our standard model

1:40:11.000 --> 1:40:15.880
 says the amount of force that the table will push back

1:40:15.880 --> 1:40:18.000
 if I were to now try to push my table sideways,

1:40:18.000 --> 1:40:19.400
 I guess I have a table here,

1:40:20.880 --> 1:40:23.000
 is proportional to the normal force.

1:40:24.040 --> 1:40:27.200
 So if I'm barely touching and I push, I'll slide,

1:40:27.200 --> 1:40:30.440
 but if I'm pushing more and I push, I'll slide less.

1:40:30.440 --> 1:40:33.720
 It's called coulomb friction is our standard model.

1:40:33.720 --> 1:40:35.520
 Now, if you don't know what the normal force is

1:40:35.520 --> 1:40:38.840
 on the four legs and you push the table,

1:40:38.840 --> 1:40:42.400
 then you don't know what the friction forces are gonna be.

1:40:43.440 --> 1:40:45.560
 And so you can't actually tell,

1:40:45.560 --> 1:40:47.960
 the laws just aren't explicit yet

1:40:47.960 --> 1:40:49.680
 about which way the table's gonna go.

1:40:49.680 --> 1:40:51.360
 It could veer off to the left,

1:40:51.360 --> 1:40:54.720
 it could veer off to the right, it could go straight.

1:40:54.720 --> 1:40:58.440
 So the rigid body assumption of contact

1:40:58.440 --> 1:40:59.840
 leaves us with some paradoxes,

1:40:59.840 --> 1:41:02.840
 which are annoying for writing simulators

1:41:02.840 --> 1:41:04.240
 and for writing controllers.

1:41:04.240 --> 1:41:07.720
 We still do that sometimes because soft contact

1:41:07.720 --> 1:41:11.400
 is potentially harder numerically or whatever,

1:41:11.400 --> 1:41:12.920
 and the best simulators do both

1:41:12.920 --> 1:41:15.240
 or do some combination of the two.

1:41:15.240 --> 1:41:17.360
 But anyways, because of these kinds of paradoxes,

1:41:17.360 --> 1:41:20.720
 there's all kinds of paradoxes in contact,

1:41:20.720 --> 1:41:23.560
 mostly due to these rigid body assumptions.

1:41:23.560 --> 1:41:27.880
 It becomes very hard to write the same kind of control laws

1:41:27.880 --> 1:41:29.600
 that we've been able to be successful with

1:41:29.600 --> 1:41:32.000
 for fighter jets.

1:41:32.000 --> 1:41:34.560
 Like fighter jets, we haven't been as successful

1:41:34.560 --> 1:41:37.440
 writing those controllers for manipulation.

1:41:37.440 --> 1:41:39.160
 And so you don't know what's going to happen

1:41:39.160 --> 1:41:41.480
 at the point of contact, at the moment of contact.

1:41:41.480 --> 1:41:42.880
 There are situations absolutely

1:41:42.880 --> 1:41:45.760
 where our laws don't tell us.

1:41:45.760 --> 1:41:47.440
 So the standard approach, that's okay.

1:41:47.440 --> 1:41:51.160
 I mean, instead of having a differential equation,

1:41:51.160 --> 1:41:53.640
 you end up with a differential inclusion, it's called.

1:41:53.640 --> 1:41:56.080
 It's a set valued equation.

1:41:56.080 --> 1:41:58.320
 It says that I'm in this configuration,

1:41:58.320 --> 1:42:00.000
 I have these forces applied on me.

1:42:00.000 --> 1:42:03.480
 And there's a set of things that could happen, right?

1:42:03.480 --> 1:42:04.320
 And you can...

1:42:04.320 --> 1:42:07.480
 And those aren't continuous, I mean, what...

1:42:07.480 --> 1:42:10.360
 So when you're saying like non smooth,

1:42:10.360 --> 1:42:14.520
 they're not only not smooth, but this is discontinuous?

1:42:14.520 --> 1:42:15.800
 The non smooth comes in

1:42:15.800 --> 1:42:18.760
 when I make or break a new contact first,

1:42:18.760 --> 1:42:21.200
 or when I transition from stick to slip.

1:42:21.200 --> 1:42:23.520
 So you typically have static friction,

1:42:23.520 --> 1:42:24.840
 and then you'll start sliding,

1:42:24.840 --> 1:42:28.920
 and that'll be a discontinuous change in philosophy.

1:42:28.920 --> 1:42:31.360
 In philosophy, for instance,

1:42:31.360 --> 1:42:33.360
 especially if you come to rest or...

1:42:33.360 --> 1:42:34.480
 That's so fascinating.

1:42:34.480 --> 1:42:37.720
 Okay, so what do you do?

1:42:37.720 --> 1:42:38.920
 Sorry, I interrupted you.

1:42:38.920 --> 1:42:39.760
 It's fine.

1:42:41.600 --> 1:42:44.160
 What's the hope under so much uncertainty

1:42:44.160 --> 1:42:45.440
 about what's going to happen?

1:42:45.440 --> 1:42:46.360
 What are you supposed to do?

1:42:46.360 --> 1:42:48.520
 I mean, control has an answer for this.

1:42:48.520 --> 1:42:50.240
 Robust control is one approach,

1:42:50.240 --> 1:42:52.640
 but roughly you can write controllers

1:42:52.640 --> 1:42:55.920
 which try to still perform the right task

1:42:55.920 --> 1:42:58.120
 despite all the things that could possibly happen.

1:42:58.120 --> 1:43:00.000
 The world might want the table to go this way and this way,

1:43:00.000 --> 1:43:03.640
 but if I write a controller that pushes a little bit more

1:43:03.640 --> 1:43:04.480
 and pushes a little bit,

1:43:04.480 --> 1:43:08.000
 I can certainly make the table go in the direction I want.

1:43:08.000 --> 1:43:10.000
 It just puts a little bit more of a burden

1:43:10.000 --> 1:43:12.120
 on the control system, right?

1:43:12.120 --> 1:43:15.440
 And this discontinuities do change the control system

1:43:15.440 --> 1:43:19.840
 because the way we write it down right now,

1:43:21.200 --> 1:43:24.320
 every different control configuration,

1:43:24.320 --> 1:43:26.200
 including sticking or sliding

1:43:26.200 --> 1:43:29.160
 or parts of my body that are in contact or not,

1:43:29.160 --> 1:43:30.840
 looks like a different system.

1:43:30.840 --> 1:43:31.880
 And I think of them,

1:43:31.880 --> 1:43:34.680
 I reason about them separately or differently

1:43:34.680 --> 1:43:38.000
 and the combinatorics of that blow up, right?

1:43:38.000 --> 1:43:41.440
 So I just don't have enough time to compute

1:43:41.440 --> 1:43:45.000
 all the possible contact configurations of my humanoid.

1:43:45.000 --> 1:43:49.000
 Interestingly, I mean, I'm a humanoid.

1:43:49.000 --> 1:43:52.400
 I have lots of degrees of freedom, lots of joints.

1:43:52.400 --> 1:43:54.960
 I've only been around for a handful of years.

1:43:54.960 --> 1:43:55.800
 It's getting up there,

1:43:55.800 --> 1:43:59.200
 but I haven't had time in my life

1:43:59.200 --> 1:44:02.080
 to visit all of the states in my system,

1:44:03.080 --> 1:44:05.240
 certainly all the contact configurations.

1:44:05.240 --> 1:44:08.320
 So if step one is to consider

1:44:08.320 --> 1:44:12.160
 every possible contact configuration that I'll ever be in,

1:44:12.160 --> 1:44:16.080
 that's probably not a problem I need to solve, right?

1:44:17.040 --> 1:44:20.560
 Just as a small tangent, what's a contact configuration?

1:44:20.560 --> 1:44:24.920
 What like, just so we can enumerate

1:44:24.920 --> 1:44:26.280
 what are we talking about?

1:44:26.280 --> 1:44:27.600
 How many are there?

1:44:27.600 --> 1:44:30.000
 The simplest example maybe would be,

1:44:30.000 --> 1:44:32.720
 imagine a robot with a flat foot.

1:44:32.720 --> 1:44:35.440
 And we think about the phases of gait

1:44:35.440 --> 1:44:40.000
 where the heel strikes and then the front toe strikes,

1:44:40.000 --> 1:44:42.480
 and then you can heel up, toe off.

1:44:43.720 --> 1:44:46.720
 Those are each different contact configurations.

1:44:46.720 --> 1:44:48.320
 I only had two different contacts,

1:44:48.320 --> 1:44:51.440
 but I ended up with four different contact configurations.

1:44:51.440 --> 1:44:56.440
 Now, of course, my robot might actually have bumps on it

1:44:57.400 --> 1:44:58.240
 or other things,

1:44:58.240 --> 1:45:00.640
 so it could be much more subtle than that, right?

1:45:00.640 --> 1:45:03.160
 But it's just even with one sort of box

1:45:03.160 --> 1:45:06.240
 interacting with the ground already in the plane

1:45:06.240 --> 1:45:07.120
 has that many, right?

1:45:07.120 --> 1:45:09.440
 And if I was just even a 3D foot,

1:45:09.440 --> 1:45:11.240
 then it probably my left toe might touch

1:45:11.240 --> 1:45:14.360
 just before my right toe and things get subtle.

1:45:14.360 --> 1:45:16.480
 Now, if I'm a dexterous hand

1:45:16.480 --> 1:45:21.480
 and I go to talk about just grabbing a water bottle,

1:45:22.280 --> 1:45:26.720
 if I have to enumerate every possible order

1:45:26.720 --> 1:45:31.000
 that my hand came into contact with the bottle,

1:45:31.000 --> 1:45:32.960
 then I'm dead in the water.

1:45:32.960 --> 1:45:35.400
 Any approach that we were able to get away with that

1:45:35.400 --> 1:45:38.480
 in walking because we mostly touched the ground

1:45:38.480 --> 1:45:40.840
 within a small number of points, for instance,

1:45:40.840 --> 1:45:43.800
 and we haven't been able to get dexterous hands that way.

1:45:43.800 --> 1:45:48.800
 So you've mentioned that people think

1:45:50.200 --> 1:45:52.520
 that contact is really hard

1:45:52.520 --> 1:45:57.520
 and that that's the reason that robotic manipulation

1:45:58.160 --> 1:46:00.560
 is problem is really hard.

1:46:00.560 --> 1:46:05.560
 Is there any flaws in that thinking?

1:46:06.560 --> 1:46:10.560
 So I think simulating contact is one aspect.

1:46:10.560 --> 1:46:12.880
 I know people often say that we don't,

1:46:12.880 --> 1:46:16.320
 that one of the reasons that we have a limit in robotics

1:46:16.320 --> 1:46:19.040
 is because we do not simulate contact accurately

1:46:19.040 --> 1:46:20.840
 in our simulators.

1:46:20.840 --> 1:46:25.600
 And I think that is the extent to which that's true

1:46:25.600 --> 1:46:27.880
 is partly because our simulators,

1:46:27.880 --> 1:46:29.920
 we haven't got mature enough simulators.

1:46:31.240 --> 1:46:34.120
 There are some things that are still hard, difficult,

1:46:34.120 --> 1:46:35.320
 that we should change,

1:46:38.200 --> 1:46:41.520
 but we actually, we know what the governing equations are.

1:46:41.520 --> 1:46:44.720
 They have some foibles like this indeterminacy,

1:46:44.720 --> 1:46:47.240
 but we should be able to simulate them accurately.

1:46:48.600 --> 1:46:51.440
 We have incredible open source community in robotics,

1:46:51.440 --> 1:46:54.360
 but it actually just takes a professional engineering team

1:46:54.360 --> 1:46:57.740
 a lot of work to write a very good simulator like that.

1:46:59.080 --> 1:47:02.160
 Now, where does, I believe you've written, Drake.

1:47:03.280 --> 1:47:04.520
 There's a team of people.

1:47:04.520 --> 1:47:07.320
 I certainly spent a lot of hours on it myself.

1:47:07.320 --> 1:47:12.060
 But what is Drake and what does it take to create

1:47:12.060 --> 1:47:17.060
 a simulation environment for the kind of difficult control

1:47:18.200 --> 1:47:19.640
 problems we're talking about?

1:47:20.740 --> 1:47:24.640
 Right, so Drake is the simulator that I've been working on.

1:47:24.640 --> 1:47:26.780
 There are other good simulators out there.

1:47:26.780 --> 1:47:29.680
 I don't like to think of Drake as just a simulator

1:47:29.680 --> 1:47:31.780
 because we write our controllers in Drake,

1:47:31.780 --> 1:47:34.360
 we write our perception systems a little bit in Drake,

1:47:34.360 --> 1:47:37.040
 but we write all of our low level control

1:47:37.040 --> 1:47:40.840
 and even planning and optimization.

1:47:40.840 --> 1:47:42.480
 So it has optimization capabilities as well?

1:47:42.480 --> 1:47:43.640
 Absolutely, yeah.

1:47:43.640 --> 1:47:46.000
 I mean, Drake is three things roughly.

1:47:46.000 --> 1:47:49.800
 It's an optimization library, which is sits on,

1:47:49.800 --> 1:47:54.240
 it provides a layer of abstraction in C++ and Python

1:47:54.240 --> 1:47:55.920
 for commercial solvers.

1:47:55.920 --> 1:48:00.760
 You can write linear programs, quadratic programs,

1:48:00.760 --> 1:48:03.340
 semi definite programs, sums of squares programs,

1:48:03.340 --> 1:48:05.660
 the ones we've used, mixed integer programs,

1:48:05.660 --> 1:48:07.960
 and it will do the work to curate those

1:48:07.960 --> 1:48:10.360
 and send them to whatever the right solver is for instance,

1:48:10.360 --> 1:48:12.500
 and it provides a level of abstraction.

1:48:13.720 --> 1:48:18.360
 The second thing is a system modeling language,

1:48:18.360 --> 1:48:20.880
 a bit like LabVIEW or Simulink,

1:48:20.880 --> 1:48:24.840
 where you can make block diagrams out of complex systems,

1:48:24.840 --> 1:48:26.640
 or it's like ROS in that sense,

1:48:26.640 --> 1:48:29.040
 where you might have lots of ROS nodes

1:48:29.040 --> 1:48:31.960
 that are each doing some part of your system,

1:48:31.960 --> 1:48:36.560
 but to contrast it with ROS, we try to write,

1:48:36.560 --> 1:48:38.960
 if you write a Drake system, then you have to,

1:48:40.120 --> 1:48:43.000
 it asks you to describe a little bit more about the system.

1:48:43.000 --> 1:48:46.240
 If you have any state, for instance, in the system,

1:48:46.240 --> 1:48:47.680
 any variables that are gonna persist,

1:48:47.680 --> 1:48:49.120
 you have to declare them.

1:48:49.120 --> 1:48:51.620
 Parameters can be declared and the like,

1:48:51.620 --> 1:48:54.160
 but the advantage of doing that is that you can,

1:48:54.160 --> 1:48:57.460
 if you like, run things all on one process,

1:48:57.460 --> 1:49:00.200
 but you can also do control design against it.

1:49:00.200 --> 1:49:03.120
 You can do, I mean, simple things like rewinding

1:49:03.120 --> 1:49:07.960
 and playing back your simulations, for instance,

1:49:07.960 --> 1:49:09.600
 these things, you get some rewards

1:49:09.600 --> 1:49:11.380
 for spending a little bit more upfront cost

1:49:11.380 --> 1:49:13.320
 in describing each system.

1:49:13.320 --> 1:49:16.920
 And I was inspired to do that

1:49:16.920 --> 1:49:20.340
 because I think the complexity of Atlas, for instance,

1:49:21.260 --> 1:49:22.600
 is just so great.

1:49:22.600 --> 1:49:24.140
 And I think, although, I mean,

1:49:24.140 --> 1:49:27.520
 ROS has been an incredible, absolutely huge fan

1:49:27.520 --> 1:49:30.720
 of what it's done for the robotics community,

1:49:30.720 --> 1:49:35.480
 but the ability to rapidly put different pieces together

1:49:35.480 --> 1:49:37.960
 and have a functioning thing is very good.

1:49:38.960 --> 1:49:42.880
 But I do think that it's hard to think clearly

1:49:42.880 --> 1:49:45.000
 about a bag of disparate parts,

1:49:45.000 --> 1:49:48.160
 Mr. Potato Head kind of software stack.

1:49:48.160 --> 1:49:53.060
 And if you can ask a little bit more

1:49:53.060 --> 1:49:54.200
 out of each of those parts,

1:49:54.200 --> 1:49:56.120
 then you can understand the way they work better.

1:49:56.120 --> 1:49:59.280
 You can try to verify them and the like,

1:50:00.160 --> 1:50:02.680
 or you can do learning against them.

1:50:02.680 --> 1:50:04.760
 And then one of those systems, the last thing,

1:50:04.760 --> 1:50:06.480
 I said the first two things that Drake is,

1:50:06.480 --> 1:50:09.680
 but the last thing is that there is a set

1:50:09.680 --> 1:50:12.560
 of multi body equations, rigid body equations,

1:50:12.560 --> 1:50:16.760
 that is trying to provide a system that simulates physics.

1:50:16.760 --> 1:50:20.060
 And we also have renderers and other things,

1:50:20.060 --> 1:50:23.300
 but I think the physics component of Drake is special

1:50:23.300 --> 1:50:27.740
 in the sense that we have done excessive amount

1:50:27.740 --> 1:50:29.840
 of engineering to make sure

1:50:29.840 --> 1:50:31.580
 that we've written the equations correctly.

1:50:31.580 --> 1:50:34.160
 Every possible tumbling satellite or spinning top

1:50:34.160 --> 1:50:37.160
 or anything that we could possibly write as a test is tested.

1:50:38.300 --> 1:50:42.000
 We are making some, I think, fundamental improvements

1:50:42.000 --> 1:50:44.240
 on the way you simulate contact.

1:50:44.240 --> 1:50:47.600
 Just what does it take to simulate contact?

1:50:47.600 --> 1:50:49.120
 I mean, it just seems,

1:50:50.920 --> 1:50:52.400
 I mean, there's something just beautiful

1:50:52.400 --> 1:50:55.240
 to the way you were like explaining contact

1:50:55.240 --> 1:50:56.720
 and you were like tapping your fingers

1:50:56.720 --> 1:51:00.720
 on the table while you're doing it, just.

1:51:00.720 --> 1:51:01.560
 Easily, right?

1:51:01.560 --> 1:51:04.800
 Easily, just like, just not even like,

1:51:04.800 --> 1:51:06.800
 it was like helping you think, I guess.

1:51:10.640 --> 1:51:12.280
 So you have this like awesome demo

1:51:12.280 --> 1:51:15.680
 of loading or unloading a dishwasher,

1:51:16.720 --> 1:51:18.840
 just picking up a plate,

1:51:18.840 --> 1:51:23.840
 or grasping it like for the first time.

1:51:26.120 --> 1:51:28.180
 That's just seems like so difficult.

1:51:29.440 --> 1:51:32.400
 What, how do you simulate any of that?

1:51:33.600 --> 1:51:35.840
 So it was really interesting that what happened was

1:51:35.840 --> 1:51:39.200
 that we started getting more professional

1:51:39.200 --> 1:51:40.520
 about our software development

1:51:40.520 --> 1:51:42.280
 during the DARPA Robotics Challenge.

1:51:43.360 --> 1:51:46.040
 I learned the value of software engineering

1:51:46.040 --> 1:51:48.640
 and how these, how to bridle complexity.

1:51:48.640 --> 1:51:52.800
 I guess that's what I want to somehow fight against

1:51:52.800 --> 1:51:54.760
 and bring some of the clear thinking of controls

1:51:54.760 --> 1:51:58.220
 into these complex systems we're building for robots.

1:52:00.460 --> 1:52:02.940
 Shortly after the DARPA Robotics Challenge,

1:52:02.940 --> 1:52:04.600
 Toyota opened a research institute,

1:52:04.600 --> 1:52:07.260
 TRI, Toyota Research Institute.

1:52:08.200 --> 1:52:10.880
 They put one of their, there's three locations.

1:52:10.880 --> 1:52:13.040
 One of them is just down the street from MIT.

1:52:13.040 --> 1:52:17.520
 And I helped ramp that up right up

1:52:17.520 --> 1:52:20.860
 as a part of my, the end of my sabbatical, I guess.

1:52:23.480 --> 1:52:28.480
 So TRI has given me, the TRI robotics effort

1:52:29.480 --> 1:52:32.640
 has made this investment in simulation in Drake.

1:52:32.640 --> 1:52:34.480
 And Michael Sherman leads a team there

1:52:34.480 --> 1:52:37.800
 of just absolutely top notch dynamics experts

1:52:37.800 --> 1:52:40.120
 that are trying to write those simulators

1:52:40.120 --> 1:52:41.960
 that can pick up the dishes.

1:52:41.960 --> 1:52:44.780
 And there's also a team working on manipulation there

1:52:44.780 --> 1:52:48.980
 that is taking problems like loading the dishwasher.

1:52:48.980 --> 1:52:53.180
 And we're using that to study these really hard corner cases

1:52:53.180 --> 1:52:55.280
 kind of problems in manipulation.

1:52:55.280 --> 1:52:59.760
 So for me, this, you know, simulating the dishes,

1:52:59.760 --> 1:53:01.580
 we could actually write a controller.

1:53:01.580 --> 1:53:05.040
 If we just cared about picking up dishes in the sink once,

1:53:05.040 --> 1:53:05.880
 we could write a controller

1:53:05.880 --> 1:53:07.760
 without any simulation whatsoever,

1:53:07.760 --> 1:53:10.040
 and we could call it done.

1:53:10.040 --> 1:53:12.140
 But we want to understand like,

1:53:12.140 --> 1:53:17.040
 what is the path you take to actually get to a robot

1:53:17.040 --> 1:53:22.040
 that could perform that for any dish in anybody's kitchen

1:53:22.120 --> 1:53:23.280
 with enough confidence

1:53:23.280 --> 1:53:26.520
 that it could be a commercial product, right?

1:53:26.520 --> 1:53:29.360
 And it has deep learning perception in the loop.

1:53:29.360 --> 1:53:31.040
 It has complex dynamics in the loop.

1:53:31.040 --> 1:53:33.240
 It has controller, it has a planner.

1:53:33.240 --> 1:53:36.320
 And how do you take all of that complexity

1:53:36.320 --> 1:53:39.020
 and put it through this engineering discipline

1:53:39.020 --> 1:53:42.440
 and verification and validation process

1:53:42.440 --> 1:53:46.440
 to actually get enough confidence to deploy?

1:53:46.440 --> 1:53:49.840
 I mean, the DARPA challenge made me realize

1:53:49.840 --> 1:53:52.000
 that that's not something you throw over the fence

1:53:52.000 --> 1:53:54.080
 and hope that somebody will harden it for you,

1:53:54.080 --> 1:53:57.380
 that there are really fundamental challenges

1:53:57.380 --> 1:53:59.840
 in closing that last gap.

1:53:59.840 --> 1:54:02.340
 They're doing the validation and the testing.

1:54:03.520 --> 1:54:06.780
 I think it might even change the way we have to think about

1:54:06.780 --> 1:54:09.580
 the way we write systems.

1:54:09.580 --> 1:54:14.200
 What happens if you have the robot running lots of tests

1:54:15.560 --> 1:54:19.040
 and it screws up, it breaks a dish, right?

1:54:19.040 --> 1:54:19.960
 How do you capture that?

1:54:19.960 --> 1:54:23.580
 I said, you can't run the same simulation

1:54:23.580 --> 1:54:27.020
 or the same experiment twice on a real robot.

1:54:27.920 --> 1:54:31.520
 Do we have to be able to bring that one off failure

1:54:31.520 --> 1:54:32.640
 back into simulation

1:54:32.640 --> 1:54:35.120
 in order to change our controllers, study it,

1:54:35.120 --> 1:54:37.240
 make sure it won't happen again?

1:54:37.240 --> 1:54:40.600
 Do we, is it enough to just try to add that

1:54:40.600 --> 1:54:43.800
 to our distribution and understand that on average,

1:54:43.800 --> 1:54:45.920
 we're gonna cover that situation again?

1:54:45.920 --> 1:54:49.960
 There's like really subtle questions at the corner cases

1:54:49.960 --> 1:54:53.240
 that I think we don't yet have satisfying answers for.

1:54:53.240 --> 1:54:55.120
 Like how do you find the corner cases?

1:54:55.120 --> 1:54:57.160
 That's one kind of, is there,

1:54:57.160 --> 1:55:01.260
 do you think that's possible to create a systematized way

1:55:01.260 --> 1:55:04.720
 of discovering corner cases efficiently?

1:55:04.720 --> 1:55:05.560
 Yes.

1:55:05.560 --> 1:55:07.600
 In whatever the problem is?

1:55:07.600 --> 1:55:10.760
 Yes, I mean, I think we have to get better at that.

1:55:10.760 --> 1:55:14.920
 I mean, control theory has for decades

1:55:14.920 --> 1:55:16.920
 talked about active experiment design.

1:55:17.840 --> 1:55:18.680
 What's that?

1:55:19.560 --> 1:55:22.080
 So people call it curiosity these days.

1:55:22.080 --> 1:55:24.800
 It's roughly this idea of trying to exploration

1:55:24.800 --> 1:55:27.600
 or exploitation, but in the active experiment design

1:55:27.600 --> 1:55:29.640
 is even, is more specific.

1:55:29.640 --> 1:55:34.120
 You could try to understand the uncertainty in your system,

1:55:34.120 --> 1:55:36.480
 design the experiment that will provide

1:55:36.480 --> 1:55:40.120
 the maximum information to reduce that uncertainty.

1:55:40.120 --> 1:55:42.360
 If there's a parameter you wanna learn about,

1:55:42.360 --> 1:55:45.440
 what is the optimal trajectory I could execute

1:55:45.440 --> 1:55:47.640
 to learn about that parameter, for instance.

1:55:49.520 --> 1:55:51.720
 Scaling that up to something that has a deep network

1:55:51.720 --> 1:55:55.660
 in the loop and a planning in the loop is tough.

1:55:55.660 --> 1:55:58.200
 We've done some work on, you know,

1:55:58.200 --> 1:56:00.280
 with Matt Okely and Aman Sinha,

1:56:00.280 --> 1:56:03.600
 we've worked on some falsification algorithms

1:56:03.600 --> 1:56:05.600
 that are trying to do rare event simulation

1:56:05.600 --> 1:56:08.120
 that try to just hammer on your simulator.

1:56:08.120 --> 1:56:10.000
 And if your simulator is good enough,

1:56:10.000 --> 1:56:13.840
 you can spend a lot of time,

1:56:13.840 --> 1:56:15.840
 or you can write good algorithms

1:56:15.840 --> 1:56:19.920
 that try to spend most of their time in the corner cases.

1:56:19.920 --> 1:56:24.920
 So you basically imagine you're building an autonomous car

1:56:25.880 --> 1:56:27.360
 and you wanna put it in, I don't know,

1:56:27.360 --> 1:56:29.400
 downtown New Delhi all the time, right?

1:56:29.400 --> 1:56:30.760
 And accelerated testing.

1:56:31.640 --> 1:56:33.340
 If you can write sampling strategies,

1:56:33.340 --> 1:56:35.400
 which figure out where your controller's

1:56:35.400 --> 1:56:37.440
 performing badly in simulation

1:56:37.440 --> 1:56:40.600
 and start generating lots of examples around that.

1:56:40.600 --> 1:56:44.060
 You know, it's just the space of possible places

1:56:44.060 --> 1:56:48.040
 where that can be, where things can go wrong is very big.

1:56:48.040 --> 1:56:49.800
 So it's hard to write those algorithms.

1:56:49.800 --> 1:56:51.720
 Yeah, rare event simulation

1:56:51.720 --> 1:56:55.760
 is just a really compelling notion, if it's possible.

1:56:55.760 --> 1:56:58.600
 We joked and we call it the black swan generator.

1:56:58.600 --> 1:57:00.080
 It's a black swan.

1:57:00.080 --> 1:57:01.680
 Because you don't just want the rare events,

1:57:01.680 --> 1:57:04.020
 you want the ones that are highly impactful.

1:57:04.020 --> 1:57:05.680
 I mean, that's the most,

1:57:06.560 --> 1:57:08.780
 those are the most sort of profound questions

1:57:08.780 --> 1:57:10.120
 we ask of our world.

1:57:10.120 --> 1:57:15.120
 Like, what's the worst that can happen?

1:57:16.720 --> 1:57:18.080
 But what we're really asking

1:57:18.080 --> 1:57:20.800
 isn't some kind of like computer science,

1:57:20.800 --> 1:57:22.560
 worst case analysis.

1:57:22.560 --> 1:57:25.600
 We're asking like, what are the millions of ways

1:57:25.600 --> 1:57:27.360
 this can go wrong?

1:57:27.360 --> 1:57:29.500
 And that's like our curiosity.

1:57:29.500 --> 1:57:34.500
 And we humans, I think are pretty bad at,

1:57:34.900 --> 1:57:36.980
 we just like run into it.

1:57:36.980 --> 1:57:38.580
 And I think there's a distributed sense

1:57:38.580 --> 1:57:41.620
 because there's now like 7.5 billion of us.

1:57:41.620 --> 1:57:42.860
 And so there's a lot of them.

1:57:42.860 --> 1:57:45.060
 And then a lot of them write blog posts

1:57:45.060 --> 1:57:46.540
 about the stupid thing they've done.

1:57:46.540 --> 1:57:48.900
 So we learn in a distributed way.

1:57:49.980 --> 1:57:50.820
 There's some.

1:57:50.820 --> 1:57:53.380
 I think that's gonna be important for robots too.

1:57:53.380 --> 1:57:55.940
 I mean, that's another massive theme

1:57:55.940 --> 1:57:58.800
 at Toyota Research for Robotics

1:57:58.800 --> 1:58:00.540
 is this fleet learning concept

1:58:00.540 --> 1:58:04.780
 is the idea that I, as a human,

1:58:04.780 --> 1:58:07.880
 I don't have enough time to visit all of my states, right?

1:58:07.880 --> 1:58:10.140
 There's just a, it's very hard for one robot

1:58:10.140 --> 1:58:11.580
 to experience all the things.

1:58:12.640 --> 1:58:15.540
 But that's not actually the problem we have to solve, right?

1:58:16.540 --> 1:58:17.700
 We're gonna have fleets of robots

1:58:17.700 --> 1:58:20.660
 that can have very similar appendages.

1:58:20.660 --> 1:58:24.160
 And at some point, maybe collectively,

1:58:24.160 --> 1:58:26.220
 they have enough data

1:58:26.220 --> 1:58:29.340
 that their computational processes

1:58:29.340 --> 1:58:31.860
 should be set up differently than ours, right?

1:58:31.860 --> 1:58:34.180
 It's this vision of just,

1:58:34.180 --> 1:58:38.880
 I mean, all these dishwasher unloading robots.

1:58:38.880 --> 1:58:42.580
 I mean, that robot dropping a plate

1:58:42.580 --> 1:58:46.860
 and a human looking at the robot probably pissed off.

1:58:46.860 --> 1:58:47.820
 Yeah.

1:58:47.820 --> 1:58:51.220
 But that's a special moment to record.

1:58:51.220 --> 1:58:54.500
 I think one thing in terms of fleet learning,

1:58:54.500 --> 1:58:57.740
 and I've seen that because I've talked to a lot of folks,

1:58:57.740 --> 1:59:01.220
 just like Tesla users or Tesla drivers,

1:59:01.220 --> 1:59:02.980
 they're another company

1:59:02.980 --> 1:59:05.300
 that's using this kind of fleet learning idea.

1:59:05.300 --> 1:59:08.220
 One hopeful thing I have about humans

1:59:08.220 --> 1:59:13.220
 is they really enjoy when a system improves, learns.

1:59:13.260 --> 1:59:14.680
 So they enjoy fleet learning.

1:59:14.680 --> 1:59:17.260
 And the reason it's hopeful for me

1:59:17.260 --> 1:59:20.300
 is they're willing to put up with something

1:59:20.300 --> 1:59:22.660
 that's kind of dumb right now.

1:59:22.660 --> 1:59:25.540
 And they're like, if it's improving,

1:59:25.540 --> 1:59:29.460
 they almost like enjoy being part of the, like teaching it.

1:59:29.460 --> 1:59:30.960
 Almost like if you have kids,

1:59:30.960 --> 1:59:33.540
 like you're teaching them something, right?

1:59:33.540 --> 1:59:35.140
 I think that's a beautiful thing

1:59:35.140 --> 1:59:36.300
 because that gives me hope

1:59:36.300 --> 1:59:38.720
 that we can put dumb robots out there.

1:59:40.100 --> 1:59:43.340
 I mean, the problem on the Tesla side with cars,

1:59:43.340 --> 1:59:45.320
 cars can kill you.

1:59:45.320 --> 1:59:47.740
 That makes the problem so much harder.

1:59:47.740 --> 1:59:50.580
 Dishwasher unloading is a little safe.

1:59:50.580 --> 1:59:54.220
 That's why home robotics is really exciting.

1:59:54.220 --> 1:59:57.580
 And just to clarify, I mean, for people who might not know,

1:59:57.580 --> 2:00:00.100
 I mean, TRI, Toyota Research Institute.

2:00:00.100 --> 2:00:03.980
 So they're, I mean, they're pretty well known

2:00:03.980 --> 2:00:06.140
 for like autonomous vehicle research,

2:00:06.140 --> 2:00:10.260
 but they're also interested in home robotics.

2:00:10.260 --> 2:00:12.780
 Yep, there's a big group working on,

2:00:12.780 --> 2:00:14.340
 multiple groups working on home robotics.

2:00:14.340 --> 2:00:17.480
 It's a major part of the portfolio.

2:00:17.480 --> 2:00:19.100
 There's also a couple other projects

2:00:19.100 --> 2:00:21.300
 in advanced materials discovery,

2:00:21.300 --> 2:00:24.420
 using AI and machine learning to discover new materials

2:00:24.420 --> 2:00:28.540
 for car batteries and the like, for instance, yeah.

2:00:28.540 --> 2:00:31.500
 And that's been actually an incredibly successful team.

2:00:31.500 --> 2:00:33.540
 There's new projects starting up too, so.

2:00:33.540 --> 2:00:38.540
 Do you see a future of where like robots are in our home

2:00:38.940 --> 2:00:43.940
 and like robots that have like actuators

2:00:44.040 --> 2:00:46.620
 that look like arms in our home

2:00:46.620 --> 2:00:49.340
 or like, you know, more like humanoid type robots?

2:00:49.340 --> 2:00:51.820
 Or is this, are we gonna do the same thing

2:00:51.820 --> 2:00:53.860
 that you just mentioned that, you know,

2:00:53.860 --> 2:00:55.980
 the dishwasher is no longer a robot.

2:00:55.980 --> 2:00:58.700
 We're going to just not even see them as robots.

2:00:58.700 --> 2:01:02.500
 But I mean, what's your vision of the home of the future

2:01:02.500 --> 2:01:06.220
 10, 20 years from now, 50 years, if you get crazy?

2:01:06.220 --> 2:01:10.720
 Yeah, I think we already have Roombas cruising around.

2:01:10.720 --> 2:01:13.700
 We have, you know, Alexis or Google Homes

2:01:13.700 --> 2:01:16.240
 on our kitchen counter.

2:01:16.240 --> 2:01:18.060
 It's only a matter of time until they spring arms

2:01:18.060 --> 2:01:20.780
 and start doing something useful like that.

2:01:21.860 --> 2:01:23.860
 So I do think it's coming.

2:01:23.860 --> 2:01:27.660
 I think lots of people have lots of motivations

2:01:27.660 --> 2:01:29.380
 for doing it.

2:01:29.380 --> 2:01:31.520
 It's been super interesting actually learning

2:01:31.520 --> 2:01:33.900
 about Toyota's vision for it,

2:01:33.900 --> 2:01:36.380
 which is about helping people age in place.

2:01:38.700 --> 2:01:41.620
 Cause I think that's not necessarily the first entry,

2:01:41.620 --> 2:01:44.340
 the most lucrative entry point,

2:01:44.340 --> 2:01:48.680
 but it's the problem maybe that we really need to solve

2:01:48.680 --> 2:01:50.020
 no matter what.

2:01:50.020 --> 2:01:53.900
 And so I think there's a real opportunity.

2:01:53.900 --> 2:01:55.740
 It's a delicate problem.

2:01:55.740 --> 2:01:59.340
 How do you work with people, help people,

2:01:59.340 --> 2:02:02.320
 keep them active, engaged, you know,

2:02:03.300 --> 2:02:05.060
 but improve their quality of life

2:02:05.060 --> 2:02:08.340
 and help them age in place, for instance.

2:02:08.340 --> 2:02:12.440
 It's interesting because older folks are also,

2:02:12.440 --> 2:02:13.700
 I mean, there's a contrast there

2:02:13.700 --> 2:02:18.080
 because they're not always the folks

2:02:18.080 --> 2:02:20.900
 who are the most comfortable with technology, for example.

2:02:20.900 --> 2:02:24.860
 So there's a division that's interesting.

2:02:24.860 --> 2:02:29.860
 You can do so much good with a robot for older folks,

2:02:32.020 --> 2:02:36.380
 but there's a gap to fill of understanding.

2:02:36.380 --> 2:02:38.380
 I mean, it's actually kind of beautiful.

2:02:39.360 --> 2:02:41.140
 Robot is learning about the human

2:02:41.140 --> 2:02:44.820
 and the human is kind of learning about this new robot thing.

2:02:44.820 --> 2:02:49.660
 And it's also with, at least with,

2:02:49.660 --> 2:02:51.460
 like when I talked to my parents about robots,

2:02:51.460 --> 2:02:54.540
 there's a little bit of a blank slate there too.

2:02:54.540 --> 2:02:58.020
 Like you can, I mean, they don't know anything

2:02:58.020 --> 2:03:02.640
 about robotics, so it's completely like wide open.

2:03:02.640 --> 2:03:03.880
 They don't have, they haven't,

2:03:03.880 --> 2:03:05.780
 my parents haven't seen Black Mirror.

2:03:06.780 --> 2:03:09.460
 So like they, it's a blank slate.

2:03:09.460 --> 2:03:11.980
 Here's a cool thing, like what can it do for me?

2:03:11.980 --> 2:03:14.380
 Yeah, so it's an exciting space.

2:03:14.380 --> 2:03:16.340
 I think it's a really important space.

2:03:16.340 --> 2:03:20.020
 I do feel like a few years ago,

2:03:20.020 --> 2:03:22.740
 drones were successful enough in academia.

2:03:22.740 --> 2:03:25.980
 They kind of broke out and started an industry

2:03:25.980 --> 2:03:29.100
 and autonomous cars have been happening.

2:03:29.100 --> 2:03:32.900
 It does feel like manipulation in logistics, of course,

2:03:32.900 --> 2:03:35.700
 first, but in the home shortly after,

2:03:35.700 --> 2:03:37.180
 seems like one of the next big things

2:03:37.180 --> 2:03:40.060
 that's gonna really pop.

2:03:40.060 --> 2:03:42.100
 So I don't think we talked about it,

2:03:42.100 --> 2:03:44.540
 but what's soft robotics?

2:03:44.540 --> 2:03:49.300
 So we talked about like rigid bodies.

2:03:49.300 --> 2:03:52.020
 Like if we can just linger on this whole touch thing.

2:03:52.940 --> 2:03:54.620
 Yeah, so what's soft robotics?

2:03:54.620 --> 2:03:59.620
 So I told you that I really dislike the fact

2:04:00.780 --> 2:04:03.140
 that robots are afraid of touching the world

2:04:03.140 --> 2:04:04.860
 all over their body.

2:04:04.860 --> 2:04:06.900
 So there's a couple reasons for that.

2:04:06.900 --> 2:04:08.740
 If you look carefully at all the places

2:04:08.740 --> 2:04:11.220
 that robots actually do touch the world,

2:04:11.220 --> 2:04:12.540
 they're almost always soft.

2:04:12.540 --> 2:04:14.700
 They have some sort of pad on their fingers

2:04:14.700 --> 2:04:16.900
 or a rubber sole on their foot.

2:04:17.900 --> 2:04:19.300
 But if you look up and down the arm,

2:04:19.300 --> 2:04:21.700
 we're just pure aluminum or something.

2:04:25.340 --> 2:04:26.660
 So that makes it hard actually.

2:04:26.660 --> 2:04:30.460
 In fact, hitting the table with your rigid arm

2:04:30.460 --> 2:04:34.580
 or nearly rigid arm has some of the problems

2:04:34.580 --> 2:04:37.260
 that we talked about in terms of simulation.

2:04:37.260 --> 2:04:39.940
 I think it fundamentally changes the mechanics of contact

2:04:39.940 --> 2:04:41.260
 when you're soft, right?

2:04:41.260 --> 2:04:45.020
 You turn point contacts into patch contacts,

2:04:45.020 --> 2:04:47.020
 which can have torsional friction.

2:04:47.020 --> 2:04:49.260
 You can have distributed load.

2:04:49.260 --> 2:04:52.460
 If I wanna pick up an egg, right?

2:04:52.460 --> 2:04:54.300
 If I pick it up with two points,

2:04:54.300 --> 2:04:56.220
 then in order to put enough force

2:04:56.220 --> 2:04:57.340
 to sustain the weight of the egg,

2:04:57.340 --> 2:04:59.980
 I might have to put a lot of force to break the egg.

2:04:59.980 --> 2:05:04.460
 If I envelop it with contact all around,

2:05:04.460 --> 2:05:07.540
 then I can distribute my force across the shell of the egg

2:05:07.540 --> 2:05:10.620
 and have a better chance of not breaking it.

2:05:10.620 --> 2:05:12.860
 So soft robotics is for me a lot about changing

2:05:12.860 --> 2:05:15.500
 the mechanics of contact.

2:05:15.500 --> 2:05:17.380
 Does it make the problem a lot harder?

2:05:19.380 --> 2:05:22.260
 Quite the opposite.

2:05:24.020 --> 2:05:26.740
 It changes the computational problem.

2:05:26.740 --> 2:05:30.460
 I think because of the, I think our world

2:05:30.460 --> 2:05:34.180
 and our mathematics has biased us towards rigid.

2:05:34.180 --> 2:05:35.020
 I see.

2:05:35.020 --> 2:05:37.620
 But it really should make things better in some ways, right?

2:05:40.740 --> 2:05:43.060
 I think the future is unwritten there.

2:05:44.620 --> 2:05:45.460
 But the other thing it can do.

2:05:45.460 --> 2:05:46.820
 I think ultimately, sorry to interrupt,

2:05:46.820 --> 2:05:49.540
 but I think ultimately it will make things simpler

2:05:49.540 --> 2:05:51.580
 if we embrace the softness of the world.

2:05:51.580 --> 2:05:55.740
 It makes things smoother, right?

2:05:55.740 --> 2:06:00.740
 So the result of small actions is less discontinuous,

2:06:00.740 --> 2:06:05.740
 but it also means potentially less instantaneously bad.

2:06:05.980 --> 2:06:09.060
 For instance, I won't necessarily contact something

2:06:09.060 --> 2:06:10.420
 and send it flying off.

2:06:12.300 --> 2:06:13.140
 The other aspect of it

2:06:13.140 --> 2:06:14.860
 that just happens to dovetail really well

2:06:14.860 --> 2:06:17.260
 is that soft robotics tends to be a place

2:06:17.260 --> 2:06:19.100
 where we can embed a lot of sensors too.

2:06:19.100 --> 2:06:23.540
 So if you change your hardware and make it more soft,

2:06:23.540 --> 2:06:25.620
 then you can potentially have a tactile sensor,

2:06:25.620 --> 2:06:27.820
 which is measuring the deformation.

2:06:27.820 --> 2:06:32.180
 So there's a team at TRI that's working on soft hands

2:06:32.180 --> 2:06:35.500
 and you get so much more information.

2:06:35.500 --> 2:06:38.820
 You can put a camera behind the skin roughly

2:06:38.820 --> 2:06:42.860
 and get fantastic tactile information,

2:06:42.860 --> 2:06:46.180
 which is, it's super important.

2:06:46.180 --> 2:06:47.020
 Like in manipulation,

2:06:47.020 --> 2:06:49.820
 one of the things that really is frustrating

2:06:49.820 --> 2:06:52.140
 is if you work super hard on your head mounted,

2:06:52.140 --> 2:06:54.540
 on your perception system for your head mounted cameras,

2:06:54.540 --> 2:06:56.060
 and then you get a lot of information

2:06:56.060 --> 2:06:57.700
 for your head mounted cameras,

2:06:57.700 --> 2:06:59.460
 and then you've identified an object,

2:06:59.460 --> 2:07:00.380
 you reach down to touch it,

2:07:00.380 --> 2:07:01.900
 and the last thing that happens,

2:07:01.900 --> 2:07:03.980
 right before the most important time,

2:07:03.980 --> 2:07:04.820
 you stick your hand

2:07:04.820 --> 2:07:07.380
 and you're occluding your head mounted sensors.

2:07:07.380 --> 2:07:10.220
 So in all the part that really matters,

2:07:10.220 --> 2:07:13.580
 all of your off board sensors are occluded.

2:07:13.580 --> 2:07:15.900
 And really, if you don't have tactile information,

2:07:15.900 --> 2:07:19.300
 then you're blind in an important way.

2:07:19.300 --> 2:07:23.140
 So it happens that soft robotics and tactile sensing

2:07:23.140 --> 2:07:25.100
 tend to go hand in hand.

2:07:25.100 --> 2:07:26.820
 I think we've kind of talked about it,

2:07:26.820 --> 2:07:31.060
 but you taught a course on underactuated robotics.

2:07:31.060 --> 2:07:32.780
 I believe that was the name of it, actually.

2:07:32.780 --> 2:07:33.620
 That's right.

2:07:34.980 --> 2:07:37.340
 Can you talk about it in that context?

2:07:37.340 --> 2:07:40.380
 What is underactuated robotics?

2:07:40.380 --> 2:07:43.740
 Right, so underactuated robotics is my graduate course.

2:07:43.740 --> 2:07:46.620
 It's online mostly now,

2:07:46.620 --> 2:07:47.460
 in the sense that the lectures.

2:07:47.460 --> 2:07:49.060
 Several versions of it, I think.

2:07:49.060 --> 2:07:49.900
 Right, the YouTube.

2:07:49.900 --> 2:07:52.060
 It's really great, I recommend it highly.

2:07:52.060 --> 2:07:55.060
 Look on YouTube for the 2020 versions.

2:07:55.060 --> 2:07:57.460
 Until March, and then you have to go back to 2019,

2:07:57.460 --> 2:07:58.900
 thanks to COVID.

2:08:00.740 --> 2:08:03.540
 No, I've poured my heart into that class.

2:08:04.820 --> 2:08:06.620
 And lecture one is basically explaining

2:08:06.620 --> 2:08:07.940
 what the word underactuated means.

2:08:07.940 --> 2:08:09.860
 So people are very kind to show up

2:08:09.860 --> 2:08:12.220
 and then maybe have to learn

2:08:12.220 --> 2:08:13.460
 what the title of the course means

2:08:13.460 --> 2:08:15.420
 over the course of the first lecture.

2:08:15.420 --> 2:08:17.500
 That first lecture is really good.

2:08:17.500 --> 2:08:18.780
 You should watch it.

2:08:18.780 --> 2:08:19.860
 Thanks.

2:08:19.860 --> 2:08:21.500
 It's a strange name,

2:08:21.500 --> 2:08:25.860
 but I thought it captured the essence

2:08:25.860 --> 2:08:27.940
 of what control was good at doing

2:08:27.940 --> 2:08:29.980
 and what control was bad at doing.

2:08:29.980 --> 2:08:31.940
 So what do I mean by underactuated?

2:08:31.940 --> 2:08:34.700
 So a mechanical system

2:08:36.340 --> 2:08:39.500
 has many degrees of freedom, for instance.

2:08:39.500 --> 2:08:41.940
 I think of a joint as a degree of freedom.

2:08:41.940 --> 2:08:46.180
 And it has some number of actuators, motors.

2:08:46.180 --> 2:08:49.220
 So if you have a robot that's bolted to the table

2:08:49.220 --> 2:08:54.100
 that has five degrees of freedom and five motors,

2:08:54.100 --> 2:08:55.860
 then you have a fully actuated robot.

2:08:57.140 --> 2:09:00.540
 If you take away one of those motors,

2:09:00.540 --> 2:09:03.180
 then you have an underactuated robot.

2:09:03.180 --> 2:09:04.940
 Now, why on earth?

2:09:04.940 --> 2:09:07.460
 I have a good friend who likes to tease me.

2:09:07.460 --> 2:09:09.500
 He said, Ross, if you had more research funding,

2:09:09.500 --> 2:09:11.740
 would you work on fully actuated robots?

2:09:11.740 --> 2:09:12.580
 Yeah.

2:09:12.580 --> 2:09:15.180
 And the answer is no.

2:09:15.180 --> 2:09:17.420
 The world gives us underactuated robots,

2:09:17.420 --> 2:09:18.460
 whether we like it or not.

2:09:18.460 --> 2:09:19.860
 I'm a human.

2:09:19.860 --> 2:09:21.500
 I'm an underactuated robot,

2:09:21.500 --> 2:09:23.540
 even though I have more muscles

2:09:23.540 --> 2:09:25.220
 than my big degrees of freedom,

2:09:25.220 --> 2:09:27.740
 because I have in some places

2:09:27.740 --> 2:09:29.940
 multiple muscles attached to the same joint.

2:09:30.820 --> 2:09:33.900
 But still, there's a really important degree of freedom

2:09:33.900 --> 2:09:37.140
 that I have, which is the location of my center of mass

2:09:37.140 --> 2:09:38.580
 in space, for instance.

2:09:39.580 --> 2:09:42.500
 All right, I can jump into the air,

2:09:42.500 --> 2:09:45.220
 and there's no motor that connects my center of mass

2:09:45.220 --> 2:09:47.220
 to the ground in that case.

2:09:47.220 --> 2:09:49.420
 So I have to think about the implications

2:09:49.420 --> 2:09:51.700
 of not having control over everything.

2:09:52.740 --> 2:09:56.540
 The passive dynamic walkers are the extreme view of that,

2:09:56.540 --> 2:09:57.860
 where you've taken away all the motors,

2:09:57.860 --> 2:09:59.980
 and you have to let physics do the work.

2:09:59.980 --> 2:10:02.220
 But it shows up in all of the walking robots,

2:10:02.220 --> 2:10:04.540
 where you have to use some of the actuators

2:10:04.540 --> 2:10:06.980
 to push and pull even the degrees of freedom

2:10:06.980 --> 2:10:08.940
 that you don't have an actuator on.

2:10:09.980 --> 2:10:13.140
 That's referring to walking if you're falling forward.

2:10:13.140 --> 2:10:16.260
 Is there a way to walk that's fully actuated?

2:10:16.260 --> 2:10:18.340
 So it's a subtle point.

2:10:18.340 --> 2:10:23.340
 When you're in contact and you have your feet on the ground,

2:10:23.940 --> 2:10:26.540
 there are still limits to what you can do, right?

2:10:26.540 --> 2:10:29.140
 Unless I have suction cups on my feet,

2:10:29.140 --> 2:10:32.620
 I cannot accelerate my center of mass towards the ground

2:10:32.620 --> 2:10:33.780
 faster than gravity,

2:10:33.780 --> 2:10:37.420
 because I can't get a force pushing me down, right?

2:10:37.420 --> 2:10:39.420
 But I can still do most of the things that I want to.

2:10:39.420 --> 2:10:42.460
 So you can get away with basically thinking of the system

2:10:42.460 --> 2:10:43.420
 as fully actuated,

2:10:43.420 --> 2:10:46.460
 unless you suddenly needed to accelerate down super fast.

2:10:47.460 --> 2:10:49.260
 But as soon as I take a step,

2:10:49.260 --> 2:10:52.980
 I get into the more nuanced territory,

2:10:52.980 --> 2:10:55.780
 and to get to really dynamic robots,

2:10:55.780 --> 2:10:59.220
 or airplanes or other things,

2:10:59.220 --> 2:11:02.620
 I think you have to embrace the underactuated dynamics.

2:11:02.620 --> 2:11:06.940
 Manipulation, people think, is manipulation underactuated?

2:11:06.940 --> 2:11:10.580
 Even if my arm is fully actuated, I have a motor,

2:11:10.580 --> 2:11:14.260
 if my goal is to control the position and orientation

2:11:14.260 --> 2:11:18.460
 of this cup, then I don't have an actuator

2:11:18.460 --> 2:11:19.300
 for that directly.

2:11:19.300 --> 2:11:21.100
 So I have to use my actuators over here

2:11:21.100 --> 2:11:22.300
 to control this thing.

2:11:23.380 --> 2:11:24.340
 Now it gets even worse,

2:11:24.340 --> 2:11:27.740
 like what if I have to button my shirt, okay?

2:11:29.300 --> 2:11:31.340
 What are the degrees of freedom of my shirt, right?

2:11:31.340 --> 2:11:34.540
 I suddenly, that's a hard question to think about.

2:11:34.540 --> 2:11:36.740
 It kind of makes me queasy

2:11:36.740 --> 2:11:40.740
 thinking about my state space control ideas.

2:11:40.740 --> 2:11:41.820
 But actually those are the problems

2:11:41.820 --> 2:11:44.540
 that make me so excited about manipulation right now,

2:11:44.540 --> 2:11:47.020
 is that it breaks some of the,

2:11:48.020 --> 2:11:50.060
 it breaks a lot of the foundational control stuff

2:11:50.060 --> 2:11:51.420
 that I've been thinking about.

2:11:51.420 --> 2:11:54.580
 Is there, what are some interesting insights

2:11:54.580 --> 2:11:58.060
 you could say about trying to solve an underactuated,

2:11:58.060 --> 2:12:02.380
 a control in an underactuated system?

2:12:02.380 --> 2:12:04.820
 So I think the philosophy there

2:12:04.820 --> 2:12:07.220
 is let physics do more of the work.

2:12:08.460 --> 2:12:12.180
 The technical approach has been optimization.

2:12:12.180 --> 2:12:14.260
 So you typically formulate your decision making

2:12:14.260 --> 2:12:17.140
 for control as an optimization problem.

2:12:17.140 --> 2:12:19.420
 And you use the language of optimal control

2:12:19.420 --> 2:12:22.780
 and sometimes often numerical optimal control

2:12:22.780 --> 2:12:26.620
 in order to make those decisions and balance,

2:12:26.620 --> 2:12:29.100
 these complicated equations of,

2:12:29.100 --> 2:12:30.900
 and in order to control,

2:12:30.900 --> 2:12:33.140
 you don't have to use optimal control

2:12:33.140 --> 2:12:34.900
 to do underactuated systems,

2:12:34.900 --> 2:12:36.340
 but that has been the technical approach

2:12:36.340 --> 2:12:39.100
 that has borne the most fruit in our,

2:12:39.100 --> 2:12:40.900
 at least in our line of work.

2:12:40.900 --> 2:12:44.060
 And there's some, so in underactuated systems,

2:12:44.060 --> 2:12:46.820
 when you say let physics do some of the work,

2:12:46.820 --> 2:12:50.380
 so there's a kind of feedback loop

2:12:50.380 --> 2:12:54.540
 that observes the state that the physics brought you to.

2:12:54.540 --> 2:12:57.780
 So like you've, there's a perception there,

2:12:57.780 --> 2:13:00.420
 there's a feedback somehow.

2:13:00.420 --> 2:13:05.420
 Do you ever loop in like complicated perception systems

2:13:05.420 --> 2:13:06.900
 into this whole picture?

2:13:06.900 --> 2:13:09.620
 Right, right around the time of the DARPA challenge,

2:13:09.620 --> 2:13:11.340
 we had a complicated perception system

2:13:11.340 --> 2:13:12.700
 in the DARPA challenge.

2:13:12.700 --> 2:13:15.580
 We also started to embrace perception

2:13:15.580 --> 2:13:17.340
 for our flying vehicles at the time.

2:13:17.340 --> 2:13:20.100
 We had a really good project

2:13:20.100 --> 2:13:21.820
 on trying to make airplanes fly

2:13:21.820 --> 2:13:23.340
 at high speeds through forests.

2:13:24.780 --> 2:13:27.460
 Sirtash Karaman was on that project

2:13:27.460 --> 2:13:30.700
 and we had, it was a really fun team to work on.

2:13:30.700 --> 2:13:34.220
 He's carried it farther, much farther forward since then.

2:13:34.220 --> 2:13:35.980
 And that's using cameras for perception?

2:13:35.980 --> 2:13:37.580
 So that was using cameras.

2:13:37.580 --> 2:13:40.300
 That was, at the time we felt like LIDAR

2:13:40.300 --> 2:13:44.860
 was too heavy and too power heavy

2:13:44.860 --> 2:13:47.740
 to be carried on a light UAV,

2:13:47.740 --> 2:13:49.220
 and we were using cameras.

2:13:49.220 --> 2:13:50.660
 And that was a big part of it was just

2:13:50.660 --> 2:13:53.100
 how do you do even stereo matching

2:13:53.100 --> 2:13:56.460
 at a fast enough rate with a small camera,

2:13:56.460 --> 2:13:57.620
 small onboard compute.

2:13:58.620 --> 2:14:00.700
 Since then we have now,

2:14:00.700 --> 2:14:02.140
 so the deep learning revolution

2:14:02.140 --> 2:14:05.540
 unquestionably changed what we can do

2:14:05.540 --> 2:14:09.020
 with perception for robotics and control.

2:14:09.020 --> 2:14:11.020
 So in manipulation, we can address,

2:14:11.020 --> 2:14:14.660
 we can use perception in I think a much deeper way.

2:14:14.660 --> 2:14:17.340
 And we get into not only,

2:14:17.340 --> 2:14:19.820
 I think the first use of it naturally

2:14:19.820 --> 2:14:22.940
 would be to ask your deep learning system

2:14:22.940 --> 2:14:25.980
 to look at the cameras and produce the state,

2:14:25.980 --> 2:14:28.900
 which is like the pose of my thing, for instance.

2:14:28.900 --> 2:14:30.460
 But I think we've quickly found out

2:14:30.460 --> 2:14:33.620
 that that's not always the right thing to do.

2:14:34.460 --> 2:14:35.620
 Why is that?

2:14:35.620 --> 2:14:38.420
 Because what's the state of my shirt?

2:14:38.420 --> 2:14:39.740
 Imagine, I've always,

2:14:39.740 --> 2:14:41.300
 Very noisy, you mean, or?

2:14:41.300 --> 2:14:46.140
 It's, if the first step of me trying to button my shirt

2:14:46.140 --> 2:14:48.580
 is estimate the full state of my shirt,

2:14:48.580 --> 2:14:50.460
 including like what's happening in the back here,

2:14:50.460 --> 2:14:51.820
 whatever, whatever.

2:14:51.820 --> 2:14:55.780
 That's just not the right specification.

2:14:55.780 --> 2:14:57.500
 There are aspects of the state

2:14:57.500 --> 2:15:00.260
 that are very important to the task.

2:15:00.260 --> 2:15:03.220
 There are many that are unobservable

2:15:03.220 --> 2:15:05.860
 and not important to the task.

2:15:05.860 --> 2:15:06.940
 So you really need,

2:15:06.940 --> 2:15:11.100
 it begs new questions about state representation.

2:15:11.100 --> 2:15:13.100
 Another example that we've been playing with in lab

2:15:13.100 --> 2:15:17.660
 has been just the idea of chopping onions, okay?

2:15:17.660 --> 2:15:19.540
 Or carrots, turns out to be better.

2:15:20.540 --> 2:15:22.500
 So onions stink up the lab.

2:15:22.500 --> 2:15:25.380
 And they're hard to see in a camera.

2:15:26.220 --> 2:15:27.900
 But so,

2:15:27.900 --> 2:15:28.740
 Details matter, yeah.

2:15:28.740 --> 2:15:30.180
 Details matter, you know?

2:15:30.180 --> 2:15:35.180
 So if I'm moving around a particular object, right?

2:15:35.220 --> 2:15:36.060
 Then I think about,

2:15:36.060 --> 2:15:38.020
 oh, it's got a position or an orientation in space.

2:15:38.020 --> 2:15:39.780
 That's the description I want.

2:15:39.780 --> 2:15:42.300
 Now, when I'm chopping an onion, okay?

2:15:42.300 --> 2:15:44.260
 Like the first chop comes down.

2:15:44.260 --> 2:15:46.820
 I have now a hundred pieces of onion.

2:15:48.420 --> 2:15:50.300
 Does my control system really need to understand

2:15:50.300 --> 2:15:52.660
 the position and orientation and even the shape

2:15:52.660 --> 2:15:56.100
 of the hundred pieces of onion in order to make a decision?

2:15:56.100 --> 2:15:56.940
 Probably not, you know?

2:15:56.940 --> 2:15:58.900
 And if I keep going, I'm just getting,

2:15:58.900 --> 2:16:01.860
 more and more is my state space getting bigger as I cut?

2:16:04.740 --> 2:16:06.020
 It's not right.

2:16:06.020 --> 2:16:08.100
 So somehow there's a,

2:16:08.100 --> 2:16:13.100
 I think there's a richer idea of state.

2:16:13.100 --> 2:16:15.740
 It's not the state that is given to us

2:16:15.740 --> 2:16:17.180
 by Lagrangian mechanics.

2:16:17.180 --> 2:16:21.340
 There is a proper Lagrangian state of the system,

2:16:21.340 --> 2:16:26.340
 but the relevant state for this is some latent state

2:16:26.460 --> 2:16:28.540
 is what we call it in machine learning.

2:16:28.540 --> 2:16:32.180
 But, you know, there's some different state representation.

2:16:32.180 --> 2:16:35.020
 Some compressed representation, some.

2:16:35.020 --> 2:16:37.260
 And that's what I worry about saying compressed

2:16:37.260 --> 2:16:38.260
 because it doesn't,

2:16:38.260 --> 2:16:41.460
 I don't mind that it's low dimensional or not,

2:16:43.020 --> 2:16:46.260
 but it has to be something that's easier to think about.

2:16:46.260 --> 2:16:47.380
 By us humans.

2:16:48.460 --> 2:16:49.300
 Or my algorithms.

2:16:49.300 --> 2:16:53.860
 Or the algorithms being like control, optimal.

2:16:53.860 --> 2:16:56.540
 So for instance, if the contact mechanics

2:16:56.540 --> 2:16:59.660
 of all of those onion pieces and all the permutations

2:16:59.660 --> 2:17:02.540
 of possible touches between those onion pieces,

2:17:02.540 --> 2:17:03.620
 you know, you can give me

2:17:03.620 --> 2:17:05.100
 a high dimensional state representation,

2:17:05.100 --> 2:17:06.780
 I'm okay if it's linear.

2:17:06.780 --> 2:17:08.660
 But if I have to think about all the possible

2:17:08.660 --> 2:17:10.760
 shattering combinatorics of that,

2:17:11.700 --> 2:17:13.860
 then my robot's gonna sit there thinking

2:17:13.860 --> 2:17:17.380
 and the soup's gonna get cold or something.

2:17:17.380 --> 2:17:20.100
 So since you taught the course,

2:17:20.100 --> 2:17:22.740
 it kind of entered my mind,

2:17:22.740 --> 2:17:25.980
 the idea of underactuated as really compelling

2:17:25.980 --> 2:17:28.700
 to see the world in this kind of way.

2:17:29.540 --> 2:17:32.420
 Do you ever, you know, if we talk about onions

2:17:32.420 --> 2:17:35.480
 or you talk about the world with people in it in general,

2:17:35.480 --> 2:17:39.980
 do you see the world as basically an underactuated system?

2:17:39.980 --> 2:17:42.380
 Do you like often look at the world in this way?

2:17:42.380 --> 2:17:44.780
 Or is this overreach?

2:17:47.040 --> 2:17:49.160
 Underactuated is a way of life, man.

2:17:49.160 --> 2:17:51.480
 Exactly, I guess that's what I'm asking.

2:17:53.560 --> 2:17:54.960
 I do think it's everywhere.

2:17:54.960 --> 2:17:57.320
 I think in some places,

2:17:58.840 --> 2:18:01.380
 we already have natural tools to deal with it.

2:18:01.380 --> 2:18:02.480
 You know, it rears its head.

2:18:02.480 --> 2:18:04.280
 I mean, in linear systems, it's not a problem.

2:18:04.280 --> 2:18:07.340
 We just, like an underactuated linear system

2:18:07.340 --> 2:18:09.000
 is really not sufficiently distinct

2:18:09.000 --> 2:18:10.760
 from a fully actuated linear system.

2:18:10.760 --> 2:18:15.600
 It's a subtle point about when that becomes a bottleneck

2:18:15.600 --> 2:18:17.220
 in what we know how to do with control.

2:18:17.220 --> 2:18:18.840
 It happens to be a bottleneck,

2:18:19.800 --> 2:18:22.500
 although we've gotten incredibly good solutions now,

2:18:22.500 --> 2:18:24.200
 but for a long time that I felt

2:18:24.200 --> 2:18:27.100
 that that was the key bottleneck in legged robots.

2:18:27.100 --> 2:18:29.200
 And roughly now the underactuated course

2:18:29.200 --> 2:18:33.840
 is me trying to tell people everything I can

2:18:33.840 --> 2:18:37.280
 about how to make Atlas do a backflip, right?

2:18:38.500 --> 2:18:39.920
 I have a second course now

2:18:39.920 --> 2:18:41.280
 that I teach in the other semesters,

2:18:41.280 --> 2:18:43.600
 which is on manipulation.

2:18:43.600 --> 2:18:45.840
 And that's where we get into now more of the,

2:18:45.840 --> 2:18:47.160
 that's a newer class.

2:18:47.160 --> 2:18:51.600
 I'm hoping to put it online this fall completely.

2:18:51.600 --> 2:18:53.700
 And that's gonna have much more aspects

2:18:53.700 --> 2:18:55.460
 about these perception problems

2:18:55.460 --> 2:18:57.200
 and the state representation questions,

2:18:57.200 --> 2:18:59.260
 and then how do you do control.

2:18:59.260 --> 2:19:04.040
 And the thing that's a little bit sad is that,

2:19:04.040 --> 2:19:07.480
 for me at least, is there's a lot of manipulation tasks

2:19:07.480 --> 2:19:09.280
 that people wanna do and should wanna do.

2:19:09.280 --> 2:19:12.740
 They could start a company with it and be very successful

2:19:12.740 --> 2:19:15.600
 that don't actually require you to think that much

2:19:15.600 --> 2:19:18.040
 about underact, or dynamics at all even,

2:19:18.040 --> 2:19:20.020
 but certainly underactuated dynamics.

2:19:20.020 --> 2:19:23.100
 Once I have, if I reach out and grab something,

2:19:23.100 --> 2:19:25.720
 if I can sort of assume it's rigidly attached to my hand,

2:19:25.720 --> 2:19:26.920
 then I can do a lot of interesting,

2:19:26.920 --> 2:19:28.800
 meaningful things with it

2:19:28.800 --> 2:19:30.960
 without really ever thinking about the dynamics

2:19:30.960 --> 2:19:32.860
 of that object.

2:19:32.860 --> 2:19:37.860
 So we've built systems that kind of reduce the need for that.

2:19:37.860 --> 2:19:39.660
 Enveloping grasps and the like.

2:19:40.780 --> 2:19:43.060
 But I think the really good problems in manipulation.

2:19:43.060 --> 2:19:48.060
 So manipulation, by the way, is more than just pick and place.

2:19:48.540 --> 2:19:51.780
 That's like a lot of people think of that, just grasping.

2:19:51.780 --> 2:19:52.620
 I don't mean that.

2:19:52.620 --> 2:19:56.500
 I mean buttoning my shirt, I mean tying shoelaces.

2:19:56.500 --> 2:19:59.060
 How do you program a robot to tie shoelaces?

2:19:59.060 --> 2:20:02.860
 And not just one shoe, but every shoe, right?

2:20:02.860 --> 2:20:05.580
 That's a really good problem.

2:20:05.580 --> 2:20:08.420
 It's tempting to write down like the infinite dimensional

2:20:08.420 --> 2:20:13.180
 state of the laces, that's probably not needed

2:20:13.180 --> 2:20:15.100
 to write a good controller.

2:20:15.100 --> 2:20:18.340
 I know we could hand design a controller that would do it,

2:20:18.340 --> 2:20:19.180
 but I don't want that.

2:20:19.180 --> 2:20:22.460
 I want to understand the principles that would allow me

2:20:22.460 --> 2:20:25.380
 to solve another problem that's kind of like that.

2:20:25.380 --> 2:20:29.820
 But I think if we can stay pure in our approach,

2:20:29.820 --> 2:20:33.820
 then the challenge of tying anybody's shoes

2:20:33.820 --> 2:20:36.300
 is a great challenge.

2:20:36.300 --> 2:20:37.220
 That's a great challenge.

2:20:37.220 --> 2:20:40.940
 I mean, and the soft touch comes into play there.

2:20:40.940 --> 2:20:43.100
 That's really interesting.

2:20:43.100 --> 2:20:46.260
 Let me ask another ridiculous question on this topic.

2:20:47.500 --> 2:20:49.780
 How important is touch?

2:20:49.780 --> 2:20:52.300
 We haven't talked much about humans,

2:20:52.300 --> 2:20:54.780
 but I have this argument with my dad

2:20:56.220 --> 2:20:59.620
 where like I think you can fall in love with a robot

2:20:59.620 --> 2:21:02.580
 based on language alone.

2:21:02.580 --> 2:21:05.380
 And he believes that touch is essential.

2:21:06.460 --> 2:21:07.660
 Touch and smell, he says.

2:21:07.660 --> 2:21:12.660
 But so in terms of robots, connecting with humans,

2:21:17.380 --> 2:21:19.660
 we can go philosophical in terms of like a deep,

2:21:19.660 --> 2:21:21.820
 meaningful connection, like love,

2:21:21.820 --> 2:21:25.580
 but even just like collaborating in an interesting way,

2:21:25.580 --> 2:21:30.580
 how important is touch like from an engineering perspective

2:21:30.580 --> 2:21:32.780
 and a philosophical one?

2:21:32.780 --> 2:21:34.460
 I think it's super important.

2:21:35.700 --> 2:21:37.020
 Even just in a practical sense,

2:21:37.020 --> 2:21:39.260
 if we forget about the emotional part of it.

2:21:40.700 --> 2:21:43.300
 But for robots to interact safely

2:21:43.300 --> 2:21:46.380
 while they're doing meaningful mechanical work

2:21:47.220 --> 2:21:52.220
 in the close contact with or vicinity of people

2:21:52.420 --> 2:21:55.220
 that need help, I think we have to have them,

2:21:55.220 --> 2:21:57.500
 we have to build them differently.

2:21:57.500 --> 2:21:59.860
 They have to be afraid, not afraid of touching the world.

2:21:59.860 --> 2:22:02.820
 So I think Baymax is just awesome.

2:22:02.820 --> 2:22:06.260
 That's just like the movie of Big Hero 6

2:22:06.260 --> 2:22:08.700
 and the concept of Baymax, that's just awesome.

2:22:08.700 --> 2:22:13.060
 I think we should, and we have some folks at Toyota

2:22:13.060 --> 2:22:14.420
 that are trying to, Toyota Research

2:22:14.420 --> 2:22:16.860
 that are trying to build Baymax roughly.

2:22:16.860 --> 2:22:21.860
 And I think it's just a fantastically good project.

2:22:21.900 --> 2:22:25.620
 I think it will change the way people physically interact.

2:22:25.620 --> 2:22:27.980
 The same way, I mean, you gave a couple examples earlier,

2:22:27.980 --> 2:22:31.940
 but if the robot that was walking around my home

2:22:31.940 --> 2:22:33.980
 looked more like a teddy bear

2:22:33.980 --> 2:22:35.980
 and a little less like the Terminator,

2:22:35.980 --> 2:22:38.900
 that could change completely the way people perceive it

2:22:38.900 --> 2:22:39.820
 and interact with it.

2:22:39.820 --> 2:22:44.340
 And maybe they'll even wanna teach it, like you said, right?

2:22:44.340 --> 2:22:47.660
 You could not quite gamify it,

2:22:47.660 --> 2:22:50.060
 but somehow instead of people judging it

2:22:50.060 --> 2:22:54.340
 and looking at it as if it's not doing as well as a human,

2:22:54.340 --> 2:22:57.060
 they're gonna try to help out the cute teddy bear, right?

2:22:57.060 --> 2:23:01.260
 Who knows, but I think we're building robots wrong

2:23:01.260 --> 2:23:06.260
 and being more soft and more contact is important, right?

2:23:07.780 --> 2:23:09.860
 Yeah, I mean, like all the magical moments

2:23:09.860 --> 2:23:12.380
 I can remember with robots,

2:23:12.380 --> 2:23:15.980
 well, first of all, just visiting your lab and seeing Atlas,

2:23:16.900 --> 2:23:21.660
 but also Spotmini, when I first saw Spotmini in person

2:23:21.660 --> 2:23:26.260
 and hung out with him, her, it,

2:23:26.260 --> 2:23:28.380
 I don't have trouble engendering robots.

2:23:28.380 --> 2:23:31.500
 I feel the robotics people really say, oh, is it it?

2:23:31.500 --> 2:23:34.460
 I kinda like the idea that it's a her or a him.

2:23:35.780 --> 2:23:38.780
 There's a magical moment, but there's no touching.

2:23:38.780 --> 2:23:41.620
 I guess the question I have, have you ever been,

2:23:41.620 --> 2:23:44.940
 like, have you had a human robot experience

2:23:44.940 --> 2:23:47.940
 where a robot touched you?

2:23:49.580 --> 2:23:51.660
 And like, it was like, wait,

2:23:51.660 --> 2:23:53.980
 like, was there a moment that you've forgotten

2:23:53.980 --> 2:23:57.740
 that a robot is a robot and like,

2:23:57.740 --> 2:24:00.820
 the anthropomorphization stepped in

2:24:00.820 --> 2:24:03.500
 and for a second you forgot that it's not human?

2:24:04.900 --> 2:24:07.820
 I mean, I think when you're in on the details,

2:24:07.820 --> 2:24:12.380
 then we, of course, anthropomorphized our work with Atlas,

2:24:12.380 --> 2:24:17.100
 but in verbal communication and the like,

2:24:17.100 --> 2:24:18.980
 I think we were pretty aware of it

2:24:18.980 --> 2:24:21.740
 as a machine that needed to be respected.

2:24:21.740 --> 2:24:26.260
 And I actually, I worry more about the smaller robots

2:24:26.260 --> 2:24:29.540
 that could still move quickly if programmed wrong

2:24:29.540 --> 2:24:31.660
 and we have to be careful actually

2:24:31.660 --> 2:24:33.740
 about safety and the like right now.

2:24:33.740 --> 2:24:36.380
 And that, if we build our robots correctly,

2:24:36.380 --> 2:24:40.300
 I think then those, a lot of those concerns could go away.

2:24:40.300 --> 2:24:41.260
 And we're seeing that trend.

2:24:41.260 --> 2:24:44.100
 We're seeing the lower cost, lighter weight arms now

2:24:44.100 --> 2:24:46.740
 that could be fundamentally safe.

2:24:46.740 --> 2:24:49.060
 I mean, I do think touch is so fundamental.

2:24:49.060 --> 2:24:51.100
 Ted Adelson is great.

2:24:51.100 --> 2:24:54.580
 He's a perceptual scientist at MIT

2:24:55.740 --> 2:24:58.180
 and he studied vision most of his life.

2:24:58.180 --> 2:25:01.220
 And he said, when I had kids,

2:25:01.220 --> 2:25:04.540
 I expected to be fascinated by their perceptual development.

2:25:06.380 --> 2:25:09.260
 But what really, what he noticed was,

2:25:09.260 --> 2:25:10.780
 felt more impressive, more dominant

2:25:10.780 --> 2:25:13.060
 was the way that they would touch everything

2:25:13.060 --> 2:25:13.900
 and lick everything.

2:25:13.900 --> 2:25:16.900
 And pick things up, stick it on their tongue and whatever.

2:25:16.900 --> 2:25:21.900
 And he said, watching his daughter convinced him

2:25:22.180 --> 2:25:25.580
 that actually he needed to study tactile sensing more.

2:25:25.580 --> 2:25:30.580
 So there's something very important.

2:25:30.580 --> 2:25:32.780
 I think it's a little bit also of the passive

2:25:32.780 --> 2:25:35.660
 versus active part of the world, right?

2:25:35.660 --> 2:25:38.460
 You can passively perceive the world.

2:25:38.460 --> 2:25:41.460
 But it's fundamentally different if you can do an experiment

2:25:41.460 --> 2:25:43.340
 and if you can change the world

2:25:43.340 --> 2:25:46.220
 and you can learn a lot more than a passive observer.

2:25:47.460 --> 2:25:51.500
 So you can in dialogue, that was your initial example,

2:25:51.500 --> 2:25:54.580
 you could have an active experiment exchange.

2:25:54.580 --> 2:25:57.460
 But I think if you're just a camera watching YouTube,

2:25:57.460 --> 2:26:00.380
 I think that's a very different problem

2:26:00.380 --> 2:26:03.700
 than if you're a robot that can apply force.

2:26:03.700 --> 2:26:05.900
 And I think that's a very different problem

2:26:05.900 --> 2:26:10.740
 than if you're a robot that can apply force and touch.

2:26:13.260 --> 2:26:15.540
 I think it's important.

2:26:15.540 --> 2:26:18.020
 Yeah, I think it's just an exciting area of research.

2:26:18.020 --> 2:26:19.260
 I think you're probably right

2:26:19.260 --> 2:26:21.540
 that this hasn't been under researched.

2:26:23.900 --> 2:26:25.780
 To me as a person who's captivated

2:26:25.780 --> 2:26:27.820
 by the idea of human robot interaction,

2:26:27.820 --> 2:26:32.820
 it feels like such a rich opportunity to explore touch.

2:26:34.140 --> 2:26:35.860
 Not even from a safety perspective,

2:26:35.860 --> 2:26:38.060
 but like you said, the emotional too.

2:26:38.060 --> 2:26:39.660
 I mean, safety comes first,

2:26:41.220 --> 2:26:46.220
 but the next step is like a real human connection.

2:26:48.300 --> 2:26:51.380
 Even in the industrial setting,

2:26:51.380 --> 2:26:55.540
 it just feels like it's nice for the robot.

2:26:55.540 --> 2:26:58.060
 I don't know, you might disagree with this,

2:26:58.060 --> 2:27:01.220
 but because I think it's important

2:27:01.220 --> 2:27:04.340
 to see robots as tools often,

2:27:04.340 --> 2:27:06.060
 but I don't know,

2:27:06.060 --> 2:27:08.540
 I think they're just always going to be more effective

2:27:08.540 --> 2:27:10.140
 once you humanize them.

2:27:11.700 --> 2:27:14.340
 Like it's convenient now to think of them as tools

2:27:14.340 --> 2:27:16.140
 because we want to focus on the safety,

2:27:16.140 --> 2:27:21.140
 but I think ultimately to create like a good experience

2:27:22.300 --> 2:27:24.860
 for the worker, for the person,

2:27:24.860 --> 2:27:27.980
 there has to be a human element.

2:27:27.980 --> 2:27:29.260
 I don't know, for me,

2:27:30.140 --> 2:27:33.140
 it feels like an industrial robotic arm

2:27:33.140 --> 2:27:34.860
 would be better if it has a human element.

2:27:34.860 --> 2:27:37.060
 I think like Rethink Robotics had that idea

2:27:37.060 --> 2:27:40.260
 with the Baxter and having eyes and so on,

2:27:40.260 --> 2:27:43.100
 having, I don't know, I'm a big believer in that.

2:27:45.220 --> 2:27:48.060
 It's not my area, but I am also a big believer.

2:27:49.300 --> 2:27:51.900
 Do you have an emotional connection to Atlas?

2:27:51.900 --> 2:27:54.940
 Like do you miss him?

2:27:54.940 --> 2:27:59.940
 I mean, yes, I don't know if I more so

2:27:59.940 --> 2:28:01.620
 than if I had a different science project

2:28:01.620 --> 2:28:03.420
 that I'd worked on super hard, right?

2:28:03.420 --> 2:28:08.420
 But yeah, I mean, the robot,

2:28:09.900 --> 2:28:11.780
 we basically had to do heart surgery on the robot

2:28:11.780 --> 2:28:14.380
 in the final competition because we melted the core.

2:28:18.380 --> 2:28:20.140
 Yeah, there was something about watching that robot

2:28:20.140 --> 2:28:20.980
 hanging there.

2:28:20.980 --> 2:28:22.540
 We know we had to compete with it in an hour

2:28:22.540 --> 2:28:25.260
 and it was getting its guts ripped out.

2:28:25.260 --> 2:28:27.460
 Those are all historic moments.

2:28:27.460 --> 2:28:30.140
 I think if you look back like a hundred years from now,

2:28:32.140 --> 2:28:35.140
 yeah, I think those are important moments in robotics.

2:28:35.140 --> 2:28:36.660
 I mean, these are the early days.

2:28:36.660 --> 2:28:37.980
 You look at like the early days

2:28:37.980 --> 2:28:39.500
 of a lot of scientific disciplines.

2:28:39.500 --> 2:28:42.020
 They look ridiculous, they're full of failure,

2:28:42.020 --> 2:28:45.060
 but it feels like robotics will be important

2:28:45.060 --> 2:28:48.940
 in the coming a hundred years.

2:28:48.940 --> 2:28:50.860
 And these are the early days.

2:28:50.860 --> 2:28:54.420
 So I think a lot of people are,

2:28:54.420 --> 2:28:57.900
 look at a brilliant person such as yourself

2:28:57.900 --> 2:29:01.740
 and are curious about the intellectual journey they've took.

2:29:01.740 --> 2:29:06.260
 Is there maybe three books, technical, fiction,

2:29:06.260 --> 2:29:10.540
 philosophical that had a big impact on your life

2:29:10.540 --> 2:29:13.340
 that you would recommend perhaps others reading?

2:29:15.260 --> 2:29:18.460
 Yeah, so I actually didn't read that much as a kid,

2:29:18.460 --> 2:29:21.260
 but I read fairly voraciously now.

2:29:21.260 --> 2:29:24.940
 There are some recent books that if you're interested

2:29:24.940 --> 2:29:29.940
 in this kind of topic, like AI Superpowers by Kai Fu Lee

2:29:29.940 --> 2:29:31.660
 is just a fantastic read.

2:29:31.660 --> 2:29:33.100
 You must read that.

2:29:35.100 --> 2:29:40.100
 Yuval Harari is just, I think that can open your mind.

2:29:40.500 --> 2:29:41.580
 Sapiens.

2:29:41.580 --> 2:29:46.580
 Sapiens is the first one, Homo Deus is the second, yeah.

2:29:46.980 --> 2:29:48.340
 We mentioned it in the book,

2:29:48.340 --> 2:29:51.060
 Homo Deus is the second, yeah.

2:29:51.060 --> 2:29:53.500
 We mentioned The Black Swan by Taleb.

2:29:53.500 --> 2:29:56.100
 I think that's a good sort of mind opener.

2:29:57.220 --> 2:30:02.220
 I actually, so there's maybe a more controversial

2:30:04.420 --> 2:30:06.220
 recommendation I could give.

2:30:06.220 --> 2:30:08.740
 Great, we love controversy.

2:30:08.740 --> 2:30:11.580
 In some sense, it's so classical it might surprise you,

2:30:11.580 --> 2:30:16.020
 but I actually recently read Mortimer Adler's

2:30:16.020 --> 2:30:19.020
 How to Read a Book, not so long, it was a while ago,

2:30:19.020 --> 2:30:22.300
 but some people hate that book.

2:30:23.220 --> 2:30:24.820
 I loved it.

2:30:24.820 --> 2:30:28.980
 I think we're in this time right now where,

2:30:30.860 --> 2:30:33.780
 boy, we're just inundated with research papers

2:30:33.780 --> 2:30:38.580
 that you could read on archive with limited peer review

2:30:38.580 --> 2:30:40.980
 and just this wealth of information.

2:30:40.980 --> 2:30:45.980
 I don't know, I think the passion of what you can get

2:30:46.460 --> 2:30:49.460
 out of a book, a really good book or a really good paper

2:30:49.460 --> 2:30:52.220
 if you find it, the attitude, the realization

2:30:52.220 --> 2:30:54.220
 that you're only gonna find a few that really

2:30:54.220 --> 2:30:58.300
 are worth all your time, but then once you find them,

2:30:58.300 --> 2:31:02.660
 you should just dig in and understand it very deeply

2:31:02.660 --> 2:31:07.660
 and it's worth marking it up and having the hard copy

2:31:07.660 --> 2:31:11.340
 writing in the side notes, side margins.

2:31:11.340 --> 2:31:16.340
 I think that was really, I read it at the right time

2:31:16.340 --> 2:31:19.260
 where I was just feeling just overwhelmed

2:31:19.260 --> 2:31:22.260
 with really low quality stuff, I guess.

2:31:23.780 --> 2:31:28.780
 And similarly, I'm just giving more than three now,

2:31:28.780 --> 2:31:31.460
 I'm sorry if I've exceeded my quota.

2:31:31.460 --> 2:31:34.140
 But on that topic just real quick is,

2:31:34.140 --> 2:31:38.140
 so basically finding a few companions to keep

2:31:38.140 --> 2:31:41.340
 for the rest of your life in terms of papers and books

2:31:41.340 --> 2:31:44.140
 and so on and those are the ones,

2:31:44.140 --> 2:31:48.900
 like not doing, what is it, FOMO, fear of missing out,

2:31:48.900 --> 2:31:50.820
 constantly trying to update yourself,

2:31:50.820 --> 2:31:53.700
 but really deeply making a life journey

2:31:53.700 --> 2:31:57.500
 of studying a particular paper, essentially, set of papers.

2:31:57.500 --> 2:32:02.500
 Yeah, I think when you really start to understand

2:32:02.500 --> 2:32:06.100
 when you really find something,

2:32:06.100 --> 2:32:07.780
 which a book that resonates with you

2:32:07.780 --> 2:32:10.420
 might not be the same book that resonates with me,

2:32:10.420 --> 2:32:13.180
 but when you really find one that resonates with you,

2:32:13.180 --> 2:32:16.260
 I think the dialogue that happens and that's what,

2:32:16.260 --> 2:32:20.140
 I loved that Adler was saying, I think Socrates and Plato

2:32:20.140 --> 2:32:25.140
 say the written word is never gonna capture

2:32:25.740 --> 2:32:28.020
 the beauty of dialogue, right?

2:32:28.020 --> 2:32:33.020
 But Adler says, no, no, a really good book

2:32:33.100 --> 2:32:35.380
 is a dialogue between you and the author

2:32:35.380 --> 2:32:39.180
 and it crosses time and space and I don't know,

2:32:39.180 --> 2:32:40.740
 I think it's a very romantic,

2:32:40.740 --> 2:32:42.740
 there's a bunch of like specific advice,

2:32:42.740 --> 2:32:44.380
 which you can just gloss over,

2:32:44.380 --> 2:32:47.260
 but the romantic view of how to read

2:32:47.260 --> 2:32:51.060
 and really appreciate it is so good.

2:32:52.140 --> 2:32:53.900
 And similarly, teaching,

2:32:53.900 --> 2:32:58.820
 yeah, I thought a lot about teaching

2:32:58.820 --> 2:33:03.300
 and so Isaac Asimov, great science fiction writer,

2:33:03.300 --> 2:33:05.340
 has also actually spent a lot of his career

2:33:05.340 --> 2:33:07.260
 writing nonfiction, right?

2:33:07.260 --> 2:33:08.740
 His memoir is fantastic.

2:33:09.940 --> 2:33:12.740
 He was passionate about explaining things, right?

2:33:12.740 --> 2:33:13.700
 He wrote all kinds of books

2:33:13.700 --> 2:33:16.100
 on all kinds of topics in science.

2:33:16.100 --> 2:33:17.740
 He was known as the great explainer

2:33:17.740 --> 2:33:22.340
 and I do really resonate with his style

2:33:22.340 --> 2:33:27.180
 and just his way of talking about,

2:33:28.420 --> 2:33:30.540
 by communicating and explaining to something

2:33:30.540 --> 2:33:32.540
 is really the way that you learn something.

2:33:32.540 --> 2:33:36.260
 I think about problems very differently

2:33:36.260 --> 2:33:39.220
 because of the way I've been given the opportunity

2:33:39.220 --> 2:33:40.460
 to teach them at MIT.

2:33:42.140 --> 2:33:45.500
 We have questions asked, the fear of the lecture,

2:33:45.500 --> 2:33:47.700
 the experience of the lecture

2:33:47.700 --> 2:33:50.220
 and the questions I get and the interactions

2:33:50.220 --> 2:33:53.140
 just forces me to be rock solid on these ideas

2:33:53.140 --> 2:33:55.060
 in a way that if I didn't have that,

2:33:55.060 --> 2:33:58.260
 I don't know, I would be in a different intellectual space.

2:33:58.260 --> 2:34:00.420
 Also, video, does that scare you

2:34:00.420 --> 2:34:02.140
 that your lectures are online

2:34:02.140 --> 2:34:05.460
 and people like me in sweatpants can sit sipping coffee

2:34:05.460 --> 2:34:08.260
 and watch you give lectures?

2:34:08.260 --> 2:34:09.980
 I think it's great.

2:34:09.980 --> 2:34:12.820
 I do think that something's changed right now,

2:34:12.820 --> 2:34:16.900
 which is, right now we're giving lectures over Zoom.

2:34:16.900 --> 2:34:21.260
 I mean, giving seminars over Zoom and everything.

2:34:21.260 --> 2:34:24.380
 I'm trying to figure out, I think it's a new medium.

2:34:24.380 --> 2:34:28.020
 I'm trying to figure out how to exploit it.

2:34:28.020 --> 2:34:33.020
 Yeah, I've been quite cynical

2:34:34.500 --> 2:34:39.500
 about human to human connection over that medium,

2:34:39.820 --> 2:34:43.420
 but I think that's because it hasn't been explored fully

2:34:43.420 --> 2:34:45.780
 and teaching is a different thing.

2:34:45.780 --> 2:34:49.100
 Every lecture is a, I'm sorry, every seminar even,

2:34:49.100 --> 2:34:53.460
 I think every talk I give is an opportunity

2:34:53.460 --> 2:34:54.980
 to give that differently.

2:34:54.980 --> 2:34:57.940
 I can deliver content directly into your browser.

2:34:57.940 --> 2:35:00.020
 You have a WebGL engine right there.

2:35:00.020 --> 2:35:04.900
 I can throw 3D content into your browser

2:35:04.900 --> 2:35:06.900
 while you're listening to me, right?

2:35:06.900 --> 2:35:10.020
 And I can assume that you have at least

2:35:10.020 --> 2:35:13.020
 a powerful enough laptop or something to watch Zoom

2:35:13.020 --> 2:35:15.460
 while I'm doing that, while I'm giving a lecture.

2:35:15.460 --> 2:35:18.060
 That's a new communication tool

2:35:18.060 --> 2:35:19.980
 that I didn't have last year, right?

2:35:19.980 --> 2:35:24.180
 And I think robotics can potentially benefit a lot

2:35:24.180 --> 2:35:25.340
 from teaching that way.

2:35:26.420 --> 2:35:28.180
 We'll see, it's gonna be an experiment this fall.

2:35:28.180 --> 2:35:29.020
 It's interesting.

2:35:29.020 --> 2:35:30.340
 I'm thinking a lot about it.

2:35:30.340 --> 2:35:35.340
 Yeah, and also like the length of lectures

2:35:35.580 --> 2:35:38.820
 or the length of like, there's something,

2:35:38.820 --> 2:35:42.900
 so like I guarantee you, it's like 80% of people

2:35:42.900 --> 2:35:44.900
 who started listening to our conversation

2:35:44.900 --> 2:35:48.180
 are still listening to now, which is crazy to me.

2:35:48.180 --> 2:35:51.140
 But so there's a patience and interest

2:35:51.140 --> 2:35:53.540
 in long form content, but at the same time,

2:35:53.540 --> 2:35:57.940
 there's a magic to forcing yourself to condense

2:35:57.940 --> 2:36:01.220
 an idea to as short as possible.

2:36:02.740 --> 2:36:04.660
 As short as possible, like clip,

2:36:04.660 --> 2:36:06.180
 it can be a part of a longer thing,

2:36:06.180 --> 2:36:09.620
 but like just like really beautifully condense an idea.

2:36:09.620 --> 2:36:11.900
 There's a lot of opportunity there

2:36:11.900 --> 2:36:16.900
 that's easier to do in remote with, I don't know,

2:36:17.500 --> 2:36:19.020
 with editing too.

2:36:19.020 --> 2:36:20.980
 Editing is an interesting thing.

2:36:20.980 --> 2:36:25.020
 Like what, you know, most professors don't get,

2:36:25.020 --> 2:36:25.860
 when they give a lecture,

2:36:25.860 --> 2:36:28.220
 they don't get to go back and edit out parts,

2:36:28.220 --> 2:36:31.580
 like crisp it up a little bit.

2:36:31.580 --> 2:36:34.180
 That's also, it can do magic.

2:36:34.180 --> 2:36:37.620
 Like if you remove like five to 10 minutes

2:36:37.620 --> 2:36:41.140
 from an hour lecture, it can actually,

2:36:41.140 --> 2:36:43.220
 it can make something special of a lecture.

2:36:43.220 --> 2:36:47.860
 I've seen that in myself and in others too,

2:36:47.860 --> 2:36:50.580
 because I edit other people's lectures to extract clips.

2:36:50.580 --> 2:36:52.740
 It's like, there's certain tangents that are like,

2:36:52.740 --> 2:36:54.420
 that lose, they're not interesting.

2:36:54.420 --> 2:36:57.180
 They're mumbling, they're just not,

2:36:57.180 --> 2:36:59.780
 they're not clarifying, they're not helpful at all.

2:36:59.780 --> 2:37:02.820
 And once you remove them, it's just, I don't know.

2:37:02.820 --> 2:37:04.580
 Editing can be magic.

2:37:04.580 --> 2:37:05.900
 It takes a lot of time.

2:37:05.900 --> 2:37:08.940
 Yeah, it takes, it depends like what is teaching,

2:37:08.940 --> 2:37:09.780
 you have to ask.

2:37:09.780 --> 2:37:13.100
 Yeah, yeah.

2:37:13.100 --> 2:37:18.020
 Cause I find the editing process is also beneficial

2:37:18.020 --> 2:37:21.620
 as for teaching, but also for your own learning.

2:37:21.620 --> 2:37:23.740
 I don't know if, have you watched yourself?

2:37:23.740 --> 2:37:24.780
 Yeah, sure.

2:37:24.780 --> 2:37:26.180
 Have you watched those videos?

2:37:26.180 --> 2:37:27.900
 I mean, not all of them.

2:37:27.900 --> 2:37:32.900
 It could be painful to see like how to improve.

2:37:33.340 --> 2:37:37.180
 So do you find that, I know you segment your podcast.

2:37:37.180 --> 2:37:40.740
 Do you think that helps people with the,

2:37:40.740 --> 2:37:42.220
 the attention span aspect of it?

2:37:42.220 --> 2:37:44.220
 Or is it the segment like sections like,

2:37:44.220 --> 2:37:46.380
 yeah, we're talking about this topic, whatever.

2:37:46.380 --> 2:37:48.260
 Nope, nope, that just helps me.

2:37:48.260 --> 2:37:49.420
 It's actually bad.

2:37:49.420 --> 2:37:52.900
 So, and you've been incredible.

2:37:53.820 --> 2:37:56.420
 So I'm learning, like I'm afraid of conversation.

2:37:56.420 --> 2:37:59.180
 This is even today, I'm terrified of talking to you.

2:37:59.180 --> 2:38:04.180
 I mean, it's something I'm trying to remove for myself.

2:38:04.180 --> 2:38:07.420
 There's a guy, I mean, I've learned from a lot of people,

2:38:07.420 --> 2:38:10.740
 but really there's been a few people

2:38:10.740 --> 2:38:14.100
 who's been inspirational to me in terms of conversation.

2:38:14.100 --> 2:38:15.700
 Whatever people think of him,

2:38:15.700 --> 2:38:17.500
 Joe Rogan has been inspirational to me

2:38:17.500 --> 2:38:20.500
 because comedians have been too.

2:38:20.500 --> 2:38:23.300
 Being able to just have fun and enjoy themselves

2:38:23.300 --> 2:38:25.580
 and lose themselves in conversation

2:38:25.580 --> 2:38:28.820
 that requires you to be a great storyteller,

2:38:28.820 --> 2:38:31.500
 to be able to pull a lot of different pieces

2:38:31.500 --> 2:38:32.820
 of information together.

2:38:32.820 --> 2:38:36.500
 But mostly just to enjoy yourself in conversations.

2:38:36.500 --> 2:38:38.060
 And I'm trying to learn that.

2:38:38.060 --> 2:38:41.660
 These notes are, you see me looking down.

2:38:41.660 --> 2:38:43.020
 That's like a safety blanket

2:38:43.020 --> 2:38:45.260
 that I'm trying to let go of more and more.

2:38:45.260 --> 2:38:46.260
 Cool.

2:38:46.260 --> 2:38:49.420
 So that's, people love just regular conversation.

2:38:49.420 --> 2:38:52.660
 That's what they, the structure is like, whatever.

2:38:52.660 --> 2:38:57.620
 I would say, I would say maybe like 10 to like,

2:38:57.620 --> 2:38:59.820
 so there's a bunch of, you know,

2:38:59.820 --> 2:39:03.820
 there's probably a couple of thousand PhD students

2:39:03.820 --> 2:39:06.980
 listening to this right now, right?

2:39:06.980 --> 2:39:09.540
 And they might know what we're talking about.

2:39:09.540 --> 2:39:12.140
 But there is somebody, I guarantee you right now,

2:39:13.460 --> 2:39:16.580
 in Russia, some kid who's just like,

2:39:16.580 --> 2:39:19.380
 who's just smoked some weed, is sitting back

2:39:19.380 --> 2:39:22.580
 and just enjoying the hell out of this conversation.

2:39:22.580 --> 2:39:23.860
 Not really understanding.

2:39:23.860 --> 2:39:25.980
 He kind of watched some Boston Dynamics videos.

2:39:25.980 --> 2:39:27.300
 He's just enjoying it.

2:39:27.300 --> 2:39:29.300
 And I salute you, sir.

2:39:29.300 --> 2:39:32.780
 No, but just like, there's so much variety of people

2:39:32.780 --> 2:39:35.260
 that just have curiosity about engineering,

2:39:35.260 --> 2:39:37.980
 about sciences, about mathematics.

2:39:37.980 --> 2:39:42.980
 And also like, I should, I mean,

2:39:43.940 --> 2:39:44.980
 enjoying it is one thing,

2:39:44.980 --> 2:39:49.180
 but also often notice it inspires people to,

2:39:49.180 --> 2:39:50.860
 there's a lot of people who are like

2:39:50.860 --> 2:39:53.660
 in their undergraduate studies trying to figure out what,

2:39:54.700 --> 2:39:56.140
 trying to figure out what to pursue.

2:39:56.140 --> 2:39:59.220
 And these conversations can really spark

2:39:59.220 --> 2:40:01.820
 the direction of their life.

2:40:01.820 --> 2:40:03.580
 And in terms of robotics, I hope it does,

2:40:03.580 --> 2:40:06.540
 because I'm excited about the possibilities

2:40:06.540 --> 2:40:07.580
 of what robotics brings.

2:40:07.580 --> 2:40:12.580
 On that topic, do you have advice?

2:40:12.580 --> 2:40:14.060
 Like what advice would you give

2:40:14.060 --> 2:40:16.860
 to a young person about life?

2:40:18.260 --> 2:40:19.380
 A young person about life

2:40:19.380 --> 2:40:22.060
 or a young person about life in robotics?

2:40:23.060 --> 2:40:24.380
 It could be in robotics.

2:40:24.380 --> 2:40:26.660
 Robotics, it could be in life in general.

2:40:26.660 --> 2:40:28.460
 It could be career.

2:40:28.460 --> 2:40:31.300
 It could be a relationship advice.

2:40:31.300 --> 2:40:32.900
 It could be running advice.

2:40:32.900 --> 2:40:36.620
 Just like they're, that's one of the things I see,

2:40:36.620 --> 2:40:38.620
 like we talked to like 20 year olds.

2:40:38.620 --> 2:40:42.500
 They're like, how do I do this thing?

2:40:42.500 --> 2:40:43.900
 What do I do?

2:40:45.620 --> 2:40:48.020
 If they come up to you, what would you tell them?

2:40:48.020 --> 2:40:53.980
 I think it's an interesting time to be a kid these days.

2:40:53.980 --> 2:40:57.860
 Everything points to this being sort of a winner,

2:40:57.860 --> 2:40:59.300
 take all economy and the like.

2:40:59.300 --> 2:41:04.300
 I think the people that will really excel in my opinion

2:41:04.500 --> 2:41:06.820
 are going to be the ones that can think deeply

2:41:06.820 --> 2:41:08.380
 about problems.

2:41:11.180 --> 2:41:13.940
 You have to be able to ask questions agilely

2:41:13.940 --> 2:41:15.820
 and use the internet for everything it's good for

2:41:15.820 --> 2:41:16.660
 and stuff like this.

2:41:16.660 --> 2:41:19.460
 And I think a lot of people will develop those skills.

2:41:19.460 --> 2:41:24.460
 I think the leaders, thought leaders,

2:41:24.820 --> 2:41:26.860
 robotics leaders, whatever,

2:41:26.860 --> 2:41:29.100
 are gonna be the ones that can do more

2:41:29.100 --> 2:41:31.340
 and they can think very deeply and critically.

2:41:32.420 --> 2:41:35.020
 And that's a harder thing to learn.

2:41:35.020 --> 2:41:38.140
 I think one path to learning that is through mathematics,

2:41:38.140 --> 2:41:39.140
 through engineering.

2:41:41.660 --> 2:41:44.180
 I would encourage people to start math early.

2:41:44.180 --> 2:41:46.900
 I mean, I didn't really start.

2:41:46.900 --> 2:41:50.460
 I mean, I was always in the better math classes

2:41:50.460 --> 2:41:51.300
 that I could take,

2:41:51.300 --> 2:41:54.700
 but I wasn't pursuing super advanced mathematics

2:41:54.700 --> 2:41:56.700
 or anything like that until I got to MIT.

2:41:56.700 --> 2:41:59.020
 I think MIT lit me up

2:41:59.020 --> 2:42:04.020
 and really started the life that I'm living now.

2:42:05.580 --> 2:42:10.580
 But yeah, I really want kids to dig deep,

2:42:10.740 --> 2:42:12.460
 really understand things, building things too.

2:42:12.460 --> 2:42:15.180
 I mean, pull things apart, put them back together.

2:42:15.180 --> 2:42:17.180
 Like that's just such a good way

2:42:17.180 --> 2:42:19.980
 to really understand things

2:42:19.980 --> 2:42:23.660
 and expect it to be a long journey, right?

2:42:23.660 --> 2:42:27.260
 It's, you don't have to know everything.

2:42:27.260 --> 2:42:29.500
 You're never gonna know everything.

2:42:29.500 --> 2:42:31.460
 So think deeply and stick with it.

2:42:32.860 --> 2:42:35.300
 Enjoy the ride, but just make sure you're not,

2:42:37.580 --> 2:42:40.580
 yeah, just make sure you're stopping

2:42:40.580 --> 2:42:43.180
 to think about why things work.

2:42:43.180 --> 2:42:45.420
 And it's true, it's easy to lose yourself

2:42:45.420 --> 2:42:49.140
 in the distractions of the world.

2:42:51.180 --> 2:42:52.740
 We're overwhelmed with content right now,

2:42:52.740 --> 2:42:56.260
 but you have to stop and pick some of it

2:42:56.260 --> 2:42:58.780
 and really understand it.

2:42:58.780 --> 2:43:00.380
 Yeah, on the book point,

2:43:00.380 --> 2:43:04.940
 I've read Animal Farm by George Orwell

2:43:04.940 --> 2:43:06.100
 a ridiculous number of times.

2:43:06.100 --> 2:43:07.860
 So for me, like that book,

2:43:07.860 --> 2:43:09.780
 I don't know if it's a good book in general,

2:43:09.780 --> 2:43:12.140
 but for me it connects deeply somehow.

2:43:13.340 --> 2:43:18.260
 It somehow connects, so I was born in the Soviet Union.

2:43:18.260 --> 2:43:20.460
 So it connects to me into the entirety of the history

2:43:20.460 --> 2:43:23.180
 of the Soviet Union and to World War II

2:43:23.180 --> 2:43:26.500
 and to the love and hatred and suffering

2:43:26.500 --> 2:43:31.500
 that went on there and the corrupting nature of power

2:43:33.140 --> 2:43:36.340
 and greed and just somehow I just,

2:43:36.340 --> 2:43:38.100
 that book has taught me more about life

2:43:38.100 --> 2:43:39.380
 than like anything else.

2:43:39.380 --> 2:43:42.860
 Even though it's just like a silly childlike book

2:43:42.860 --> 2:43:46.980
 about pigs, I don't know why,

2:43:46.980 --> 2:43:49.300
 it just connects and inspires.

2:43:49.300 --> 2:43:53.780
 The same, there's a few technical books too

2:43:53.780 --> 2:43:58.020
 and algorithms that just, yeah, you return to often.

2:43:58.020 --> 2:43:59.700
 I'm with you.

2:44:01.900 --> 2:44:04.100
 Yeah, there's, and I've been losing that

2:44:04.100 --> 2:44:05.380
 because of the internet.

2:44:05.380 --> 2:44:09.700
 I've been like going on, I've been going on archive

2:44:09.700 --> 2:44:12.420
 and blog posts and GitHub and the new thing

2:44:12.420 --> 2:44:17.420
 and you lose your ability to really master an idea.

2:44:18.100 --> 2:44:18.940
 Right.

2:44:18.940 --> 2:44:19.780
 Wow.

2:44:19.780 --> 2:44:21.100
 Exactly right.

2:44:21.100 --> 2:44:23.540
 What's a fond memory from childhood?

2:44:24.940 --> 2:44:27.460
 When baby Russ Tedrick.

2:44:29.540 --> 2:44:33.940
 Well, I guess I just said that at least my current life

2:44:33.940 --> 2:44:36.780
 began when I got to MIT.

2:44:36.780 --> 2:44:38.900
 If I have to go farther than that.

2:44:38.900 --> 2:44:41.300
 Yeah, what was, was there a life before MIT?

2:44:42.260 --> 2:44:47.260
 Oh, absolutely, but let me actually tell you

2:44:47.380 --> 2:44:48.900
 what happened when I first got to MIT

2:44:48.900 --> 2:44:52.220
 because that I think might be relevant here,

2:44:52.220 --> 2:44:57.220
 but I had taken a computer engineering degree at Michigan.

2:44:57.540 --> 2:45:00.420
 I enjoyed it immensely, learned a bunch of stuff.

2:45:00.420 --> 2:45:04.580
 I liked computers, I liked programming,

2:45:04.580 --> 2:45:07.340
 but when I did get to MIT and started working

2:45:07.340 --> 2:45:10.300
 with Sebastian Sung, theoretical physicist,

2:45:10.300 --> 2:45:15.180
 computational neuroscientist, the culture here

2:45:15.180 --> 2:45:17.220
 was just different.

2:45:17.220 --> 2:45:20.260
 It demanded more of me, certainly mathematically

2:45:20.260 --> 2:45:22.660
 and in the critical thinking.

2:45:22.660 --> 2:45:27.660
 And I remember the day that I borrowed one of the books

2:45:27.700 --> 2:45:29.780
 from my advisor's office and walked down

2:45:29.780 --> 2:45:32.140
 to the Charles River and was like,

2:45:32.140 --> 2:45:33.540
 I'm getting my butt kicked.

2:45:36.620 --> 2:45:38.180
 And I think that's gonna happen to everybody

2:45:38.180 --> 2:45:40.220
 who's doing this kind of stuff.

2:45:40.220 --> 2:45:45.220
 I think I expected you to ask me the meaning of life.

2:45:46.020 --> 2:45:51.020
 I think that somehow I think that's gotta be part of it.

2:45:52.780 --> 2:45:53.940
 Doing hard things?

2:45:55.140 --> 2:45:56.460
 Yeah.

2:45:56.460 --> 2:45:58.220
 Did you consider quitting at any point?

2:45:58.220 --> 2:45:59.780
 Did you consider this isn't for me?

2:45:59.780 --> 2:46:01.740
 No, never that.

2:46:01.740 --> 2:46:06.740
 I was working hard, but I was loving it.

2:46:07.180 --> 2:46:08.860
 I think there's this magical thing

2:46:08.860 --> 2:46:11.900
 where I'm lucky to surround myself with people

2:46:11.900 --> 2:46:16.900
 that basically almost every day I'll see something,

2:46:17.900 --> 2:46:20.340
 I'll be told something or something that I realize,

2:46:20.340 --> 2:46:22.020
 wow, I don't understand that.

2:46:22.020 --> 2:46:24.180
 And if I could just understand that,

2:46:24.180 --> 2:46:26.020
 there's something else to learn.

2:46:26.020 --> 2:46:28.140
 That if I could just learn that thing,

2:46:28.140 --> 2:46:30.220
 I would connect another piece of the puzzle.

2:46:30.220 --> 2:46:35.220
 And I think that is just such an important aspect

2:46:36.220 --> 2:46:40.260
 and being willing to understand what you can and can't do

2:46:40.260 --> 2:46:43.580
 and loving the journey of going

2:46:43.580 --> 2:46:44.820
 and learning those other things.

2:46:44.820 --> 2:46:46.260
 I think that's the best part.

2:46:47.340 --> 2:46:51.500
 I don't think there's a better way to end it, Russ.

2:46:51.500 --> 2:46:55.580
 You've been an inspiration to me since I showed up at MIT.

2:46:55.580 --> 2:46:57.700
 Your work has been an inspiration to the world.

2:46:57.700 --> 2:46:59.740
 This conversation was amazing.

2:46:59.740 --> 2:47:01.700
 I can't wait to see what you do next

2:47:01.700 --> 2:47:03.220
 with robotics, home robots.

2:47:03.220 --> 2:47:05.780
 I hope to see you work in my home one day.

2:47:05.780 --> 2:47:08.100
 So thanks so much for talking today, it's been awesome.

2:47:08.100 --> 2:47:09.480
 Cheers.

2:47:09.480 --> 2:47:11.060
 Thanks for listening to this conversation

2:47:11.060 --> 2:47:14.180
 with Russ Tedrick and thank you to our sponsors,

2:47:14.180 --> 2:47:18.220
 Magic Spoon Cereal, BetterHelp and ExpressVPN.

2:47:18.220 --> 2:47:20.180
 Please consider supporting this podcast

2:47:20.180 --> 2:47:23.420
 by going to magicspoon.com slash Lex

2:47:23.420 --> 2:47:25.500
 and using code Lex at checkout.

2:47:25.500 --> 2:47:27.780
 Go into betterhelp.com slash Lex

2:47:27.780 --> 2:47:32.780
 and signing up at expressvpn.com slash Lex pod.

2:47:32.820 --> 2:47:36.180
 Click the links, buy the stuff, get the discount.

2:47:36.180 --> 2:47:39.380
 It really is the best way to support this podcast.

2:47:39.380 --> 2:47:41.520
 If you enjoy this thing, subscribe on YouTube,

2:47:41.520 --> 2:47:43.700
 review it with five stars and up a podcast,

2:47:43.700 --> 2:47:46.540
 support on Patreon or connect with me on Twitter

2:47:46.540 --> 2:47:50.620
 at Lex Friedman spelled somehow without the E

2:47:50.620 --> 2:47:53.460
 just F R I D M A N.

2:47:53.460 --> 2:47:55.100
 And now let me leave you with some words

2:47:55.100 --> 2:47:58.540
 from Neil deGrasse Tyson talking about robots in space

2:47:58.540 --> 2:48:00.680
 and the emphasis we humans put

2:48:00.680 --> 2:48:03.640
 on human based space exploration.

2:48:03.640 --> 2:48:05.680
 Robots are important.

2:48:05.680 --> 2:48:07.980
 If I don my pure scientist hat,

2:48:07.980 --> 2:48:10.020
 I would say just send robots.

2:48:10.020 --> 2:48:12.340
 I'll stay down here and get the data.

2:48:12.340 --> 2:48:15.080
 But nobody's ever given a parade for a robot.

2:48:15.080 --> 2:48:17.940
 Nobody's ever named a high school after a robot.

2:48:17.940 --> 2:48:20.180
 So when I don my public educator hat,

2:48:20.180 --> 2:48:22.780
 I have to recognize the elements of exploration

2:48:22.780 --> 2:48:24.180
 that excite people.

2:48:24.180 --> 2:48:26.980
 It's not only the discoveries and the beautiful photos

2:48:26.980 --> 2:48:29.020
 that come down from the heavens.

2:48:29.020 --> 2:48:33.020
 It's the vicarious participation in discovery itself.

2:48:33.020 --> 2:48:54.020
 Thank you for listening and hope to see you next time.

