WEBVTT

00:00.000 --> 00:02.840
 The following is a conversation with Max Tegmark,

00:02.840 --> 00:04.760
 his second time on the podcast.

00:04.760 --> 00:07.120
 In fact, the previous conversation

00:07.120 --> 00:10.960
 was episode number one of this very podcast.

00:10.960 --> 00:14.800
 He is a physicist and artificial intelligence researcher

00:14.800 --> 00:18.840
 at MIT, cofounder of the Future of Life Institute,

00:18.840 --> 00:21.360
 and author of Life 3.0,

00:21.360 --> 00:24.560
 Being Human in the Age of Artificial Intelligence.

00:24.560 --> 00:27.120
 He's also the head of a bunch of other huge,

00:27.120 --> 00:29.240
 fascinating projects and has written

00:29.240 --> 00:30.560
 a lot of different things

00:30.560 --> 00:32.080
 that you should definitely check out.

00:32.080 --> 00:34.480
 He has been one of the key humans

00:34.480 --> 00:37.400
 who has been outspoken about longterm existential risks

00:37.400 --> 00:40.500
 of AI and also its exciting possibilities

00:40.500 --> 00:42.900
 and solutions to real world problems.

00:42.900 --> 00:46.440
 Most recently at the intersection of AI and physics,

00:46.440 --> 00:50.000
 and also in reengineering the algorithms

00:50.000 --> 00:53.200
 that divide us by controlling the information we see

00:53.200 --> 00:56.160
 and thereby creating bubbles and all other kinds

00:56.160 --> 00:59.640
 of complex social phenomena that we see today.

00:59.640 --> 01:01.440
 In general, he's one of the most passionate

01:01.440 --> 01:04.340
 and brilliant people I have the fortune of knowing.

01:04.340 --> 01:06.180
 I hope to talk to him many more times

01:06.180 --> 01:08.280
 on this podcast in the future.

01:08.280 --> 01:10.000
 Quick mention of our sponsors,

01:10.000 --> 01:12.160
 The Jordan Harbinger Show,

01:12.160 --> 01:14.360
 Four Sigmatic Mushroom Coffee,

01:14.360 --> 01:18.480
 BetterHelp Online Therapy, and ExpressVPN.

01:18.480 --> 01:23.560
 So the choices, wisdom, caffeine, sanity, or privacy.

01:23.560 --> 01:25.880
 Choose wisely, my friends, and if you wish,

01:25.880 --> 01:28.360
 click the sponsor links below to get a discount

01:28.360 --> 01:30.560
 and to support this podcast.

01:30.560 --> 01:33.900
 As a side note, let me say that much of the researchers

01:33.900 --> 01:35.400
 in the machine learning

01:35.400 --> 01:37.760
 and artificial intelligence communities

01:37.760 --> 01:40.400
 do not spend much time thinking deeply

01:40.400 --> 01:42.720
 about existential risks of AI.

01:42.720 --> 01:46.160
 Because our current algorithms are seen as useful but dumb,

01:46.160 --> 01:49.240
 it's difficult to imagine how they may become destructive

01:49.240 --> 01:51.240
 to the fabric of human civilization

01:51.240 --> 01:53.040
 in the foreseeable future.

01:53.040 --> 01:56.120
 I understand this mindset, but it's very troublesome.

01:56.120 --> 02:00.480
 To me, this is both a dangerous and uninspiring perspective,

02:00.480 --> 02:03.980
 reminiscent of a lobster sitting in a pot of lukewarm water

02:03.980 --> 02:06.160
 that a minute ago was cold.

02:06.160 --> 02:08.640
 I feel a kinship with this lobster.

02:08.640 --> 02:10.560
 I believe that already the algorithms

02:10.560 --> 02:12.960
 that drive our interaction on social media

02:12.960 --> 02:14.980
 have an intelligence and power

02:14.980 --> 02:17.360
 that far outstrip the intelligence and power

02:17.360 --> 02:19.220
 of any one human being.

02:19.220 --> 02:21.640
 Now really is the time to think about this,

02:21.640 --> 02:24.140
 to define the trajectory of the interplay

02:24.140 --> 02:26.940
 of technology and human beings in our society.

02:26.940 --> 02:29.680
 I think that the future of human civilization

02:29.680 --> 02:32.820
 very well may be at stake over this very question

02:32.820 --> 02:36.240
 of the role of artificial intelligence in our society.

02:36.240 --> 02:38.160
 If you enjoy this thing, subscribe on YouTube,

02:38.160 --> 02:40.960
 review it on Apple Podcasts, follow on Spotify,

02:40.960 --> 02:43.840
 support on Patreon, or connect with me on Twitter

02:43.840 --> 02:45.260
 at Lex Friedman.

02:45.260 --> 02:48.840
 And now, here's my conversation with Max Tegmark.

02:49.880 --> 02:51.440
 So people might not know this,

02:51.440 --> 02:55.280
 but you were actually episode number one of this podcast

02:55.280 --> 02:59.280
 just a couple of years ago, and now we're back.

02:59.280 --> 03:02.280
 And it so happens that a lot of exciting things happened

03:02.280 --> 03:05.600
 in both physics and artificial intelligence,

03:05.600 --> 03:08.480
 both fields that you're super passionate about.

03:08.480 --> 03:11.640
 Can we try to catch up to some of the exciting things

03:11.640 --> 03:14.080
 happening in artificial intelligence,

03:14.080 --> 03:17.340
 especially in the context of the way it's cracking,

03:17.340 --> 03:20.020
 open the different problems of the sciences?

03:20.020 --> 03:24.520
 Yeah, I'd love to, especially now as we start 2021 here,

03:24.520 --> 03:26.280
 it's a really fun time to think about

03:26.280 --> 03:29.560
 what were the biggest breakthroughs in AI,

03:29.560 --> 03:31.800
 not the ones necessarily that media wrote about,

03:31.800 --> 03:35.200
 but that really matter, and what does that mean

03:35.200 --> 03:37.440
 for our ability to do better science?

03:37.440 --> 03:39.920
 What does it mean for our ability

03:39.920 --> 03:43.160
 to help people around the world?

03:43.160 --> 03:46.440
 And what does it mean for new problems

03:46.440 --> 03:48.440
 that they could cause if we're not smart enough

03:48.440 --> 03:51.880
 to avoid them, so what do we learn basically from this?

03:51.880 --> 03:52.720
 Yes, absolutely.

03:52.720 --> 03:54.960
 So one of the amazing things you're a part of

03:54.960 --> 03:57.680
 is the AI Institute for Artificial Intelligence

03:57.680 --> 04:00.160
 and Fundamental Interactions.

04:00.160 --> 04:02.280
 What's up with this institute?

04:02.280 --> 04:03.680
 What are you working on?

04:03.680 --> 04:05.000
 What are you thinking about?

04:05.000 --> 04:09.080
 The idea is something I'm very on fire with,

04:09.080 --> 04:11.920
 which is basically AI meets physics.

04:11.920 --> 04:15.360
 And it's been almost five years now

04:15.360 --> 04:18.360
 since I shifted my own MIT research

04:18.360 --> 04:20.680
 from physics to machine learning.

04:20.680 --> 04:22.880
 And in the beginning, I noticed that a lot of my colleagues,

04:22.880 --> 04:24.280
 even though they were polite about it,

04:24.280 --> 04:27.760
 were like kind of, what is Max doing?

04:27.760 --> 04:29.040
 What is this weird stuff?

04:29.040 --> 04:30.880
 He's lost his mind.

04:30.880 --> 04:35.080
 But then gradually, I, together with some colleagues,

04:35.080 --> 04:40.080
 were able to persuade more and more of the other professors

04:40.080 --> 04:42.520
 in our physics department to get interested in this.

04:42.520 --> 04:46.320
 And now we've got this amazing NSF Center,

04:46.320 --> 04:50.000
 so 20 million bucks for the next five years, MIT,

04:50.000 --> 04:53.200
 and a bunch of neighboring universities here also.

04:53.200 --> 04:55.280
 And I noticed now those colleagues

04:55.280 --> 04:57.040
 who were looking at me funny have stopped

04:57.040 --> 05:00.320
 asking what the point is of this,

05:00.320 --> 05:02.400
 because it's becoming more clear.

05:02.400 --> 05:05.560
 And I really believe that, of course,

05:05.560 --> 05:09.440
 AI can help physics a lot to do better physics.

05:09.440 --> 05:13.160
 But physics can also help AI a lot,

05:13.160 --> 05:16.440
 both by building better hardware.

05:16.440 --> 05:18.840
 My colleague, Marin Soljacic, for example,

05:18.840 --> 05:23.000
 is working on an optical chip for much faster machine

05:23.000 --> 05:25.360
 learning, where the computation is done

05:25.360 --> 05:30.240
 not by moving electrons around, but by moving photons around,

05:30.240 --> 05:34.240
 dramatically less energy use, faster, better.

05:34.240 --> 05:37.560
 We can also help AI a lot, I think,

05:37.560 --> 05:42.840
 by having a different set of tools

05:42.840 --> 05:46.440
 and a different, maybe more audacious attitude.

05:46.440 --> 05:51.560
 AI has, to a significant extent, been an engineering discipline

05:51.560 --> 05:54.240
 where you're just trying to make things that work

05:54.240 --> 05:56.560
 and being more interested in maybe selling them

05:56.560 --> 06:00.280
 than in figuring out exactly how they work

06:00.280 --> 06:03.680
 and proving theorems about that they will always work.

06:03.680 --> 06:05.240
 Contrast that with physics.

06:05.240 --> 06:08.920
 When Elon Musk sends a rocket to the International Space

06:08.920 --> 06:12.080
 Station, they didn't just train with machine learning.

06:12.080 --> 06:14.080
 Oh, let's fire it a little bit more to the left,

06:14.080 --> 06:14.920
 a bit more to the right.

06:14.920 --> 06:15.800
 Oh, that also missed.

06:15.800 --> 06:16.920
 Let's try here.

06:16.920 --> 06:23.400
 No, we figured out Newton's laws of gravitation and other things

06:23.400 --> 06:26.800
 and got a really deep fundamental understanding.

06:26.800 --> 06:30.840
 And that's what gives us such confidence in rockets.

06:30.840 --> 06:36.040
 And my vision is that in the future,

06:36.040 --> 06:38.840
 all machine learning systems that actually have impact

06:38.840 --> 06:40.960
 on people's lives will be understood

06:40.960 --> 06:43.120
 at a really, really deep level.

06:43.120 --> 06:46.960
 So we trust them, not because some sales rep told us to,

06:46.960 --> 06:50.360
 but because they've earned our trust.

06:50.360 --> 06:51.800
 And really safety critical things

06:51.800 --> 06:55.600
 even prove that they will always do what we expect them to do.

06:55.600 --> 06:57.040
 That's very much the physics mindset.

06:57.040 --> 07:00.160
 So it's interesting, if you look at big breakthroughs

07:00.160 --> 07:03.680
 that have happened in machine learning this year,

07:03.680 --> 07:08.200
 from dancing robots, it's pretty fantastic.

07:08.200 --> 07:10.280
 Not just because it's cool, but if you just

07:10.280 --> 07:12.880
 think about not that many years ago,

07:12.880 --> 07:16.680
 this YouTube video at this DARPA challenge with the MIT robot

07:16.680 --> 07:20.680
 comes out of the car and face plants.

07:20.680 --> 07:23.840
 How far we've come in just a few years.

07:23.840 --> 07:30.360
 Similarly, Alpha Fold 2, crushing the protein folding

07:30.360 --> 07:31.160
 problem.

07:31.160 --> 07:33.080
 We can talk more about implications

07:33.080 --> 07:34.400
 for medical research and stuff.

07:34.400 --> 07:39.240
 But hey, that's huge progress.

07:39.240 --> 07:44.120
 You can look at the GPT3 that can spout off

07:44.120 --> 07:48.840
 English text, which sometimes really, really blows you away.

07:48.840 --> 07:52.920
 You can look at DeepMind's MuZero,

07:52.920 --> 07:57.920
 which doesn't just kick our butt in Go and Chess and Shogi,

07:57.920 --> 07:59.760
 but also in all these Atari games.

07:59.760 --> 08:02.960
 And you don't even have to teach it the rules now.

08:02.960 --> 08:06.920
 What all of those have in common is, besides being powerful,

08:06.920 --> 08:10.520
 is we don't fully understand how they work.

08:10.520 --> 08:13.160
 And that's fine if it's just some dancing robots.

08:13.160 --> 08:16.440
 And the worst thing that can happen is they face plant.

08:16.440 --> 08:19.120
 Or if they're playing Go, and the worst thing that can happen

08:19.120 --> 08:22.240
 is that they make a bad move and lose the game.

08:22.240 --> 08:25.400
 It's less fine if that's what's controlling

08:25.400 --> 08:29.120
 your self driving car or your nuclear power plant.

08:29.120 --> 08:33.600
 And we've seen already that even though Hollywood

08:33.600 --> 08:35.040
 had all these movies where they try

08:35.040 --> 08:37.000
 to make us worry about the wrong things,

08:37.000 --> 08:41.080
 like machines turning evil, the actual bad things that

08:41.080 --> 08:43.480
 have happened with automation have not

08:43.480 --> 08:45.440
 been machines turning evil.

08:45.440 --> 08:48.840
 They've been caused by overtrust in things

08:48.840 --> 08:51.440
 we didn't understand as well as we thought we did.

08:51.440 --> 08:54.320
 Even very simple automated systems

08:54.320 --> 09:00.440
 like what Boeing put into the 737 MAX killed a lot of people.

09:00.440 --> 09:02.960
 Was it that that little simple system was evil?

09:02.960 --> 09:03.920
 Of course not.

09:03.920 --> 09:07.400
 But we didn't understand it as well as we should have.

09:07.400 --> 09:10.640
 And we trusted without understanding.

09:10.640 --> 09:11.440
 Exactly.

09:11.440 --> 09:12.440
 That's the overtrust.

09:12.440 --> 09:15.720
 We didn't even understand that we didn't understand.

09:15.720 --> 09:19.880
 The humility is really at the core of being a scientist.

09:19.880 --> 09:21.880
 I think step one, if you want to be a scientist,

09:21.880 --> 09:25.000
 is don't ever fool yourself into thinking you understand things

09:25.000 --> 09:27.080
 when you actually don't.

09:27.080 --> 09:29.480
 That's probably good advice for humans in general.

09:29.480 --> 09:31.320
 I think humility in general can do us good.

09:31.320 --> 09:33.240
 But in science, it's so spectacular.

09:33.240 --> 09:35.880
 Why did we have the wrong theory of gravity

09:35.880 --> 09:40.520
 ever from Aristotle onward until Galileo's time?

09:40.520 --> 09:43.680
 Why would we believe something so dumb as that if I throw

09:43.680 --> 09:47.280
 this water bottle, it's going to go up with constant speed

09:47.280 --> 09:49.680
 until it realizes that its natural motion is down?

09:49.680 --> 09:51.040
 It changes its mind.

09:51.040 --> 09:55.320
 Because people just kind of assumed Aristotle was right.

09:55.320 --> 09:56.120
 He's an authority.

09:56.120 --> 09:57.720
 We understand that.

09:57.720 --> 09:59.920
 Why did we believe things like that the sun is

09:59.920 --> 10:01.880
 going around the Earth?

10:01.880 --> 10:04.000
 Why did we believe that time flows

10:04.000 --> 10:06.440
 at the same rate for everyone until Einstein?

10:06.440 --> 10:08.560
 Same exact mistake over and over again.

10:08.560 --> 10:12.320
 We just weren't humble enough to acknowledge that we actually

10:12.320 --> 10:13.920
 didn't know for sure.

10:13.920 --> 10:15.720
 We assumed we knew.

10:15.720 --> 10:17.760
 So we didn't discover the truth because we

10:17.760 --> 10:20.560
 assumed there was nothing there to be discovered, right?

10:20.560 --> 10:24.400
 There was something to be discovered about the 737 Max.

10:24.400 --> 10:26.480
 And if you had been a bit more suspicious

10:26.480 --> 10:28.680
 and tested it better, we would have found it.

10:28.680 --> 10:30.600
 And it's the same thing with most harm

10:30.600 --> 10:33.760
 that's been done by automation so far, I would say.

10:33.760 --> 10:35.720
 So I don't know if you heard here of a company called

10:35.720 --> 10:38.000
 Knight Capital?

10:38.000 --> 10:38.760
 So good.

10:38.760 --> 10:42.080
 That means you didn't invest in them earlier.

10:42.080 --> 10:45.560
 They deployed this automated trading system,

10:45.560 --> 10:47.000
 all nice and shiny.

10:47.000 --> 10:49.480
 They didn't understand it as well as they thought.

10:49.480 --> 10:51.320
 And it went about losing $10 million

10:51.320 --> 10:55.520
 per minute for 44 minutes straight

10:55.520 --> 10:59.520
 until someone presumably was like, oh, no, shut this up.

10:59.520 --> 11:00.480
 Was it evil?

11:00.480 --> 11:01.040
 No.

11:01.040 --> 11:04.400
 It was, again, misplaced trust, something they didn't fully

11:04.400 --> 11:05.240
 understand, right?

11:05.240 --> 11:09.040
 And there have been so many, even when people

11:09.040 --> 11:12.640
 have been killed by robots, which is quite rare still,

11:12.640 --> 11:15.680
 but in factory accidents, it's in every single case

11:15.680 --> 11:19.080
 been not malice, just that the robot didn't understand

11:19.080 --> 11:24.400
 that a human is different from an auto part or whatever.

11:24.400 --> 11:28.000
 So this is why I think there's so much opportunity

11:28.000 --> 11:32.040
 for a physics approach, where you just aim for a higher

11:32.040 --> 11:33.600
 level of understanding.

11:33.600 --> 11:36.200
 And if you look at all these systems

11:36.200 --> 11:40.680
 that we talked about from reinforcement learning

11:40.680 --> 11:44.240
 systems and dancing robots to all these neural networks

11:44.240 --> 11:49.600
 that power GPT3 and go playing software and stuff,

11:49.600 --> 11:53.480
 they're all basically black boxes,

11:53.480 --> 11:55.920
 not so different from if you teach a human something,

11:55.920 --> 11:58.120
 you have no idea how their brain works, right?

11:58.120 --> 11:59.960
 Except the human brain, at least,

11:59.960 --> 12:03.800
 has been error corrected during many, many centuries

12:03.800 --> 12:06.560
 of evolution in a way that some of these systems have not,

12:06.560 --> 12:07.560
 right?

12:07.560 --> 12:10.640
 And my MIT research is entirely focused

12:10.640 --> 12:14.440
 on demystifying this black box, intelligible intelligence

12:14.440 --> 12:15.960
 is my slogan.

12:15.960 --> 12:18.440
 That's a good line, intelligible intelligence.

12:18.440 --> 12:20.360
 Yeah, that we shouldn't settle for something

12:20.360 --> 12:22.160
 that seems intelligent, but it should

12:22.160 --> 12:24.280
 be intelligible so that we actually trust it

12:24.280 --> 12:26.640
 because we understand it, right?

12:26.640 --> 12:28.880
 Like, again, Elon trusts his rockets

12:28.880 --> 12:31.600
 because he understands Newton's laws and thrust

12:31.600 --> 12:33.800
 and how everything works.

12:33.800 --> 12:36.880
 And can I tell you why I'm optimistic about this?

12:36.880 --> 12:37.520
 Yes.

12:37.520 --> 12:41.280
 I think we've made a bit of a mistake

12:41.280 --> 12:44.880
 where some people still think that somehow we're never going

12:44.880 --> 12:47.320
 to understand neural networks.

12:47.320 --> 12:49.520
 We're just going to have to learn to live with this.

12:49.520 --> 12:52.240
 It's this very powerful black box.

12:52.240 --> 12:55.840
 Basically, for those who haven't spent time

12:55.840 --> 12:59.000
 building their own, it's super simple what happens inside.

12:59.000 --> 13:01.280
 You send in a long list of numbers,

13:01.280 --> 13:04.880
 and then you do a bunch of operations on them,

13:04.880 --> 13:06.880
 multiply by matrices, et cetera, et cetera,

13:06.880 --> 13:09.840
 and some other numbers come out that's output of it.

13:09.840 --> 13:13.520
 And then there are a bunch of knobs you can tune.

13:13.520 --> 13:16.680
 And when you change them, it affects the computation,

13:16.680 --> 13:18.080
 the input output relation.

13:18.080 --> 13:19.560
 And then you just give the computer

13:19.560 --> 13:22.680
 some definition of good, and it keeps optimizing these knobs

13:22.680 --> 13:24.760
 until it performs as good as possible.

13:24.760 --> 13:27.160
 And often, you go like, wow, that's really good.

13:27.160 --> 13:29.480
 This robot can dance, or this machine

13:29.480 --> 13:31.960
 is beating me at chess now.

13:31.960 --> 13:33.400
 And in the end, you have something

13:33.400 --> 13:35.240
 which, even though you can look inside it,

13:35.240 --> 13:38.680
 you have very little idea of how it works.

13:38.680 --> 13:42.040
 You can print out tables of all the millions of parameters

13:42.040 --> 13:43.240
 in there.

13:43.240 --> 13:45.000
 Is it crystal clear now how it's working?

13:45.000 --> 13:46.840
 No, of course not.

13:46.840 --> 13:49.080
 Many of my colleagues seem willing to settle for that.

13:49.080 --> 13:51.560
 And I'm like, no, that's like the halfway point.

13:54.360 --> 13:57.560
 Some have even gone as far as sort of guessing

13:57.560 --> 14:00.800
 that the mistrutability of this is

14:00.800 --> 14:02.760
 where some of the power comes from,

14:02.760 --> 14:05.120
 and some sort of mysticism.

14:05.120 --> 14:06.840
 I think that's total nonsense.

14:06.840 --> 14:10.240
 I think the real power of neural networks

14:10.240 --> 14:15.040
 comes not from inscrutability, but from differentiability.

14:15.040 --> 14:17.640
 And what I mean by that is simply

14:17.640 --> 14:23.880
 that the output changes only smoothly if you tweak your knobs.

14:23.880 --> 14:26.640
 And then you can use all these powerful methods

14:26.640 --> 14:28.320
 we have for optimization in science.

14:28.320 --> 14:30.160
 We can just tweak them a little bit and see,

14:30.160 --> 14:31.680
 did that get better or worse?

14:31.680 --> 14:33.920
 That's the fundamental idea of machine learning,

14:33.920 --> 14:36.080
 that the machine itself can keep optimizing

14:36.080 --> 14:37.240
 until it gets better.

14:37.240 --> 14:41.920
 Suppose you wrote this algorithm instead in Python

14:41.920 --> 14:43.720
 or some other programming language,

14:43.720 --> 14:46.280
 and then what the knobs did was they just changed

14:46.280 --> 14:49.920
 random letters in your code.

14:49.920 --> 14:51.440
 Now it would just epically fail.

14:51.440 --> 14:53.560
 You change one thing, and instead of saying print,

14:53.560 --> 14:56.840
 it says, synth, syntax error.

14:56.840 --> 14:58.720
 You don't even know, was that for the better

14:58.720 --> 14:59.920
 or for the worse, right?

14:59.920 --> 15:02.720
 This, to me, is what I believe is

15:02.720 --> 15:05.240
 the fundamental power of neural networks.

15:05.240 --> 15:06.640
 And just to clarify, the changing

15:06.640 --> 15:08.400
 of the different letters in a program

15:08.400 --> 15:10.600
 would not be a differentiable process.

15:10.600 --> 15:13.760
 It would make it an invalid program, typically.

15:13.760 --> 15:16.800
 And then you wouldn't even know if you changed more letters

15:16.800 --> 15:18.560
 if it would make it work again, right?

15:18.560 --> 15:23.360
 So that's the magic of neural networks, the inscrutability.

15:23.360 --> 15:26.600
 The differentiability, that every setting of the parameters

15:26.600 --> 15:29.040
 is a program, and you can tell is it better or worse, right?

15:29.040 --> 15:31.040
 And so.

15:31.040 --> 15:33.680
 So you don't like the poetry of the mystery of neural networks

15:33.680 --> 15:35.120
 as the source of its power?

15:35.120 --> 15:37.880
 I generally like poetry, but.

15:37.880 --> 15:39.200
 Not in this case.

15:39.200 --> 15:40.440
 It's so misleading.

15:40.440 --> 15:42.880
 And above all, it shortchanges us.

15:42.880 --> 15:46.440
 It makes us underestimate the good things

15:46.440 --> 15:47.880
 we can accomplish.

15:47.880 --> 15:49.400
 So what we've been doing in my group

15:49.400 --> 15:53.000
 is basically step one, train the mysterious neural network

15:53.000 --> 15:54.920
 to do something well.

15:54.920 --> 15:59.560
 And then step two, do some additional AI techniques

15:59.560 --> 16:03.280
 to see if we can now transform this black box into something

16:03.280 --> 16:07.120
 equally intelligent that you can actually understand.

16:07.120 --> 16:09.800
 So for example, I'll give you one example, this AI Feynman

16:09.800 --> 16:11.560
 project that we just published, right?

16:11.560 --> 16:18.080
 So we took the 100 most famous or complicated equations

16:18.080 --> 16:20.880
 from one of my favorite physics textbooks,

16:20.880 --> 16:22.560
 in fact, the one that got me into physics

16:22.560 --> 16:25.760
 in the first place, the Feynman lectures on physics.

16:25.760 --> 16:28.520
 And so you have a formula.

16:28.520 --> 16:31.680
 Maybe it has what goes into the formula

16:31.680 --> 16:35.960
 as six different variables, and then what comes out as one.

16:35.960 --> 16:38.000
 So then you can make a giant Excel spreadsheet

16:38.000 --> 16:39.600
 with seven columns.

16:39.600 --> 16:41.680
 You put in just random numbers for the six columns

16:41.680 --> 16:43.880
 for those six input variables, and then you

16:43.880 --> 16:46.880
 calculate with a formula the seventh column, the output.

16:46.880 --> 16:50.440
 So maybe it's like the force equals in the last column

16:50.440 --> 16:51.720
 some function of the other.

16:51.720 --> 16:53.840
 And now the task is, OK, if I don't tell you

16:53.840 --> 16:57.320
 what the formula was, can you figure that out

16:57.320 --> 17:00.080
 from looking at my spreadsheet I gave you?

17:00.080 --> 17:04.400
 This problem is called symbolic regression.

17:04.400 --> 17:05.800
 If I tell you that the formula is

17:05.800 --> 17:08.160
 what we call a linear formula, so it's just

17:08.160 --> 17:14.760
 that the output is sum of all the things, input, the times,

17:14.760 --> 17:17.440
 some constants, that's the famous easy problem

17:17.440 --> 17:18.680
 we can solve.

17:18.680 --> 17:21.360
 We do it all the time in science and engineering.

17:21.360 --> 17:24.480
 But the general one, if it's more complicated functions

17:24.480 --> 17:27.920
 with logarithms or cosines or other math,

17:27.920 --> 17:30.560
 it's a very, very hard one and probably impossible

17:30.560 --> 17:34.560
 to do fast in general, just because the number of formulas

17:34.560 --> 17:37.360
 with n symbols just grows exponentially,

17:37.360 --> 17:38.760
 just like the number of passwords

17:38.760 --> 17:43.320
 you can make grow dramatically with length.

17:43.320 --> 17:46.160
 But we had this idea that if you first

17:46.160 --> 17:48.480
 have a neural network that can actually approximate

17:48.480 --> 17:49.880
 the formula, you just trained it,

17:49.880 --> 17:51.960
 even if you don't understand how it works,

17:51.960 --> 17:56.560
 that can be the first step towards actually understanding

17:56.560 --> 17:58.280
 how it works.

17:58.280 --> 18:00.600
 So that's what we do first.

18:00.600 --> 18:03.240
 And then we study that neural network now

18:03.240 --> 18:04.880
 and put in all sorts of other data

18:04.880 --> 18:06.720
 that wasn't in the original training data

18:06.720 --> 18:09.400
 and use that to discover simplifying

18:09.400 --> 18:11.460
 properties of the formula.

18:11.460 --> 18:13.160
 And that lets us break it apart, often

18:13.160 --> 18:15.560
 into many simpler pieces in a kind of divide

18:15.560 --> 18:17.480
 and conquer approach.

18:17.480 --> 18:20.120
 So we were able to solve all of those 100 formulas,

18:20.120 --> 18:22.160
 discover them automatically, plus a whole bunch

18:22.160 --> 18:22.720
 of other ones.

18:22.720 --> 18:26.320
 And it's actually kind of humbling

18:26.320 --> 18:29.480
 to see that this code, which anyone who wants now

18:29.480 --> 18:33.200
 is listening to this, can type pip install AI Feynman

18:33.200 --> 18:34.560
 on the computer and run it.

18:34.560 --> 18:38.360
 It can actually do what Johannes Kepler spent four years doing

18:38.360 --> 18:40.800
 when he stared at Mars data until he was like,

18:40.800 --> 18:44.520
 finally, Eureka, this is an ellipse.

18:44.520 --> 18:46.960
 This will do it automatically for you in one hour.

18:46.960 --> 18:51.600
 Or Max Planck, he was looking at how much radiation comes out

18:51.600 --> 18:54.160
 from different wavelengths from a hot object

18:54.160 --> 18:57.400
 and discovered the famous blackbody formula.

18:57.400 --> 19:00.400
 This discovers it automatically.

19:00.400 --> 19:05.120
 I'm actually excited about seeing

19:05.120 --> 19:08.640
 if we can discover not just old formulas again,

19:08.640 --> 19:12.000
 but new formulas that no one has seen before.

19:12.000 --> 19:14.680
 I do like this process of using kind of a neural network

19:14.680 --> 19:18.440
 to find some basic insights and then dissecting

19:18.440 --> 19:21.680
 the neural network to then gain the final.

19:21.680 --> 19:30.680
 So in that way, you've forcing the explainability issue,

19:30.680 --> 19:34.880
 really trying to analyze the neural network for the things

19:34.880 --> 19:38.360
 it knows in order to come up with the final beautiful,

19:38.360 --> 19:42.240
 simple theory underlying the initial system

19:42.240 --> 19:43.080
 that you were looking at.

19:43.080 --> 19:44.280
 I love that.

19:44.280 --> 19:47.440
 And the reason I'm so optimistic that it

19:47.440 --> 19:49.040
 can be generalized to so much more

19:49.040 --> 19:53.480
 is because that's exactly what we do as human scientists.

19:53.480 --> 19:55.680
 Think of Galileo, whom we mentioned, right?

19:55.680 --> 19:58.760
 I bet when he was a little kid, if his dad threw him an apple,

19:58.760 --> 20:01.080
 he would catch it.

20:01.080 --> 20:01.560
 Why?

20:01.560 --> 20:04.480
 Because he had a neural network in his brain

20:04.480 --> 20:07.960
 that he had trained to predict the parabolic orbit of apples

20:07.960 --> 20:09.960
 that are thrown under gravity.

20:09.960 --> 20:12.000
 If you throw a tennis ball to a dog,

20:12.000 --> 20:15.360
 it also has this same ability of deep learning

20:15.360 --> 20:18.160
 to figure out how the ball is going to move and catch it.

20:18.160 --> 20:21.960
 But Galileo went one step further when he got older.

20:21.960 --> 20:26.040
 He went back and was like, wait a minute.

20:26.040 --> 20:27.880
 I can write down a formula for this.

20:27.880 --> 20:31.560
 Y equals x squared, a parabola.

20:31.560 --> 20:36.520
 And he helped revolutionize physics as we know it, right?

20:36.520 --> 20:38.200
 So there was a basic neural network

20:38.200 --> 20:43.360
 in there from childhood that captured the experiences

20:43.360 --> 20:46.440
 of observing different kinds of trajectories.

20:46.440 --> 20:48.240
 And then he was able to go back in

20:48.240 --> 20:51.000
 with another extra little neural network

20:51.000 --> 20:53.480
 and analyze all those experiences and be like,

20:53.480 --> 20:54.600
 wait a minute.

20:54.600 --> 20:56.240
 There's a deeper rule here.

20:56.240 --> 20:56.960
 Exactly.

20:56.960 --> 21:00.720
 He was able to distill out in symbolic form

21:00.720 --> 21:03.960
 what that complicated black box neural network was doing.

21:03.960 --> 21:07.320
 Not only did the formula he got ultimately

21:07.320 --> 21:09.840
 become more accurate, and similarly, this

21:09.840 --> 21:12.000
 is how Newton got Newton's laws, which

21:12.000 --> 21:15.600
 is why Elon can send rockets to the space station now, right?

21:15.600 --> 21:19.480
 So it's not only more accurate, but it's also simpler,

21:19.480 --> 21:20.120
 much simpler.

21:20.120 --> 21:22.320
 And it's so simple that we can actually describe it

21:22.320 --> 21:26.080
 to our friends and each other, right?

21:26.080 --> 21:28.800
 We've talked about it just in the context of physics now.

21:28.800 --> 21:31.560
 But hey, isn't this what we're doing when we're

21:31.560 --> 21:33.360
 talking to each other also?

21:33.360 --> 21:35.440
 We go around with our neural networks,

21:35.440 --> 21:38.760
 just like dogs and cats and chipmunks and Blue Jays.

21:38.760 --> 21:41.920
 And we experience things in the world.

21:41.920 --> 21:43.840
 But then we humans do this additional step

21:43.840 --> 21:46.720
 on top of that, where we then distill out

21:46.720 --> 21:50.280
 certain high level knowledge that we've extracted from this

21:50.280 --> 21:52.240
 in a way that we can communicate it

21:52.240 --> 21:56.600
 to each other in a symbolic form in English in this case, right?

21:56.600 --> 21:59.960
 So if we can do it and we believe

21:59.960 --> 22:02.880
 that we are information processing entities,

22:02.880 --> 22:04.960
 then we should be able to make machine learning that

22:04.960 --> 22:07.160
 does it also.

22:07.160 --> 22:10.200
 Well, do you think the entire thing could be learning?

22:10.200 --> 22:14.160
 Because this dissection process, like for AI Feynman,

22:14.160 --> 22:19.240
 the secondary stage feels like something like reasoning.

22:19.240 --> 22:23.400
 And the initial step feels more like the more basic kind

22:23.400 --> 22:25.280
 of differentiable learning.

22:25.280 --> 22:27.680
 Do you think the whole thing could be differentiable

22:27.680 --> 22:28.720
 learning?

22:28.720 --> 22:31.120
 Do you think the whole thing could be basically neural

22:31.120 --> 22:32.320
 networks on top of each other?

22:32.320 --> 22:33.800
 It's like turtles all the way down.

22:33.800 --> 22:35.920
 Could it be neural networks all the way down?

22:35.920 --> 22:37.920
 I mean, that's a really interesting question.

22:37.920 --> 22:41.040
 We know that in your case, it is neural networks all the way

22:41.040 --> 22:42.960
 down because that's all you have in your skull

22:42.960 --> 22:45.880
 is a bunch of neurons doing their thing, right?

22:45.880 --> 22:50.320
 But if you ask the question more generally,

22:50.320 --> 22:54.120
 what algorithms are being used in your brain,

22:54.120 --> 22:56.160
 I think it's super interesting to compare.

22:56.160 --> 22:58.760
 I think we've gone a little bit backwards historically

22:58.760 --> 23:02.800
 because we humans first discovered good old fashioned

23:02.800 --> 23:06.880
 AI, the logic based AI that we often call GoFi

23:06.880 --> 23:09.080
 for good old fashioned AI.

23:09.080 --> 23:12.600
 And then more recently, we did machine learning

23:12.600 --> 23:14.160
 because it required bigger computers.

23:14.160 --> 23:15.960
 So we had to discover it later.

23:15.960 --> 23:19.160
 So we think of machine learning with neural networks

23:19.160 --> 23:21.840
 as the modern thing and the logic based AI

23:21.840 --> 23:24.280
 as the old fashioned thing.

23:24.280 --> 23:27.800
 But if you look at evolution on Earth,

23:27.800 --> 23:29.800
 it's actually been the other way around.

23:29.800 --> 23:34.120
 I would say that, for example, an eagle

23:34.120 --> 23:38.680
 has a better vision system than I have using.

23:38.680 --> 23:42.360
 And dogs are just as good at casting tennis balls as I am.

23:42.360 --> 23:45.920
 All this stuff which is done by training in neural network

23:45.920 --> 23:49.920
 and not interpreting it in words is

23:49.920 --> 23:51.880
 something so many of our animal friends can do,

23:51.880 --> 23:53.680
 at least as well as us, right?

23:53.680 --> 23:56.560
 What is it that we humans can do that the chipmunks

23:56.560 --> 23:58.880
 and the eagles cannot?

23:58.880 --> 24:01.600
 It's more to do with this logic based stuff, right,

24:01.600 --> 24:04.840
 where we can extract out information

24:04.840 --> 24:10.240
 in symbols, in language, and now even with equations

24:10.240 --> 24:12.160
 if you're a scientist, right?

24:12.160 --> 24:13.920
 So basically what happened was first we

24:13.920 --> 24:16.880
 built these computers that could multiply numbers real fast

24:16.880 --> 24:18.080
 and manipulate symbols.

24:18.080 --> 24:20.520
 And we felt they were pretty dumb.

24:20.520 --> 24:22.800
 And then we made neural networks that

24:22.800 --> 24:25.280
 can see as well as a cat can and do

24:25.280 --> 24:30.040
 a lot of this inscrutable black box neural networks.

24:30.040 --> 24:33.000
 What we humans can do also is put the two together

24:33.000 --> 24:34.040
 in a useful way.

24:34.040 --> 24:36.120
 Yes, in our own brain.

24:36.120 --> 24:37.360
 Yes, in our own brain.

24:37.360 --> 24:40.920
 So if we ever want to get artificial general intelligence

24:40.920 --> 24:45.160
 that can do all jobs as well as humans can, right,

24:45.160 --> 24:47.040
 then that's what's going to be required

24:47.040 --> 24:53.120
 to be able to combine the neural networks with symbolic,

24:53.120 --> 24:55.200
 combine the old AI with the new AI in a good way.

24:55.200 --> 24:57.200
 We do it in our brains.

24:57.200 --> 24:59.760
 And there seems to be basically two strategies

24:59.760 --> 25:01.000
 I see in industry now.

25:01.000 --> 25:03.600
 One scares the heebie jeebies out of me,

25:03.600 --> 25:05.840
 and the other one I find much more encouraging.

25:05.840 --> 25:07.080
 OK, which one?

25:07.080 --> 25:08.320
 Can we break them apart?

25:08.320 --> 25:09.600
 Which of the two?

25:09.600 --> 25:11.640
 The one that scares the heebie jeebies out of me

25:11.640 --> 25:12.880
 is this attitude that we're just going

25:12.880 --> 25:14.720
 to make ever bigger systems that we still

25:14.720 --> 25:19.280
 don't understand until they can be as smart as humans.

25:19.280 --> 25:22.200
 What could possibly go wrong?

25:22.200 --> 25:24.200
 I think it's just such a reckless thing to do.

25:24.200 --> 25:27.000
 And unfortunately, if we actually

25:27.000 --> 25:30.120
 succeed as a species to build artificial general intelligence,

25:30.120 --> 25:31.840
 then we still have no clue how it works.

25:31.840 --> 25:35.440
 I think at least 50% chance we're

25:35.440 --> 25:37.040
 going to be extinct before too long.

25:37.040 --> 25:40.480
 It's just going to be an utter epic own goal.

25:40.480 --> 25:46.600
 So it's that 44 minute losing money problem or the paper clip

25:46.600 --> 25:49.480
 problem where we don't understand how it works,

25:49.480 --> 25:51.280
 and it just in a matter of seconds

25:51.280 --> 25:52.760
 runs away in some kind of direction

25:52.760 --> 25:54.440
 that's going to be very problematic.

25:54.440 --> 25:57.640
 Even long before, you have to worry about the machines

25:57.640 --> 26:01.400
 themselves somehow deciding to do things.

26:01.400 --> 26:06.840
 And to us, we have to worry about people using machines

26:06.840 --> 26:09.840
 that are short of AGI and power to do bad things.

26:09.840 --> 26:13.080
 I mean, just take a moment.

26:13.080 --> 26:18.040
 And if anyone is not worried particularly about advanced AI,

26:18.040 --> 26:20.800
 just take 10 seconds and just think

26:20.800 --> 26:23.720
 about your least favorite leader on the planet right now.

26:23.720 --> 26:25.120
 Don't tell me who it is.

26:25.120 --> 26:26.760
 I want to keep this apolitical.

26:26.760 --> 26:28.840
 But just see the face in front of you,

26:28.840 --> 26:30.480
 that person, for 10 seconds.

26:30.480 --> 26:35.280
 Now imagine that that person has this incredibly powerful AI

26:35.280 --> 26:37.120
 under their control and can use it

26:37.120 --> 26:38.760
 to impose their will on the whole planet.

26:38.760 --> 26:39.960
 How does that make you feel?

26:42.840 --> 26:44.280
 Yeah.

26:44.280 --> 26:49.480
 So can we break that apart just briefly?

26:49.480 --> 26:51.720
 For the 50% chance that we'll run

26:51.720 --> 26:53.880
 to trouble with this approach, do you

26:53.880 --> 26:58.040
 see the bigger worry in that leader or humans

26:58.040 --> 27:00.600
 using the system to do damage?

27:00.600 --> 27:05.360
 Or are you more worried, and I think I'm in this camp,

27:05.360 --> 27:09.800
 more worried about accidental, unintentional destruction

27:09.800 --> 27:10.840
 of everything?

27:10.840 --> 27:14.960
 So humans trying to do good, and in a way

27:14.960 --> 27:17.400
 where everyone agrees it's kind of good,

27:17.400 --> 27:20.040
 it's just they're trying to do good without understanding.

27:20.040 --> 27:22.480
 Because I think every evil leader in history

27:22.480 --> 27:24.560
 thought they're, to some degree, thought

27:24.560 --> 27:25.600
 they're trying to do good.

27:25.600 --> 27:25.880
 Oh, yeah.

27:25.880 --> 27:28.080
 I'm sure Hitler thought he was doing good.

27:28.080 --> 27:29.480
 Yeah.

27:29.480 --> 27:31.120
 I've been reading a lot about Stalin.

27:31.120 --> 27:34.240
 I'm sure Stalin is from, he legitimately

27:34.240 --> 27:36.560
 thought that communism was good for the world,

27:36.560 --> 27:37.760
 and that he was doing good.

27:37.760 --> 27:39.960
 I think Mao Zedong thought what he was doing with the Great

27:39.960 --> 27:41.200
 Leap Forward was good too.

27:41.200 --> 27:42.880
 Yeah.

27:42.880 --> 27:45.560
 I'm actually concerned about both of those.

27:45.560 --> 27:48.440
 Before, I promised to answer this in detail,

27:48.440 --> 27:50.320
 but before we do that, let me finish

27:50.320 --> 27:51.240
 answering the first question.

27:51.240 --> 27:53.520
 Because I told you that there were two different routes we

27:53.520 --> 27:55.400
 could get to artificial general intelligence,

27:55.400 --> 27:57.240
 and one scares the hell out of me,

27:57.240 --> 27:59.320
 which is this one where we build something,

27:59.320 --> 28:02.040
 we just say bigger neural networks, ever more hardware,

28:02.040 --> 28:03.760
 and just train the heck out of more data,

28:03.760 --> 28:07.240
 and poof, now it's very powerful.

28:07.240 --> 28:11.800
 That, I think, is the most unsafe and reckless approach.

28:11.800 --> 28:16.480
 The alternative to that is the intelligible intelligence

28:16.480 --> 28:22.840
 approach instead, where we say neural networks is just

28:22.840 --> 28:27.000
 a tool for the first step to get the intuition,

28:27.000 --> 28:29.120
 but then we're going to spend also

28:29.120 --> 28:33.280
 serious resources on other AI techniques

28:33.280 --> 28:35.960
 for demystifying this black box and figuring out

28:35.960 --> 28:38.680
 what it's actually doing so we can convert it

28:38.680 --> 28:41.040
 into something that's equally intelligent,

28:41.040 --> 28:44.040
 but that we actually understand what it's doing.

28:44.040 --> 28:45.960
 Maybe we can even prove theorems about it,

28:45.960 --> 28:50.120
 that this car here will never be hacked when it's driving,

28:50.120 --> 28:53.800
 because here is the proof.

28:53.800 --> 28:55.160
 There is a whole science of this.

28:55.160 --> 28:57.040
 It doesn't work for neural networks

28:57.040 --> 28:58.800
 that are big black boxes, but it works well

28:58.800 --> 29:02.880
 and works with certain other kinds of codes, right?

29:02.880 --> 29:05.160
 That approach, I think, is much more promising.

29:05.160 --> 29:07.160
 That's exactly why I'm working on it, frankly,

29:07.160 --> 29:09.400
 not just because I think it's cool for science,

29:09.400 --> 29:14.080
 but because I think the more we understand these systems,

29:14.080 --> 29:16.160
 the better the chances that we can

29:16.160 --> 29:18.400
 make them do the things that are good for us

29:18.400 --> 29:21.520
 that are actually intended, not unintended.

29:21.520 --> 29:24.280
 So you think it's possible to prove things

29:24.280 --> 29:27.360
 about something as complicated as a neural network?

29:27.360 --> 29:28.440
 That's the hope?

29:28.440 --> 29:30.840
 Well, ideally, there's no reason it

29:30.840 --> 29:34.320
 has to be a neural network in the end either, right?

29:34.320 --> 29:36.480
 We discovered Newton's laws of gravity

29:36.480 --> 29:40.040
 with neural network in Newton's head.

29:40.040 --> 29:44.080
 But that's not the way it's programmed into the navigation

29:44.080 --> 29:46.600
 system of Elon Musk's rocket anymore.

29:46.600 --> 29:49.200
 It's written in C++, or I don't know

29:49.200 --> 29:51.360
 what language he uses exactly.

29:51.360 --> 29:53.400
 And then there are software tools called symbolic

29:53.400 --> 29:54.640
 verification.

29:54.640 --> 29:59.080
 DARPA and the US military has done a lot of really great

29:59.080 --> 30:01.160
 research on this, because they really

30:01.160 --> 30:03.760
 want to understand that when they build weapon systems,

30:03.760 --> 30:07.480
 they don't just go fire at random or malfunction, right?

30:07.480 --> 30:10.720
 And there is even a whole operating system

30:10.720 --> 30:12.920
 called Cell 3 that's been developed by a DARPA grant,

30:12.920 --> 30:16.160
 where you can actually mathematically prove

30:16.160 --> 30:18.800
 that this thing can never be hacked.

30:18.800 --> 30:20.360
 Wow.

30:20.360 --> 30:22.280
 One day, I hope that will be something

30:22.280 --> 30:25.120
 you can say about the OS that's running on our laptops too.

30:25.120 --> 30:27.040
 As you know, we're not there.

30:27.040 --> 30:30.080
 But I think we should be ambitious, frankly.

30:30.080 --> 30:34.120
 And if we can use machine learning

30:34.120 --> 30:36.320
 to help do the proofs and so on as well,

30:36.320 --> 30:40.040
 then it's much easier to verify that a proof is correct

30:40.040 --> 30:42.960
 than to come up with a proof in the first place.

30:42.960 --> 30:45.000
 That's really the core idea here.

30:45.000 --> 30:47.480
 If someone comes on your podcast and says

30:47.480 --> 30:49.760
 they proved the Riemann hypothesis

30:49.760 --> 30:55.480
 or some sensational new theorem, it's

30:55.480 --> 30:58.640
 much easier for someone else, take some smart grad,

30:58.640 --> 31:01.000
 math grad students to check, oh, there's an error here

31:01.000 --> 31:04.000
 on equation five, or this really checks out,

31:04.000 --> 31:07.080
 than it was to discover the proof.

31:07.080 --> 31:09.000
 Yeah, although some of those proofs are pretty complicated.

31:09.000 --> 31:11.080
 But yes, it's still nevertheless much easier

31:11.080 --> 31:12.880
 to verify the proof.

31:12.880 --> 31:14.680
 I love the optimism.

31:14.680 --> 31:17.480
 We kind of, even with the security of systems,

31:17.480 --> 31:21.760
 there's a kind of cynicism that pervades people

31:21.760 --> 31:24.920
 who think about this, which is like, oh, it's hopeless.

31:24.920 --> 31:27.080
 I mean, in the same sense, exactly like you're saying

31:27.080 --> 31:29.000
 when you own networks, oh, it's hopeless to understand

31:29.000 --> 31:30.440
 what's happening.

31:30.440 --> 31:32.560
 With security, people are just like, well,

31:32.560 --> 31:35.040
 it's always going, there's always going to be

31:36.240 --> 31:40.800
 attack vectors, like ways to attack the system.

31:40.800 --> 31:42.200
 But you're right, we're just very new

31:42.200 --> 31:44.080
 with these computational systems.

31:44.080 --> 31:46.400
 We're new with these intelligent systems.

31:46.400 --> 31:49.560
 And it's not out of the realm of possibly,

31:49.560 --> 31:51.840
 just like people that understand the movement

31:51.840 --> 31:54.600
 of the stars and the planets and so on.

31:54.600 --> 31:58.320
 It's entirely possible that within, hopefully soon,

31:58.320 --> 32:00.360
 but it could be within 100 years,

32:00.360 --> 32:03.600
 we start to have an obvious laws of gravity

32:03.600 --> 32:08.600
 about intelligence and God forbid about consciousness too.

32:09.280 --> 32:10.960
 That one is...

32:10.960 --> 32:12.320
 Agreed.

32:12.320 --> 32:15.240
 I think, of course, if you're selling computers

32:15.240 --> 32:16.720
 that get hacked a lot, that's in your interest

32:16.720 --> 32:18.640
 as a company that people think it's impossible

32:18.640 --> 32:20.640
 to make it safe, but he's going to get the idea

32:20.640 --> 32:21.480
 of suing you.

32:21.480 --> 32:24.840
 I want to really inject optimism here.

32:24.840 --> 32:29.480
 It's absolutely possible to do much better

32:29.480 --> 32:30.320
 than we're doing now.

32:30.320 --> 32:34.840
 And your laptop does so much stuff.

32:34.840 --> 32:37.960
 You don't need the music player to be super safe

32:37.960 --> 32:42.120
 in your future self driving car, right?

32:42.120 --> 32:43.840
 If someone hacks it and starts playing music

32:43.840 --> 32:47.880
 you don't like, the world won't end.

32:47.880 --> 32:49.560
 But what you can do is you can break out

32:49.560 --> 32:53.080
 and say that your drive computer that controls your safety

32:53.080 --> 32:55.920
 must be completely physically decoupled entirely

32:55.920 --> 32:57.600
 from the entertainment system.

32:57.600 --> 33:01.080
 And it must physically be such that it can't take on

33:01.080 --> 33:03.040
 over the air updates while you're driving.

33:03.040 --> 33:08.040
 And it can have ultimately some operating system on it

33:09.920 --> 33:12.280
 which is symbolically verified and proven

33:13.320 --> 33:17.760
 that it's always going to do what it's supposed to do, right?

33:17.760 --> 33:19.960
 We can basically have, and companies should take

33:19.960 --> 33:20.800
 that attitude too.

33:20.800 --> 33:22.440
 They should look at everything they do and say

33:22.440 --> 33:25.840
 what are the few systems in our company

33:25.840 --> 33:27.400
 that threaten the whole life of the company

33:27.400 --> 33:31.800
 if they get hacked and have the highest standards for them.

33:31.800 --> 33:34.560
 And then they can save money by going for the el cheapo

33:34.560 --> 33:36.920
 poorly understood stuff for the rest.

33:36.920 --> 33:38.920
 This is very feasible, I think.

33:38.920 --> 33:41.720
 And coming back to the bigger question

33:41.720 --> 33:45.000
 that you worried about that there'll be unintentional

33:45.000 --> 33:47.720
 failures, I think there are two quite separate risks here.

33:47.720 --> 33:48.560
 Right?

33:48.560 --> 33:49.600
 We talked a lot about one of them

33:49.600 --> 33:52.640
 which is that the goals are noble of the human.

33:52.640 --> 33:56.920
 The human says, I want this airplane to not crash

33:56.920 --> 33:58.640
 because this is not Muhammad Atta

33:58.640 --> 34:00.480
 now flying the airplane, right?

34:00.480 --> 34:03.240
 And now there's this technical challenge

34:03.240 --> 34:05.960
 of making sure that the autopilot is actually

34:05.960 --> 34:08.320
 gonna behave as the pilot wants.

34:11.000 --> 34:13.360
 If you set that aside, there's also the separate question.

34:13.360 --> 34:17.440
 How do you make sure that the goals of the pilot

34:17.440 --> 34:19.680
 are actually aligned with the goals of the passenger?

34:19.680 --> 34:22.480
 How do you make sure very much more broadly

34:22.480 --> 34:24.640
 that if we can all agree as a species

34:24.640 --> 34:26.200
 that we would like things to kind of go well

34:26.200 --> 34:30.320
 for humanity as a whole, that the goals are aligned here.

34:30.320 --> 34:31.560
 The alignment problem.

34:31.560 --> 34:36.000
 And yeah, there's been a lot of progress

34:36.000 --> 34:39.880
 in the sense that there's suddenly huge amounts

34:39.880 --> 34:42.040
 of research going on on it about it.

34:42.040 --> 34:43.400
 I'm very grateful to Elon Musk

34:43.400 --> 34:44.960
 for giving us that money five years ago

34:44.960 --> 34:46.680
 so we could launch the first research program

34:46.680 --> 34:49.480
 on technical AI safety and alignment.

34:49.480 --> 34:51.280
 There's a lot of stuff happening.

34:51.280 --> 34:54.920
 But I think we need to do more than just make sure

34:54.920 --> 34:57.280
 little machines do always what their owners do.

34:58.200 --> 35:00.240
 That wouldn't have prevented September 11th

35:00.240 --> 35:03.040
 if Muhammad Atta said, okay, autopilot,

35:03.040 --> 35:05.600
 please fly into World Trade Center.

35:06.720 --> 35:07.680
 And it's like, okay.

35:08.960 --> 35:11.840
 That even happened in a different situation.

35:11.840 --> 35:15.680
 There was this depressed pilot named Andreas Lubitz, right?

35:15.680 --> 35:17.640
 Who told his German wings passenger jet

35:17.640 --> 35:19.040
 to fly into the Alps.

35:19.040 --> 35:21.640
 He just told the computer to change the altitude

35:21.640 --> 35:23.280
 to a hundred meters or something like that.

35:23.280 --> 35:25.360
 And you know what the computer said?

35:25.360 --> 35:26.600
 Okay.

35:26.600 --> 35:29.560
 And it had the freaking topographical map of the Alps

35:29.560 --> 35:31.440
 in there, it had GPS, everything.

35:31.440 --> 35:33.120
 No one had bothered teaching it

35:33.120 --> 35:35.600
 even the basic kindergarten ethics of like,

35:35.600 --> 35:39.600
 no, we never want airplanes to fly into mountains

35:39.600 --> 35:41.040
 under any circumstances.

35:41.040 --> 35:46.040
 And so we have to think beyond just the technical issues

35:48.520 --> 35:51.120
 and think about how do we align in general incentives

35:51.120 --> 35:53.760
 on this planet for the greater good?

35:53.760 --> 35:55.520
 So starting with simple stuff like that,

35:55.520 --> 35:58.160
 every airplane that has a computer in it

35:58.160 --> 36:00.840
 should be taught whatever kindergarten ethics

36:00.840 --> 36:02.280
 that's smart enough to understand.

36:02.280 --> 36:05.040
 Like, no, don't fly into fixed objects

36:05.040 --> 36:07.280
 if the pilot tells you to do so.

36:07.280 --> 36:10.000
 Then go on autopilot mode.

36:10.000 --> 36:13.480
 Send an email to the cops and land at the latest airport,

36:13.480 --> 36:14.840
 nearest airport, you know.

36:14.840 --> 36:18.240
 Any car with a forward facing camera

36:18.240 --> 36:20.720
 should just be programmed by the manufacturer

36:20.720 --> 36:23.560
 so that it will never accelerate into a human ever.

36:24.760 --> 36:28.760
 That would avoid things like the NIS attack

36:28.760 --> 36:31.120
 and many horrible terrorist vehicle attacks

36:31.120 --> 36:33.720
 where they deliberately did that, right?

36:33.720 --> 36:35.160
 This was not some sort of thing,

36:35.160 --> 36:38.400
 oh, you know, US and China, different views on,

36:38.400 --> 36:41.880
 no, there was not a single car manufacturer

36:41.880 --> 36:44.080
 in the world, right, who wanted the cars to do this.

36:44.080 --> 36:45.920
 They just hadn't thought to do the alignment.

36:45.920 --> 36:48.520
 And if you look at more broadly problems

36:48.520 --> 36:49.880
 that happen on this planet,

36:51.280 --> 36:53.840
 the vast majority have to do a poor alignment.

36:53.840 --> 36:57.160
 I mean, think about, let's go back really big

36:57.160 --> 36:59.080
 because I know you're so good at that.

36:59.080 --> 36:59.920
 Let's go big, yeah.

36:59.920 --> 37:03.840
 Yeah, so long ago in evolution, we had these genes.

37:03.840 --> 37:06.400
 And they wanted to make copies of themselves.

37:06.400 --> 37:07.640
 That's really all they cared about.

37:07.640 --> 37:12.640
 So some genes said, hey, I'm gonna build a brain

37:13.160 --> 37:15.880
 on this body I'm in so that I can get better

37:15.880 --> 37:17.280
 at making copies of myself.

37:17.280 --> 37:20.240
 And then they decided for their benefit

37:20.240 --> 37:23.320
 to get copied more, to align your brain's incentives

37:23.320 --> 37:24.560
 with their incentives.

37:24.560 --> 37:28.200
 So it didn't want you to starve to death.

37:29.080 --> 37:31.520
 So it gave you an incentive to eat

37:31.520 --> 37:35.080
 and it wanted you to make copies of the genes.

37:35.080 --> 37:37.680
 So it gave you incentive to fall in love

37:37.680 --> 37:40.960
 and do all sorts of naughty things

37:40.960 --> 37:44.120
 to make copies of itself, right?

37:44.120 --> 37:47.720
 So that was successful value alignment done on the genes.

37:47.720 --> 37:50.440
 They created something more intelligent than themselves,

37:50.440 --> 37:52.920
 but they made sure to try to align the values.

37:52.920 --> 37:55.800
 But then something went a little bit wrong

37:55.800 --> 37:58.400
 against the idea of what the genes wanted

37:58.400 --> 38:00.360
 because a lot of humans discovered,

38:00.360 --> 38:03.280
 hey, you know, yeah, we really like this business

38:03.280 --> 38:06.640
 about sex that the genes have made us enjoy,

38:06.640 --> 38:09.440
 but we don't wanna have babies right now.

38:09.440 --> 38:13.800
 So we're gonna hack the genes and use birth control.

38:13.800 --> 38:18.640
 And I really feel like drinking a Coca Cola right now,

38:18.640 --> 38:20.080
 but I don't wanna get a potbelly,

38:20.080 --> 38:21.960
 so I'm gonna drink Diet Coke.

38:21.960 --> 38:24.600
 We have all these things we've figured out

38:24.600 --> 38:26.400
 because we're smarter than the genes,

38:26.400 --> 38:29.040
 how we can actually subvert their intentions.

38:29.040 --> 38:33.440
 So it's not surprising that we humans now,

38:33.440 --> 38:34.800
 when we are in the role of these genes,

38:34.800 --> 38:37.640
 creating other nonhuman entities with a lot of power,

38:37.640 --> 38:39.400
 have to face the same exact challenge.

38:39.400 --> 38:41.720
 How do we make other powerful entities

38:41.720 --> 38:45.280
 have incentives that are aligned with ours?

38:45.280 --> 38:47.000
 And so they won't hack them.

38:47.000 --> 38:48.720
 Corporations, for example, right?

38:48.720 --> 38:51.280
 We humans decided to create corporations

38:51.280 --> 38:53.440
 because it can benefit us greatly.

38:53.440 --> 38:55.120
 Now all of a sudden there's a supermarket.

38:55.120 --> 38:56.240
 I can go buy food there.

38:56.240 --> 38:57.200
 I don't have to hunt.

38:57.200 --> 39:02.200
 Awesome, and then to make sure that this corporation

39:02.880 --> 39:05.960
 would do things that were good for us and not bad for us,

39:05.960 --> 39:08.280
 we created institutions to keep them in check.

39:08.280 --> 39:12.520
 Like if the local supermarket sells poisonous food,

39:12.520 --> 39:17.520
 then the owners of the supermarket

39:17.920 --> 39:22.160
 have to spend some years reflecting behind bars, right?

39:22.160 --> 39:25.720
 So we created incentives to align them.

39:25.720 --> 39:27.480
 But of course, just like we were able to see

39:27.480 --> 39:30.640
 through this thing and you develop birth control,

39:30.640 --> 39:31.840
 if you're a powerful corporation,

39:31.840 --> 39:35.080
 you also have an incentive to try to hack the institutions

39:35.080 --> 39:36.320
 that are supposed to govern you.

39:36.320 --> 39:38.160
 Because you ultimately, as a corporation,

39:38.160 --> 39:40.920
 have an incentive to maximize your profit.

39:40.920 --> 39:42.080
 Just like you have an incentive

39:42.080 --> 39:44.160
 to maximize the enjoyment your brain has,

39:44.160 --> 39:46.000
 not for your genes.

39:46.000 --> 39:50.480
 So if they can figure out a way of bribing regulators,

39:50.480 --> 39:52.400
 then they're gonna do that.

39:52.400 --> 39:54.440
 In the US, we kind of caught onto that

39:54.440 --> 39:57.200
 and made laws against corruption and bribery.

39:58.560 --> 40:03.560
 Then in the late 1800s, Teddy Roosevelt realized that,

40:03.760 --> 40:05.360
 no, we were still being kind of hacked

40:05.360 --> 40:07.280
 because the Massachusetts Railroad companies

40:07.280 --> 40:10.120
 had like a bigger budget than the state of Massachusetts

40:10.120 --> 40:13.600
 and they were doing a lot of very corrupt stuff.

40:13.600 --> 40:15.480
 So he did the whole trust busting thing

40:15.480 --> 40:18.440
 to try to align these other nonhuman entities,

40:18.440 --> 40:19.440
 the companies, again,

40:19.440 --> 40:23.040
 more with the incentives of Americans as a whole.

40:23.040 --> 40:24.080
 It's not surprising, though,

40:24.080 --> 40:26.160
 that this is a battle you have to keep fighting.

40:26.160 --> 40:30.560
 Now we have even larger companies than we ever had before.

40:30.560 --> 40:33.440
 And of course, they're gonna try to, again,

40:34.320 --> 40:37.800
 subvert the institutions.

40:37.800 --> 40:41.040
 Not because, I think people make a mistake

40:41.040 --> 40:42.040
 of getting all too,

40:44.280 --> 40:46.960
 thinking about things in terms of good and evil.

40:46.960 --> 40:50.360
 Like arguing about whether corporations are good or evil,

40:50.360 --> 40:53.080
 or whether robots are good or evil.

40:53.080 --> 40:57.040
 A robot isn't good or evil, it's a tool.

40:57.040 --> 40:58.400
 And you can use it for great things

40:58.400 --> 41:01.080
 like robotic surgery or for bad things.

41:01.080 --> 41:04.120
 And a corporation also is a tool, of course.

41:04.120 --> 41:06.480
 And if you have good incentives to the corporation,

41:06.480 --> 41:07.520
 it'll do great things,

41:07.520 --> 41:10.000
 like start a hospital or a grocery store.

41:10.000 --> 41:11.760
 If you have any bad incentives,

41:12.680 --> 41:15.800
 then it's gonna start maybe marketing addictive drugs

41:15.800 --> 41:18.600
 to people and you'll have an opioid epidemic, right?

41:18.600 --> 41:19.880
 It's all about,

41:21.440 --> 41:23.480
 we should not make the mistake of getting into

41:23.480 --> 41:25.640
 some sort of fairytale, good, evil thing

41:25.640 --> 41:27.920
 about corporations or robots.

41:27.920 --> 41:30.800
 We should focus on putting the right incentives in place.

41:30.800 --> 41:33.320
 My optimistic vision is that if we can do that,

41:34.280 --> 41:35.840
 then we can really get good things.

41:35.840 --> 41:38.000
 We're not doing so great with that right now,

41:38.000 --> 41:39.240
 either on AI, I think,

41:39.240 --> 41:42.680
 or on other intelligent nonhuman entities,

41:42.680 --> 41:43.920
 like big companies, right?

41:43.920 --> 41:47.440
 We just have a new second generation of AI

41:47.440 --> 41:51.160
 and a secretary of defense who's gonna start up now

41:51.160 --> 41:53.640
 in the Biden administration,

41:53.640 --> 41:58.120
 who was an active member of the board of Raytheon,

41:58.120 --> 41:59.240
 for example.

41:59.240 --> 42:03.360
 So, I have nothing against Raytheon.

42:04.720 --> 42:05.680
 I'm not a pacifist,

42:05.680 --> 42:08.560
 but there's an obvious conflict of interest

42:08.560 --> 42:12.360
 if someone is in the job where they decide

42:12.360 --> 42:14.240
 who they're gonna contract with.

42:14.240 --> 42:16.680
 And I think somehow we have,

42:16.680 --> 42:19.520
 maybe we need another Teddy Roosevelt to come along again

42:19.520 --> 42:20.640
 and say, hey, you know,

42:20.640 --> 42:23.480
 we want what's good for all Americans,

42:23.480 --> 42:26.600
 and we need to go do some serious realigning again

42:26.600 --> 42:29.840
 of the incentives that we're giving to these big companies.

42:30.760 --> 42:33.880
 And then we're gonna be better off.

42:33.880 --> 42:35.800
 It seems that naturally with human beings,

42:35.800 --> 42:37.720
 just like you beautifully described the history

42:37.720 --> 42:38.880
 of this whole thing,

42:38.880 --> 42:40.760
 of it all started with the genes

42:40.760 --> 42:42.680
 and they're probably pretty upset

42:42.680 --> 42:45.600
 by all the unintended consequences that happened since.

42:45.600 --> 42:48.680
 But it seems that it kind of works out,

42:48.680 --> 42:51.120
 like it's in this collective intelligence

42:51.120 --> 42:53.480
 that emerges at the different levels.

42:53.480 --> 42:56.920
 It seems to find sometimes last minute

42:56.920 --> 43:00.920
 a way to realign the values or keep the values aligned.

43:00.920 --> 43:03.800
 It's almost, it finds a way,

43:03.800 --> 43:07.560
 like different leaders, different humans pop up

43:07.560 --> 43:10.680
 all over the place that reset the system.

43:10.680 --> 43:15.240
 Do you want, I mean, do you have an explanation why that is?

43:15.240 --> 43:17.240
 Or is that just survivor bias?

43:17.240 --> 43:19.600
 And also is that different,

43:19.600 --> 43:23.120
 somehow fundamentally different than with AI systems

43:23.120 --> 43:26.440
 where you're no longer dealing with something

43:26.440 --> 43:30.200
 that was a direct, maybe companies are the same,

43:30.200 --> 43:33.360
 a direct byproduct of the evolutionary process?

43:33.360 --> 43:36.200
 I think there is one thing which has changed.

43:36.200 --> 43:40.280
 That's why I'm not all optimistic.

43:40.280 --> 43:42.280
 That's why I think there's about a 50% chance

43:42.280 --> 43:46.120
 if we take the dumb route with artificial intelligence

43:46.120 --> 43:50.240
 that humanity will be extinct in this century.

43:51.680 --> 43:53.320
 First, just the big picture.

43:53.320 --> 43:56.720
 Yeah, companies need to have the right incentives.

43:57.880 --> 43:59.000
 Even governments, right?

43:59.000 --> 44:00.920
 We used to have governments,

44:02.120 --> 44:04.200
 usually there were just some king,

44:04.200 --> 44:07.160
 who was the king because his dad was the king.

44:07.160 --> 44:10.600
 And then there were some benefits

44:10.600 --> 44:15.280
 of having this powerful kingdom or empire of any sort

44:15.280 --> 44:17.960
 because then it could prevent a lot of local squabbles.

44:17.960 --> 44:19.360
 So at least everybody in that region

44:19.360 --> 44:20.800
 would stop warring against each other.

44:20.800 --> 44:24.200
 And their incentives of different cities in the kingdom

44:24.200 --> 44:25.160
 became more aligned, right?

44:25.160 --> 44:27.200
 That was the whole selling point.

44:27.200 --> 44:31.520
 Harare, Noel Harare has a beautiful piece

44:31.520 --> 44:35.320
 on how empires were collaboration enablers.

44:35.320 --> 44:36.760
 And then we also, Harare says,

44:36.760 --> 44:38.280
 invented money for that reason

44:38.280 --> 44:40.640
 so we could have better alignment

44:40.640 --> 44:44.160
 and we could do trade even with people we didn't know.

44:44.160 --> 44:45.840
 So this sort of stuff has been playing out

44:45.840 --> 44:47.880
 since time immemorial, right?

44:47.880 --> 44:51.520
 What's changed is that it happens on ever larger scales,

44:51.520 --> 44:52.360
 right?

44:52.360 --> 44:53.480
 The technology keeps getting better

44:53.480 --> 44:54.760
 because science gets better.

44:54.760 --> 44:57.600
 So now we can communicate over larger distances,

44:57.600 --> 44:59.840
 transport things fast over larger distances.

44:59.840 --> 45:02.960
 And so the entities get ever bigger,

45:02.960 --> 45:05.480
 but our planet is not getting bigger anymore.

45:05.480 --> 45:08.120
 So in the past, you could have one experiment

45:08.120 --> 45:11.040
 that just totally screwed up like Easter Island,

45:11.920 --> 45:15.160
 where they actually managed to have such poor alignment

45:15.160 --> 45:17.600
 that when they went extinct, people there,

45:17.600 --> 45:21.520
 there was no one else to come back and replace them, right?

45:21.520 --> 45:24.000
 If Elon Musk doesn't get us to Mars

45:24.000 --> 45:27.680
 and then we go extinct on a global scale,

45:27.680 --> 45:28.920
 then we're not coming back.

45:28.920 --> 45:31.480
 That's the fundamental difference.

45:31.480 --> 45:35.800
 And that's a mistake we don't make for that reason.

45:35.800 --> 45:39.800
 In the past, of course, history is full of fiascos, right?

45:39.800 --> 45:42.160
 But it was never the whole planet.

45:42.160 --> 45:45.960
 And then, okay, now there's this nice uninhabited land here.

45:45.960 --> 45:49.400
 Some other people could move in and organize things better.

45:49.400 --> 45:50.720
 This is different.

45:50.720 --> 45:52.680
 The second thing, which is also different

45:52.680 --> 45:57.680
 is that technology gives us so much more empowerment, right?

45:58.200 --> 46:00.520
 Both to do good things and also to screw up.

46:00.520 --> 46:02.920
 In the stone age, even if you had someone

46:02.920 --> 46:04.760
 whose goals were really poorly aligned,

46:04.760 --> 46:06.680
 like maybe he was really pissed off

46:06.680 --> 46:08.760
 because his stone age girlfriend dumped him

46:08.760 --> 46:09.920
 and he just wanted to,

46:09.920 --> 46:12.640
 if he wanted to kill as many people as he could,

46:12.640 --> 46:15.160
 how many could he really take out with a rock and a stick

46:15.160 --> 46:17.200
 before he was overpowered, right?

46:17.200 --> 46:18.920
 Just handful, right?

46:18.920 --> 46:23.760
 Now, with today's technology,

46:23.760 --> 46:25.640
 if we have an accidental nuclear war

46:25.640 --> 46:27.880
 between Russia and the US,

46:27.880 --> 46:31.080
 which we almost have about a dozen times,

46:31.080 --> 46:32.280
 and then we have a nuclear winter,

46:32.280 --> 46:34.760
 it could take out seven billion people

46:34.760 --> 46:37.280
 or six billion people, we don't know.

46:37.280 --> 46:40.440
 So the scale of the damage is bigger that we can do.

46:40.440 --> 46:45.440
 And there's obviously no law of physics

46:45.520 --> 46:48.080
 that says that technology will never get powerful enough

46:48.080 --> 46:51.720
 that we could wipe out our species entirely.

46:51.720 --> 46:53.640
 That would just be fantasy to think

46:53.640 --> 46:55.080
 that science is somehow doomed

46:55.080 --> 46:57.240
 to not get more powerful than that, right?

46:57.240 --> 47:00.280
 And it's not at all unfeasible in our lifetime

47:00.280 --> 47:03.120
 that someone could design a designer pandemic

47:03.120 --> 47:04.640
 which spreads as easily as COVID,

47:04.640 --> 47:06.880
 but just basically kills everybody.

47:06.880 --> 47:08.480
 We already had smallpox.

47:08.480 --> 47:10.680
 It killed one third of everybody who got it.

47:13.000 --> 47:15.320
 What do you think of the, here's an intuition,

47:15.320 --> 47:16.840
 maybe it's completely naive

47:16.840 --> 47:18.960
 and this optimistic intuition I have,

47:18.960 --> 47:22.880
 which it seems, and maybe it's a biased experience

47:22.880 --> 47:25.920
 that I have, but it seems like the most brilliant people

47:25.920 --> 47:30.920
 I've met in my life all are really like

47:31.600 --> 47:33.680
 fundamentally good human beings.

47:33.680 --> 47:37.440
 And not like naive good, like they really wanna do good

47:37.440 --> 47:39.880
 for the world in a way that, well, maybe is aligned

47:39.880 --> 47:41.800
 to my sense of what good means.

47:41.800 --> 47:45.800
 And so I have a sense that the people

47:47.840 --> 47:51.000
 that will be defining the very cutting edge of technology,

47:51.000 --> 47:53.960
 there'll be much more of the ones that are doing good

47:53.960 --> 47:55.840
 versus the ones that are doing evil.

47:55.840 --> 48:00.160
 So the race, I'm optimistic on the,

48:00.160 --> 48:03.080
 us always like last minute coming up with a solution.

48:03.080 --> 48:06.480
 So if there's an engineered pandemic

48:06.480 --> 48:09.280
 that has the capability to destroy

48:09.280 --> 48:11.640
 most of the human civilization,

48:11.640 --> 48:15.880
 it feels like to me either leading up to that before

48:15.880 --> 48:19.240
 or as it's going on, there will be,

48:19.240 --> 48:22.520
 we're able to rally the collective genius

48:22.520 --> 48:23.800
 of the human species.

48:23.800 --> 48:26.160
 I can tell by your smile that you're

48:26.160 --> 48:30.080
 at least some percentage doubtful,

48:30.080 --> 48:35.000
 but could that be a fundamental law of human nature?

48:35.000 --> 48:40.000
 That evolution only creates, like karma is beneficial,

48:40.880 --> 48:44.280
 good is beneficial, and therefore we'll be all right.

48:44.280 --> 48:46.960
 I hope you're right.

48:46.960 --> 48:48.720
 I would really love it if you're right,

48:48.720 --> 48:51.000
 if there's some sort of law of nature that says

48:51.000 --> 48:53.080
 that we always get lucky in the last second

48:53.080 --> 48:58.080
 with karma, but I prefer not playing it so close

49:01.160 --> 49:03.040
 and gambling on that.

49:03.040 --> 49:06.480
 And I think, in fact, I think it can be dangerous

49:06.480 --> 49:08.120
 to have too strong faith in that

49:08.120 --> 49:10.800
 because it makes us complacent.

49:10.800 --> 49:12.520
 Like if someone tells you, you never have to worry

49:12.520 --> 49:13.760
 about your house burning down,

49:13.760 --> 49:15.360
 then you're not gonna put in a smoke detector

49:15.360 --> 49:17.000
 because why would you need to?

49:17.000 --> 49:19.040
 Even if it's sometimes very simple precautions,

49:19.040 --> 49:20.000
 we don't take them.

49:20.000 --> 49:22.360
 If you're like, oh, the government is gonna take care

49:22.360 --> 49:24.760
 of everything for us, I can always trust my politicians.

49:24.760 --> 49:27.520
 I can always, we advocate our own responsibility.

49:27.520 --> 49:29.080
 I think it's a healthier attitude to say,

49:29.080 --> 49:30.840
 yeah, maybe things will work out.

49:30.840 --> 49:33.560
 Maybe I'm actually gonna have to myself step up

49:33.560 --> 49:35.080
 and take responsibility.

49:37.160 --> 49:38.360
 And the stakes are so huge.

49:38.360 --> 49:41.840
 I mean, if we do this right, we can develop

49:41.840 --> 49:43.640
 all this ever more powerful technology

49:43.640 --> 49:46.360
 and cure all diseases and create a future

49:46.360 --> 49:48.040
 where humanity is healthy and wealthy

49:48.040 --> 49:50.080
 for not just the next election cycle,

49:50.080 --> 49:52.960
 but like billions of years throughout our universe.

49:52.960 --> 49:54.760
 That's really worth working hard for

49:54.760 --> 49:58.000
 and not just sitting and hoping

49:58.000 --> 49:59.520
 for some sort of fairytale karma.

49:59.520 --> 50:01.600
 Well, I just mean, so you're absolutely right.

50:01.600 --> 50:03.080
 From the perspective of the individual,

50:03.080 --> 50:05.600
 like for me, the primary thing should be

50:05.600 --> 50:09.720
 to take responsibility and to build the solutions

50:09.720 --> 50:11.320
 that your skillset allows.

50:11.320 --> 50:12.720
 Yeah, which is a lot.

50:12.720 --> 50:14.560
 I think we underestimate often very much

50:14.560 --> 50:16.360
 how much good we can do.

50:16.360 --> 50:19.520
 If you or anyone listening to this

50:19.520 --> 50:23.000
 is completely confident that our government

50:23.000 --> 50:25.720
 would do a perfect job on handling any future crisis

50:25.720 --> 50:29.920
 with engineered pandemics or future AI,

50:29.920 --> 50:34.920
 I actually reflect a bit on what actually happened in 2020.

50:36.360 --> 50:39.680
 Do you feel that the government by and large

50:39.680 --> 50:42.680
 around the world has handled this flawlessly?

50:42.680 --> 50:45.160
 That's a really sad and disappointing reality

50:45.160 --> 50:48.720
 that hopefully is a wake up call for everybody.

50:48.720 --> 50:52.280
 For the scientists, for the engineers,

50:52.280 --> 50:54.240
 for the researchers in AI especially,

50:54.240 --> 50:59.240
 it was disappointing to see how inefficient we were

51:01.000 --> 51:04.120
 at collecting the right amount of data

51:04.120 --> 51:07.080
 in a privacy preserving way and spreading that data

51:07.080 --> 51:09.200
 and utilizing that data to make decisions,

51:09.200 --> 51:10.440
 all that kind of stuff.

51:10.440 --> 51:13.360
 Yeah, I think when something bad happens to me,

51:13.360 --> 51:17.280
 I made myself a promise many years ago

51:17.280 --> 51:21.760
 that I would not be a whiner.

51:21.760 --> 51:23.680
 So when something bad happens to me,

51:23.680 --> 51:27.280
 of course it's a process of disappointment,

51:27.280 --> 51:30.520
 but then I try to focus on what did I learn from this

51:30.520 --> 51:32.600
 that can make me a better person in the future.

51:32.600 --> 51:35.720
 And there's usually something to be learned when I fail.

51:35.720 --> 51:38.200
 And I think we should all ask ourselves,

51:38.200 --> 51:41.480
 what can we learn from the pandemic

51:41.480 --> 51:43.400
 about how we can do better in the future?

51:43.400 --> 51:46.360
 And you mentioned there a really good lesson.

51:46.360 --> 51:49.440
 We were not as resilient as we thought we were

51:50.480 --> 51:53.960
 and we were not as prepared maybe as we wish we were.

51:53.960 --> 51:57.280
 You can even see very stark contrast around the planet.

51:57.280 --> 52:01.760
 South Korea, they have over 50 million people.

52:01.760 --> 52:03.520
 Do you know how many deaths they have from COVID

52:03.520 --> 52:04.600
 last time I checked?

52:05.600 --> 52:06.440
 No.

52:06.440 --> 52:07.280
 It's about 500.

52:08.880 --> 52:10.280
 Why is that?

52:10.280 --> 52:15.280
 Well, the short answer is that they had prepared.

52:16.760 --> 52:19.200
 They were incredibly quick,

52:19.200 --> 52:21.520
 incredibly quick to get on it

52:21.520 --> 52:25.520
 with very rapid testing and contact tracing and so on,

52:25.520 --> 52:28.080
 which is why they never had more cases

52:28.080 --> 52:30.040
 than they could contract trace effectively, right?

52:30.040 --> 52:32.040
 They never even had to have the kind of big lockdowns

52:32.040 --> 52:33.720
 we had in the West.

52:33.720 --> 52:36.560
 But the deeper answer to,

52:36.560 --> 52:39.080
 it's not just the Koreans are just somehow better people.

52:39.080 --> 52:40.800
 The reason I think they were better prepared

52:40.800 --> 52:45.320
 was because they had already had a pretty bad hit

52:45.320 --> 52:47.560
 from the SARS pandemic,

52:47.560 --> 52:49.920
 or which never became a pandemic,

52:49.920 --> 52:52.120
 something like 17 years ago, I think.

52:52.120 --> 52:53.400
 So it was kind of fresh memory

52:53.400 --> 52:56.000
 that we need to be prepared for pandemics.

52:56.000 --> 52:57.000
 So they were, right?

52:59.080 --> 53:01.240
 So maybe this is a lesson here

53:01.240 --> 53:03.280
 for all of us to draw from COVID

53:03.280 --> 53:06.360
 that rather than just wait for the next pandemic

53:06.360 --> 53:09.840
 or the next problem with AI getting out of control

53:09.840 --> 53:11.320
 or anything else,

53:11.320 --> 53:14.720
 maybe we should just actually set aside

53:14.720 --> 53:16.800
 a tiny fraction of our GDP

53:17.680 --> 53:19.320
 to have people very systematically

53:19.320 --> 53:20.680
 do some horizon scanning and say,

53:20.680 --> 53:23.320
 okay, what are the things that could go wrong?

53:23.320 --> 53:24.600
 And let's duke it out and see

53:24.600 --> 53:25.800
 which are the more likely ones

53:25.800 --> 53:28.760
 and which are the ones that are actually actionable

53:28.760 --> 53:29.800
 and then be prepared.

53:29.800 --> 53:34.800
 So one of the observations as one little ant slash human

53:36.560 --> 53:38.560
 that I am of disappointment

53:38.560 --> 53:43.560
 is the political division over information

53:44.040 --> 53:47.440
 that has been observed, that I observed this year,

53:47.440 --> 53:50.880
 that it seemed the discussion was less about

53:54.040 --> 53:57.600
 sort of what happened and understanding

53:57.600 --> 54:00.680
 what happened deeply and more about

54:00.680 --> 54:04.080
 there's different truths out there.

54:04.080 --> 54:05.400
 And it's like an argument,

54:05.400 --> 54:07.640
 my truth is better than your truth.

54:07.640 --> 54:10.840
 And it's like red versus blue or different.

54:10.840 --> 54:13.280
 It was like this ridiculous discourse

54:13.280 --> 54:16.520
 that doesn't seem to get at any kind of notion of the truth.

54:16.520 --> 54:19.000
 It's not like some kind of scientific process.

54:19.000 --> 54:21.000
 Even science got politicized in ways

54:21.000 --> 54:24.360
 that's very heartbreaking to me.

54:24.360 --> 54:28.680
 You have an exciting project on the AI front

54:28.680 --> 54:32.560
 of trying to rethink one of the,

54:32.560 --> 54:34.240
 you mentioned corporations.

54:34.240 --> 54:37.360
 There's one of the other collective intelligence systems

54:37.360 --> 54:40.480
 that have emerged through all of this is social networks.

54:40.480 --> 54:43.600
 And just the spread, the internet is the spread

54:43.600 --> 54:46.400
 of information on the internet,

54:46.400 --> 54:48.320
 our ability to share that information.

54:48.320 --> 54:50.640
 There's all different kinds of news sources and so on.

54:50.640 --> 54:53.200
 And so you said like that's from first principles,

54:53.200 --> 54:57.320
 let's rethink how we think about the news,

54:57.320 --> 54:59.080
 how we think about information.

54:59.080 --> 55:02.480
 Can you talk about this amazing effort

55:02.480 --> 55:03.640
 that you're undertaking?

55:03.640 --> 55:04.560
 Oh, I'd love to.

55:04.560 --> 55:06.400
 This has been my big COVID project

55:06.400 --> 55:10.720
 and nights and weekends on ever since the lockdown.

55:11.920 --> 55:13.080
 To segue into this actually,

55:13.080 --> 55:14.520
 let me come back to what you said earlier

55:14.520 --> 55:17.040
 that you had this hope that in your experience,

55:17.040 --> 55:18.800
 people who you felt were very talented

55:18.800 --> 55:21.240
 were often idealistic and wanted to do good.

55:21.240 --> 55:25.160
 Frankly, I feel the same about all people by and large,

55:25.160 --> 55:26.120
 there are always exceptions,

55:26.120 --> 55:28.480
 but I think the vast majority of everybody,

55:28.480 --> 55:30.320
 regardless of education and whatnot,

55:30.320 --> 55:33.280
 really are fundamentally good, right?

55:33.280 --> 55:36.920
 So how can it be that people still do so much nasty stuff?

55:37.920 --> 55:40.040
 I think it has everything to do with this,

55:40.040 --> 55:41.920
 with the information that we're given.

55:41.920 --> 55:42.760
 Yes.

55:42.760 --> 55:46.240
 If you go into Sweden 500 years ago

55:46.240 --> 55:47.360
 and you start telling all the farmers

55:47.360 --> 55:49.160
 that those Danes in Denmark,

55:49.160 --> 55:52.840
 they're so terrible people, and we have to invade them

55:52.840 --> 55:55.320
 because they've done all these terrible things

55:55.320 --> 55:56.840
 that you can't fact check yourself.

55:56.840 --> 55:59.720
 A lot of people, Swedes did that, right?

55:59.720 --> 56:04.720
 And we're seeing so much of this today in the world,

56:06.680 --> 56:11.680
 both geopolitically, where we are told that China is bad

56:11.760 --> 56:13.960
 and Russia is bad and Venezuela is bad,

56:13.960 --> 56:16.000
 and people in those countries are often told

56:16.000 --> 56:17.320
 that we are bad.

56:17.320 --> 56:21.840
 And we also see it at a micro level where people are told

56:21.840 --> 56:24.640
 that, oh, those who voted for the other party are bad people.

56:24.640 --> 56:26.480
 It's not just an intellectual disagreement,

56:26.480 --> 56:31.480
 but they're bad people and we're getting ever more divided.

56:32.880 --> 56:37.880
 So how do you reconcile this with this intrinsic goodness

56:39.000 --> 56:39.840
 in people?

56:39.840 --> 56:41.640
 I think it's pretty obvious that it has, again,

56:41.640 --> 56:46.280
 to do with the information that we're fed and given, right?

56:46.280 --> 56:49.800
 We evolved to live in small groups

56:49.800 --> 56:52.080
 where you might know 30 people in total, right?

56:52.080 --> 56:55.440
 So you then had a system that was quite good

56:55.440 --> 56:57.760
 for assessing who you could trust and who you could not.

56:57.760 --> 57:02.760
 And if someone told you that Joe there is a jerk,

57:02.840 --> 57:05.000
 but you had interacted with him yourself

57:05.000 --> 57:06.400
 and seen him in action,

57:06.400 --> 57:08.320
 and you would quickly realize maybe

57:08.320 --> 57:11.680
 that that's actually not quite accurate, right?

57:11.680 --> 57:13.520
 But now that the most people on the planet

57:13.520 --> 57:15.280
 are people we've never met,

57:15.280 --> 57:17.200
 it's very important that we have a way

57:17.200 --> 57:19.400
 of trusting the information we're given.

57:19.400 --> 57:23.160
 And so, okay, so where does the news project come in?

57:23.160 --> 57:26.560
 Well, throughout history, you can go read Machiavelli,

57:26.560 --> 57:28.680
 from the 1400s, and you'll see how already then

57:28.680 --> 57:30.040
 they were busy manipulating people

57:30.040 --> 57:31.640
 with propaganda and stuff.

57:31.640 --> 57:35.720
 Propaganda is not new at all.

57:35.720 --> 57:37.720
 And the incentives to manipulate people

57:37.720 --> 57:40.040
 is just not new at all.

57:40.040 --> 57:41.240
 What is it that's new?

57:41.240 --> 57:44.680
 What's new is machine learning meets propaganda.

57:44.680 --> 57:45.760
 That's what's new.

57:45.760 --> 57:47.880
 That's why this has gotten so much worse.

57:47.880 --> 57:50.680
 Some people like to blame certain individuals,

57:50.680 --> 57:53.120
 like in my liberal university bubble,

57:53.120 --> 57:55.960
 many people blame Donald Trump and say it was his fault.

57:56.920 --> 57:58.080
 I see it differently.

57:59.120 --> 58:03.840
 I think Donald Trump just had this extreme skill

58:03.840 --> 58:07.560
 at playing this game in the machine learning algorithm age.

58:07.560 --> 58:09.920
 A game he couldn't have played 10 years ago.

58:09.920 --> 58:10.920
 So what's changed?

58:10.920 --> 58:13.200
 What's changed is, well, Facebook and Google

58:13.200 --> 58:16.640
 and other companies, and I'm not badmouthing them,

58:16.640 --> 58:18.800
 I have a lot of friends who work for these companies,

58:18.800 --> 58:22.720
 good people, they deployed machine learning algorithms

58:22.720 --> 58:24.280
 just to increase their profit a little bit,

58:24.280 --> 58:28.520
 to just maximize the time people spent watching ads.

58:28.520 --> 58:30.520
 And they had totally underestimated

58:30.520 --> 58:32.360
 how effective they were gonna be.

58:32.360 --> 58:37.560
 This was, again, the black box, non intelligible intelligence.

58:37.560 --> 58:39.360
 They just noticed, oh, we're getting more ad revenue.

58:39.360 --> 58:40.200
 Great.

58:40.200 --> 58:42.400
 It took a long time until they even realized why and how

58:42.400 --> 58:45.760
 and how damaging this was for society.

58:45.760 --> 58:47.960
 Because of course, what the machine learning figured out

58:47.960 --> 58:52.080
 was that the by far most effective way of gluing you

58:52.080 --> 58:55.040
 to your little rectangle was to show you things

58:55.040 --> 58:59.800
 that triggered strong emotions, anger, et cetera, resentment,

58:59.800 --> 59:04.720
 and if it was true or not, it didn't really matter.

59:04.720 --> 59:07.520
 It was also easier to find stories that weren't true.

59:07.520 --> 59:09.320
 If you weren't limited, that's just the limitation,

59:09.320 --> 59:10.600
 is to show people.

59:10.600 --> 59:12.360
 That's a very limiting fact.

59:12.360 --> 59:16.960
 And before long, we got these amazing filter bubbles

59:16.960 --> 59:18.960
 on a scale we had never seen before.

59:18.960 --> 59:24.600
 A couple of days to the fact that also the online news media

59:24.600 --> 59:27.560
 were so effective that they killed a lot of people

59:27.560 --> 59:30.200
 that were so effective that they killed a lot of print

59:30.200 --> 59:30.800
 journalism.

59:30.800 --> 59:34.120
 There's less than half as many journalists

59:34.120 --> 59:39.640
 now in America, I believe, as there was a generation ago.

59:39.640 --> 59:42.800
 You just couldn't compete with the online advertising.

59:42.800 --> 59:47.240
 So all of a sudden, most people are not

59:47.240 --> 59:48.680
 getting even reading newspapers.

59:48.680 --> 59:51.320
 They get their news from social media.

59:51.320 --> 59:55.000
 And most people only get news in their little bubble.

59:55.000 --> 59:58.400
 So along comes now some people like Donald Trump,

59:58.400 --> 1:00:01.560
 who figured out, among the first successful politicians,

1:00:01.560 --> 1:00:04.080
 to figure out how to really play this new game

1:00:04.080 --> 1:00:05.960
 and become very, very influential.

1:00:05.960 --> 1:00:09.600
 But I think Donald Trump was as simple.

1:00:09.600 --> 1:00:11.120
 He took advantage of it.

1:00:11.120 --> 1:00:14.520
 He didn't create the fundamental conditions

1:00:14.520 --> 1:00:19.040
 were created by machine learning taking over the news media.

1:00:19.040 --> 1:00:22.920
 So this is what motivated my little COVID project here.

1:00:22.920 --> 1:00:27.120
 So I said before, machine learning and tech in general

1:00:27.120 --> 1:00:29.040
 is not evil, but it's also not good.

1:00:29.040 --> 1:00:31.400
 It's just a tool that you can use

1:00:31.400 --> 1:00:32.680
 for good things or bad things.

1:00:32.680 --> 1:00:36.000
 And as it happens, machine learning and news

1:00:36.000 --> 1:00:39.680
 was mainly used by the big players, big tech,

1:00:39.680 --> 1:00:43.240
 to manipulate people and to watch as many ads as possible,

1:00:43.240 --> 1:00:45.720
 which had this unintended consequence of really screwing

1:00:45.720 --> 1:00:50.440
 up our democracy and fragmenting it into filter bubbles.

1:00:50.440 --> 1:00:53.200
 So I thought, well, machine learning algorithms

1:00:53.200 --> 1:00:54.400
 are basically free.

1:00:54.400 --> 1:00:56.200
 They can run on your smartphone for free also

1:00:56.200 --> 1:00:57.840
 if someone gives them away to you, right?

1:00:57.840 --> 1:01:00.480
 There's no reason why they only have to help the big guy

1:01:01.840 --> 1:01:02.960
 to manipulate the little guy.

1:01:02.960 --> 1:01:05.280
 They can just as well help the little guy

1:01:05.280 --> 1:01:07.880
 to see through all the manipulation attempts

1:01:07.880 --> 1:01:08.720
 from the big guy.

1:01:08.720 --> 1:01:10.600
 So this project is called,

1:01:10.600 --> 1:01:12.800
 you can go to improvethenews.org.

1:01:12.800 --> 1:01:16.600
 The first thing we've built is this little news aggregator.

1:01:16.600 --> 1:01:17.880
 Looks a bit like Google News,

1:01:17.880 --> 1:01:20.200
 except it has these sliders on it to help you break out

1:01:20.200 --> 1:01:21.760
 of your filter bubble.

1:01:21.760 --> 1:01:24.440
 So if you're reading, you can click, click

1:01:24.440 --> 1:01:25.960
 and go to your favorite topic.

1:01:27.080 --> 1:01:31.120
 And then if you just slide the left, right slider

1:01:31.120 --> 1:01:32.560
 away all the way over to the left.

1:01:32.560 --> 1:01:33.720
 There's two sliders, right?

1:01:33.720 --> 1:01:36.280
 Yeah, there's the one, the most obvious one

1:01:36.280 --> 1:01:38.800
 is the one that has left, right labeled on it.

1:01:38.800 --> 1:01:40.560
 You go to the left, you get one set of articles,

1:01:40.560 --> 1:01:43.360
 you go to the right, you see a very different truth

1:01:43.360 --> 1:01:44.200
 appearing.

1:01:44.200 --> 1:01:47.640
 Oh, that's literally left and right on the political spectrum.

1:01:47.640 --> 1:01:48.480
 On the political spectrum.

1:01:48.480 --> 1:01:51.560
 So if you're reading about immigration, for example,

1:01:52.720 --> 1:01:55.560
 it's very, very noticeable.

1:01:55.560 --> 1:01:57.080
 And I think step one always,

1:01:57.080 --> 1:02:00.960
 if you wanna not get manipulated is just to be able

1:02:00.960 --> 1:02:02.960
 to recognize the techniques people use.

1:02:02.960 --> 1:02:05.880
 So it's very helpful to just see how they spin things

1:02:05.880 --> 1:02:06.760
 on the two sides.

1:02:08.160 --> 1:02:11.240
 I think many people are under the misconception

1:02:11.240 --> 1:02:14.080
 that the main problem is fake news.

1:02:14.080 --> 1:02:14.920
 It's not.

1:02:14.920 --> 1:02:17.520
 I had an amazing team of MIT students

1:02:17.520 --> 1:02:20.360
 where we did an academic project to use machine learning

1:02:20.360 --> 1:02:23.080
 to detect the main kinds of bias over the summer.

1:02:23.080 --> 1:02:25.640
 And yes, of course, sometimes there's fake news

1:02:25.640 --> 1:02:29.120
 where someone just claims something that's false, right?

1:02:30.000 --> 1:02:32.920
 Like, oh, Hillary Clinton just got divorced or something.

1:02:32.920 --> 1:02:36.160
 But what we see much more of is actually just omissions.

1:02:37.800 --> 1:02:41.920
 If you go to, there's some stories which just won't be

1:02:41.920 --> 1:02:45.520
 mentioned by the left or the right, because it doesn't suit

1:02:45.520 --> 1:02:46.360
 their agenda.

1:02:46.360 --> 1:02:49.680
 And then they'll mention other ones very, very, very much.

1:02:49.680 --> 1:02:54.680
 So for example, we've had a number of stories

1:02:54.680 --> 1:02:58.640
 about the Trump family's financial dealings.

1:02:59.600 --> 1:03:01.560
 And then there's been a bunch of stories

1:03:01.560 --> 1:03:05.280
 about the Biden family's, Hunter Biden's financial dealings.

1:03:05.280 --> 1:03:07.520
 Surprise, surprise, they don't get equal coverage

1:03:07.520 --> 1:03:08.920
 on the left and the right.

1:03:08.920 --> 1:03:13.320
 One side loves to cover the Biden, Hunter Biden's stuff,

1:03:13.320 --> 1:03:15.360
 and one side loves to cover the Trump.

1:03:15.360 --> 1:03:17.320
 You can never guess which is which, right?

1:03:17.320 --> 1:03:21.560
 But the great news is if you're a normal American citizen

1:03:21.560 --> 1:03:24.080
 and you dislike corruption in all its forms,

1:03:24.960 --> 1:03:28.560
 then slide, slide, you can just look at both sides

1:03:28.560 --> 1:03:32.520
 and you'll see all those political corruption stories.

1:03:32.520 --> 1:03:37.520
 It's really liberating to just take in the both sides,

1:03:37.520 --> 1:03:39.440
 the spin on both sides.

1:03:39.440 --> 1:03:42.720
 It somehow unlocks your mind to think on your own,

1:03:42.720 --> 1:03:47.040
 to realize that, I don't know, it's the same thing

1:03:47.040 --> 1:03:49.840
 that was useful, right, in the Soviet Union times

1:03:49.840 --> 1:03:54.360
 for when everybody was much more aware

1:03:54.360 --> 1:03:57.200
 that they're surrounded by propaganda, right?

1:03:57.200 --> 1:04:00.600
 That is so interesting what you're saying, actually.

1:04:00.600 --> 1:04:04.000
 So Noam Chomsky, used to be our MIT colleague,

1:04:04.000 --> 1:04:07.640
 once said that propaganda is to democracy

1:04:07.640 --> 1:04:11.960
 what violence is to totalitarianism.

1:04:11.960 --> 1:04:15.080
 And what he means by that is if you have

1:04:15.080 --> 1:04:16.680
 a really totalitarian government,

1:04:16.680 --> 1:04:18.040
 you don't need propaganda.

1:04:19.680 --> 1:04:22.880
 People will do what you want them to do anyway,

1:04:22.880 --> 1:04:24.320
 but out of fear, right?

1:04:24.320 --> 1:04:28.080
 But otherwise, you need propaganda.

1:04:28.080 --> 1:04:29.880
 So I would say actually that the propaganda

1:04:29.880 --> 1:04:32.560
 is much higher quality in democracies,

1:04:32.560 --> 1:04:34.240
 much more believable.

1:04:34.240 --> 1:04:36.960
 And it's really, it's really striking.

1:04:36.960 --> 1:04:39.520
 When I talk to colleagues, science colleagues

1:04:39.520 --> 1:04:41.160
 like from Russia and China and so on,

1:04:42.200 --> 1:04:45.120
 I notice they are actually much more aware

1:04:45.120 --> 1:04:47.200
 of the propaganda in their own media

1:04:47.200 --> 1:04:48.840
 than many of my American colleagues are

1:04:48.840 --> 1:04:51.120
 about the propaganda in Western media.

1:04:51.120 --> 1:04:51.960
 That's brilliant.

1:04:51.960 --> 1:04:53.880
 That means the propaganda in the Western media

1:04:53.880 --> 1:04:54.720
 is just better.

1:04:54.720 --> 1:04:55.560
 Yes.

1:04:55.560 --> 1:04:56.400
 That's so brilliant.

1:04:56.400 --> 1:04:58.200
 Everything's better in the West, even the propaganda.

1:04:58.200 --> 1:05:03.200
 But once you realize that,

1:05:07.360 --> 1:05:09.280
 you realize there's also something very optimistic there

1:05:09.280 --> 1:05:10.480
 that you can do about it, right?

1:05:10.480 --> 1:05:12.920
 Because first of all, omissions,

1:05:14.040 --> 1:05:16.920
 as long as there's no outright censorship,

1:05:16.920 --> 1:05:18.480
 you can just look at both sides

1:05:19.840 --> 1:05:22.760
 and pretty quickly piece together

1:05:22.760 --> 1:05:26.120
 a much more accurate idea of what's actually going on, right?

1:05:26.120 --> 1:05:28.040
 And develop a natural skepticism too.

1:05:28.040 --> 1:05:28.880
 Yeah.

1:05:28.880 --> 1:05:31.600
 Just an analytical scientific mind

1:05:31.600 --> 1:05:32.880
 about the way you're taking the information.

1:05:32.880 --> 1:05:33.720
 Yeah.

1:05:33.720 --> 1:05:35.480
 And I think, I have to say,

1:05:35.480 --> 1:05:38.480
 sometimes I feel that some of us in the academic bubble

1:05:38.480 --> 1:05:41.440
 are too arrogant about this and somehow think,

1:05:41.440 --> 1:05:44.560
 oh, it's just people who aren't as educated

1:05:44.560 --> 1:05:45.800
 as the dots are pulled.

1:05:45.800 --> 1:05:48.240
 When we are often just as gullible also,

1:05:48.240 --> 1:05:52.080
 we read only our media and don't see through things.

1:05:52.080 --> 1:05:53.960
 Anyone who looks at both sides like this

1:05:53.960 --> 1:05:56.320
 and compares a little will immediately start noticing

1:05:56.320 --> 1:05:58.080
 the shenanigans being pulled.

1:05:58.080 --> 1:06:01.840
 And I think what I tried to do with this app

1:06:01.840 --> 1:06:05.760
 is that the big tech has to some extent

1:06:05.760 --> 1:06:08.960
 tried to blame the individual for being manipulated,

1:06:08.960 --> 1:06:12.320
 much like big tobacco tried to blame the individuals

1:06:12.320 --> 1:06:13.680
 entirely for smoking.

1:06:13.680 --> 1:06:16.880
 And then later on, our government stepped up and say,

1:06:16.880 --> 1:06:19.560
 actually, you can't just blame little kids

1:06:19.560 --> 1:06:20.400
 for starting to smoke.

1:06:20.400 --> 1:06:22.400
 We have to have more responsible advertising

1:06:22.400 --> 1:06:23.480
 and this and that.

1:06:23.480 --> 1:06:24.600
 I think it's a bit the same here.

1:06:24.600 --> 1:06:27.600
 It's very convenient for a big tech to blame.

1:06:27.600 --> 1:06:29.960
 So it's just people who are so dumb and get fooled.

1:06:32.160 --> 1:06:34.160
 The blame usually comes in saying,

1:06:34.160 --> 1:06:36.000
 oh, it's just human psychology.

1:06:36.000 --> 1:06:38.360
 People just wanna hear what they already believe.

1:06:38.360 --> 1:06:43.160
 But professor David Rand at MIT actually partly debunked that

1:06:43.160 --> 1:06:45.280
 with a really nice study showing that people

1:06:45.280 --> 1:06:47.640
 tend to be interested in hearing things

1:06:47.640 --> 1:06:49.880
 that go against what they believe,

1:06:49.880 --> 1:06:52.680
 if it's presented in a respectful way.

1:06:52.680 --> 1:06:57.560
 Suppose, for example, that you have a company

1:06:57.560 --> 1:06:59.120
 and you're just about to launch this project

1:06:59.120 --> 1:07:00.280
 and you're convinced it's gonna work.

1:07:00.280 --> 1:07:02.280
 And someone says, you know, Lex,

1:07:03.520 --> 1:07:05.640
 I hate to tell you this, but this is gonna fail.

1:07:05.640 --> 1:07:06.640
 And here's why.

1:07:06.640 --> 1:07:08.920
 Would you be like, shut up, I don't wanna hear it.

1:07:08.920 --> 1:07:10.640
 La, la, la, la, la, la, la, la, la.

1:07:10.640 --> 1:07:11.480
 Would you?

1:07:11.480 --> 1:07:13.000
 You would be interested, right?

1:07:13.000 --> 1:07:15.480
 And also if you're on an airplane,

1:07:16.360 --> 1:07:19.000
 back in the pre COVID times,

1:07:19.000 --> 1:07:20.240
 and the guy next to you

1:07:20.240 --> 1:07:23.160
 is clearly from the opposite side of the political spectrum,

1:07:24.160 --> 1:07:26.720
 but is very respectful and polite to you.

1:07:26.720 --> 1:07:28.840
 Wouldn't you be kind of interested to hear a bit about

1:07:28.840 --> 1:07:31.960
 how he or she thinks about things?

1:07:31.960 --> 1:07:32.800
 Of course.

1:07:32.800 --> 1:07:35.360
 But it's not so easy to find out

1:07:35.360 --> 1:07:36.760
 respectful disagreement now,

1:07:36.760 --> 1:07:40.440
 because like, for example, if you are a Democrat

1:07:40.440 --> 1:07:41.920
 and you're like, oh, I wanna see something

1:07:41.920 --> 1:07:42.760
 on the other side,

1:07:42.760 --> 1:07:45.080
 so you just go Breitbart.com.

1:07:45.080 --> 1:07:46.960
 And then after the first 10 seconds,

1:07:46.960 --> 1:07:49.400
 you feel deeply insulted by something.

1:07:49.400 --> 1:07:52.480
 And they, it's not gonna work.

1:07:52.480 --> 1:07:54.480
 Or if you take someone who votes Republican

1:07:55.640 --> 1:07:57.400
 and they go to something on the left,

1:07:57.400 --> 1:08:00.120
 then they just get very offended very quickly

1:08:00.120 --> 1:08:02.200
 by them having put a deliberately ugly picture

1:08:02.200 --> 1:08:04.320
 of Donald Trump on the front page or something.

1:08:04.320 --> 1:08:05.640
 It doesn't really work.

1:08:05.640 --> 1:08:09.800
 So this news aggregator also has this nuance slider,

1:08:09.800 --> 1:08:11.440
 which you can pull to the right

1:08:11.440 --> 1:08:13.960
 and then sort of make it easier to get exposed

1:08:13.960 --> 1:08:16.120
 to actually more sort of academic style

1:08:16.120 --> 1:08:17.200
 or more respectful,

1:08:17.200 --> 1:08:19.480
 portrayals of different views.

1:08:19.480 --> 1:08:22.080
 And finally, the one kind of bias

1:08:22.080 --> 1:08:25.440
 I think people are mostly aware of is the left, right,

1:08:25.440 --> 1:08:26.280
 because it's so obvious,

1:08:26.280 --> 1:08:30.600
 because both left and right are very powerful here, right?

1:08:30.600 --> 1:08:33.920
 Both of them have well funded TV stations and newspapers,

1:08:33.920 --> 1:08:35.520
 and it's kind of hard to miss.

1:08:35.520 --> 1:08:39.000
 But there's another one, the establishment slider,

1:08:39.000 --> 1:08:41.320
 which is also really fun.

1:08:41.320 --> 1:08:42.840
 I love to play with it.

1:08:42.840 --> 1:08:44.360
 And that's more about corruption.

1:08:44.360 --> 1:08:45.200
 Yeah, yeah.

1:08:45.200 --> 1:08:47.600
 I love that one. Yes.

1:08:47.600 --> 1:08:51.360
 Because if you have a society

1:08:53.240 --> 1:08:57.320
 where almost all the powerful entities

1:08:57.320 --> 1:08:59.480
 want you to believe a certain thing,

1:08:59.480 --> 1:09:01.840
 that's what you're gonna read in both the big media,

1:09:01.840 --> 1:09:04.640
 mainstream media on the left and on the right, of course.

1:09:04.640 --> 1:09:08.200
 And the powerful companies can push back very hard,

1:09:08.200 --> 1:09:10.160
 like tobacco companies push back very hard

1:09:10.160 --> 1:09:12.160
 back in the day when some newspapers

1:09:12.160 --> 1:09:15.400
 started writing articles about tobacco being dangerous,

1:09:15.400 --> 1:09:17.000
 so that it was hard to get a lot of coverage

1:09:17.000 --> 1:09:18.480
 about it initially.

1:09:18.480 --> 1:09:20.880
 And also if you look geopolitically, right,

1:09:20.880 --> 1:09:23.120
 of course, in any country, when you read their media,

1:09:23.120 --> 1:09:24.880
 you're mainly gonna be reading a lot of articles

1:09:24.880 --> 1:09:27.360
 about how our country is the good guy

1:09:27.360 --> 1:09:30.400
 and the other countries are the bad guys, right?

1:09:30.400 --> 1:09:33.360
 So if you wanna have a really more nuanced understanding,

1:09:33.360 --> 1:09:37.040
 like the Germans used to be told that the British

1:09:37.040 --> 1:09:38.840
 used to be told that the French were the bad guys

1:09:38.840 --> 1:09:39.880
 and the French used to be told

1:09:39.880 --> 1:09:41.880
 that the British were the bad guys.

1:09:41.880 --> 1:09:45.680
 Now they visit each other's countries a lot

1:09:45.680 --> 1:09:47.360
 and have a much more nuanced understanding.

1:09:47.360 --> 1:09:48.840
 I don't think there's gonna be any more wars

1:09:48.840 --> 1:09:50.120
 between France and Germany.

1:09:50.120 --> 1:09:51.840
 But on the geopolitical scale,

1:09:53.000 --> 1:09:54.520
 it's just as much as ever, you know,

1:09:54.520 --> 1:09:57.600
 big Cold War, now US, China, and so on.

1:09:57.600 --> 1:10:01.200
 And if you wanna get a more nuanced understanding

1:10:01.200 --> 1:10:03.520
 of what's happening geopolitically,

1:10:03.520 --> 1:10:05.960
 then it's really fun to look at this establishment slider

1:10:05.960 --> 1:10:09.360
 because it turns out there are tons of little newspapers,

1:10:09.360 --> 1:10:11.360
 both on the left and on the right,

1:10:11.360 --> 1:10:14.480
 who sometimes challenge establishment and say,

1:10:14.480 --> 1:10:17.800
 you know, maybe we shouldn't actually invade Iraq right now.

1:10:17.800 --> 1:10:20.400
 Maybe this weapons of mass destruction thing is BS.

1:10:20.400 --> 1:10:23.680
 If you look at the journalism research afterwards,

1:10:23.680 --> 1:10:25.360
 you can actually see that quite clearly.

1:10:25.360 --> 1:10:29.200
 Both CNN and Fox were very pro.

1:10:29.200 --> 1:10:30.640
 Let's get rid of Saddam.

1:10:30.640 --> 1:10:32.560
 There are weapons of mass destruction.

1:10:32.560 --> 1:10:34.680
 Then there were a lot of smaller newspapers.

1:10:34.680 --> 1:10:36.200
 They were like, wait a minute,

1:10:36.200 --> 1:10:40.240
 this evidence seems a bit sketchy and maybe we...

1:10:40.240 --> 1:10:42.240
 But of course they were so hard to find.

1:10:42.240 --> 1:10:44.560
 Most people didn't even know they existed, right?

1:10:44.560 --> 1:10:47.400
 Yet it would have been better for American national security

1:10:47.400 --> 1:10:50.160
 if those voices had also come up.

1:10:50.160 --> 1:10:52.560
 I think it harmed America's national security actually

1:10:52.560 --> 1:10:53.800
 that we invaded Iraq.

1:10:53.800 --> 1:10:55.560
 And arguably there's a lot more interest

1:10:55.560 --> 1:11:00.480
 in that kind of thinking too, from those small sources.

1:11:00.480 --> 1:11:02.600
 So like when you say big,

1:11:02.600 --> 1:11:07.600
 it's more about kind of the reach of the broadcast,

1:11:07.600 --> 1:11:12.040
 but it's not big in terms of the interest.

1:11:12.040 --> 1:11:14.120
 I think there's a lot of interest

1:11:14.120 --> 1:11:16.200
 in that kind of anti establishment

1:11:16.200 --> 1:11:18.840
 or like skepticism towards, you know,

1:11:18.840 --> 1:11:20.360
 out of the box thinking.

1:11:20.360 --> 1:11:22.000
 There's a lot of interest in that kind of thing.

1:11:22.000 --> 1:11:26.920
 Do you see this news project or something like it

1:11:26.920 --> 1:11:30.600
 being basically taken over the world

1:11:30.600 --> 1:11:32.920
 as the main way we consume information?

1:11:32.920 --> 1:11:35.120
 Like how do we get there?

1:11:35.120 --> 1:11:37.320
 Like how do we, you know?

1:11:37.320 --> 1:11:39.000
 So, okay, the idea is brilliant.

1:11:39.000 --> 1:11:44.000
 It's a, you're calling it your little project in 2020,

1:11:44.000 --> 1:11:48.480
 but how does that become the new way we consume information?

1:11:48.480 --> 1:11:51.000
 I hope, first of all, just to plant a little seed there

1:11:51.000 --> 1:11:55.920
 because normally the big barrier of doing anything in media

1:11:55.920 --> 1:11:59.280
 is you need a ton of money, but this costs no money at all.

1:11:59.280 --> 1:12:00.640
 I've just been paying myself.

1:12:00.640 --> 1:12:03.080
 You pay a tiny amount of money each month to Amazon

1:12:03.080 --> 1:12:04.640
 to run the thing in their cloud.

1:12:04.640 --> 1:12:06.920
 We're not, there never will never be any ads.

1:12:06.920 --> 1:12:09.360
 The point is not to make any money off of it.

1:12:09.360 --> 1:12:11.640
 And we just train machine learning algorithms

1:12:11.640 --> 1:12:13.160
 to classify the articles and stuff.

1:12:13.160 --> 1:12:14.840
 So it just kind of runs by itself.

1:12:14.840 --> 1:12:17.760
 So if it actually gets good enough at some point

1:12:17.760 --> 1:12:20.720
 that it starts catching on, it could scale.

1:12:20.720 --> 1:12:23.120
 And if other people carbon copy it

1:12:23.120 --> 1:12:24.960
 and make other versions that are better,

1:12:24.960 --> 1:12:28.200
 that's the more the merrier.

1:12:28.200 --> 1:12:32.920
 I think there's a real opportunity for machine learning

1:12:32.920 --> 1:12:39.880
 to empower the individual against the powerful players.

1:12:39.880 --> 1:12:41.600
 As I said in the beginning here, it's

1:12:41.600 --> 1:12:43.280
 been mostly the other way around so far,

1:12:43.280 --> 1:12:46.960
 that the big players have the AI and then they tell people,

1:12:46.960 --> 1:12:49.600
 this is the truth, this is how it is.

1:12:49.600 --> 1:12:52.200
 But it can just as well go the other way around.

1:12:52.200 --> 1:12:54.320
 And when the internet was born, actually, a lot of people

1:12:54.320 --> 1:12:56.280
 had this hope that maybe this will be

1:12:56.280 --> 1:12:58.120
 a great thing for democracy, make it easier

1:12:58.120 --> 1:12:59.480
 to find out about things.

1:12:59.480 --> 1:13:02.320
 And maybe machine learning and things like this

1:13:02.320 --> 1:13:03.720
 can actually help again.

1:13:03.720 --> 1:13:07.080
 And I have to say, I think it's more important than ever now

1:13:07.080 --> 1:13:12.160
 because this is very linked also to the whole future of life

1:13:12.160 --> 1:13:13.920
 as we discussed earlier.

1:13:13.920 --> 1:13:17.280
 We're getting this ever more powerful tech.

1:13:17.280 --> 1:13:19.040
 Frank, it's pretty clear if you look

1:13:19.040 --> 1:13:21.920
 on the one or two generation, three generation timescale

1:13:21.920 --> 1:13:24.880
 that there are only two ways this can end geopolitically.

1:13:24.880 --> 1:13:27.640
 Either it ends great for all humanity

1:13:27.640 --> 1:13:31.640
 or it ends terribly for all of us.

1:13:31.640 --> 1:13:33.680
 There's really no in between.

1:13:33.680 --> 1:13:37.560
 And we're so stuck in that because technology

1:13:37.560 --> 1:13:39.080
 knows no borders.

1:13:39.080 --> 1:13:42.200
 And you can't have people fighting

1:13:42.200 --> 1:13:44.480
 when the weapons just keep getting ever more

1:13:44.480 --> 1:13:47.040
 powerful indefinitely.

1:13:47.040 --> 1:13:50.280
 Eventually, the luck runs out.

1:13:50.280 --> 1:13:55.480
 And right now we have, I love America,

1:13:55.480 --> 1:13:59.840
 but the fact of the matter is what's good for America

1:13:59.840 --> 1:14:02.000
 is not opposite in the long term to what's

1:14:02.000 --> 1:14:04.600
 good for other countries.

1:14:04.600 --> 1:14:07.400
 It would be if this was some sort of zero sum game

1:14:07.400 --> 1:14:10.960
 like it was thousands of years ago when the only way one

1:14:10.960 --> 1:14:13.440
 country could get more resources was

1:14:13.440 --> 1:14:14.960
 to take land from other countries

1:14:14.960 --> 1:14:17.640
 because that was basically the resource.

1:14:17.640 --> 1:14:18.920
 Look at the map of Europe.

1:14:18.920 --> 1:14:21.400
 Some countries kept getting bigger and smaller,

1:14:21.400 --> 1:14:23.280
 endless wars.

1:14:23.280 --> 1:14:26.400
 But then since 1945, there hasn't been any war

1:14:26.400 --> 1:14:27.160
 in Western Europe.

1:14:27.160 --> 1:14:29.920
 And they all got way richer because of tech.

1:14:29.920 --> 1:14:34.760
 So the optimistic outcome is that the big winner

1:14:34.760 --> 1:14:38.200
 in this century is going to be America and China and Russia

1:14:38.200 --> 1:14:40.200
 and everybody else because technology just makes

1:14:40.200 --> 1:14:41.760
 us all healthier and wealthier.

1:14:41.760 --> 1:14:44.680
 And we just find some way of keeping the peace

1:14:44.680 --> 1:14:46.640
 on this planet.

1:14:46.640 --> 1:14:48.760
 But I think, unfortunately, there

1:14:48.760 --> 1:14:50.440
 are some pretty powerful forces right now

1:14:50.440 --> 1:14:52.560
 that are pushing in exactly the opposite direction

1:14:52.560 --> 1:14:55.920
 and trying to demonize other countries, which just makes

1:14:55.920 --> 1:14:58.360
 it more likely that this ever more powerful tech we're

1:14:58.360 --> 1:15:02.200
 building is going to be used in disastrous ways.

1:15:02.200 --> 1:15:04.400
 Yeah, for aggression versus cooperation,

1:15:04.400 --> 1:15:05.200
 that kind of thing.

1:15:05.200 --> 1:15:09.560
 Yeah, even look at just military AI now.

1:15:09.560 --> 1:15:12.160
 It was so awesome to see these dancing robots.

1:15:12.160 --> 1:15:14.000
 I loved it.

1:15:14.000 --> 1:15:17.080
 But one of the biggest growth areas in robotics

1:15:17.080 --> 1:15:19.480
 now is, of course, autonomous weapons.

1:15:19.480 --> 1:15:23.200
 And 2020 was like the best marketing year

1:15:23.200 --> 1:15:24.400
 ever for autonomous weapons.

1:15:24.400 --> 1:15:27.520
 Because in both Libya, it's a civil war,

1:15:27.520 --> 1:15:34.440
 and in Nagorno Karabakh, they made the decisive difference.

1:15:34.440 --> 1:15:36.280
 And everybody else is watching this.

1:15:36.280 --> 1:15:38.920
 Oh, yeah, we want to build autonomous weapons, too.

1:15:38.920 --> 1:15:45.080
 In Libya, you had, on one hand, our ally,

1:15:45.080 --> 1:15:47.080
 the United Arab Emirates that were flying

1:15:47.080 --> 1:15:50.640
 their autonomous weapons that they bought from China,

1:15:50.640 --> 1:15:51.880
 bombing Libyans.

1:15:51.880 --> 1:15:54.280
 And on the other side, you had our other ally, Turkey,

1:15:54.280 --> 1:15:57.200
 flying their drones.

1:15:57.200 --> 1:16:00.480
 And they had no skin in the game,

1:16:00.480 --> 1:16:01.680
 any of these other countries.

1:16:01.680 --> 1:16:04.160
 And of course, it was the Libyans who really got screwed.

1:16:04.160 --> 1:16:09.280
 In Nagorno Karabakh, you had actually, again,

1:16:09.280 --> 1:16:12.400
 Turkey is sending drones built by this company that

1:16:12.400 --> 1:16:17.080
 was actually founded by a guy who went to MIT AeroAstro.

1:16:17.080 --> 1:16:17.800
 Do you know that?

1:16:17.800 --> 1:16:18.280
 No.

1:16:18.280 --> 1:16:18.960
 Bacratyar.

1:16:18.960 --> 1:16:19.520
 Yeah.

1:16:19.520 --> 1:16:21.480
 So MIT has a direct responsibility

1:16:21.480 --> 1:16:22.680
 for ultimately this.

1:16:22.680 --> 1:16:25.680
 And a lot of civilians were killed there.

1:16:25.680 --> 1:16:29.640
 So because it was militarily so effective,

1:16:29.640 --> 1:16:31.240
 now suddenly there's a huge push.

1:16:31.240 --> 1:16:35.680
 Oh, yeah, yeah, let's go build ever more autonomy

1:16:35.680 --> 1:16:39.440
 into these weapons, and it's going to be great.

1:16:39.440 --> 1:16:44.640
 And I think, actually, people who

1:16:44.640 --> 1:16:47.760
 are obsessed about some sort of future Terminator scenario

1:16:47.760 --> 1:16:51.640
 right now should start focusing on the fact

1:16:51.640 --> 1:16:54.000
 that we have two much more urgent threats happening

1:16:54.000 --> 1:16:54.960
 from machine learning.

1:16:54.960 --> 1:16:57.880
 One of them is the whole destruction of democracy

1:16:57.880 --> 1:17:01.600
 that we've talked about now, where

1:17:01.600 --> 1:17:03.560
 our flow of information is being manipulated

1:17:03.560 --> 1:17:04.400
 by machine learning.

1:17:04.400 --> 1:17:06.960
 And the other one is that right now,

1:17:06.960 --> 1:17:10.440
 this is the year when the big arms race and out of control

1:17:10.440 --> 1:17:12.800
 arms race in at least Thomas Weapons is going to start,

1:17:12.800 --> 1:17:14.640
 or it's going to stop.

1:17:14.640 --> 1:17:18.480
 So you have a sense that there is like 2020

1:17:18.480 --> 1:17:24.280
 was an instrumental catalyst for the autonomous weapons race.

1:17:24.280 --> 1:17:26.560
 Yeah, because it was the first year when they proved

1:17:26.560 --> 1:17:28.360
 decisive in the battlefield.

1:17:28.360 --> 1:17:31.400
 And these ones are still not fully autonomous, mostly.

1:17:31.400 --> 1:17:32.640
 They're remote controlled, right?

1:17:32.640 --> 1:17:38.720
 But we could very quickly make things

1:17:38.720 --> 1:17:43.280
 about the size and cost of a smartphone, which you just put

1:17:43.280 --> 1:17:45.160
 in the GPS coordinates or the face of the one

1:17:45.160 --> 1:17:47.000
 you want to kill, a skin color or whatever,

1:17:47.000 --> 1:17:48.480
 and it flies away and does it.

1:17:48.480 --> 1:17:53.920
 And the real good reason why the US and all

1:17:53.920 --> 1:17:57.040
 the other superpowers should put the kibosh on this

1:17:57.040 --> 1:18:01.680
 is the same reason we decided to put the kibosh on bioweapons.

1:18:01.680 --> 1:18:05.000
 So we gave the Future of Life Award

1:18:05.000 --> 1:18:07.200
 that we can talk more about later to Matthew Messelson

1:18:07.200 --> 1:18:08.680
 from Harvard before for convincing

1:18:08.680 --> 1:18:10.320
 Nixon to ban bioweapons.

1:18:10.320 --> 1:18:13.600
 And I asked him, how did you do it?

1:18:13.600 --> 1:18:16.560
 And he was like, well, I just said, look,

1:18:16.560 --> 1:18:20.520
 we don't want there to be a $500 weapon of mass destruction

1:18:20.520 --> 1:18:26.560
 that all our enemies can afford, even nonstate actors.

1:18:26.560 --> 1:18:32.120
 And Nixon was like, good point.

1:18:32.120 --> 1:18:34.520
 It's in America's interest that the powerful weapons are all

1:18:34.520 --> 1:18:37.600
 really expensive, so only we can afford them,

1:18:37.600 --> 1:18:41.080
 or maybe some more stable adversaries, right?

1:18:41.080 --> 1:18:42.960
 Nuclear weapons are like that.

1:18:42.960 --> 1:18:44.920
 But bioweapons were not like that.

1:18:44.920 --> 1:18:46.400
 That's why we banned them.

1:18:46.400 --> 1:18:48.400
 And that's why you never hear about them now.

1:18:48.400 --> 1:18:50.280
 That's why we love biology.

1:18:50.280 --> 1:18:55.440
 So you have a sense that it's possible for the big power

1:18:55.440 --> 1:18:58.480
 houses in terms of the big nations in the world

1:18:58.480 --> 1:19:02.360
 to agree that autonomous weapons is not a race we want to be on,

1:19:02.360 --> 1:19:03.680
 that it doesn't end well.

1:19:03.680 --> 1:19:05.320
 Yeah, because we know it's just going

1:19:05.320 --> 1:19:06.560
 to end in mass proliferation.

1:19:06.560 --> 1:19:08.560
 And every terrorist everywhere is

1:19:08.560 --> 1:19:10.280
 going to have these super cheap weapons

1:19:10.280 --> 1:19:13.440
 that they will use against us.

1:19:13.440 --> 1:19:15.960
 And our politicians have to constantly worry

1:19:15.960 --> 1:19:18.240
 about being assassinated every time they go outdoors

1:19:18.240 --> 1:19:21.040
 by some anonymous little mini drone.

1:19:21.040 --> 1:19:21.840
 We don't want that.

1:19:21.840 --> 1:19:25.920
 And even if the US and China and everyone else

1:19:25.920 --> 1:19:27.840
 could just agree that you can only

1:19:27.840 --> 1:19:31.560
 build these weapons if they cost at least $10 million,

1:19:31.560 --> 1:19:34.760
 that would be a huge win for the superpowers

1:19:34.760 --> 1:19:38.800
 and, frankly, for everybody.

1:19:38.800 --> 1:19:41.000
 And people often push back and say, well, it's

1:19:41.000 --> 1:19:43.200
 so hard to prevent cheating.

1:19:43.200 --> 1:19:45.800
 But hey, you could say the same about bioweapons.

1:19:45.800 --> 1:19:49.360
 Take any of your MIT colleagues in biology.

1:19:49.360 --> 1:19:52.000
 Of course, they could build some nasty bioweapon

1:19:52.000 --> 1:19:53.560
 if they really wanted to.

1:19:53.560 --> 1:19:55.280
 But first of all, they don't want to

1:19:55.280 --> 1:19:57.640
 because they think it's disgusting because of the stigma.

1:19:57.640 --> 1:20:02.120
 And second, even if there's some sort of nutcase and want to,

1:20:02.120 --> 1:20:04.160
 it's very likely that some of their grad students

1:20:04.160 --> 1:20:06.560
 or someone would rat them out because everyone else thinks

1:20:06.560 --> 1:20:08.000
 it's so disgusting.

1:20:08.000 --> 1:20:11.480
 And in fact, we now know there was even a fair bit of cheating

1:20:11.480 --> 1:20:13.480
 on the bioweapons ban.

1:20:13.480 --> 1:20:17.520
 But no countries used them because it was so stigmatized

1:20:17.520 --> 1:20:22.400
 that it just wasn't worth revealing that they had cheated.

1:20:22.400 --> 1:20:24.840
 You talk about drones, but you kind of

1:20:24.840 --> 1:20:28.960
 think that drones is a remote operation.

1:20:28.960 --> 1:20:30.680
 Which they are, mostly, still.

1:20:30.680 --> 1:20:34.600
 But you're not taking the next intellectual step

1:20:34.600 --> 1:20:36.320
 of where does this go.

1:20:36.320 --> 1:20:38.760
 You're kind of saying the problem with drones

1:20:38.760 --> 1:20:42.400
 is that you're removing yourself from direct violence.

1:20:42.400 --> 1:20:44.920
 Therefore, you're not able to sort of maintain

1:20:44.920 --> 1:20:46.720
 the common humanity required to make

1:20:46.720 --> 1:20:48.720
 the proper decisions strategically.

1:20:48.720 --> 1:20:51.360
 But that's the criticism as opposed to like,

1:20:51.360 --> 1:20:55.520
 if this is automated, and just exactly as you said,

1:20:55.520 --> 1:20:58.640
 if you automate it and there's a race,

1:20:58.640 --> 1:21:01.280
 then the technology's gonna get better and better and better

1:21:01.280 --> 1:21:03.720
 which means getting cheaper and cheaper and cheaper.

1:21:03.720 --> 1:21:06.080
 And unlike, perhaps, nuclear weapons

1:21:06.080 --> 1:21:10.240
 which is connected to resources in a way,

1:21:10.240 --> 1:21:13.760
 like it's hard to engineer, yeah.

1:21:13.760 --> 1:21:17.600
 It feels like there's too much overlap

1:21:17.600 --> 1:21:20.400
 between the tech industry and autonomous weapons

1:21:20.400 --> 1:21:24.400
 to where you could have smartphone type of cheapness.

1:21:24.400 --> 1:21:29.280
 If you look at drones, for $1,000,

1:21:29.280 --> 1:21:30.800
 you can have an incredible system

1:21:30.800 --> 1:21:34.600
 that's able to maintain flight autonomously for you

1:21:34.600 --> 1:21:36.240
 and take pictures and stuff.

1:21:36.240 --> 1:21:39.440
 You could see that going into the autonomous weapons space

1:21:39.440 --> 1:21:43.240
 that's, but why is that not thought about

1:21:43.240 --> 1:21:45.640
 or discussed enough in the public, do you think?

1:21:45.640 --> 1:21:48.960
 You see those dancing Boston Dynamics robots

1:21:48.960 --> 1:21:50.760
 and everybody has this kind of,

1:21:52.600 --> 1:21:55.360
 as if this is like a far future.

1:21:55.360 --> 1:21:58.640
 They have this fear like, oh, this'll be Terminator

1:21:58.640 --> 1:22:03.080
 in like some, I don't know, unspecified 20, 30, 40 years.

1:22:03.080 --> 1:22:05.640
 And they don't think about, well, this is like

1:22:05.640 --> 1:22:09.120
 some much less dramatic version of that

1:22:09.120 --> 1:22:11.160
 is actually happening now.

1:22:11.160 --> 1:22:14.840
 It's not gonna be legged, it's not gonna be dancing,

1:22:14.840 --> 1:22:17.160
 but it already has the capability

1:22:17.160 --> 1:22:20.240
 to use artificial intelligence to kill humans.

1:22:20.240 --> 1:22:22.880
 Yeah, the Boston Dynamics legged robots,

1:22:22.880 --> 1:22:24.960
 I think the reason we imagine them holding guns

1:22:24.960 --> 1:22:28.440
 is just because you've all seen Arnold Schwarzenegger, right?

1:22:28.440 --> 1:22:30.600
 That's our reference point.

1:22:30.600 --> 1:22:32.680
 That's pretty useless.

1:22:32.680 --> 1:22:35.360
 That's not gonna be the main military use of them.

1:22:35.360 --> 1:22:38.720
 They might be useful in law enforcement in the future

1:22:38.720 --> 1:22:40.280
 and then there's a whole debate about,

1:22:40.280 --> 1:22:42.640
 do you want robots showing up at your house with guns

1:22:42.640 --> 1:22:45.440
 telling you who'll be perfectly obedient

1:22:45.440 --> 1:22:47.560
 to whatever dictator controls them?

1:22:47.560 --> 1:22:49.240
 But let's leave that aside for a moment

1:22:49.240 --> 1:22:51.320
 and look at what's actually relevant now.

1:22:51.320 --> 1:22:54.760
 So there's a spectrum of things you can do

1:22:54.760 --> 1:22:55.760
 with AI in the military.

1:22:55.760 --> 1:22:57.560
 And again, to put my card on the table,

1:22:57.560 --> 1:23:01.240
 I'm not the pacifist, I think we should have good defense.

1:23:03.480 --> 1:23:08.480
 So for example, a predator drone is basically

1:23:08.480 --> 1:23:11.720
 a fancy little remote controlled airplane, right?

1:23:11.720 --> 1:23:16.040
 There's a human piloting it and the decision ultimately

1:23:16.040 --> 1:23:17.280
 about whether to kill somebody with it

1:23:17.280 --> 1:23:19.400
 is made by a human still.

1:23:19.400 --> 1:23:23.880
 And this is a line I think we should never cross.

1:23:23.880 --> 1:23:25.880
 There's a current DOD policy.

1:23:25.880 --> 1:23:27.920
 Again, you have to have a human in the loop.

1:23:27.920 --> 1:23:30.680
 I think algorithms should never make life

1:23:30.680 --> 1:23:33.120
 or death decisions, they should be left to humans.

1:23:34.120 --> 1:23:37.720
 Now, why might we cross that line?

1:23:37.720 --> 1:23:40.520
 Well, first of all, these are expensive, right?

1:23:40.520 --> 1:23:45.520
 So for example, when Azerbaijan had all these drones

1:23:46.560 --> 1:23:48.280
 and Armenia didn't have any, they start trying

1:23:48.280 --> 1:23:51.760
 to jerry rig little cheap things, fly around.

1:23:51.760 --> 1:23:54.040
 But then of course, the Armenians would jam them

1:23:54.040 --> 1:23:55.600
 or the Azeris would jam them.

1:23:55.600 --> 1:23:58.320
 And remote control things can be jammed,

1:23:58.320 --> 1:24:00.040
 that makes them inferior.

1:24:00.040 --> 1:24:02.960
 Also, there's a bit of a time delay between,

1:24:02.960 --> 1:24:05.400
 if we're piloting something from far away,

1:24:05.400 --> 1:24:08.640
 speed of light, and the human has a reaction time as well,

1:24:08.640 --> 1:24:11.560
 it would be nice to eliminate that jamming possibility

1:24:11.560 --> 1:24:14.320
 in the time that they by having it fully autonomous.

1:24:14.320 --> 1:24:17.080
 But now you might be, so then if you do,

1:24:17.080 --> 1:24:19.400
 but now you might be crossing that exact line.

1:24:19.400 --> 1:24:22.360
 You might program it to just, oh yeah, the air drone,

1:24:22.360 --> 1:24:25.280
 go hover over this country for a while

1:24:25.280 --> 1:24:28.760
 and whenever you find someone who is a bad guy,

1:24:28.760 --> 1:24:30.080
 kill them.

1:24:30.960 --> 1:24:33.480
 Now the machine is making these sort of decisions

1:24:33.480 --> 1:24:36.120
 and some people who defend this still say,

1:24:36.120 --> 1:24:39.960
 well, that's morally fine because we are the good guys

1:24:39.960 --> 1:24:43.000
 and we will tell it the definition of bad guy

1:24:43.000 --> 1:24:44.320
 that we think is moral.

1:24:45.640 --> 1:24:48.720
 But now it would be very naive to think

1:24:48.720 --> 1:24:51.480
 that if ISIS buys that same drone,

1:24:51.480 --> 1:24:54.040
 that they're gonna use our definition of bad guy.

1:24:54.040 --> 1:24:55.840
 Maybe for them, bad guy is someone wearing

1:24:55.840 --> 1:25:00.840
 a US army uniform or maybe there will be some,

1:25:00.840 --> 1:25:04.680
 weird ethnic group who decides that someone

1:25:04.680 --> 1:25:07.160
 of another ethnic group, they are the bad guys, right?

1:25:07.160 --> 1:25:11.320
 The thing is human soldiers with all our faults,

1:25:11.320 --> 1:25:14.080
 we still have some basic wiring in us.

1:25:14.080 --> 1:25:18.040
 Like, no, it's not okay to kill kids and civilians.

1:25:20.040 --> 1:25:21.760
 And Thomas Weprin has none of that.

1:25:21.760 --> 1:25:23.600
 It's just gonna do whatever is programmed.

1:25:23.600 --> 1:25:27.720
 It's like the perfect Adolf Eichmann on steroids.

1:25:27.720 --> 1:25:30.840
 Like they told him, Adolf Eichmann, you know,

1:25:30.840 --> 1:25:32.240
 he wanted to do this and this and this

1:25:32.240 --> 1:25:33.680
 to make the Holocaust more efficient.

1:25:33.680 --> 1:25:37.840
 And he was like, yeah, and off he went and did it, right?

1:25:37.840 --> 1:25:41.120
 Do we really wanna make machines that are like that,

1:25:41.120 --> 1:25:44.240
 like completely amoral and we'll take the user's definition

1:25:44.240 --> 1:25:45.720
 of who is the bad guy?

1:25:45.720 --> 1:25:47.920
 And do we then wanna make them so cheap

1:25:47.920 --> 1:25:49.640
 that all our adversaries can have them?

1:25:49.640 --> 1:25:52.720
 Like what could possibly go wrong?

1:25:52.720 --> 1:25:56.720
 That's I think the big ordeal of the whole thing.

1:25:56.720 --> 1:26:00.200
 I think the big argument for why we wanna,

1:26:00.200 --> 1:26:03.520
 this year really put the kibosh on this.

1:26:03.520 --> 1:26:06.360
 And I think you can tell there's a lot

1:26:06.360 --> 1:26:10.120
 of very active debate even going on within the US military

1:26:10.120 --> 1:26:13.080
 and undoubtedly in other militaries around the world also

1:26:13.080 --> 1:26:14.200
 about whether we should have some sort

1:26:14.200 --> 1:26:16.760
 of international agreement to at least require

1:26:16.760 --> 1:26:20.640
 that these weapons have to be above a certain size

1:26:20.640 --> 1:26:27.320
 and cost, you know, so that things just don't totally spiral

1:26:27.320 --> 1:26:29.800
 out of control.

1:26:29.800 --> 1:26:31.600
 And finally, just for your question,

1:26:31.600 --> 1:26:33.560
 but is it possible to stop it?

1:26:33.560 --> 1:26:37.000
 Because some people tell me, oh, just give up, you know.

1:26:37.000 --> 1:26:41.560
 But again, so Matthew Messelsen again from Harvard, right,

1:26:41.560 --> 1:26:46.640
 who the bioweapons hero, he had exactly this criticism

1:26:46.640 --> 1:26:47.760
 also with bioweapons.

1:26:47.760 --> 1:26:49.920
 People were like, how can you check for sure

1:26:49.920 --> 1:26:52.960
 that the Russians aren't cheating?

1:26:52.960 --> 1:26:58.560
 And he told me this, I think really ingenious insight.

1:26:58.560 --> 1:27:01.200
 He said, you know, Max, some people

1:27:01.200 --> 1:27:03.640
 think you have to have inspections and things

1:27:03.640 --> 1:27:06.760
 and you have to make sure that you can catch the cheaters

1:27:06.760 --> 1:27:08.960
 with 100% chance.

1:27:08.960 --> 1:27:10.800
 You don't need 100%, he said.

1:27:10.800 --> 1:27:14.080
 1% is usually enough.

1:27:14.080 --> 1:27:19.240
 Because if it's another big state,

1:27:19.240 --> 1:27:23.480
 suppose China and the US have signed the treaty drawing

1:27:23.480 --> 1:27:26.200
 a certain line and saying, yeah, these kind of drones are OK,

1:27:26.200 --> 1:27:28.800
 but these fully autonomous ones are not.

1:27:28.800 --> 1:27:34.400
 Now suppose you are China and you have cheated and secretly

1:27:34.400 --> 1:27:36.000
 developed some clandestine little thing

1:27:36.000 --> 1:27:37.560
 or you're thinking about doing it.

1:27:37.560 --> 1:27:39.200
 What's your calculation that you do?

1:27:39.200 --> 1:27:41.880
 Well, you're like, OK, what's the probability

1:27:41.880 --> 1:27:44.920
 that we're going to get caught?

1:27:44.920 --> 1:27:49.120
 If the probability is 100%, of course, we're not going to do it.

1:27:49.120 --> 1:27:52.720
 But if the probability is 5% that we're going to get caught,

1:27:52.720 --> 1:27:55.560
 then it's going to be like a huge embarrassment for us.

1:27:55.560 --> 1:28:00.120
 And we still have our nuclear weapons anyway,

1:28:00.120 --> 1:28:05.160
 so it doesn't really make an enormous difference in terms

1:28:05.160 --> 1:28:07.520
 of deterring the US.

1:28:07.520 --> 1:28:11.640
 And that feeds the stigma that you kind of established,

1:28:11.640 --> 1:28:14.720
 like this fabric, this universal stigma over the thing.

1:28:14.720 --> 1:28:15.520
 Exactly.

1:28:15.520 --> 1:28:18.080
 It's very reasonable for them to say, well, we probably

1:28:18.080 --> 1:28:18.800
 get away with it.

1:28:18.800 --> 1:28:21.320
 If we don't, then the US will know we cheated,

1:28:21.320 --> 1:28:23.720
 and then they're going to go full tilt with their program

1:28:23.720 --> 1:28:25.000
 and say, look, the Chinese are cheaters,

1:28:25.000 --> 1:28:27.080
 and now we have all these weapons against us,

1:28:27.080 --> 1:28:27.920
 and that's bad.

1:28:27.920 --> 1:28:32.160
 So the stigma alone is very, very powerful.

1:28:32.160 --> 1:28:34.520
 And again, look what happened with bioweapons.

1:28:34.520 --> 1:28:36.880
 It's been 50 years now.

1:28:36.880 --> 1:28:40.120
 When was the last time you read about a bioterrorism attack?

1:28:40.120 --> 1:28:42.680
 The only deaths I really know about with bioweapons

1:28:42.680 --> 1:28:45.200
 that have happened when we Americans managed

1:28:45.200 --> 1:28:47.200
 to kill some of our own with anthrax,

1:28:47.200 --> 1:28:49.760
 or the idiot who sent them to Tom Daschle and others

1:28:49.760 --> 1:28:50.880
 in letters, right?

1:28:50.880 --> 1:28:55.960
 And similarly in Sverdlovsk in the Soviet Union,

1:28:55.960 --> 1:28:57.960
 they had some anthrax in some lab there.

1:28:57.960 --> 1:29:00.000
 Maybe they were cheating or who knows,

1:29:00.000 --> 1:29:02.520
 and it leaked out and killed a bunch of Russians.

1:29:02.520 --> 1:29:04.480
 I'd say that's a pretty good success, right?

1:29:04.480 --> 1:29:08.360
 50 years, just two own goals by the superpowers,

1:29:08.360 --> 1:29:09.560
 and then nothing.

1:29:09.560 --> 1:29:12.120
 And that's why whenever I ask anyone

1:29:12.120 --> 1:29:15.160
 what they think about biology, they think it's great.

1:29:15.160 --> 1:29:18.080
 They associate it with new cures, new diseases,

1:29:18.080 --> 1:29:19.720
 maybe a good vaccine.

1:29:19.720 --> 1:29:22.680
 This is how I want to think about AI in the future.

1:29:22.680 --> 1:29:24.840
 And I want others to think about AI too,

1:29:24.840 --> 1:29:27.840
 as a source of all these great solutions to our problems,

1:29:27.840 --> 1:29:31.920
 not as, oh, AI, oh yeah, that's the reason

1:29:31.920 --> 1:29:34.600
 I feel scared going outside these days.

1:29:34.600 --> 1:29:37.920
 Yeah, it's kind of brilliant that bioweapons

1:29:37.920 --> 1:29:40.760
 and nuclear weapons, we've figured out,

1:29:40.760 --> 1:29:43.320
 I mean, of course there's still a huge source of danger,

1:29:43.320 --> 1:29:47.760
 but we figured out some way of creating rules

1:29:47.760 --> 1:29:51.440
 and social stigma over these weapons

1:29:51.440 --> 1:29:54.600
 that then creates a stability to our,

1:29:54.600 --> 1:29:57.640
 whatever that game theoretic stability that occurs.

1:29:57.640 --> 1:29:59.200
 And we don't have that with AI,

1:29:59.200 --> 1:30:03.760
 and you're kind of screaming from the top of the mountain

1:30:03.760 --> 1:30:05.520
 about this, that we need to find that

1:30:05.520 --> 1:30:10.520
 because it's very possible with the future of life,

1:30:10.520 --> 1:30:15.000
 as you point out, Institute Awards pointed out

1:30:15.000 --> 1:30:17.920
 that with nuclear weapons,

1:30:17.920 --> 1:30:21.040
 we could have destroyed ourselves quite a few times.

1:30:21.040 --> 1:30:26.040
 And it's a learning experience that is very costly.

1:30:28.520 --> 1:30:30.960
 We gave this Future Life Award,

1:30:30.960 --> 1:30:34.640
 we gave it the first time to this guy, Vasily Arkhipov.

1:30:34.640 --> 1:30:37.480
 He was on, most people haven't even heard of him.

1:30:37.480 --> 1:30:38.640
 Yeah, can you say who he is?

1:30:38.640 --> 1:30:43.640
 Vasily Arkhipov, he has, in my opinion,

1:30:44.080 --> 1:30:47.480
 made the greatest positive contribution to humanity

1:30:47.480 --> 1:30:50.200
 of any human in modern history.

1:30:50.200 --> 1:30:51.880
 And maybe it sounds like hyperbole here,

1:30:51.880 --> 1:30:53.320
 like I'm just over the top,

1:30:53.320 --> 1:30:56.080
 but let me tell you the story and I think maybe you'll agree.

1:30:56.080 --> 1:30:58.200
 So during the Cuban Missile Crisis,

1:31:00.000 --> 1:31:01.800
 we Americans first didn't know

1:31:01.800 --> 1:31:04.240
 that the Russians had sent four submarines,

1:31:05.160 --> 1:31:06.720
 but we caught two of them.

1:31:06.720 --> 1:31:09.160
 And we didn't know that,

1:31:09.160 --> 1:31:11.040
 so we dropped practice depth charges

1:31:11.040 --> 1:31:12.360
 on the one that he was on,

1:31:12.360 --> 1:31:13.840
 try to force it to the surface.

1:31:15.440 --> 1:31:17.680
 But we didn't know that this nuclear submarine

1:31:17.680 --> 1:31:20.560
 actually was a nuclear submarine with a nuclear torpedo.

1:31:20.560 --> 1:31:22.640
 We also didn't know that they had authorization

1:31:22.640 --> 1:31:25.120
 to launch it without clearance from Moscow.

1:31:25.120 --> 1:31:26.040
 And we also didn't know

1:31:26.040 --> 1:31:28.240
 that they were running out of electricity.

1:31:28.240 --> 1:31:29.880
 Their batteries were almost dead.

1:31:29.880 --> 1:31:31.840
 They were running out of oxygen.

1:31:31.840 --> 1:31:34.280
 Sailors were fainting left and right.

1:31:34.280 --> 1:31:39.120
 The temperature was about 110, 120 Fahrenheit on board.

1:31:39.120 --> 1:31:40.920
 It was really hellish conditions,

1:31:40.920 --> 1:31:43.240
 really just a kind of doomsday.

1:31:43.240 --> 1:31:44.520
 And at that point,

1:31:44.520 --> 1:31:46.280
 these giant explosions start happening

1:31:46.280 --> 1:31:48.160
 from the Americans dropping these.

1:31:48.160 --> 1:31:50.680
 The captain thought World War III had begun.

1:31:50.680 --> 1:31:53.720
 They decided they were gonna launch the nuclear torpedo.

1:31:53.720 --> 1:31:55.360
 And one of them shouted,

1:31:55.360 --> 1:31:56.200
 we're all gonna die,

1:31:56.200 --> 1:31:58.920
 but we're not gonna disgrace our Navy.

1:31:58.920 --> 1:32:00.120
 We don't know what would have happened

1:32:00.120 --> 1:32:03.400
 if there had been a giant mushroom cloud all of a sudden

1:32:03.400 --> 1:32:04.760
 against the Americans.

1:32:04.760 --> 1:32:07.360
 But since everybody had their hands on the triggers,

1:32:09.200 --> 1:32:10.800
 you don't have to be too creative to think

1:32:10.800 --> 1:32:13.080
 that it could have led to an all out nuclear war,

1:32:13.080 --> 1:32:15.680
 in which case we wouldn't be having this conversation now.

1:32:15.680 --> 1:32:17.600
 What actually took place was

1:32:17.600 --> 1:32:21.040
 they needed three people to approve this.

1:32:21.040 --> 1:32:22.200
 The captain had said yes.

1:32:22.200 --> 1:32:24.120
 There was the Communist Party political officer.

1:32:24.120 --> 1:32:26.000
 He also said, yes, let's do it.

1:32:26.000 --> 1:32:29.040
 And the third man was this guy, Vasily Arkhipov,

1:32:29.040 --> 1:32:29.880
 who said, no.

1:32:29.880 --> 1:32:32.720
 For some reason, he was just more chill than the others

1:32:32.720 --> 1:32:34.240
 and he was the right man at the right time.

1:32:34.240 --> 1:32:38.120
 I don't want us as a species rely on the right person

1:32:38.120 --> 1:32:40.720
 being there at the right time, you know.

1:32:40.720 --> 1:32:42.920
 We tracked down his family

1:32:42.920 --> 1:32:46.320
 living in relative poverty outside Moscow.

1:32:47.320 --> 1:32:48.800
 When he flew his daughter,

1:32:48.800 --> 1:32:52.720
 he had passed away and flew them to London.

1:32:52.720 --> 1:32:54.000
 They had never been to the West even.

1:32:54.000 --> 1:32:57.160
 It was incredibly moving to get to honor them for this.

1:32:57.160 --> 1:32:59.320
 The next year we gave them a medal.

1:32:59.320 --> 1:33:01.800
 The next year we gave this Future Life Award

1:33:01.800 --> 1:33:04.160
 to Stanislav Petrov.

1:33:04.160 --> 1:33:05.000
 Have you heard of him?

1:33:05.000 --> 1:33:05.840
 Yes.

1:33:05.840 --> 1:33:10.000
 So he was in charge of the Soviet early warning station,

1:33:10.000 --> 1:33:12.880
 which was built with Soviet technology

1:33:12.880 --> 1:33:14.760
 and honestly not that reliable.

1:33:14.760 --> 1:33:17.320
 It said that there were five US missiles coming in.

1:33:18.280 --> 1:33:21.440
 Again, if they had launched at that point,

1:33:21.440 --> 1:33:23.440
 we probably wouldn't be having this conversation.

1:33:23.440 --> 1:33:29.440
 He decided based on just mainly gut instinct

1:33:29.440 --> 1:33:32.640
 to just not escalate this.

1:33:32.640 --> 1:33:35.200
 And I'm very glad he wasn't replaced by an AI

1:33:35.200 --> 1:33:37.600
 that was just automatically following orders.

1:33:37.600 --> 1:33:39.840
 And then we gave the third one to Matthew Messelson.

1:33:39.840 --> 1:33:44.360
 Last year, we gave this award to these guys

1:33:44.360 --> 1:33:46.760
 who actually use technology for good,

1:33:46.760 --> 1:33:50.120
 not avoiding something bad, but for something good.

1:33:50.120 --> 1:33:52.120
 The guys who eliminated this disease,

1:33:52.120 --> 1:33:55.440
 it was way worse than COVID that had killed

1:33:55.440 --> 1:33:58.520
 half a billion people in its final century.

1:33:58.520 --> 1:33:59.440
 Smallpox, right?

1:33:59.440 --> 1:34:01.200
 So you mentioned it earlier.

1:34:01.200 --> 1:34:05.320
 COVID on average kills less than 1% of people who get it.

1:34:05.320 --> 1:34:08.240
 Smallpox, about 30%.

1:34:08.240 --> 1:34:14.160
 And they just ultimately, Viktor Zhdanov and Bill Foege,

1:34:14.160 --> 1:34:17.560
 most of my colleagues have never heard of either of them,

1:34:17.560 --> 1:34:22.080
 one American, one Russian, they did this amazing effort

1:34:22.080 --> 1:34:25.200
 not only was Zhdanov able to get the US and the Soviet Union

1:34:25.200 --> 1:34:27.920
 to team up against smallpox during the Cold War,

1:34:27.920 --> 1:34:30.320
 but Bill Foege came up with this ingenious strategy

1:34:30.320 --> 1:34:32.840
 for making it actually go all the way

1:34:32.840 --> 1:34:36.560
 to defeat the disease without funding

1:34:36.560 --> 1:34:37.600
 for vaccinating everyone.

1:34:37.600 --> 1:34:40.040
 And as a result, we haven't had any,

1:34:40.040 --> 1:34:42.680
 we went from 15 million deaths the year

1:34:42.680 --> 1:34:44.280
 I was born in smallpox.

1:34:44.280 --> 1:34:45.640
 So what do we have in COVID now?

1:34:45.640 --> 1:34:47.240
 A little bit short of 2 million, right?

1:34:47.240 --> 1:34:48.120
 Yes.

1:34:48.120 --> 1:34:52.040
 To zero deaths, of course, this year and forever.

1:34:52.040 --> 1:34:53.960
 There have been 200 million people,

1:34:53.960 --> 1:34:57.200
 we estimate, who would have died since then by smallpox

1:34:57.200 --> 1:34:58.080
 had it not been for this.

1:34:58.080 --> 1:35:02.160
 So isn't science awesome when you use it for good?

1:35:02.160 --> 1:35:04.280
 The reason we wanna celebrate these sort of people

1:35:04.280 --> 1:35:05.680
 is to remind them of this.

1:35:05.680 --> 1:35:10.160
 Science is so awesome when you use it for good.

1:35:10.160 --> 1:35:13.520
 And those awards actually, the variety there,

1:35:13.520 --> 1:35:14.920
 it's a very interesting picture.

1:35:14.920 --> 1:35:19.360
 So the first two are looking at,

1:35:19.360 --> 1:35:22.680
 it's kind of exciting to think that these average humans

1:35:22.680 --> 1:35:26.200
 in some sense, they're products of billions

1:35:26.200 --> 1:35:30.200
 of other humans that came before them, evolution,

1:35:30.200 --> 1:35:33.360
 and some little, you said gut,

1:35:33.360 --> 1:35:35.320
 but there's something in there

1:35:35.320 --> 1:35:40.320
 that stopped the annihilation of the human race.

1:35:41.080 --> 1:35:43.040
 And that's a magical thing,

1:35:43.040 --> 1:35:45.240
 but that's like this deeply human thing.

1:35:45.240 --> 1:35:47.400
 And then there's the other aspect

1:35:47.400 --> 1:35:49.800
 where that's also very human,

1:35:49.800 --> 1:35:51.440
 which is to build solution

1:35:51.440 --> 1:35:55.240
 to the existential crises that we're facing,

1:35:55.240 --> 1:35:57.520
 like to build it, to take the responsibility

1:35:57.520 --> 1:36:00.600
 and to come up with different technologies and so on.

1:36:00.600 --> 1:36:04.080
 And both of those are deeply human,

1:36:04.080 --> 1:36:07.400
 the gut and the mind, whatever that is that creates.

1:36:07.400 --> 1:36:08.640
 The best is when they work together.

1:36:08.640 --> 1:36:11.400
 Arkhipov, I wish I could have met him, of course,

1:36:11.400 --> 1:36:13.200
 but he had passed away.

1:36:13.200 --> 1:36:16.720
 He was really a fantastic military officer,

1:36:16.720 --> 1:36:18.680
 combining all the best traits

1:36:18.680 --> 1:36:21.000
 that we in America admire in our military.

1:36:21.000 --> 1:36:23.160
 Because first of all, he was very loyal, of course.

1:36:23.160 --> 1:36:26.280
 He never even told anyone about this during his whole life,

1:36:26.280 --> 1:36:28.440
 even though you think he had some bragging rights, right?

1:36:28.440 --> 1:36:30.000
 But he just was like, this is just business,

1:36:30.000 --> 1:36:31.560
 just doing my job.

1:36:31.560 --> 1:36:34.320
 It only came out later after his death.

1:36:34.320 --> 1:36:37.120
 And second, the reason he did the right thing

1:36:37.120 --> 1:36:39.240
 was not because he was some sort of liberal

1:36:39.240 --> 1:36:43.960
 or some sort of, not because he was just,

1:36:43.960 --> 1:36:47.360
 oh, peace and love.

1:36:47.360 --> 1:36:49.800
 It was partly because he had been the captain

1:36:49.800 --> 1:36:53.080
 on another submarine that had a nuclear reactor meltdown.

1:36:53.080 --> 1:36:58.000
 And it was his heroism that helped contain this.

1:36:58.000 --> 1:36:59.760
 That's why he died of cancer later also.

1:36:59.760 --> 1:37:01.480
 But he had seen many of his crew members die.

1:37:01.480 --> 1:37:04.160
 And I think for him, that gave him this gut feeling

1:37:04.160 --> 1:37:06.200
 that if there's a nuclear war

1:37:06.200 --> 1:37:08.400
 between the US and the Soviet Union,

1:37:08.400 --> 1:37:11.080
 the whole world is gonna go through

1:37:11.080 --> 1:37:13.760
 what I saw my dear crew members suffer through.

1:37:13.760 --> 1:37:15.840
 It wasn't just an abstract thing for him.

1:37:15.840 --> 1:37:17.680
 I think it was real.

1:37:17.680 --> 1:37:20.640
 And second though, not just the gut, the mind, right?

1:37:20.640 --> 1:37:23.960
 He was, for some reason, very levelheaded personality

1:37:23.960 --> 1:37:25.080
 and very smart guy,

1:37:25.960 --> 1:37:29.240
 which is exactly what we want our best fighter pilots

1:37:29.240 --> 1:37:30.120
 to be also, right?

1:37:30.120 --> 1:37:32.880
 I never forget Neil Armstrong when he's landing on the moon

1:37:32.880 --> 1:37:34.560
 and almost running out of gas.

1:37:34.560 --> 1:37:37.440
 And he doesn't even change when they say 30 seconds,

1:37:37.440 --> 1:37:39.680
 he doesn't even change the tone of voice, just keeps going.

1:37:39.680 --> 1:37:41.840
 Arkhipov, I think was just like that.

1:37:41.840 --> 1:37:43.480
 So when the explosions start going off

1:37:43.480 --> 1:37:45.520
 and his captain is screaming and we should nuke them

1:37:45.520 --> 1:37:47.400
 and all, he's like,

1:37:50.960 --> 1:37:54.280
 I don't think the Americans are trying to sink us.

1:37:54.280 --> 1:37:56.480
 I think they're trying to send us a message.

1:37:58.080 --> 1:37:59.200
 That's pretty bad ass.

1:37:59.200 --> 1:38:00.040
 Yes.

1:38:00.040 --> 1:38:02.720
 Coolness, because he said, if they wanted to sink us,

1:38:03.680 --> 1:38:06.920
 and he said, listen, listen, it's alternating

1:38:06.920 --> 1:38:10.160
 one loud explosion on the left, one on the right,

1:38:10.160 --> 1:38:12.120
 one on the left, one on the right.

1:38:12.120 --> 1:38:14.320
 He was the only one who noticed this pattern.

1:38:15.840 --> 1:38:17.880
 And he's like, I think this is,

1:38:17.880 --> 1:38:19.400
 I'm trying to send us a signal

1:38:20.640 --> 1:38:22.840
 that they want it to surface

1:38:22.840 --> 1:38:24.360
 and they're not gonna sink us.

1:38:25.800 --> 1:38:27.400
 And somehow,

1:38:29.320 --> 1:38:32.160
 this is how he then managed it ultimately

1:38:32.160 --> 1:38:34.640
 with his combination of gut

1:38:34.640 --> 1:38:37.960
 and also just cool analytical thinking,

1:38:37.960 --> 1:38:40.120
 was able to deescalate the whole thing.

1:38:40.120 --> 1:38:44.240
 And yeah, so this is some of the best in humanity.

1:38:44.240 --> 1:38:45.880
 I guess coming back to what we talked about earlier,

1:38:45.880 --> 1:38:47.400
 it's the combination of the neural network,

1:38:47.400 --> 1:38:50.960
 the instinctive, with, I'm getting teary up here,

1:38:50.960 --> 1:38:53.240
 getting emotional, but he was just,

1:38:53.240 --> 1:38:54.800
 he is one of my superheroes,

1:38:56.120 --> 1:39:00.440
 having both the heart and the mind combined.

1:39:00.440 --> 1:39:03.760
 And especially in that time, there's something about the,

1:39:03.760 --> 1:39:05.440
 I mean, this is a very, in America,

1:39:05.440 --> 1:39:06.880
 people are used to this kind of idea

1:39:06.880 --> 1:39:11.160
 of being the individual of like on your own thinking.

1:39:12.040 --> 1:39:15.480
 I think under, in the Soviet Union under communism,

1:39:15.480 --> 1:39:17.600
 it's actually much harder to do that.

1:39:17.600 --> 1:39:19.960
 Oh yeah, he didn't even, he even got,

1:39:19.960 --> 1:39:21.840
 he didn't get any accolades either

1:39:21.840 --> 1:39:24.240
 when he came back for this, right?

1:39:24.240 --> 1:39:25.880
 They just wanted to hush the whole thing up.

1:39:25.880 --> 1:39:28.000
 Yeah, there's echoes of that with Chernobyl,

1:39:28.000 --> 1:39:29.160
 there's all kinds of,

1:39:30.920 --> 1:39:34.400
 that's one, that's a really hopeful thing

1:39:34.400 --> 1:39:37.520
 that amidst big centralized powers,

1:39:37.520 --> 1:39:39.920
 whether it's companies or states,

1:39:39.920 --> 1:39:42.480
 there's still the power of the individual

1:39:42.480 --> 1:39:43.880
 to think on their own, to act.

1:39:43.880 --> 1:39:46.880
 But I think we need to think of people like this,

1:39:46.880 --> 1:39:50.160
 not as a panacea we can always count on,

1:39:50.160 --> 1:39:54.080
 but rather as a wake up call.

1:39:55.720 --> 1:39:58.560
 So because of them, because of Arkhipov,

1:39:58.560 --> 1:40:01.320
 we are alive to learn from this lesson,

1:40:01.320 --> 1:40:03.120
 to learn from the fact that we shouldn't keep playing

1:40:03.120 --> 1:40:04.840
 Russian roulette and almost have a nuclear war

1:40:04.840 --> 1:40:06.600
 by mistake now and then,

1:40:06.600 --> 1:40:09.600
 because relying on luck is not a good longterm strategy.

1:40:09.600 --> 1:40:11.360
 If you keep playing Russian roulette over and over again,

1:40:11.360 --> 1:40:13.560
 the probability of surviving just drops exponentially

1:40:13.560 --> 1:40:14.400
 with time.

1:40:14.400 --> 1:40:15.240
 Yeah.

1:40:15.240 --> 1:40:16.680
 And if you have some probability

1:40:16.680 --> 1:40:18.640
 of having an accidental nuke war every year,

1:40:18.640 --> 1:40:21.200
 the probability of not having one also drops exponentially.

1:40:21.200 --> 1:40:22.840
 I think we can do better than that.

1:40:22.840 --> 1:40:26.000
 So I think the message is very clear,

1:40:26.000 --> 1:40:27.840
 once in a while shit happens,

1:40:27.840 --> 1:40:31.320
 and there's a lot of very concrete things we can do

1:40:31.320 --> 1:40:34.920
 to reduce the risk of things like that happening

1:40:34.920 --> 1:40:36.520
 in the first place.

1:40:36.520 --> 1:40:39.600
 On the AI front, if we just link on that for a second.

1:40:39.600 --> 1:40:40.960
 Yeah.

1:40:40.960 --> 1:40:44.120
 So you're friends with, you often talk with Elon Musk

1:40:44.120 --> 1:40:46.680
 throughout history, you've did a lot

1:40:46.680 --> 1:40:48.680
 of interesting things together.

1:40:48.680 --> 1:40:52.280
 He has a set of fears about the future

1:40:52.280 --> 1:40:54.960
 of artificial intelligence, AGI.

1:40:55.840 --> 1:40:59.720
 Do you have a sense, we've already talked about

1:40:59.720 --> 1:41:01.560
 the things we should be worried about with AI,

1:41:01.560 --> 1:41:04.040
 do you have a sense of the shape of his fears

1:41:04.040 --> 1:41:06.880
 in particular about AI,

1:41:06.880 --> 1:41:10.160
 of which subset of what we've talked about,

1:41:10.160 --> 1:41:14.480
 whether it's creating, it's that direction

1:41:14.480 --> 1:41:17.520
 of creating sort of these giant competition systems

1:41:17.520 --> 1:41:19.160
 that are not explainable,

1:41:19.160 --> 1:41:21.800
 they're not intelligible intelligence,

1:41:21.800 --> 1:41:26.720
 or is it the...

1:41:26.720 --> 1:41:28.840
 And then like as a branch of that,

1:41:28.840 --> 1:41:31.840
 is it the manipulation by big corporations of that

1:41:31.840 --> 1:41:35.400
 or individual evil people to use that for destruction

1:41:35.400 --> 1:41:37.480
 or the unintentional consequences?

1:41:37.480 --> 1:41:40.280
 Do you have a sense of where his thinking is on this?

1:41:40.280 --> 1:41:42.440
 From my many conversations with Elon,

1:41:42.440 --> 1:41:47.400
 yeah, I certainly have a model of how he thinks.

1:41:47.400 --> 1:41:49.880
 It's actually very much like the way I think also,

1:41:49.880 --> 1:41:51.080
 I'll elaborate on it a bit.

1:41:51.080 --> 1:41:54.680
 I just wanna push back on when you said evil people,

1:41:54.680 --> 1:41:58.520
 I don't think it's a very helpful concept.

1:41:58.520 --> 1:42:02.320
 Evil people, sometimes people do very, very bad things,

1:42:02.320 --> 1:42:05.440
 but they usually do it because they think it's a good thing

1:42:05.440 --> 1:42:07.760
 because somehow other people had told them

1:42:07.760 --> 1:42:08.640
 that that was a good thing

1:42:08.640 --> 1:42:13.380
 or given them incorrect information or whatever, right?

1:42:15.440 --> 1:42:18.400
 I believe in the fundamental goodness of humanity

1:42:18.400 --> 1:42:21.680
 that if we educate people well

1:42:21.680 --> 1:42:24.240
 and they find out how things really are,

1:42:24.240 --> 1:42:27.240
 people generally wanna do good and be good.

1:42:27.240 --> 1:42:30.360
 Hence the value alignment,

1:42:30.360 --> 1:42:33.660
 as opposed to it's about information, about knowledge,

1:42:33.660 --> 1:42:35.320
 and then once we have that,

1:42:35.320 --> 1:42:39.960
 we'll likely be able to do good

1:42:39.960 --> 1:42:41.600
 in the way that's aligned with everybody else

1:42:41.600 --> 1:42:42.440
 who thinks differently.

1:42:42.440 --> 1:42:44.000
 Yeah, and it's not just the individual people

1:42:44.000 --> 1:42:44.960
 we have to align.

1:42:44.960 --> 1:42:49.600
 So we don't just want people to be educated

1:42:49.600 --> 1:42:51.200
 to know the way things actually are

1:42:51.200 --> 1:42:53.200
 and to treat each other well,

1:42:53.200 --> 1:42:56.280
 but we also need to align other nonhuman entities.

1:42:56.280 --> 1:42:58.560
 We talked about corporations, there has to be institutions

1:42:58.560 --> 1:42:59.960
 so that what they do is actually good

1:42:59.960 --> 1:43:00.880
 for the country they're in

1:43:00.880 --> 1:43:03.480
 and we should align, make sure that what countries do

1:43:03.480 --> 1:43:07.780
 is actually good for the species as a whole, et cetera.

1:43:07.780 --> 1:43:08.680
 Coming back to Elon,

1:43:08.680 --> 1:43:13.600
 yeah, my understanding of how Elon sees this

1:43:13.600 --> 1:43:15.240
 is really quite similar to my own,

1:43:15.240 --> 1:43:18.200
 which is one of the reasons I like him so much

1:43:18.200 --> 1:43:19.320
 and enjoy talking with him so much.

1:43:19.320 --> 1:43:22.960
 I feel he's quite different from most people

1:43:22.960 --> 1:43:27.720
 in that he thinks much more than most people

1:43:27.720 --> 1:43:29.840
 about the really big picture,

1:43:29.840 --> 1:43:32.540
 not just what's gonna happen in the next election cycle,

1:43:32.540 --> 1:43:36.020
 but in millennia, millions and billions of years from now.

1:43:36.840 --> 1:43:39.280
 And when you look in this more cosmic perspective,

1:43:39.280 --> 1:43:43.080
 it's so obvious that we are gazing out into this universe

1:43:43.080 --> 1:43:46.280
 that as far as we can tell is mostly dead

1:43:46.280 --> 1:43:49.800
 with life being almost imperceptibly tiny perturbation,

1:43:49.800 --> 1:43:52.640
 and he sees this enormous opportunity

1:43:52.640 --> 1:43:54.280
 for our universe to come alive,

1:43:54.280 --> 1:43:56.480
 first to become an interplanetary species.

1:43:56.480 --> 1:44:01.480
 Mars is obviously just first stop on this cosmic journey.

1:44:02.120 --> 1:44:05.020
 And precisely because he thinks more long term,

1:44:06.760 --> 1:44:09.560
 it's much more clear to him than to most people

1:44:09.560 --> 1:44:11.340
 that what we do with this Russian roulette thing

1:44:11.340 --> 1:44:15.320
 we keep playing with our nukes is a really poor strategy,

1:44:15.320 --> 1:44:16.720
 really reckless strategy.

1:44:16.720 --> 1:44:18.620
 And also that we're just building

1:44:18.620 --> 1:44:21.640
 these ever more powerful AI systems that we don't understand

1:44:21.640 --> 1:44:23.840
 is also just a really reckless strategy.

1:44:23.840 --> 1:44:26.640
 I feel Elon is very much a humanist

1:44:26.640 --> 1:44:30.880
 in the sense that he wants an awesome future for humanity.

1:44:30.880 --> 1:44:35.880
 He wants it to be us that control the machines

1:44:35.960 --> 1:44:38.280
 rather than the machines that control us.

1:44:39.400 --> 1:44:42.080
 And why shouldn't we insist on that?

1:44:42.080 --> 1:44:44.560
 We're building them after all, right?

1:44:44.560 --> 1:44:46.520
 Why should we build things that just make us

1:44:46.520 --> 1:44:48.440
 into some little cog in the machinery

1:44:48.440 --> 1:44:50.240
 that has no further say in the matter, right?

1:44:50.240 --> 1:44:54.560
 That's not my idea of an inspiring future either.

1:44:54.560 --> 1:44:57.880
 Yeah, if you think on the cosmic scale

1:44:57.880 --> 1:44:59.820
 in terms of both time and space,

1:45:00.720 --> 1:45:02.600
 so much is put into perspective.

1:45:02.600 --> 1:45:04.220
 Yeah.

1:45:04.220 --> 1:45:06.440
 Whenever I have a bad day, that's what I think about.

1:45:06.440 --> 1:45:09.200
 It immediately makes me feel better.

1:45:09.200 --> 1:45:13.520
 It makes me sad that for us individual humans,

1:45:13.520 --> 1:45:16.400
 at least for now, the ride ends too quickly.

1:45:16.400 --> 1:45:20.080
 That we don't get to experience the cosmic scale.

1:45:20.080 --> 1:45:22.280
 Yeah, I mean, I think of our universe sometimes

1:45:22.280 --> 1:45:25.200
 as an organism that has only begun to wake up a tiny bit,

1:45:26.080 --> 1:45:30.120
 just like the very first little glimmers of consciousness

1:45:30.120 --> 1:45:32.120
 you have in the morning when you start coming around.

1:45:32.120 --> 1:45:33.160
 Before the coffee.

1:45:33.160 --> 1:45:35.880
 Before the coffee, even before you get out of bed,

1:45:35.880 --> 1:45:37.280
 before you even open your eyes.

1:45:37.280 --> 1:45:40.320
 You start to wake up a little bit.

1:45:40.320 --> 1:45:41.320
 There's something here.

1:45:43.440 --> 1:45:47.120
 That's very much how I think of where we are.

1:45:47.120 --> 1:45:48.600
 All those galaxies out there,

1:45:48.600 --> 1:45:51.160
 I think they're really beautiful,

1:45:51.160 --> 1:45:52.840
 but why are they beautiful?

1:45:52.840 --> 1:45:55.040
 They're beautiful because conscious entities

1:45:55.040 --> 1:45:57.000
 are actually observing them,

1:45:57.000 --> 1:45:59.080
 experiencing them through our telescopes.

1:46:01.720 --> 1:46:05.880
 I define consciousness as subjective experience,

1:46:05.880 --> 1:46:09.420
 whether it be colors or emotions or sounds.

1:46:09.420 --> 1:46:12.340
 So beauty is an experience.

1:46:12.340 --> 1:46:13.800
 Meaning is an experience.

1:46:13.800 --> 1:46:15.880
 Purpose is an experience.

1:46:15.880 --> 1:46:18.000
 If there was no conscious experience,

1:46:18.000 --> 1:46:20.320
 observing these galaxies, they wouldn't be beautiful.

1:46:20.320 --> 1:46:24.960
 If we do something dumb with advanced AI in the future here

1:46:24.960 --> 1:46:29.360
 and Earth originating, life goes extinct.

1:46:29.360 --> 1:46:30.480
 And that was it for this.

1:46:30.480 --> 1:46:33.560
 If there is nothing else with telescopes in our universe,

1:46:33.560 --> 1:46:36.600
 then it's kind of game over for beauty

1:46:36.600 --> 1:46:38.120
 and meaning and purpose in our whole universe.

1:46:38.120 --> 1:46:39.880
 And I think that would be just such

1:46:39.880 --> 1:46:41.800
 an opportunity lost, frankly.

1:46:41.800 --> 1:46:46.080
 And I think when Elon points this out,

1:46:46.080 --> 1:46:49.640
 he gets very unfairly maligned in the media

1:46:49.640 --> 1:46:52.440
 for all the dumb media bias reasons we talked about.

1:46:52.440 --> 1:46:55.680
 They want to print precisely the things about Elon

1:46:55.680 --> 1:46:58.800
 out of context that are really click baity.

1:46:58.800 --> 1:47:00.440
 He has gotten so much flack

1:47:00.440 --> 1:47:03.420
 for this summoning the demon statement.

1:47:04.720 --> 1:47:07.680
 I happen to know exactly the context

1:47:07.680 --> 1:47:09.720
 because I was in the front row when he gave that talk.

1:47:09.720 --> 1:47:11.280
 It was at MIT, you'll be pleased to know,

1:47:11.280 --> 1:47:13.880
 it was the AeroAstro anniversary.

1:47:13.880 --> 1:47:16.800
 They had Buzz Aldrin there from the moon landing,

1:47:16.800 --> 1:47:19.000
 a whole house, a Kresge auditorium

1:47:19.000 --> 1:47:20.840
 packed with MIT students.

1:47:20.840 --> 1:47:23.920
 And he had this amazing Q&A, it might've gone for an hour.

1:47:23.920 --> 1:47:27.160
 And they talked about rockets and Mars and everything.

1:47:27.160 --> 1:47:29.600
 At the very end, this one student

1:47:29.600 --> 1:47:33.200
 who has actually hit my class asked him, what about AI?

1:47:33.200 --> 1:47:35.240
 Elon makes this one comment

1:47:35.240 --> 1:47:39.440
 and they take this out of context, print it, goes viral.

1:47:39.440 --> 1:47:40.600
 What is it like with AI,

1:47:40.600 --> 1:47:42.920
 we're summoning the demons, something like that.

1:47:42.920 --> 1:47:47.480
 And try to cast him as some sort of doom and gloom dude.

1:47:47.480 --> 1:47:51.960
 You know Elon, he's not the doom and gloom dude.

1:47:51.960 --> 1:47:54.000
 He is such a positive visionary.

1:47:54.000 --> 1:47:55.680
 And the whole reason he warns about this

1:47:55.680 --> 1:47:57.720
 is because he realizes more than most

1:47:57.720 --> 1:47:59.880
 what the opportunity cost is of screwing up.

1:47:59.880 --> 1:48:02.360
 That there is so much awesomeness in the future

1:48:02.360 --> 1:48:05.480
 that we can and our descendants can enjoy

1:48:05.480 --> 1:48:07.760
 if we don't screw up, right?

1:48:07.760 --> 1:48:10.320
 I get so pissed off when people try to cast him

1:48:10.320 --> 1:48:15.320
 as some sort of technophobic Luddite.

1:48:15.320 --> 1:48:18.480
 And at this point, it's kind of ludicrous

1:48:18.480 --> 1:48:21.640
 when I hear people say that people who worry about

1:48:21.640 --> 1:48:24.560
 artificial general intelligence are Luddites

1:48:24.560 --> 1:48:27.000
 because of course, if you look more closely,

1:48:27.000 --> 1:48:32.000
 you have some of the most outspoken people making warnings

1:48:32.920 --> 1:48:35.640
 are people like Professor Stuart Russell from Berkeley

1:48:35.640 --> 1:48:38.360
 who's written the bestselling AI textbook, you know.

1:48:38.360 --> 1:48:43.360
 So claiming that he's a Luddite who doesn't understand AI

1:48:43.360 --> 1:48:46.520
 is the joke is really on the people who said it.

1:48:46.520 --> 1:48:48.200
 But I think more broadly,

1:48:48.200 --> 1:48:50.800
 this message is really not sunk in at all.

1:48:50.800 --> 1:48:52.640
 What it is that people worry about,

1:48:52.640 --> 1:48:56.680
 they think that Elon and Stuart Russell and others

1:48:56.680 --> 1:49:01.680
 are worried about the dancing robots picking up an AR 15

1:49:02.280 --> 1:49:04.360
 and going on a rampage, right?

1:49:04.360 --> 1:49:08.440
 They think they're worried about robots turning evil.

1:49:08.440 --> 1:49:10.360
 They're not, I'm not.

1:49:10.360 --> 1:49:15.360
 The risk is not malice, it's competence.

1:49:15.880 --> 1:49:17.560
 The risk is just that we build some systems

1:49:17.560 --> 1:49:18.760
 that are incredibly competent,

1:49:18.760 --> 1:49:20.040
 which means they're always gonna get

1:49:20.040 --> 1:49:22.000
 their goals accomplished,

1:49:22.000 --> 1:49:24.080
 even if they clash with our goals.

1:49:24.080 --> 1:49:25.040
 That's the risk.

1:49:25.920 --> 1:49:30.920
 Why did we humans drive the West African black rhino extinct?

1:49:30.920 --> 1:49:34.840
 Is it because we're malicious, evil rhinoceros haters?

1:49:34.840 --> 1:49:38.000
 No, it's just because our goals didn't align

1:49:38.000 --> 1:49:39.240
 with the goals of those rhinos

1:49:39.240 --> 1:49:41.440
 and tough luck for the rhinos, you know.

1:49:42.360 --> 1:49:46.720
 So the point is just we don't wanna put ourselves

1:49:46.720 --> 1:49:48.120
 in the position of those rhinos

1:49:48.120 --> 1:49:51.240
 creating something more powerful than us

1:49:51.240 --> 1:49:53.880
 if we haven't first figured out how to align the goals.

1:49:53.880 --> 1:49:54.920
 And I am optimistic.

1:49:54.920 --> 1:49:56.880
 I think we could do it if we worked really hard on it,

1:49:56.880 --> 1:49:59.200
 because I spent a lot of time

1:49:59.200 --> 1:50:01.800
 around intelligent entities that were more intelligent

1:50:01.800 --> 1:50:04.040
 than me, my mom and my dad.

1:50:05.960 --> 1:50:07.560
 And I was little and that was fine

1:50:07.560 --> 1:50:09.160
 because their goals were actually aligned

1:50:09.160 --> 1:50:10.200
 with mine quite well.

1:50:11.280 --> 1:50:15.440
 But we've seen today many examples of where the goals

1:50:15.440 --> 1:50:17.200
 of our powerful systems are not so aligned.

1:50:17.200 --> 1:50:22.200
 So those click through optimization algorithms

1:50:22.960 --> 1:50:24.560
 that are polarized social media, right?

1:50:24.560 --> 1:50:26.160
 They were actually pretty poorly aligned

1:50:26.160 --> 1:50:28.760
 with what was good for democracy, it turned out.

1:50:28.760 --> 1:50:31.520
 And again, almost all problems we've had

1:50:31.520 --> 1:50:33.640
 in the machine learning again came so far,

1:50:33.640 --> 1:50:35.520
 not from malice, but from poor alignment.

1:50:35.520 --> 1:50:38.240
 And that's exactly why that's why we should be concerned

1:50:38.240 --> 1:50:39.320
 about it in the future.

1:50:39.320 --> 1:50:43.240
 Do you think it's possible that with systems

1:50:43.240 --> 1:50:47.320
 like Neuralink and brain computer interfaces,

1:50:47.320 --> 1:50:49.280
 you know, again, thinking of the cosmic scale,

1:50:49.280 --> 1:50:52.600
 Elon's talked about this, but others have as well

1:50:52.600 --> 1:50:57.240
 throughout history of figuring out how the exact mechanism

1:50:57.240 --> 1:51:00.000
 of how to achieve that kind of alignment.

1:51:00.000 --> 1:51:03.160
 So one of them is having a symbiosis with AI,

1:51:03.160 --> 1:51:05.560
 which is like coming up with clever ways

1:51:05.560 --> 1:51:10.360
 where we're like stuck together in this weird relationship,

1:51:10.360 --> 1:51:14.200
 whether it's biological or in some kind of other way.

1:51:14.200 --> 1:51:17.240
 Do you think that's a possibility

1:51:17.240 --> 1:51:19.200
 of having that kind of symbiosis?

1:51:19.200 --> 1:51:20.960
 Or do we wanna instead kind of focus

1:51:20.960 --> 1:51:27.960
 on this distinct entities of us humans talking

1:51:28.200 --> 1:51:31.720
 to these intelligible, self doubting AIs,

1:51:31.720 --> 1:51:33.600
 maybe like Stuart Russell thinks about it,

1:51:33.600 --> 1:51:37.640
 like we're self doubting and full of uncertainty

1:51:37.640 --> 1:51:39.760
 and our AI systems are full of uncertainty.

1:51:39.760 --> 1:51:41.520
 We communicate back and forth

1:51:41.520 --> 1:51:43.760
 and in that way achieve symbiosis.

1:51:44.680 --> 1:51:46.200
 I honestly don't know.

1:51:46.200 --> 1:51:48.600
 I would say that because we don't know for sure

1:51:48.600 --> 1:51:52.200
 what if any of our, which of any of our ideas will work.

1:51:52.200 --> 1:51:55.200
 But we do know that if we don't,

1:51:55.200 --> 1:51:56.880
 I'm pretty convinced that if we don't get any

1:51:56.880 --> 1:51:59.840
 of these things to work and just barge ahead,

1:51:59.840 --> 1:52:01.440
 then our species is, you know,

1:52:01.440 --> 1:52:03.720
 probably gonna go extinct this century.

1:52:03.720 --> 1:52:04.600
 I think it's...

1:52:04.600 --> 1:52:06.320
 This century, you think like,

1:52:06.320 --> 1:52:09.720
 you think we're facing this crisis

1:52:09.720 --> 1:52:11.320
 is a 21st century crisis.

1:52:11.320 --> 1:52:13.520
 Like this century will be remembered.

1:52:13.520 --> 1:52:18.520
 But on a hard drive and a hard drive somewhere

1:52:18.720 --> 1:52:22.280
 or maybe by future generations is like,

1:52:22.280 --> 1:52:26.240
 like there'll be future Future of Life Institute awards

1:52:26.240 --> 1:52:30.640
 for people that have done something about AI.

1:52:30.640 --> 1:52:31.880
 It could also end even worse,

1:52:31.880 --> 1:52:33.720
 whether we're not superseded

1:52:33.720 --> 1:52:35.280
 by leaving any AI behind either.

1:52:35.280 --> 1:52:37.040
 We just totally wipe out, you know,

1:52:37.040 --> 1:52:38.480
 like on Easter Island.

1:52:38.480 --> 1:52:39.880
 Our century is long.

1:52:39.880 --> 1:52:44.280
 You know, there are still 79 years left of it, right?

1:52:44.280 --> 1:52:47.680
 Think about how far we've come just in the last 30 years.

1:52:47.680 --> 1:52:52.680
 So we can talk more about what might go wrong,

1:52:53.080 --> 1:52:54.600
 but you asked me this really good question

1:52:54.600 --> 1:52:55.800
 about what's the best strategy.

1:52:55.800 --> 1:52:59.800
 Is it Neuralink or Russell's approach or whatever?

1:52:59.800 --> 1:53:04.800
 I think, you know, when we did the Manhattan project,

1:53:05.480 --> 1:53:08.480
 we didn't know if any of our four ideas

1:53:08.480 --> 1:53:11.760
 for enriching uranium and getting out the uranium 235

1:53:11.760 --> 1:53:12.880
 were gonna work.

1:53:12.880 --> 1:53:14.800
 But we felt this was really important

1:53:14.800 --> 1:53:16.680
 to get it before Hitler did.

1:53:16.680 --> 1:53:17.520
 So, you know what we did?

1:53:17.520 --> 1:53:19.520
 We tried all four of them.

1:53:19.520 --> 1:53:21.960
 Here, I think it's analogous

1:53:21.960 --> 1:53:24.360
 where there's the greatest threat

1:53:24.360 --> 1:53:25.920
 that's ever faced our species.

1:53:25.920 --> 1:53:29.240
 And of course, US national security by implication.

1:53:29.240 --> 1:53:31.480
 We don't know if we don't have any method

1:53:31.480 --> 1:53:34.680
 that's guaranteed to work, but we have a lot of ideas.

1:53:34.680 --> 1:53:35.960
 So we should invest pretty heavily

1:53:35.960 --> 1:53:38.040
 in pursuing all of them with an open mind

1:53:38.040 --> 1:53:40.560
 and hope that one of them at least works.

1:53:40.560 --> 1:53:45.360
 These are, the good news is the century is long,

1:53:45.360 --> 1:53:47.880
 and it might take decades

1:53:47.880 --> 1:53:50.160
 until we have artificial general intelligence.

1:53:50.160 --> 1:53:52.760
 So we have some time hopefully,

1:53:52.760 --> 1:53:55.240
 but it takes a long time to solve

1:53:55.240 --> 1:53:57.120
 these very, very difficult problems.

1:53:57.120 --> 1:53:58.080
 It's gonna actually be the,

1:53:58.080 --> 1:53:59.160
 it's the most difficult problem

1:53:59.160 --> 1:54:01.320
 we were ever trying to solve as a species.

1:54:01.320 --> 1:54:03.400
 So we have to start now.

1:54:03.400 --> 1:54:05.840
 So we don't have, rather than begin thinking about it

1:54:05.840 --> 1:54:08.720
 the night before some people who've had too much Red Bull

1:54:08.720 --> 1:54:09.560
 switch it on.

1:54:09.560 --> 1:54:11.840
 And we have to, coming back to your question,

1:54:11.840 --> 1:54:14.240
 we have to pursue all of these different avenues and see.

1:54:14.240 --> 1:54:16.800
 If you were my investment advisor

1:54:16.800 --> 1:54:19.920
 and I was trying to invest in the future,

1:54:19.920 --> 1:54:22.120
 how do you think the human species

1:54:23.040 --> 1:54:27.560
 is most likely to destroy itself in the century?

1:54:29.440 --> 1:54:32.120
 Yeah, so if the crises,

1:54:32.120 --> 1:54:34.680
 many of the crises we're facing are really before us

1:54:34.680 --> 1:54:37.160
 within the next hundred years,

1:54:37.160 --> 1:54:41.200
 how do we make explicit,

1:54:42.320 --> 1:54:46.640
 make known the unknowns and solve those problems

1:54:46.640 --> 1:54:48.200
 to avoid the biggest,

1:54:49.560 --> 1:54:51.920
 starting with the biggest existential crisis?

1:54:51.920 --> 1:54:53.160
 So as your investment advisor,

1:54:53.160 --> 1:54:55.680
 how are you planning to make money on us

1:54:55.680 --> 1:54:56.640
 destroying ourselves?

1:54:56.640 --> 1:54:57.480
 I have to ask.

1:54:57.480 --> 1:54:58.320
 I don't know.

1:54:58.320 --> 1:55:00.040
 It might be the Russian origins.

1:55:01.080 --> 1:55:02.840
 Somehow it's involved.

1:55:02.840 --> 1:55:04.760
 At the micro level of detailed strategies,

1:55:04.760 --> 1:55:06.680
 of course, these are unsolved problems.

1:55:08.640 --> 1:55:09.680
 For AI alignment,

1:55:09.680 --> 1:55:12.240
 we can break it into three sub problems

1:55:12.240 --> 1:55:13.480
 that are all unsolved.

1:55:13.480 --> 1:55:16.720
 I think you want first to make machines

1:55:16.720 --> 1:55:18.400
 understand our goals,

1:55:18.400 --> 1:55:23.400
 then adopt our goals and then retain our goals.

1:55:23.600 --> 1:55:26.160
 So to hit on all three real quickly.

1:55:27.400 --> 1:55:31.080
 The problem when Andreas Lubitz told his autopilot

1:55:31.080 --> 1:55:34.320
 to fly into the Alps was that the computer

1:55:34.320 --> 1:55:39.040
 didn't even understand anything about his goals.

1:55:39.040 --> 1:55:40.520
 It was too dumb.

1:55:40.520 --> 1:55:42.720
 It could have understood actually,

1:55:42.720 --> 1:55:45.280
 but you would have had to put some effort in

1:55:45.280 --> 1:55:48.880
 as a systems designer to don't fly into mountains.

1:55:48.880 --> 1:55:49.920
 So that's the first challenge.

1:55:49.920 --> 1:55:54.480
 How do you program into computers human values,

1:55:54.480 --> 1:55:55.320
 human goals?

1:55:56.240 --> 1:55:58.280
 We can start rather than saying,

1:55:58.280 --> 1:55:59.120
 oh, it's so hard.

1:55:59.120 --> 1:56:01.400
 We should start with the simple stuff, as I said,

1:56:02.400 --> 1:56:04.120
 self driving cars, airplanes,

1:56:04.120 --> 1:56:07.240
 just put in all the goals that we all agree on already,

1:56:07.240 --> 1:56:10.560
 and then have a habit of whenever machines get smarter

1:56:10.560 --> 1:56:14.280
 so they can understand one level higher goals,

1:56:15.480 --> 1:56:16.960
 put them into.

1:56:16.960 --> 1:56:20.840
 The second challenge is getting them to adopt the goals.

1:56:20.840 --> 1:56:22.320
 It's easy for situations like that

1:56:22.320 --> 1:56:23.280
 where you just program it in,

1:56:23.280 --> 1:56:26.040
 but when you have self learning systems like children,

1:56:26.040 --> 1:56:29.320
 you know, any parent knows

1:56:29.320 --> 1:56:33.440
 that there was a difference between getting our kids

1:56:33.440 --> 1:56:34.840
 to understand what we want them to do

1:56:34.840 --> 1:56:37.600
 and to actually adopt our goals, right?

1:56:37.600 --> 1:56:40.040
 With humans, with children, fortunately,

1:56:40.040 --> 1:56:44.000
 they go through this phase.

1:56:44.000 --> 1:56:45.480
 First, they're too dumb to understand

1:56:45.480 --> 1:56:46.800
 what we want our goals are.

1:56:46.800 --> 1:56:50.360
 And then they have this period of some years

1:56:50.360 --> 1:56:52.080
 when they're both smart enough to understand them

1:56:52.080 --> 1:56:53.520
 and malleable enough that we have a chance

1:56:53.520 --> 1:56:55.400
 to raise them well.

1:56:55.400 --> 1:56:59.160
 And then they become teenagers kind of too late.

1:56:59.160 --> 1:57:01.360
 But we have this window with machines,

1:57:01.360 --> 1:57:04.120
 the challenges, the intelligence might grow so fast

1:57:04.120 --> 1:57:05.880
 that that window is pretty short.

1:57:06.800 --> 1:57:08.480
 So that's a research problem.

1:57:08.480 --> 1:57:11.320
 The third one is how do you make sure they keep the goals

1:57:11.320 --> 1:57:13.680
 if they keep learning more and getting smarter?

1:57:14.520 --> 1:57:17.360
 Many sci fi movies are about how you have something

1:57:17.360 --> 1:57:18.520
 in which initially was aligned,

1:57:18.520 --> 1:57:20.320
 but then things kind of go off keel.

1:57:20.320 --> 1:57:24.680
 And, you know, my kids were very, very excited

1:57:24.680 --> 1:57:27.360
 about their Legos when they were little.

1:57:27.360 --> 1:57:29.800
 Now they're just gathering dust in the basement.

1:57:29.800 --> 1:57:32.560
 If we create machines that are really on board

1:57:32.560 --> 1:57:34.320
 with the goal of taking care of humanity,

1:57:34.320 --> 1:57:36.080
 we don't want them to get as bored with us

1:57:36.080 --> 1:57:39.480
 as my kids got with Legos.

1:57:39.480 --> 1:57:41.920
 So this is another research challenge.

1:57:41.920 --> 1:57:43.400
 How can you make some sort of recursively

1:57:43.400 --> 1:57:47.440
 self improving system retain certain basic goals?

1:57:47.440 --> 1:57:50.880
 That said, a lot of adult people still play with Legos.

1:57:50.880 --> 1:57:52.720
 So maybe we succeeded with the Legos.

1:57:52.720 --> 1:57:55.320
 Maybe, I like your optimism.

1:57:55.320 --> 1:57:56.160
 But above all.

1:57:56.160 --> 1:57:59.120
 So not all AI systems have to maintain the goals, right?

1:57:59.120 --> 1:58:00.200
 Just some fraction.

1:58:00.200 --> 1:58:04.920
 Yeah, so there's a lot of talented AI researchers now

1:58:04.920 --> 1:58:07.280
 who have heard of this and want to work on it.

1:58:07.280 --> 1:58:08.880
 Not so much funding for it yet.

1:58:10.960 --> 1:58:13.880
 Of the billions that go into building AI more powerful,

1:58:14.800 --> 1:58:16.240
 it's only a minuscule fraction

1:58:16.240 --> 1:58:18.280
 so far going into this safety research.

1:58:18.280 --> 1:58:20.880
 My attitude is generally we should not try to slow down

1:58:20.880 --> 1:58:22.840
 the technology, but we should greatly accelerate

1:58:22.840 --> 1:58:25.880
 the investment in this sort of safety research.

1:58:25.880 --> 1:58:29.320
 And also, this was very embarrassing last year,

1:58:29.320 --> 1:58:31.840
 but the NSF decided to give out

1:58:31.840 --> 1:58:33.680
 six of these big institutes.

1:58:33.680 --> 1:58:37.040
 We got one of them for AI and science, you asked me about.

1:58:37.040 --> 1:58:39.640
 Another one was supposed to be for AI safety research.

1:58:40.720 --> 1:58:43.520
 And they gave it to people studying oceans

1:58:43.520 --> 1:58:44.760
 and climate and stuff.

1:58:46.920 --> 1:58:49.320
 So I'm all for studying oceans and climates,

1:58:49.320 --> 1:58:51.120
 but we need to actually have some money

1:58:51.120 --> 1:58:53.360
 that actually goes into AI safety research also

1:58:53.360 --> 1:58:55.400
 and doesn't just get grabbed by whatever.

1:58:56.400 --> 1:58:57.960
 That's a fantastic investment.

1:58:57.960 --> 1:59:00.480
 And then at the higher level, you asked this question,

1:59:00.480 --> 1:59:02.680
 okay, what can we do?

1:59:02.680 --> 1:59:04.000
 What are the biggest risks?

1:59:05.240 --> 1:59:08.760
 I think we cannot just consider this

1:59:08.760 --> 1:59:11.000
 to be only a technical problem.

1:59:11.000 --> 1:59:13.640
 Again, because if you solve only the technical problem,

1:59:13.640 --> 1:59:14.680
 can I play with your robot?

1:59:14.680 --> 1:59:15.520
 Yes, please.

1:59:15.520 --> 1:59:20.520
 If we can get our machines to just blindly obey

1:59:20.560 --> 1:59:21.880
 the orders we give them,

1:59:22.760 --> 1:59:25.280
 so we can always trust that it will do what we want.

1:59:26.160 --> 1:59:28.440
 That might be great for the owner of the robot.

1:59:28.440 --> 1:59:31.400
 That might not be so great for the rest of humanity

1:59:31.400 --> 1:59:34.080
 if that person is that least favorite world leader

1:59:34.080 --> 1:59:35.680
 or whatever you imagine, right?

1:59:36.600 --> 1:59:39.200
 So we have to also take a look at the,

1:59:39.200 --> 1:59:41.960
 apply alignment, not just to machines,

1:59:41.960 --> 1:59:44.560
 but to all the other powerful structures.

1:59:44.560 --> 1:59:45.720
 That's why it's so important

1:59:45.720 --> 1:59:47.040
 to strengthen our democracy again,

1:59:47.040 --> 1:59:48.520
 as I said, to have institutions,

1:59:48.520 --> 1:59:51.440
 make sure that the playing field is not rigged

1:59:51.440 --> 1:59:54.800
 so that corporations are given the right incentives

1:59:54.800 --> 1:59:57.240
 to do the things that both make profit

1:59:57.240 --> 1:59:58.880
 and are good for people,

1:59:58.880 --> 2:00:00.920
 to make sure that countries have incentives

2:00:00.920 --> 2:00:03.320
 to do things that are both good for their people

2:00:03.320 --> 2:00:06.840
 and don't screw up the rest of the world.

2:00:06.840 --> 2:00:10.280
 And this is not just something for AI nerds to geek out on.

2:00:10.280 --> 2:00:13.080
 This is an interesting challenge for political scientists,

2:00:13.080 --> 2:00:16.800
 economists, and so many other thinkers.

2:00:16.800 --> 2:00:18.680
 So one of the magical things

2:00:18.680 --> 2:00:23.680
 that perhaps makes this earth quite unique

2:00:25.240 --> 2:00:28.840
 is that it's home to conscious beings.

2:00:28.840 --> 2:00:30.400
 So you mentioned consciousness.

2:00:31.640 --> 2:00:35.000
 Perhaps as a small aside,

2:00:35.000 --> 2:00:36.720
 because we didn't really get specific

2:00:36.720 --> 2:00:39.440
 to how we might do the alignment.

2:00:39.440 --> 2:00:40.280
 Like you said,

2:00:40.280 --> 2:00:41.840
 is there just a really important research problem,

2:00:41.840 --> 2:00:44.720
 but do you think engineering consciousness

2:00:44.720 --> 2:00:49.720
 into AI systems is a possibility,

2:00:49.880 --> 2:00:53.040
 is something that we might one day do,

2:00:53.040 --> 2:00:56.800
 or is there something fundamental to consciousness

2:00:56.800 --> 2:00:59.880
 that is, is there something about consciousness

2:00:59.880 --> 2:01:02.360
 that is fundamental to humans and humans only?

2:01:03.400 --> 2:01:04.640
 I think it's possible.

2:01:04.640 --> 2:01:08.320
 I think both consciousness and intelligence

2:01:08.320 --> 2:01:10.760
 are information processing.

2:01:10.760 --> 2:01:13.480
 Certain types of information processing.

2:01:13.480 --> 2:01:15.160
 And that fundamentally,

2:01:15.160 --> 2:01:17.320
 it doesn't matter whether the information is processed

2:01:17.320 --> 2:01:21.280
 by carbon atoms in the neurons and brains

2:01:21.280 --> 2:01:25.920
 or by silicon atoms and so on in our technology.

2:01:27.280 --> 2:01:28.280
 Some people disagree.

2:01:28.280 --> 2:01:30.240
 This is what I think as a physicist.

2:01:32.960 --> 2:01:34.960
 That consciousness is the same kind of,

2:01:34.960 --> 2:01:37.720
 you said consciousness is information processing.

2:01:37.720 --> 2:01:42.720
 So meaning, I think you had a quote of something like

2:01:43.000 --> 2:01:47.760
 it's information knowing itself, that kind of thing.

2:01:47.760 --> 2:01:49.280
 I think consciousness is, yeah,

2:01:49.280 --> 2:01:51.960
 is the way information feels when it's being processed.

2:01:51.960 --> 2:01:53.520
 One's being put in complex ways.

2:01:53.520 --> 2:01:56.120
 We don't know exactly what those complex ways are.

2:01:56.120 --> 2:01:59.240
 It's clear that most of the information processing

2:01:59.240 --> 2:02:01.720
 in our brains does not create an experience.

2:02:01.720 --> 2:02:03.600
 We're not even aware of it, right?

2:02:03.600 --> 2:02:05.520
 Like for example,

2:02:05.520 --> 2:02:07.880
 you're not aware of your heartbeat regulation right now,

2:02:07.880 --> 2:02:10.600
 even though it's clearly being done by your body, right?

2:02:10.600 --> 2:02:12.120
 It's just kind of doing its own thing.

2:02:12.120 --> 2:02:13.680
 When you go jogging,

2:02:13.680 --> 2:02:15.280
 there's a lot of complicated stuff

2:02:15.280 --> 2:02:18.720
 about how you put your foot down and we know it's hard.

2:02:18.720 --> 2:02:20.560
 That's why robots used to fall over so much,

2:02:20.560 --> 2:02:22.720
 but you're mostly unaware about it.

2:02:22.720 --> 2:02:25.760
 Your brain, your CEO consciousness module

2:02:25.760 --> 2:02:26.600
 just sends an email,

2:02:26.600 --> 2:02:29.160
 hey, I'm gonna keep jogging along this path.

2:02:29.160 --> 2:02:31.560
 The rest is on autopilot, right?

2:02:31.560 --> 2:02:33.200
 So most of it is not conscious,

2:02:33.200 --> 2:02:36.640
 but somehow there is some of the information processing,

2:02:36.640 --> 2:02:41.640
 which is we don't know what exactly.

2:02:41.680 --> 2:02:44.120
 I think this is a science problem

2:02:44.120 --> 2:02:47.680
 that I hope one day we'll have some equation for

2:02:47.680 --> 2:02:49.080
 or something so we can be able to build

2:02:49.080 --> 2:02:51.040
 a consciousness detector and say, yeah,

2:02:51.040 --> 2:02:53.920
 here there is some consciousness, here there's not.

2:02:53.920 --> 2:02:56.640
 Oh, don't boil that lobster because it's feeling pain

2:02:56.640 --> 2:02:59.880
 or it's okay because it's not feeling pain.

2:02:59.880 --> 2:03:03.440
 Right now we treat this as sort of just metaphysics,

2:03:03.440 --> 2:03:06.920
 but it would be very useful in emergency rooms

2:03:06.920 --> 2:03:09.760
 to know if a patient has locked in syndrome

2:03:09.760 --> 2:03:14.560
 and is conscious or if they are actually just out.

2:03:14.560 --> 2:03:17.720
 And in the future, if you build a very, very intelligent

2:03:17.720 --> 2:03:20.120
 helper robot to take care of you,

2:03:20.120 --> 2:03:21.480
 I think you'd like to know

2:03:21.480 --> 2:03:24.120
 if you should feel guilty about shutting it down

2:03:24.120 --> 2:03:27.080
 or if it's just like a zombie going through the motions

2:03:27.080 --> 2:03:29.720
 like a fancy tape recorder, right?

2:03:29.720 --> 2:03:32.800
 And once we can make progress

2:03:32.800 --> 2:03:34.040
 on the science of consciousness

2:03:34.040 --> 2:03:38.320
 and figure out what is conscious and what isn't,

2:03:38.320 --> 2:03:43.320
 then assuming we want to create positive experiences

2:03:45.960 --> 2:03:48.880
 and not suffering, we'll probably choose to build

2:03:48.880 --> 2:03:51.760
 some machines that are deliberately unconscious

2:03:51.760 --> 2:03:56.760
 that do incredibly boring, repetitive jobs

2:03:56.760 --> 2:03:59.680
 in an iron mine somewhere or whatever.

2:03:59.680 --> 2:04:03.120
 And maybe we'll choose to create helper robots

2:04:03.120 --> 2:04:05.360
 for the elderly that are conscious

2:04:05.360 --> 2:04:07.080
 so that people don't just feel creeped out

2:04:07.080 --> 2:04:09.280
 that the robot is just faking it

2:04:10.160 --> 2:04:12.160
 when it acts like it's sad or happy.

2:04:12.160 --> 2:04:13.440
 Like you said, elderly,

2:04:13.440 --> 2:04:16.920
 I think everybody gets pretty deeply lonely in this world.

2:04:16.920 --> 2:04:19.640
 And so there's a place I think for everybody

2:04:19.640 --> 2:04:21.640
 to have a connection with conscious beings,

2:04:21.640 --> 2:04:24.400
 whether they're human or otherwise.

2:04:24.400 --> 2:04:26.920
 But I know for sure that I would,

2:04:26.920 --> 2:04:29.960
 if I had a robot, if I was gonna develop any kind

2:04:29.960 --> 2:04:32.760
 of personal emotional connection with it,

2:04:32.760 --> 2:04:33.840
 I would be very creeped out

2:04:33.840 --> 2:04:35.280
 if I knew it in an intellectual level

2:04:35.280 --> 2:04:36.840
 that the whole thing was just a fraud.

2:04:36.840 --> 2:04:41.840
 Now today you can buy a little talking doll for a kid

2:04:43.000 --> 2:04:46.120
 which will say things and the little child will often think

2:04:46.120 --> 2:04:47.840
 that this is actually conscious

2:04:47.840 --> 2:04:50.440
 and even real secrets to it that then go on the internet

2:04:50.440 --> 2:04:52.520
 and with lots of the creepy repercussions.

2:04:52.520 --> 2:04:57.520
 I would not wanna be just hacked and tricked like this.

2:04:58.040 --> 2:05:01.560
 If I was gonna be developing real emotional connections

2:05:01.560 --> 2:05:04.200
 with the robot, I would wanna know

2:05:04.200 --> 2:05:05.440
 that this is actually real.

2:05:05.440 --> 2:05:08.080
 It's acting conscious, acting happy

2:05:08.080 --> 2:05:09.880
 because it actually feels it.

2:05:09.880 --> 2:05:11.400
 And I think this is not sci fi.

2:05:11.400 --> 2:05:15.560
 I think it's possible to measure, to come up with tools.

2:05:15.560 --> 2:05:17.560
 After we understand the science of consciousness,

2:05:17.560 --> 2:05:19.760
 you're saying we'll be able to come up with tools

2:05:19.760 --> 2:05:21.400
 that can measure consciousness

2:05:21.400 --> 2:05:25.120
 and definitively say like this thing is experiencing

2:05:25.120 --> 2:05:27.360
 the things it says it's experiencing.

2:05:27.360 --> 2:05:28.320
 Kind of by definition.

2:05:28.320 --> 2:05:31.560
 If it is a physical phenomenon, information processing

2:05:31.560 --> 2:05:34.040
 and we know that some information processing is conscious

2:05:34.040 --> 2:05:36.000
 and some isn't, well, then there is something there

2:05:36.000 --> 2:05:38.040
 to be discovered with the methods of science.

2:05:38.040 --> 2:05:41.120
 Giulio Tononi has stuck his neck out the farthest

2:05:41.120 --> 2:05:43.640
 and written down some equations for a theory.

2:05:43.640 --> 2:05:45.680
 Maybe that's right, maybe it's wrong.

2:05:45.680 --> 2:05:46.920
 We certainly don't know.

2:05:46.920 --> 2:05:50.760
 But I applaud that kind of efforts to sort of take this,

2:05:50.760 --> 2:05:53.960
 say this is not just something that philosophers

2:05:53.960 --> 2:05:56.320
 can have beer and muse about,

2:05:56.320 --> 2:05:58.720
 but something we can measure and study.

2:05:58.720 --> 2:06:00.560
 And coming, bringing that back to us,

2:06:00.560 --> 2:06:03.000
 I think what we would probably choose to do, as I said,

2:06:03.000 --> 2:06:04.600
 is if we cannot figure this out,

2:06:05.680 --> 2:06:09.000
 choose to make, to be quite mindful

2:06:09.000 --> 2:06:11.280
 about what sort of consciousness, if any,

2:06:11.280 --> 2:06:13.720
 we put in different machines that we have.

2:06:16.080 --> 2:06:19.000
 And certainly, we wouldn't wanna make,

2:06:19.000 --> 2:06:21.760
 we should not be making much machines that suffer

2:06:21.760 --> 2:06:23.640
 without us even knowing it, right?

2:06:23.640 --> 2:06:28.320
 And if at any point someone decides to upload themselves

2:06:28.320 --> 2:06:30.120
 like Ray Kurzweil wants to do,

2:06:30.120 --> 2:06:31.440
 I don't know if you've had him on your show.

2:06:31.440 --> 2:06:33.040
 We agree, but then COVID happens,

2:06:33.040 --> 2:06:34.680
 so we're waiting it out a little bit.

2:06:34.680 --> 2:06:38.520
 Suppose he uploads himself into this robo Ray

2:06:38.520 --> 2:06:42.200
 and it talks like him and acts like him and laughs like him.

2:06:42.200 --> 2:06:44.840
 And before he powers off his biological body,

2:06:46.480 --> 2:06:47.760
 he would probably be pretty disturbed

2:06:47.760 --> 2:06:49.600
 if he realized that there's no one home.

2:06:49.600 --> 2:06:52.760
 This robot is not having any subjective experience, right?

2:06:53.760 --> 2:06:58.760
 If humanity gets replaced by machine descendants,

2:06:59.840 --> 2:07:02.320
 which do all these cool things and build spaceships

2:07:02.320 --> 2:07:05.640
 and go to intergalactic rock concerts,

2:07:05.640 --> 2:07:10.000
 and it turns out that they are all unconscious,

2:07:10.000 --> 2:07:11.440
 just going through the motions,

2:07:11.440 --> 2:07:16.160
 wouldn't that be like the ultimate zombie apocalypse, right?

2:07:16.160 --> 2:07:18.040
 Just a play for empty benches?

2:07:18.040 --> 2:07:21.200
 Yeah, I have a sense that there's some kind of,

2:07:21.200 --> 2:07:22.800
 once we understand consciousness better,

2:07:22.800 --> 2:07:25.640
 we'll understand that there's some kind of continuum

2:07:25.640 --> 2:07:28.000
 and it would be a greater appreciation.

2:07:28.000 --> 2:07:30.440
 And we'll probably understand, just like you said,

2:07:30.440 --> 2:07:32.400
 it'd be unfortunate if it's a trick.

2:07:32.400 --> 2:07:33.920
 We'll probably definitely understand

2:07:33.920 --> 2:07:37.760
 that love is indeed a trick that we'll play on each other,

2:07:37.760 --> 2:07:40.960
 that we humans are, we convince ourselves we're conscious,

2:07:40.960 --> 2:07:45.240
 but we're really, us and trees and dolphins

2:07:45.240 --> 2:07:46.600
 are all the same kind of consciousness.

2:07:46.600 --> 2:07:48.160
 Can I try to cheer you up a little bit

2:07:48.160 --> 2:07:50.280
 with a philosophical thought here about the love part?

2:07:50.280 --> 2:07:51.360
 Yes, let's do it.

2:07:51.360 --> 2:07:53.920
 You know, you might say,

2:07:53.920 --> 2:07:56.960
 okay, yeah, love is just a collaboration enabler.

2:07:58.120 --> 2:08:01.800
 And then maybe you can go and get depressed about that.

2:08:01.800 --> 2:08:04.640
 But I think that would be the wrong conclusion, actually.

2:08:04.640 --> 2:08:08.640
 You know, I know that the only reason I enjoy food

2:08:08.640 --> 2:08:11.000
 is because my genes hacked me

2:08:11.000 --> 2:08:13.720
 and they don't want me to starve to death.

2:08:13.720 --> 2:08:17.280
 Not because they care about me consciously

2:08:17.280 --> 2:08:21.080
 enjoying succulent delights of pistachio ice cream,

2:08:21.080 --> 2:08:23.360
 but they just want me to make copies of them.

2:08:23.360 --> 2:08:24.520
 The whole thing, so in a sense,

2:08:24.520 --> 2:08:28.960
 the whole enjoyment of food is also a scam like this.

2:08:28.960 --> 2:08:31.280
 But does that mean I shouldn't take pleasure

2:08:31.280 --> 2:08:32.560
 in this pistachio ice cream?

2:08:32.560 --> 2:08:34.040
 I love pistachio ice cream.

2:08:34.040 --> 2:08:38.200
 And I can tell you, I know this is an experimental fact.

2:08:38.200 --> 2:08:41.600
 I enjoy pistachio ice cream every bit as much,

2:08:41.600 --> 2:08:45.560
 even though I scientifically know exactly why,

2:08:45.560 --> 2:08:46.880
 what kind of scam this was.

2:08:46.880 --> 2:08:48.640
 Your genes really appreciate

2:08:48.640 --> 2:08:50.440
 that you like the pistachio ice cream.

2:08:50.440 --> 2:08:53.080
 Well, but I, my mind appreciates it too, you know?

2:08:53.080 --> 2:08:55.800
 And I have a conscious experience right now.

2:08:55.800 --> 2:08:58.640
 Ultimately, all of my brain is also just something

2:08:58.640 --> 2:09:00.440
 the genes built to copy themselves.

2:09:00.440 --> 2:09:01.600
 But so what?

2:09:01.600 --> 2:09:03.200
 You know, I'm grateful that,

2:09:03.200 --> 2:09:04.960
 yeah, thanks genes for doing this,

2:09:04.960 --> 2:09:07.600
 but you know, now it's my brain that's in charge here

2:09:07.600 --> 2:09:09.520
 and I'm gonna enjoy my conscious experience,

2:09:09.520 --> 2:09:10.360
 thank you very much.

2:09:10.360 --> 2:09:12.480
 And not just the pistachio ice cream,

2:09:12.480 --> 2:09:15.440
 but also the love I feel for my amazing wife

2:09:15.440 --> 2:09:19.280
 and all the other delights of being conscious.

2:09:19.280 --> 2:09:22.240
 I don't, actually Richard Feynman,

2:09:22.240 --> 2:09:25.080
 I think said this so well.

2:09:25.080 --> 2:09:28.000
 He is also the guy, you know, really got me into physics.

2:09:29.680 --> 2:09:31.240
 Some art friend said that,

2:09:31.240 --> 2:09:34.520
 oh, science kind of just is the party pooper.

2:09:34.520 --> 2:09:36.240
 It's kind of ruins the fun, right?

2:09:36.240 --> 2:09:39.680
 When like you have a beautiful flowers as the artist

2:09:39.680 --> 2:09:41.600
 and then the scientist is gonna deconstruct that

2:09:41.600 --> 2:09:44.160
 into just a blob of quarks and electrons.

2:09:44.160 --> 2:09:47.480
 And Feynman pushed back on that in such a beautiful way,

2:09:47.480 --> 2:09:49.920
 which I think also can be used to push back

2:09:49.920 --> 2:09:53.440
 and make you not feel guilty about falling in love.

2:09:53.440 --> 2:09:55.000
 So here's what Feynman basically said.

2:09:55.000 --> 2:09:56.920
 He said to his friend, you know,

2:09:56.920 --> 2:09:59.080
 yeah, I can also as a scientist see

2:09:59.080 --> 2:10:00.960
 that this is a beautiful flower, thank you very much.

2:10:00.960 --> 2:10:03.280
 Maybe I can't draw as good a painting as you

2:10:03.280 --> 2:10:04.560
 because I'm not as talented an artist,

2:10:04.560 --> 2:10:06.800
 but yeah, I can really see the beauty in it.

2:10:06.800 --> 2:10:09.360
 And it just, it also looks beautiful to me.

2:10:09.360 --> 2:10:12.200
 But in addition to that, Feynman said, as a scientist,

2:10:12.200 --> 2:10:16.960
 I see even more beauty that the artist did not see, right?

2:10:16.960 --> 2:10:21.120
 Suppose this is a flower on a blossoming apple tree.

2:10:21.120 --> 2:10:23.840
 You could say this tree has more beauty in it

2:10:23.840 --> 2:10:26.400
 than just the colors and the fragrance.

2:10:26.400 --> 2:10:29.040
 This tree is made of air, Feynman wrote.

2:10:29.040 --> 2:10:31.240
 This is one of my favorite Feynman quotes ever.

2:10:31.240 --> 2:10:33.760
 And it took the carbon out of the air

2:10:33.760 --> 2:10:36.160
 and bound it in using the flaming heat of the sun,

2:10:36.160 --> 2:10:38.600
 you know, to turn the air into a tree.

2:10:38.600 --> 2:10:42.760
 And when you burn logs in your fireplace,

2:10:42.760 --> 2:10:45.120
 it's really beautiful to think that this is being reversed.

2:10:45.120 --> 2:10:48.600
 Now the tree is going, the wood is going back into air.

2:10:48.600 --> 2:10:52.520
 And in this flaming, beautiful dance of the fire

2:10:52.520 --> 2:10:56.000
 that the artist can see is the flaming light of the sun

2:10:56.000 --> 2:10:59.120
 that was bound in to turn the air into tree.

2:10:59.120 --> 2:11:01.480
 And then the ashes is the little residue

2:11:01.480 --> 2:11:02.560
 that didn't come from the air

2:11:02.560 --> 2:11:04.280
 that the tree sucked out of the ground, you know.

2:11:04.280 --> 2:11:06.160
 Feynman said, these are beautiful things.

2:11:06.160 --> 2:11:10.040
 And science just adds, it doesn't subtract.

2:11:10.040 --> 2:11:12.760
 And I feel exactly that way about love

2:11:12.760 --> 2:11:14.800
 and about pistachio ice cream also.

2:11:16.000 --> 2:11:18.680
 I can understand that there is even more nuance

2:11:18.680 --> 2:11:20.480
 to the whole thing, right?

2:11:20.480 --> 2:11:22.480
 At this very visceral level,

2:11:22.480 --> 2:11:24.560
 you can fall in love just as much as someone

2:11:24.560 --> 2:11:26.400
 who knows nothing about neuroscience.

2:11:27.680 --> 2:11:31.840
 But you can also appreciate this even greater beauty in it.

2:11:31.840 --> 2:11:35.600
 Just like, isn't it remarkable that it came about

2:11:35.600 --> 2:11:38.560
 from this completely lifeless universe,

2:11:38.560 --> 2:11:43.080
 just a bunch of hot blob of plasma expanding.

2:11:43.080 --> 2:11:46.160
 And then over the eons, you know, gradually,

2:11:46.160 --> 2:11:48.440
 first the strong nuclear force decided

2:11:48.440 --> 2:11:50.920
 to combine quarks together into nuclei.

2:11:50.920 --> 2:11:53.040
 And then the electric force bound in electrons

2:11:53.040 --> 2:11:53.880
 and made atoms.

2:11:53.880 --> 2:11:55.240
 And then they clustered from gravity

2:11:55.240 --> 2:11:57.720
 and you got planets and stars and this and that.

2:11:57.720 --> 2:12:00.040
 And then natural selection came along

2:12:00.040 --> 2:12:01.800
 and the genes had their little thing.

2:12:01.800 --> 2:12:04.640
 And you started getting what went from seeming

2:12:04.640 --> 2:12:06.240
 like a completely pointless universe

2:12:06.240 --> 2:12:08.040
 that we're just trying to increase entropy

2:12:08.040 --> 2:12:10.160
 and approach heat death into something

2:12:10.160 --> 2:12:11.720
 that looked more goal oriented.

2:12:11.720 --> 2:12:13.280
 Isn't that kind of beautiful?

2:12:13.280 --> 2:12:15.760
 And then this goal orientedness through evolution

2:12:15.760 --> 2:12:18.720
 got ever more sophisticated where you got ever more.

2:12:18.720 --> 2:12:20.120
 And then you started getting this thing,

2:12:20.120 --> 2:12:25.120
 which is kind of like DeepMind's mu zero and steroids,

2:12:25.280 --> 2:12:29.400
 the ultimate self play is not what DeepMind's AI

2:12:29.400 --> 2:12:32.080
 does against itself to get better at go.

2:12:32.080 --> 2:12:34.440
 It's what all these little quark blobs did

2:12:34.440 --> 2:12:38.920
 against each other in the game of survival of the fittest.

2:12:38.920 --> 2:12:42.000
 Now, when you had really dumb bacteria

2:12:42.000 --> 2:12:44.040
 living in a simple environment,

2:12:44.040 --> 2:12:46.440
 there wasn't much incentive to get intelligent,

2:12:46.440 --> 2:12:50.880
 but then the life made environment more complex.

2:12:50.880 --> 2:12:53.520
 And then there was more incentive to get even smarter.

2:12:53.520 --> 2:12:56.600
 And that gave the other organisms more of incentive

2:12:56.600 --> 2:12:57.520
 to also get smarter.

2:12:57.520 --> 2:12:59.880
 And then here we are now,

2:12:59.880 --> 2:13:04.880
 just like mu zero learned to become world master at go

2:13:05.040 --> 2:13:07.200
 and chess from playing against itself

2:13:07.200 --> 2:13:08.560
 by just playing against itself.

2:13:08.560 --> 2:13:10.680
 All the quirks here on our planet,

2:13:10.680 --> 2:13:15.000
 the electrons have created giraffes and elephants

2:13:15.000 --> 2:13:17.640
 and humans and love.

2:13:17.640 --> 2:13:20.280
 I just find that really beautiful.

2:13:20.280 --> 2:13:24.200
 And to me, that just adds to the enjoyment of love.

2:13:24.200 --> 2:13:25.640
 It doesn't subtract anything.

2:13:25.640 --> 2:13:27.320
 Do you feel a little more careful now?

2:13:27.320 --> 2:13:30.640
 I feel way better, that was incredible.

2:13:30.640 --> 2:13:33.920
 So this self play of quirks,

2:13:33.920 --> 2:13:36.320
 taking back to the beginning of our conversation

2:13:36.320 --> 2:13:39.520
 a little bit, there's so many exciting possibilities

2:13:39.520 --> 2:13:42.040
 about artificial intelligence understanding

2:13:42.040 --> 2:13:44.240
 the basic laws of physics.

2:13:44.240 --> 2:13:47.400
 Do you think AI will help us unlock?

2:13:47.400 --> 2:13:49.240
 There's been quite a bit of excitement

2:13:49.240 --> 2:13:50.440
 throughout the history of physics

2:13:50.440 --> 2:13:55.440
 of coming up with more and more general simple laws

2:13:55.440 --> 2:13:58.400
 that explain the nature of our reality.

2:13:58.400 --> 2:14:01.120
 And then the ultimate of that would be a theory

2:14:01.120 --> 2:14:03.680
 of everything that combines everything together.

2:14:03.680 --> 2:14:07.440
 Do you think it's possible that one, we humans,

2:14:07.440 --> 2:14:12.440
 but perhaps AI systems will figure out a theory of physics

2:14:13.640 --> 2:14:16.200
 that unifies all the laws of physics?

2:14:17.120 --> 2:14:19.920
 Yeah, I think it's absolutely possible.

2:14:19.920 --> 2:14:21.360
 I think it's very clear

2:14:21.360 --> 2:14:24.960
 that we're gonna see a great boost to science.

2:14:24.960 --> 2:14:26.720
 We're already seeing a boost actually

2:14:26.720 --> 2:14:28.760
 from machine learning helping science.

2:14:28.760 --> 2:14:30.280
 Alpha fold was an example,

2:14:30.280 --> 2:14:33.360
 the decades old protein folding problem.

2:14:34.440 --> 2:14:38.160
 So, and gradually, yeah, unless we go extinct

2:14:38.160 --> 2:14:39.720
 by doing something dumb like we discussed,

2:14:39.720 --> 2:14:44.040
 I think it's very likely

2:14:44.040 --> 2:14:48.040
 that our understanding of physics will become so good

2:14:48.040 --> 2:14:53.040
 that our technology will no longer be limited

2:14:53.040 --> 2:14:55.200
 by human intelligence,

2:14:55.200 --> 2:14:57.400
 but instead be limited by the laws of physics.

2:14:58.240 --> 2:15:00.120
 So our tech today is limited

2:15:00.120 --> 2:15:02.120
 by what we've been able to invent, right?

2:15:02.120 --> 2:15:04.920
 I think as AI progresses,

2:15:04.920 --> 2:15:07.200
 it'll just be limited by the speed of light

2:15:07.200 --> 2:15:09.240
 and other physical limits,

2:15:09.240 --> 2:15:13.960
 which would mean it's gonna be just dramatically beyond

2:15:13.960 --> 2:15:15.280
 where we are now.

2:15:15.280 --> 2:15:18.560
 Do you think it's a fundamentally mathematical pursuit

2:15:18.560 --> 2:15:22.120
 of trying to understand like the laws

2:15:22.120 --> 2:15:25.760
 of our universe from a mathematical perspective?

2:15:25.760 --> 2:15:28.000
 So almost like if it's AI,

2:15:28.000 --> 2:15:31.640
 it's exploring the space of like theorems

2:15:31.640 --> 2:15:33.480
 and those kinds of things,

2:15:33.480 --> 2:15:38.480
 or is there some other more computational ideas,

2:15:39.760 --> 2:15:41.280
 more sort of empirical ideas?

2:15:41.280 --> 2:15:43.120
 They're both, I would say.

2:15:43.120 --> 2:15:45.920
 It's really interesting to look out at the landscape

2:15:45.920 --> 2:15:48.000
 of everything we call science today.

2:15:48.000 --> 2:15:50.200
 So here you come now with this big new hammer.

2:15:50.200 --> 2:15:51.480
 It says machine learning on it

2:15:51.480 --> 2:15:53.360
 and that's, you know, where are there some nails

2:15:53.360 --> 2:15:56.600
 that you can help with here that you can hammer?

2:15:56.600 --> 2:16:00.120
 Ultimately, if machine learning gets the point

2:16:00.120 --> 2:16:02.800
 that it can do everything better than us,

2:16:02.800 --> 2:16:06.000
 it will be able to help across the whole space of science.

2:16:06.000 --> 2:16:08.120
 But maybe we can anchor it by starting a little bit

2:16:08.120 --> 2:16:11.640
 right now near term and see how we kind of move forward.

2:16:11.640 --> 2:16:14.840
 So like right now, first of all,

2:16:14.840 --> 2:16:17.360
 you have a lot of big data science, right?

2:16:17.360 --> 2:16:19.360
 Where, for example, with telescopes,

2:16:19.360 --> 2:16:24.120
 we are able to collect way more data every hour

2:16:24.120 --> 2:16:26.720
 than a grad student can just pour over

2:16:26.720 --> 2:16:28.760
 like in the old times, right?

2:16:28.760 --> 2:16:31.040
 And machine learning is already being used very effectively,

2:16:31.040 --> 2:16:34.680
 even at MIT, to find planets around other stars,

2:16:34.680 --> 2:16:36.560
 to detect exciting new signatures

2:16:36.560 --> 2:16:38.760
 of new particle physics in the sky,

2:16:38.760 --> 2:16:42.960
 to detect the ripples in the fabric of space time

2:16:42.960 --> 2:16:44.640
 that we call gravitational waves

2:16:44.640 --> 2:16:46.520
 caused by enormous black holes

2:16:46.520 --> 2:16:48.120
 crashing into each other halfway

2:16:48.120 --> 2:16:49.920
 across the observable universe.

2:16:49.920 --> 2:16:52.680
 Machine learning is running and ticking right now,

2:16:52.680 --> 2:16:53.800
 doing all these things,

2:16:53.800 --> 2:16:57.560
 and it's really helping all these experimental fields.

2:16:58.440 --> 2:17:01.880
 There is a separate front of physics,

2:17:01.880 --> 2:17:03.240
 computational physics,

2:17:03.240 --> 2:17:05.680
 which is getting an enormous boost also.

2:17:05.680 --> 2:17:09.520
 So we had to do all our computations by hand, right?

2:17:09.520 --> 2:17:11.240
 People would have these giant books

2:17:11.240 --> 2:17:12.800
 with tables of logarithms,

2:17:12.800 --> 2:17:16.720
 and oh my God, it pains me to even think

2:17:16.720 --> 2:17:19.880
 how long time it would have taken to do simple stuff.

2:17:19.880 --> 2:17:23.560
 Then we started to get little calculators and computers

2:17:23.560 --> 2:17:26.520
 that could do some basic math for us.

2:17:26.520 --> 2:17:28.840
 Now, what we're starting to see is

2:17:31.160 --> 2:17:35.600
 kind of a shift from GOFI, computational physics,

2:17:35.600 --> 2:17:40.000
 to neural network, computational physics.

2:17:40.000 --> 2:17:44.520
 What I mean by that is most computational physics

2:17:44.520 --> 2:17:48.480
 would be done by humans programming in

2:17:48.480 --> 2:17:50.200
 the intelligence of how to do the computation

2:17:50.200 --> 2:17:51.160
 into the computer.

2:17:52.440 --> 2:17:55.400
 Just as when Garry Kasparov got his posterior kicked

2:17:55.400 --> 2:17:56.920
 by IBM's Deep Blue in chess,

2:17:56.920 --> 2:17:59.880
 humans had programmed in exactly how to play chess.

2:17:59.880 --> 2:18:01.160
 Intelligence came from the humans.

2:18:01.160 --> 2:18:02.400
 It wasn't learned, right?

2:18:03.840 --> 2:18:08.480
 Mu zero can be not only Kasparov in chess,

2:18:08.480 --> 2:18:09.880
 but also Stockfish,

2:18:09.880 --> 2:18:12.560
 which is the best sort of GOFI chess program.

2:18:12.560 --> 2:18:16.560
 By learning, and we're seeing more of that now,

2:18:16.560 --> 2:18:18.320
 that shift beginning to happen in physics.

2:18:18.320 --> 2:18:20.520
 So let me give you an example.

2:18:20.520 --> 2:18:24.120
 So lattice QCD is an area of physics

2:18:24.120 --> 2:18:27.320
 whose goal is basically to take the periodic table

2:18:27.320 --> 2:18:30.120
 and just compute the whole thing from first principles.

2:18:31.120 --> 2:18:33.920
 This is not the search for theory of everything.

2:18:33.920 --> 2:18:36.360
 We already know the theory

2:18:36.360 --> 2:18:39.720
 that's supposed to produce as output the periodic table,

2:18:39.720 --> 2:18:42.720
 which atoms are stable, how heavy they are,

2:18:42.720 --> 2:18:44.840
 all that good stuff, their spectral lines.

2:18:45.840 --> 2:18:48.120
 It's a theory, lattice QCD,

2:18:48.120 --> 2:18:50.000
 you can put it on your tshirt.

2:18:50.000 --> 2:18:51.160
 Our colleague Frank Wilczek

2:18:51.160 --> 2:18:53.120
 got the Nobel Prize for working on it.

2:18:54.520 --> 2:18:56.600
 But the math is just too hard for us to solve.

2:18:56.600 --> 2:18:58.640
 We have not been able to start with these equations

2:18:58.640 --> 2:19:01.440
 and solve them to the extent that we can predict, oh yeah.

2:19:01.440 --> 2:19:03.360
 And then there is carbon,

2:19:03.360 --> 2:19:07.000
 and this is what the spectrum of the carbon atom looks like.

2:19:07.000 --> 2:19:09.960
 But awesome people are building

2:19:09.960 --> 2:19:12.040
 these supercomputer simulations

2:19:12.040 --> 2:19:14.960
 where you just put in these equations

2:19:14.960 --> 2:19:19.960
 and you make a big cubic lattice of space,

2:19:20.680 --> 2:19:22.080
 or actually it's a very small lattice

2:19:22.080 --> 2:19:25.640
 because you're going down to the subatomic scale,

2:19:25.640 --> 2:19:26.880
 and you try to solve it.

2:19:26.880 --> 2:19:28.960
 But it's just so computationally expensive

2:19:28.960 --> 2:19:31.840
 that we still haven't been able to calculate things

2:19:31.840 --> 2:19:34.960
 as accurately as we measure them in many cases.

2:19:34.960 --> 2:19:37.520
 And now machine learning is really revolutionizing this.

2:19:37.520 --> 2:19:40.040
 So my colleague Fiala Shanahan at MIT, for example,

2:19:40.040 --> 2:19:43.280
 she's been using this really cool

2:19:43.280 --> 2:19:47.560
 machine learning technique called normalizing flows,

2:19:47.560 --> 2:19:49.800
 where she's realized she can actually speed up

2:19:49.800 --> 2:19:52.160
 the calculation dramatically

2:19:52.160 --> 2:19:54.620
 by having the AI learn how to do things faster.

2:19:55.680 --> 2:19:57.280
 Another area like this

2:19:57.280 --> 2:20:02.280
 where we suck up an enormous amount of supercomputer time

2:20:02.280 --> 2:20:05.480
 to do physics is black hole collisions.

2:20:05.480 --> 2:20:06.880
 So now that we've done the sexy stuff

2:20:06.880 --> 2:20:09.960
 of detecting a bunch of this with LIGO and other experiments,

2:20:09.960 --> 2:20:13.360
 we want to be able to know what we're seeing.

2:20:13.360 --> 2:20:16.480
 And so it's a very simple conceptual problem.

2:20:16.480 --> 2:20:18.000
 It's the two body problem.

2:20:19.000 --> 2:20:23.000
 Newton solved it for classical gravity hundreds of years ago,

2:20:23.000 --> 2:20:26.080
 but the two body problem is still not fully solved.

2:20:26.080 --> 2:20:26.920
 For black holes.

2:20:26.920 --> 2:20:29.000
 Black holes, yes, and Einstein's gravity

2:20:29.000 --> 2:20:31.120
 because they won't just orbit in space,

2:20:31.120 --> 2:20:33.560
 they won't just orbit each other forever anymore,

2:20:33.560 --> 2:20:36.080
 two things, they give off gravitational waves

2:20:36.080 --> 2:20:37.800
 and make sure they crash into each other.

2:20:37.800 --> 2:20:40.320
 And the game, what you want to do is you want to figure out,

2:20:40.320 --> 2:20:43.480
 okay, what kind of wave comes out

2:20:43.480 --> 2:20:46.320
 as a function of the masses of the two black holes,

2:20:46.320 --> 2:20:48.120
 as a function of how they're spinning,

2:20:48.120 --> 2:20:50.720
 relative to each other, et cetera.

2:20:50.720 --> 2:20:52.040
 And that is so hard.

2:20:52.040 --> 2:20:54.200
 It can take months of supercomputer time

2:20:54.200 --> 2:20:56.200
 and massive numbers of cores to do it.

2:20:56.200 --> 2:21:00.000
 Now, wouldn't it be great if you can use machine learning

2:21:01.240 --> 2:21:03.280
 to greatly speed that up, right?

2:21:04.760 --> 2:21:09.360
 Now you can use the expensive old GoFi calculation

2:21:09.360 --> 2:21:11.920
 as the truth, and then see if machine learning

2:21:11.920 --> 2:21:13.600
 can figure out a smarter, faster way

2:21:13.600 --> 2:21:15.000
 of getting the right answer.

2:21:16.320 --> 2:21:20.000
 Yet another area, like computational physics.

2:21:20.000 --> 2:21:22.280
 These are probably the big three

2:21:22.280 --> 2:21:24.240
 that suck up the most computer time.

2:21:24.240 --> 2:21:27.160
 Lattice QCD, black hole collisions,

2:21:27.160 --> 2:21:29.560
 and cosmological simulations,

2:21:29.560 --> 2:21:32.280
 where you take not a subatomic thing

2:21:32.280 --> 2:21:34.400
 and try to figure out the mass of the proton,

2:21:34.400 --> 2:21:37.680
 but you take something enormous

2:21:37.680 --> 2:21:41.320
 and try to look at how all the galaxies get formed in there.

2:21:41.320 --> 2:21:44.720
 There again, there are a lot of very cool ideas right now

2:21:44.720 --> 2:21:46.080
 about how you can use machine learning

2:21:46.080 --> 2:21:48.000
 to do this sort of stuff better.

2:21:49.760 --> 2:21:51.560
 The difference between this and the big data

2:21:51.560 --> 2:21:54.560
 is you kind of make the data yourself, right?

2:21:54.560 --> 2:21:58.440
 So, and then finally,

2:21:58.440 --> 2:22:00.200
 we're looking over the physics landscape

2:22:00.200 --> 2:22:02.120
 and seeing what can we hammer with machine learning, right?

2:22:02.120 --> 2:22:05.520
 So we talked about experimental data, big data,

2:22:05.520 --> 2:22:07.880
 discovering cool stuff that we humans

2:22:07.880 --> 2:22:09.520
 then look more closely at.

2:22:09.520 --> 2:22:13.440
 Then we talked about taking the expensive computations

2:22:13.440 --> 2:22:15.040
 we're doing now and figuring out

2:22:15.040 --> 2:22:18.560
 how to do them much faster and better with AI.

2:22:18.560 --> 2:22:21.920
 And finally, let's go really theoretical.

2:22:21.920 --> 2:22:24.000
 So things like discovering equations,

2:22:25.000 --> 2:22:27.560
 having deep fundamental insights,

2:22:30.240 --> 2:22:33.040
 this is something closest to what I've been doing

2:22:33.040 --> 2:22:33.880
 in my group.

2:22:33.880 --> 2:22:35.920
 We talked earlier about the whole AI Feynman project,

2:22:35.920 --> 2:22:37.920
 where if you just have some data,

2:22:37.920 --> 2:22:39.840
 how do you automatically discover equations

2:22:39.840 --> 2:22:42.160
 that seem to describe this well,

2:22:42.160 --> 2:22:44.120
 that you can then go back as a human

2:22:44.120 --> 2:22:46.640
 and then work with and test and explore.

2:22:46.640 --> 2:22:50.320
 And you asked a really good question also

2:22:50.320 --> 2:22:54.000
 about if this is sort of a search problem in some sense.

2:22:54.000 --> 2:22:56.880
 That's very deep actually what you said, because it is.

2:22:56.880 --> 2:23:00.320
 Suppose I ask you to prove some mathematical theorem.

2:23:01.680 --> 2:23:02.960
 What is a proof in math?

2:23:02.960 --> 2:23:05.360
 It's just a long string of steps, logical steps

2:23:05.360 --> 2:23:06.960
 that you can write out with symbols.

2:23:07.920 --> 2:23:10.240
 And once you find it, it's very easy to write a program

2:23:10.240 --> 2:23:12.960
 to check whether it's a valid proof or not.

2:23:14.640 --> 2:23:16.080
 So why is it so hard to prove it?

2:23:16.080 --> 2:23:19.040
 Well, because there are ridiculously many possible

2:23:19.040 --> 2:23:21.600
 candidate proofs you could write down, right?

2:23:21.600 --> 2:23:25.440
 If the proof contains 10,000 symbols,

2:23:25.440 --> 2:23:27.760
 even if there were only 10 options

2:23:27.760 --> 2:23:29.120
 for what each symbol could be,

2:23:29.120 --> 2:23:33.440
 that's 10 to the power of 1,000 possible proofs,

2:23:33.440 --> 2:23:36.080
 which is way more than there are atoms in our universe.

2:23:36.080 --> 2:23:38.400
 So you could say it's trivial to prove these things.

2:23:38.400 --> 2:23:41.200
 You just write a computer, generate all strings,

2:23:41.200 --> 2:23:43.680
 and then check, is this a valid proof?

2:23:43.680 --> 2:23:44.520
 No.

2:23:44.520 --> 2:23:45.360
 Is this a valid proof?

2:23:45.360 --> 2:23:46.400
 Is this a valid proof?

2:23:46.400 --> 2:23:47.720
 No.

2:23:47.720 --> 2:23:51.040
 And then you just keep doing this forever.

2:23:51.960 --> 2:23:53.160
 But there are a lot of,

2:23:53.160 --> 2:23:55.120
 but it is fundamentally a search problem.

2:23:55.120 --> 2:23:57.000
 You just want to search the space of all those,

2:23:57.000 --> 2:24:02.000
 all strings of symbols to find one that is the proof, right?

2:24:03.880 --> 2:24:08.800
 And there's a whole area of machine learning called search.

2:24:08.800 --> 2:24:10.600
 How do you search through some giant space

2:24:10.600 --> 2:24:12.400
 to find the needle in the haystack?

2:24:12.400 --> 2:24:14.800
 And it's easier in cases

2:24:14.800 --> 2:24:17.160
 where there's a clear measure of good,

2:24:17.160 --> 2:24:18.800
 like you're not just right or wrong,

2:24:18.800 --> 2:24:20.640
 but this is better and this is worse,

2:24:20.640 --> 2:24:21.800
 so you can maybe get some hints

2:24:21.800 --> 2:24:23.800
 as to which direction to go in.

2:24:23.800 --> 2:24:27.000
 That's why we talked about neural networks work so well.

2:24:28.400 --> 2:24:30.680
 I mean, that's such a human thing

2:24:30.680 --> 2:24:32.280
 of that moment of genius

2:24:32.280 --> 2:24:37.280
 of figuring out the intuition of good, essentially.

2:24:37.360 --> 2:24:38.680
 I mean, we thought that that was...

2:24:38.680 --> 2:24:40.120
 Or is it?

2:24:40.120 --> 2:24:41.320
 Maybe it's not, right?

2:24:41.320 --> 2:24:42.720
 We thought that about chess, right?

2:24:42.720 --> 2:24:46.880
 That the ability to see like 10, 15,

2:24:46.880 --> 2:24:50.680
 sometimes 20 steps ahead was not a calculation

2:24:50.680 --> 2:24:51.760
 that humans were performing.

2:24:51.760 --> 2:24:53.720
 It was some kind of weird intuition

2:24:53.720 --> 2:24:57.280
 about different patterns, about board positions,

2:24:57.280 --> 2:24:59.440
 about the relative positions,

2:24:59.440 --> 2:25:01.640
 somehow stitching stuff together.

2:25:01.640 --> 2:25:03.920
 And a lot of it is just like intuition,

2:25:03.920 --> 2:25:05.960
 but then you have like alpha,

2:25:05.960 --> 2:25:10.400
 I guess zero be the first one that did the self play.

2:25:10.400 --> 2:25:12.160
 It just came up with this.

2:25:12.160 --> 2:25:14.560
 It was able to learn through self play mechanism,

2:25:14.560 --> 2:25:16.040
 this kind of intuition.

2:25:16.040 --> 2:25:16.880
 Exactly.

2:25:16.880 --> 2:25:19.960
 But just like you said, it's so fascinating to think,

2:25:19.960 --> 2:25:24.640
 well, they're in the space of totally new ideas.

2:25:24.640 --> 2:25:28.960
 Can that be done in developing theorems?

2:25:28.960 --> 2:25:30.800
 We know it can be done by neural networks

2:25:30.800 --> 2:25:32.280
 because we did it with the neural networks

2:25:32.280 --> 2:25:36.280
 in the craniums of the great mathematicians of humanity.

2:25:36.280 --> 2:25:38.640
 And I'm so glad you brought up alpha zero

2:25:38.640 --> 2:25:39.960
 because that's the counter example.

2:25:39.960 --> 2:25:41.840
 It turned out we were flattering ourselves

2:25:41.840 --> 2:25:45.360
 when we said intuition is something different.

2:25:45.360 --> 2:25:46.520
 Only humans can do it.

2:25:46.520 --> 2:25:48.120
 It's not information processing.

2:25:50.880 --> 2:25:52.320
 It used to be that way.

2:25:53.720 --> 2:25:56.200
 Again, it's really instructive, I think,

2:25:56.200 --> 2:25:58.480
 to compare the chess computer Deep Blue

2:25:58.480 --> 2:26:02.040
 that beat Kasparov with alpha zero

2:26:02.040 --> 2:26:04.280
 that beat Lisa Dahl at Go.

2:26:04.280 --> 2:26:08.640
 Because for Deep Blue, there was no intuition.

2:26:08.640 --> 2:26:12.000
 There was some, humans had programmed in some intuition.

2:26:12.000 --> 2:26:13.600
 After humans had played a lot of games,

2:26:13.600 --> 2:26:16.520
 they told the computer, count the pawn as one point,

2:26:16.520 --> 2:26:19.920
 the bishop is three points, rook is five points,

2:26:19.920 --> 2:26:21.120
 and so on, you add it all up,

2:26:21.120 --> 2:26:23.400
 and then you add some extra points for past pawns

2:26:23.400 --> 2:26:27.320
 and subtract if the opponent has it and blah, blah, blah.

2:26:28.280 --> 2:26:31.560
 And then what Deep Blue did was just search.

2:26:32.520 --> 2:26:34.960
 Just very brute force and tried many, many moves ahead,

2:26:34.960 --> 2:26:37.400
 all these combinations and a prune tree search.

2:26:37.400 --> 2:26:41.120
 And it could think much faster than Kasparov, and it won.

2:26:42.680 --> 2:26:45.440
 And that, I think, inflated our egos

2:26:45.440 --> 2:26:46.560
 in a way it shouldn't have,

2:26:46.560 --> 2:26:48.760
 because people started to say, yeah, yeah,

2:26:48.760 --> 2:26:51.360
 it's just brute force search, but it has no intuition.

2:26:52.280 --> 2:26:57.280
 Alpha zero really popped our bubble there,

2:26:57.760 --> 2:27:00.880
 because what alpha zero does,

2:27:00.880 --> 2:27:03.880
 yes, it does also do some of that tree search,

2:27:03.880 --> 2:27:06.560
 but it also has this intuition module,

2:27:06.560 --> 2:27:09.560
 which in geek speak is called a value function,

2:27:09.560 --> 2:27:11.120
 where it just looks at the board

2:27:11.120 --> 2:27:13.960
 and comes up with a number for how good is that position.

2:27:14.960 --> 2:27:17.960
 The difference was no human told it

2:27:17.960 --> 2:27:21.120
 how good the position is, it just learned it.

2:27:22.480 --> 2:27:26.840
 And mu zero is the coolest or scariest of all,

2:27:26.840 --> 2:27:28.320
 depending on your mood,

2:27:28.320 --> 2:27:32.080
 because the same basic AI system

2:27:33.040 --> 2:27:35.320
 will learn what the good board position is,

2:27:35.320 --> 2:27:38.640
 regardless of whether it's chess or Go or Shogi

2:27:38.640 --> 2:27:42.920
 or Pacman or Lady Pacman or Breakout or Space Invaders

2:27:42.920 --> 2:27:45.000
 or any number, a bunch of other games.

2:27:45.000 --> 2:27:45.840
 You don't tell it anything,

2:27:45.840 --> 2:27:49.760
 and it gets this intuition after a while for what's good.

2:27:49.760 --> 2:27:52.760
 So this is very hopeful for science, I think,

2:27:52.760 --> 2:27:55.240
 because if it can get intuition

2:27:55.240 --> 2:27:57.280
 for what's a good position there,

2:27:57.280 --> 2:27:58.880
 maybe it can also get intuition

2:27:58.880 --> 2:28:00.640
 for what are some good directions to go

2:28:00.640 --> 2:28:03.040
 if you're trying to prove something.

2:28:03.040 --> 2:28:06.400
 I often, one of the most fun things in my science career

2:28:06.400 --> 2:28:08.600
 is when I've been able to prove some theorem about something

2:28:08.600 --> 2:28:12.160
 and it's very heavily intuition guided, of course.

2:28:12.160 --> 2:28:14.080
 I don't sit and try all random strings.

2:28:14.080 --> 2:28:16.280
 I have a hunch that, you know,

2:28:16.280 --> 2:28:18.840
 this reminds me a little bit of about this other proof

2:28:18.840 --> 2:28:19.920
 I've seen for this thing.

2:28:19.920 --> 2:28:22.520
 So maybe I first, what if I try this?

2:28:22.520 --> 2:28:24.720
 Nah, that didn't work out.

2:28:24.720 --> 2:28:25.840
 But this reminds me actually,

2:28:25.840 --> 2:28:28.560
 the way this failed reminds me of that.

2:28:28.560 --> 2:28:33.560
 So combining the intuition with all these brute force

2:28:33.880 --> 2:28:38.520
 capabilities, I think it's gonna be able to help physics too.

2:28:38.520 --> 2:28:42.880
 Do you think there'll be a day when an AI system

2:28:42.880 --> 2:28:46.400
 being the primary contributor, let's say 90% plus,

2:28:46.400 --> 2:28:48.200
 wins the Nobel Prize in physics?

2:28:50.400 --> 2:28:51.960
 Obviously they'll give it to the humans

2:28:51.960 --> 2:28:54.800
 because we humans don't like to give prizes to machines.

2:28:54.800 --> 2:28:57.560
 It'll give it to the humans behind the system.

2:28:57.560 --> 2:28:59.920
 You could argue that AI has already been involved

2:28:59.920 --> 2:29:01.560
 in some Nobel Prizes, probably,

2:29:01.560 --> 2:29:03.560
 maybe something with black holes and stuff like that.

2:29:03.560 --> 2:29:07.160
 Yeah, we don't like giving prizes to other life forms.

2:29:07.160 --> 2:29:09.720
 If someone wins a horse racing contest,

2:29:09.720 --> 2:29:11.360
 they don't give the prize to the horse either.

2:29:11.360 --> 2:29:12.200
 That's true.

2:29:13.400 --> 2:29:16.000
 But do you think that we might be able to see

2:29:16.000 --> 2:29:19.200
 something like that in our lifetimes when AI,

2:29:19.200 --> 2:29:21.840
 so like the first system I would say

2:29:21.840 --> 2:29:25.360
 that makes us think about a Nobel Prize seriously

2:29:25.360 --> 2:29:28.760
 is like Alpha Fold is making us think about

2:29:28.760 --> 2:29:31.960
 in medicine, physiology, a Nobel Prize,

2:29:31.960 --> 2:29:34.080
 perhaps discoveries that are direct result

2:29:34.080 --> 2:29:36.640
 of something that's discovered by Alpha Fold.

2:29:36.640 --> 2:29:39.560
 Do you think in physics we might be able

2:29:39.560 --> 2:29:41.520
 to see that in our lifetimes?

2:29:41.520 --> 2:29:43.520
 I think what's probably gonna happen

2:29:43.520 --> 2:29:46.880
 is more of a blurring of the distinctions.

2:29:46.880 --> 2:29:51.880
 So today if somebody uses a computer

2:29:53.000 --> 2:29:54.920
 to do a computation that gives them the Nobel Prize,

2:29:54.920 --> 2:29:57.160
 nobody's gonna dream of giving the prize to the computer.

2:29:57.160 --> 2:29:59.000
 They're gonna be like, that was just a tool.

2:29:59.000 --> 2:30:02.120
 I think for these things also,

2:30:02.120 --> 2:30:04.000
 people are just gonna for a long time

2:30:04.000 --> 2:30:06.120
 view the computer as a tool.

2:30:06.120 --> 2:30:11.120
 But what's gonna change is the ubiquity of machine learning.

2:30:11.120 --> 2:30:16.120
 I think at some point in my lifetime,

2:30:17.120 --> 2:30:21.400
 finding a human physicist who knows nothing

2:30:21.400 --> 2:30:23.800
 about machine learning is gonna be almost as hard

2:30:23.800 --> 2:30:25.960
 as it is today finding a human physicist

2:30:25.960 --> 2:30:29.160
 who doesn't says, oh, I don't know anything about computers

2:30:29.160 --> 2:30:30.880
 or I don't use math.

2:30:30.880 --> 2:30:32.880
 That would just be a ridiculous concept.

2:30:34.000 --> 2:30:38.240
 You see, but the thing is there is a magic moment though,

2:30:38.240 --> 2:30:42.320
 like with Alpha Zero, when the system surprises us

2:30:42.320 --> 2:30:45.640
 in a way where the best people in the world

2:30:46.680 --> 2:30:48.960
 truly learn something from the system

2:30:48.960 --> 2:30:52.480
 in a way where you feel like it's another entity.

2:30:52.480 --> 2:30:54.920
 Like the way people, the way Magnus Carlsen,

2:30:54.920 --> 2:30:58.080
 the way certain people are looking at the work of Alpha Zero,

2:30:58.080 --> 2:31:02.960
 it's like, it truly is no longer a tool

2:31:02.960 --> 2:31:06.680
 in the sense that it doesn't feel like a tool.

2:31:06.680 --> 2:31:08.960
 It feels like some other entity.

2:31:08.960 --> 2:31:13.320
 So there's a magic difference like where you're like,

2:31:13.320 --> 2:31:17.320
 if an AI system is able to come up with an insight

2:31:17.320 --> 2:31:22.320
 that surprises everybody in some like major way

2:31:23.760 --> 2:31:25.960
 that's a phase shift in our understanding

2:31:25.960 --> 2:31:27.760
 of some particular science

2:31:27.760 --> 2:31:30.040
 or some particular aspect of physics,

2:31:30.040 --> 2:31:32.680
 I feel like that is no longer a tool.

2:31:32.680 --> 2:31:34.840
 And then you can start to say

2:31:35.800 --> 2:31:38.720
 that like it perhaps deserves the prize.

2:31:38.720 --> 2:31:40.640
 So for sure, the more important

2:31:40.640 --> 2:31:43.120
 and the more fundamental transformation

2:31:43.120 --> 2:31:46.640
 of the 21st century science is exactly what you're saying,

2:31:46.640 --> 2:31:50.680
 which is probably everybody will be doing machine learning.

2:31:50.680 --> 2:31:51.560
 It's to some degree.

2:31:51.560 --> 2:31:54.760
 Like if you want to be successful

2:31:54.760 --> 2:31:57.560
 at unlocking the mysteries of science,

2:31:57.560 --> 2:31:58.800
 you should be doing machine learning.

2:31:58.800 --> 2:32:01.440
 But it's just exciting to think about like,

2:32:01.440 --> 2:32:03.080
 whether there'll be one that comes along

2:32:03.080 --> 2:32:08.000
 that's super surprising and they'll make us question

2:32:08.000 --> 2:32:10.320
 like who the real inventors are in this world.

2:32:10.320 --> 2:32:11.640
 Yeah.

2:32:11.640 --> 2:32:14.240
 Yeah, I think the question of,

2:32:14.240 --> 2:32:15.880
 isn't if it's gonna happen, but when?

2:32:15.880 --> 2:32:17.960
 And, but it's important.

2:32:17.960 --> 2:32:20.840
 Honestly, in my mind, the time when that happens

2:32:20.840 --> 2:32:23.360
 is also more or less the same time

2:32:23.360 --> 2:32:25.560
 when we get artificial general intelligence.

2:32:25.560 --> 2:32:28.160
 And then we have a lot bigger things to worry about

2:32:28.160 --> 2:32:31.000
 than whether we should get the Nobel prize or not, right?

2:32:31.000 --> 2:32:31.840
 Yeah.

2:32:31.840 --> 2:32:35.000
 Because when you have machines

2:32:35.000 --> 2:32:39.360
 that can outperform our best scientists at science,

2:32:39.360 --> 2:32:41.040
 they can probably outperform us

2:32:41.040 --> 2:32:44.440
 at a lot of other stuff as well,

2:32:44.440 --> 2:32:46.440
 which can at a minimum make them

2:32:46.440 --> 2:32:49.440
 incredibly powerful agents in the world.

2:32:49.440 --> 2:32:53.160
 And I think it's a mistake to think

2:32:53.160 --> 2:32:57.040
 we only have to start worrying about loss of control

2:32:57.040 --> 2:32:59.720
 when machines get to AGI across the board,

2:32:59.720 --> 2:33:02.160
 where they can do everything, all our jobs.

2:33:02.160 --> 2:33:07.160
 Long before that, they'll be hugely influential.

2:33:07.880 --> 2:33:12.560
 We talked at length about how the hacking of our minds

2:33:12.560 --> 2:33:17.560
 with algorithms trying to get us glued to our screens,

2:33:18.440 --> 2:33:22.320
 right, has already had a big impact on society.

2:33:22.320 --> 2:33:24.080
 That was an incredibly dumb algorithm

2:33:24.080 --> 2:33:25.840
 in the grand scheme of things, right?

2:33:25.840 --> 2:33:27.840
 The supervised machine learning,

2:33:27.840 --> 2:33:29.520
 yet that had huge impact.

2:33:29.520 --> 2:33:32.080
 So I just don't want us to be lulled

2:33:32.080 --> 2:33:33.280
 into false sense of security

2:33:33.280 --> 2:33:35.560
 and think there won't be any societal impact

2:33:35.560 --> 2:33:37.040
 until things reach human level,

2:33:37.040 --> 2:33:38.280
 because it's happening already.

2:33:38.280 --> 2:33:40.560
 And I was just thinking the other week,

2:33:40.560 --> 2:33:44.880
 when I see some scaremonger going,

2:33:44.880 --> 2:33:47.080
 oh, the robots are coming,

2:33:47.080 --> 2:33:50.280
 the implication is always that they're coming to kill us.

2:33:50.280 --> 2:33:51.120
 Yeah.

2:33:51.120 --> 2:33:52.360
 And maybe you should have worried about that

2:33:52.360 --> 2:33:54.720
 if you were in Nagorno Karabakh

2:33:54.720 --> 2:33:55.720
 during the recent war there.

2:33:55.720 --> 2:34:00.720
 But more seriously, the robots are coming right now,

2:34:01.440 --> 2:34:03.160
 but they're mainly not coming to kill us.

2:34:03.160 --> 2:34:04.400
 They're coming to hack us.

2:34:06.000 --> 2:34:08.200
 They're coming to hack our minds,

2:34:08.200 --> 2:34:11.280
 into buying things that maybe we didn't need,

2:34:11.280 --> 2:34:13.200
 to vote for people who may not have

2:34:13.200 --> 2:34:15.360
 our best interest in mind.

2:34:15.360 --> 2:34:17.600
 And it's kind of humbling, I think,

2:34:17.600 --> 2:34:20.120
 actually, as a human being to admit

2:34:20.120 --> 2:34:22.360
 that it turns out that our minds are actually

2:34:22.360 --> 2:34:24.760
 much more hackable than we thought.

2:34:24.760 --> 2:34:27.040
 And the ultimate insult is that we are actually

2:34:27.040 --> 2:34:30.400
 getting hacked by the machine learning algorithms

2:34:30.400 --> 2:34:31.560
 that are, in some objective sense,

2:34:31.560 --> 2:34:33.960
 much dumber than us, you know?

2:34:33.960 --> 2:34:35.720
 But maybe we shouldn't be so surprised

2:34:35.720 --> 2:34:40.520
 because, you know, how do you feel about cute puppies?

2:34:40.520 --> 2:34:41.600
 Love them.

2:34:41.600 --> 2:34:43.640
 So, you know, you would probably argue

2:34:43.640 --> 2:34:46.120
 that in some across the board measure,

2:34:46.120 --> 2:34:47.680
 you're more intelligent than they are,

2:34:47.680 --> 2:34:50.760
 but boy, are cute puppies good at hacking us, right?

2:34:50.760 --> 2:34:51.600
 Yeah.

2:34:51.600 --> 2:34:53.720
 They move into our house, persuade us to feed them

2:34:53.720 --> 2:34:54.560
 and do all these things.

2:34:54.560 --> 2:34:56.600
 And what do they ever do but for us?

2:34:56.600 --> 2:34:57.440
 Yeah.

2:34:57.440 --> 2:35:00.520
 Other than being cute and making us feel good, right?

2:35:00.520 --> 2:35:03.080
 So if puppies can hack us,

2:35:03.080 --> 2:35:04.920
 maybe we shouldn't be so surprised

2:35:04.920 --> 2:35:09.040
 if pretty dumb machine learning algorithms can hack us too.

2:35:09.040 --> 2:35:11.680
 Not to speak of cats, which is another level.

2:35:11.680 --> 2:35:13.400
 And I think we should,

2:35:13.400 --> 2:35:15.640
 to counter your previous point about there,

2:35:15.640 --> 2:35:18.040
 let us not think about evil creatures in this world.

2:35:18.040 --> 2:35:20.480
 We can all agree that cats are as close

2:35:20.480 --> 2:35:22.960
 to objective evil as we can get.

2:35:22.960 --> 2:35:24.400
 But that's just me saying that.

2:35:24.400 --> 2:35:25.480
 Okay, so you have.

2:35:25.480 --> 2:35:27.320
 Have you seen the cartoon?

2:35:27.320 --> 2:35:29.840
 I think it's maybe the onion

2:35:31.760 --> 2:35:33.720
 with this incredibly cute kitten.

2:35:33.720 --> 2:35:36.840
 And it just says, it's underneath something

2:35:36.840 --> 2:35:38.920
 that thinks about murder all day.

2:35:38.920 --> 2:35:41.560
 Exactly.

2:35:41.560 --> 2:35:43.080
 That's accurate.

2:35:43.080 --> 2:35:45.200
 You've mentioned offline that there might be a link

2:35:45.200 --> 2:35:47.960
 between post biological AGI and SETI.

2:35:47.960 --> 2:35:51.280
 So last time we talked,

2:35:52.520 --> 2:35:54.920
 you've talked about this intuition

2:35:54.920 --> 2:35:59.280
 that we humans might be quite unique

2:35:59.280 --> 2:36:02.360
 in our galactic neighborhood.

2:36:02.360 --> 2:36:03.680
 Perhaps our galaxy,

2:36:03.680 --> 2:36:06.360
 perhaps the entirety of the observable universe

2:36:06.360 --> 2:36:10.840
 who might be the only intelligent civilization here,

2:36:10.840 --> 2:36:15.840
 which is, and you argue pretty well for that thought.

2:36:17.720 --> 2:36:21.240
 So I have a few little questions around this.

2:36:21.240 --> 2:36:24.680
 One, the scientific question,

2:36:24.680 --> 2:36:29.240
 in which way would you be,

2:36:29.240 --> 2:36:33.120
 if you were wrong in that intuition,

2:36:33.960 --> 2:36:36.680
 in which way do you think you would be surprised?

2:36:36.680 --> 2:36:38.520
 Like why were you wrong?

2:36:38.520 --> 2:36:41.600
 We find out that you ended up being wrong.

2:36:41.600 --> 2:36:43.880
 Like in which dimension?

2:36:43.880 --> 2:36:48.400
 So like, is it because we can't see them?

2:36:48.400 --> 2:36:51.320
 Is it because the nature of their intelligence

2:36:51.320 --> 2:36:54.760
 or the nature of their life is totally different

2:36:54.760 --> 2:36:56.760
 than we can possibly imagine?

2:36:56.760 --> 2:37:00.680
 Is it because the,

2:37:00.680 --> 2:37:02.640
 I mean, something about the great filters

2:37:02.640 --> 2:37:04.440
 and surviving them,

2:37:04.440 --> 2:37:08.760
 or maybe because we're being protected from signals,

2:37:08.760 --> 2:37:13.760
 all those explanations for why we haven't heard

2:37:15.120 --> 2:37:20.120
 a big, loud, like red light that says we're here.

2:37:21.680 --> 2:37:23.520
 So there are actually two separate things there

2:37:23.520 --> 2:37:24.720
 that I could be wrong about,

2:37:24.720 --> 2:37:26.840
 two separate claims that I made, right?

2:37:28.920 --> 2:37:32.240
 One of them is, I made the claim,

2:37:32.240 --> 2:37:35.520
 I think most civilizations,

2:37:36.960 --> 2:37:41.800
 when you're going from simple bacteria like things

2:37:41.800 --> 2:37:46.800
 to space colonizing civilizations,

2:37:47.840 --> 2:37:50.840
 they spend only a very, very tiny fraction

2:37:50.840 --> 2:37:55.160
 of their life being where we are.

2:37:55.160 --> 2:37:57.280
 That I could be wrong about.

2:37:57.280 --> 2:37:58.760
 The other one I could be wrong about

2:37:58.760 --> 2:38:01.520
 is the quite different statement that I think that actually

2:38:01.520 --> 2:38:04.680
 I'm guessing that we are the only civilization

2:38:04.680 --> 2:38:06.120
 in our observable universe

2:38:06.120 --> 2:38:08.240
 from which light has reached us so far

2:38:08.240 --> 2:38:12.320
 that's actually gotten far enough to invent telescopes.

2:38:12.320 --> 2:38:13.960
 So let's talk about maybe both of them in turn

2:38:13.960 --> 2:38:15.000
 because they really are different.

2:38:15.000 --> 2:38:19.880
 The first one, if you look at the N equals one,

2:38:19.880 --> 2:38:22.080
 the data point we have on this planet, right?

2:38:22.080 --> 2:38:25.880
 So we spent four and a half billion years

2:38:25.880 --> 2:38:28.240
 fluxing around on this planet with life, right?

2:38:28.240 --> 2:38:32.080
 We got, and most of it was pretty lame stuff

2:38:32.080 --> 2:38:33.640
 from an intelligence perspective,

2:38:33.640 --> 2:38:38.160
 you know, it was bacteria and then the dinosaurs spent,

2:38:39.200 --> 2:38:41.280
 then the things gradually accelerated, right?

2:38:41.280 --> 2:38:43.600
 Then the dinosaurs spent over a hundred million years

2:38:43.600 --> 2:38:46.960
 stomping around here without even inventing smartphones.

2:38:46.960 --> 2:38:50.240
 And then very recently, you know,

2:38:50.240 --> 2:38:52.120
 it's only, we've only spent 400 years

2:38:52.120 --> 2:38:55.320
 going from Newton to us, right?

2:38:55.320 --> 2:38:56.480
 In terms of technology.

2:38:56.480 --> 2:39:00.160
 And look what we've done even, you know,

2:39:00.160 --> 2:39:02.600
 when I was a little kid, there was no internet even.

2:39:02.600 --> 2:39:05.880
 So it's, I think it's pretty likely for,

2:39:05.880 --> 2:39:08.160
 in this case of this planet, right?

2:39:08.160 --> 2:39:12.200
 That we're either gonna really get our act together

2:39:12.200 --> 2:39:15.080
 and start spreading life into space, the century,

2:39:15.080 --> 2:39:16.440
 and doing all sorts of great things,

2:39:16.440 --> 2:39:18.880
 or we're gonna wipe out.

2:39:18.880 --> 2:39:20.080
 It's a little hard.

2:39:20.080 --> 2:39:23.480
 If I, I could be wrong in the sense that maybe

2:39:23.480 --> 2:39:25.800
 what happened on this earth is very atypical.

2:39:25.800 --> 2:39:28.520
 And for some reason, what's more common on other planets

2:39:28.520 --> 2:39:31.440
 is that they spend an enormously long time

2:39:31.440 --> 2:39:33.720
 futzing around with the ham radio and things,

2:39:33.720 --> 2:39:36.200
 but they just never really take it to the next level

2:39:36.200 --> 2:39:38.400
 for reasons I don't, I haven't understood.

2:39:38.400 --> 2:39:40.200
 I'm humble and open to that.

2:39:40.200 --> 2:39:42.880
 But I would bet at least 10 to one

2:39:42.880 --> 2:39:45.160
 that our situation is more typical

2:39:45.160 --> 2:39:46.760
 because the whole thing with Moore's law

2:39:46.760 --> 2:39:48.160
 and accelerating technology,

2:39:48.160 --> 2:39:50.040
 it's pretty obvious why it's happening.

2:39:51.200 --> 2:39:52.880
 Everything that grows exponentially,

2:39:52.880 --> 2:39:54.080
 we call it an explosion,

2:39:54.080 --> 2:39:56.640
 whether it's a population explosion or a nuclear explosion,

2:39:56.640 --> 2:39:58.000
 it's always caused by the same thing.

2:39:58.000 --> 2:40:01.480
 It's that the next step triggers a step after that.

2:40:01.480 --> 2:40:04.320
 So I, we, tomorrow's technology,

2:40:04.320 --> 2:40:06.760
 today's technology enables tomorrow's technology

2:40:06.760 --> 2:40:09.080
 and that enables the next level.

2:40:09.080 --> 2:40:13.800
 And as I think, because the technology is always better,

2:40:13.800 --> 2:40:16.160
 of course, the steps can come faster and faster.

2:40:17.200 --> 2:40:19.160
 On the other question that I might be wrong about,

2:40:19.160 --> 2:40:22.320
 that's the much more controversial one, I think.

2:40:22.320 --> 2:40:24.920
 But before we close out on this thing about,

2:40:24.920 --> 2:40:27.080
 if, the first one, if it's true

2:40:27.080 --> 2:40:30.520
 that most civilizations spend only a very short amount

2:40:30.520 --> 2:40:32.880
 of their total time in the stage, say,

2:40:32.880 --> 2:40:35.440
 between inventing

2:40:37.320 --> 2:40:40.760
 telescopes or mastering electricity

2:40:40.760 --> 2:40:43.880
 and leaving there and doing space travel,

2:40:43.880 --> 2:40:46.200
 if that's actually generally true,

2:40:46.200 --> 2:40:49.000
 then that should apply also elsewhere out there.

2:40:49.000 --> 2:40:51.040
 So we should be very, very,

2:40:51.040 --> 2:40:52.920
 we should be very, very surprised

2:40:52.920 --> 2:40:55.480
 if we find some random civilization

2:40:55.480 --> 2:40:56.920
 and we happen to catch them exactly

2:40:56.920 --> 2:40:58.800
 in that very, very short stage.

2:40:58.800 --> 2:40:59.640
 It's much more likely

2:40:59.640 --> 2:41:02.960
 that we find a planet full of bacteria.

2:41:02.960 --> 2:41:05.560
 Or that we find some civilization

2:41:05.560 --> 2:41:07.480
 that's already post biological

2:41:07.480 --> 2:41:11.880
 and has done some really cool galactic construction projects

2:41:11.880 --> 2:41:13.360
 in their galaxy.

2:41:13.360 --> 2:41:15.200
 Would we be able to recognize them, do you think?

2:41:15.200 --> 2:41:17.480
 Is it possible that we just can't,

2:41:17.480 --> 2:41:20.040
 I mean, this post biological world,

2:41:21.120 --> 2:41:23.520
 could it be just existing in some other dimension?

2:41:23.520 --> 2:41:26.280
 It could be just all a virtual reality game

2:41:26.280 --> 2:41:28.480
 for them or something, I don't know,

2:41:28.480 --> 2:41:30.560
 that it changes completely

2:41:30.560 --> 2:41:32.880
 where we won't be able to detect.

2:41:32.880 --> 2:41:35.280
 We have to be honestly very humble about this.

2:41:35.280 --> 2:41:39.000
 I think I said earlier the number one principle

2:41:39.000 --> 2:41:40.840
 of being a scientist is you have to be humble

2:41:40.840 --> 2:41:42.960
 and willing to acknowledge that everything we think,

2:41:42.960 --> 2:41:45.040
 guess might be totally wrong.

2:41:45.040 --> 2:41:46.960
 Of course, you could imagine some civilization

2:41:46.960 --> 2:41:48.640
 where they all decide to become Buddhists

2:41:48.640 --> 2:41:49.880
 and very inward looking

2:41:49.880 --> 2:41:52.360
 and just move into their little virtual reality

2:41:52.360 --> 2:41:55.120
 and not disturb the flora and fauna around them

2:41:55.120 --> 2:41:58.120
 and we might not notice them.

2:41:58.120 --> 2:41:59.960
 But this is a numbers game, right?

2:41:59.960 --> 2:42:02.280
 If you have millions of civilizations out there

2:42:02.280 --> 2:42:03.680
 or billions of them,

2:42:03.680 --> 2:42:08.080
 all it takes is one with a more ambitious mentality

2:42:08.080 --> 2:42:10.280
 that decides, hey, we are gonna go out

2:42:10.280 --> 2:42:15.280
 and settle a bunch of other solar systems

2:42:15.520 --> 2:42:17.560
 and maybe galaxies.

2:42:17.560 --> 2:42:18.440
 And then it doesn't matter

2:42:18.440 --> 2:42:19.640
 if they're a bunch of quiet Buddhists,

2:42:19.640 --> 2:42:23.040
 we're still gonna notice that expansionist one, right?

2:42:23.040 --> 2:42:26.560
 And it seems like quite the stretch to assume that,

2:42:26.560 --> 2:42:28.120
 now we know even in our own galaxy

2:42:28.120 --> 2:42:33.120
 that there are probably a billion or more planets

2:42:33.120 --> 2:42:35.280
 that are pretty Earth like.

2:42:35.280 --> 2:42:37.680
 And many of them are formed over a billion years

2:42:37.680 --> 2:42:40.640
 before ours, so had a big head start.

2:42:40.640 --> 2:42:43.600
 So if you actually assume also

2:42:43.600 --> 2:42:46.120
 that life happens kind of automatically

2:42:46.120 --> 2:42:47.360
 on an Earth like planet,

2:42:48.440 --> 2:42:52.080
 I think it's quite the stretch to then go and say,

2:42:52.080 --> 2:42:55.280
 okay, so there are another billion civilizations out there

2:42:55.280 --> 2:42:56.840
 that also have our level of tech

2:42:56.840 --> 2:42:59.280
 and they all decided to become Buddhists

2:42:59.280 --> 2:43:02.880
 and not a single one decided to go Hitler on the galaxy

2:43:02.880 --> 2:43:05.280
 and say, we need to go out and colonize

2:43:05.280 --> 2:43:08.840
 or not a single one decided for more benevolent reasons

2:43:08.840 --> 2:43:10.520
 to go out and get more resources.

2:43:11.480 --> 2:43:13.840
 That seems like a bit of a stretch, frankly.

2:43:13.840 --> 2:43:16.560
 And this leads into the second thing

2:43:16.560 --> 2:43:18.560
 you challenged me that I might be wrong about,

2:43:18.560 --> 2:43:22.320
 how rare or common is life, you know?

2:43:22.320 --> 2:43:25.120
 So Francis Drake, when he wrote down the Drake equation,

2:43:25.120 --> 2:43:27.560
 multiplied together a huge number of factors

2:43:27.560 --> 2:43:29.320
 and then we don't know any of them.

2:43:29.320 --> 2:43:31.480
 So we know even less about what you get

2:43:31.480 --> 2:43:33.680
 when you multiply together the whole product.

2:43:35.120 --> 2:43:37.200
 Since then, a lot of those factors

2:43:37.200 --> 2:43:38.880
 have become much better known.

2:43:38.880 --> 2:43:40.840
 One of his big uncertainties was

2:43:40.840 --> 2:43:44.360
 how common is it that a solar system even has a planet?

2:43:44.360 --> 2:43:46.280
 Well, now we know it very common.

2:43:46.280 --> 2:43:48.320
 Earth like planets, we know we have better.

2:43:48.320 --> 2:43:50.440
 There are a dime a dozen, there are many, many of them,

2:43:50.440 --> 2:43:52.080
 even in our galaxy.

2:43:52.080 --> 2:43:55.000
 At the same time, you know, we have thanks to,

2:43:55.000 --> 2:43:58.840
 I'm a big supporter of the SETI project and its cousins

2:43:58.840 --> 2:44:00.520
 and I think we should keep doing this

2:44:00.520 --> 2:44:02.400
 and we've learned a lot.

2:44:02.400 --> 2:44:03.800
 We've learned that so far,

2:44:03.800 --> 2:44:08.040
 all we have is still unconvincing hints, nothing more, right?

2:44:08.040 --> 2:44:10.320
 And there are certainly many scenarios

2:44:10.320 --> 2:44:13.080
 where it would be dead obvious.

2:44:13.080 --> 2:44:15.920
 If there were a hundred million

2:44:15.920 --> 2:44:19.000
 other human like civilizations in our galaxy,

2:44:19.000 --> 2:44:21.600
 it would not be that hard to notice some of them

2:44:21.600 --> 2:44:23.440
 with today's technology and we haven't, right?

2:44:23.440 --> 2:44:27.720
 So what we can say is, well, okay,

2:44:27.720 --> 2:44:30.560
 we can rule out that there is a human level of civilization

2:44:30.560 --> 2:44:34.120
 on the moon and in fact, the many nearby solar systems

2:44:34.120 --> 2:44:37.600
 where we cannot rule out, of course,

2:44:37.600 --> 2:44:41.560
 that there is something like Earth sitting in a galaxy

2:44:41.560 --> 2:44:43.120
 five billion light years away.

2:44:45.120 --> 2:44:46.400
 But we've ruled out a lot

2:44:46.400 --> 2:44:48.480
 and that's already kind of shocking

2:44:48.480 --> 2:44:50.320
 given that there are all these planets there, you know?

2:44:50.320 --> 2:44:51.440
 So like, where are they?

2:44:51.440 --> 2:44:52.280
 Where are they all?

2:44:52.280 --> 2:44:54.880
 That's the classic Fermi paradox.

2:44:54.880 --> 2:44:59.240
 And so my argument, which might very well be wrong,

2:44:59.240 --> 2:45:01.400
 it's very simple really, it just goes like this.

2:45:01.400 --> 2:45:03.000
 Okay, we have no clue about this.

2:45:05.240 --> 2:45:07.800
 It could be the probability of getting life

2:45:07.800 --> 2:45:11.240
 on a random planet, it could be 10 to the minus one

2:45:11.240 --> 2:45:14.680
 a priori or 10 to the minus five, 10, 10 to the minus 20,

2:45:14.680 --> 2:45:17.400
 10 to the minus 30, 10 to the minus 40.

2:45:17.400 --> 2:45:20.240
 Basically every order of magnitude is about equally likely.

2:45:21.400 --> 2:45:24.120
 When then do the math and ask the question,

2:45:24.120 --> 2:45:27.400
 how close is our nearest neighbor?

2:45:27.400 --> 2:45:30.520
 It's again, equally likely that it's 10 to the 10 meters away,

2:45:30.520 --> 2:45:33.440
 10 to 20 meters away, 10 to the 30 meters away.

2:45:33.440 --> 2:45:35.640
 We have some nerdy ways of talking about this

2:45:35.640 --> 2:45:38.080
 with Bayesian statistics and a uniform log prior,

2:45:38.080 --> 2:45:39.360
 but that's irrelevant.

2:45:39.360 --> 2:45:42.040
 This is the simple basic argument.

2:45:42.040 --> 2:45:43.320
 And now comes the data.

2:45:43.320 --> 2:45:46.320
 So we can say, okay, there are all these orders

2:45:46.320 --> 2:45:49.280
 of magnitude, 10 to the 26 meters away,

2:45:49.280 --> 2:45:51.960
 there's the edge of our observable universe.

2:45:51.960 --> 2:45:54.840
 If it's farther than that, light hasn't even reached us yet.

2:45:54.840 --> 2:45:58.040
 If it's less than 10 to the 16 meters away,

2:45:58.040 --> 2:46:02.320
 well, it's within Earth's,

2:46:02.320 --> 2:46:03.800
 it's no farther away than the sun.

2:46:03.800 --> 2:46:05.240
 We can definitely rule that out.

2:46:07.200 --> 2:46:08.520
 So I think about it like this,

2:46:08.520 --> 2:46:10.840
 a priori before we looked at the telescopes,

2:46:11.840 --> 2:46:14.320
 it could be 10 to the 10 meters, 10 to the 20,

2:46:14.320 --> 2:46:16.520
 10 to the 30, 10 to the 40, 10 to the 50, 10 to blah, blah, blah.

2:46:16.520 --> 2:46:18.040
 Equally likely anywhere here.

2:46:18.040 --> 2:46:21.760
 And now we've ruled out like this chunk.

2:46:21.760 --> 2:46:26.760
 And here is the edge of our observable universe already.

2:46:27.880 --> 2:46:30.560
 So I'm certainly not saying I don't think

2:46:30.560 --> 2:46:32.480
 there's any life elsewhere in space.

2:46:32.480 --> 2:46:33.680
 If space is infinite,

2:46:33.680 --> 2:46:35.640
 then you're basically a hundred percent guaranteed

2:46:35.640 --> 2:46:39.200
 that there is, but the probability that there is life,

2:46:41.200 --> 2:46:42.280
 that the nearest neighbor,

2:46:42.280 --> 2:46:43.760
 it happens to be in this little region

2:46:43.760 --> 2:46:47.120
 between where we would have seen it already

2:46:47.120 --> 2:46:48.680
 and where we will never see it.

2:46:48.680 --> 2:46:51.920
 There's actually significantly less than one, I think.

2:46:51.920 --> 2:46:54.280
 And I think there's a moral lesson from this,

2:46:54.280 --> 2:46:55.840
 which is really important,

2:46:55.840 --> 2:47:00.120
 which is to be good stewards of this planet

2:47:00.120 --> 2:47:01.440
 and this shot we've had.

2:47:01.440 --> 2:47:03.640
 It can be very dangerous to say,

2:47:03.640 --> 2:47:07.640
 oh, it's fine if we nuke our planet or ruin the climate

2:47:07.640 --> 2:47:10.280
 or mess it up with unaligned AI,

2:47:10.280 --> 2:47:15.160
 because I know there is this nice Star Trek fleet out there.

2:47:15.160 --> 2:47:18.040
 They're gonna swoop in and take over where we failed.

2:47:18.040 --> 2:47:19.840
 Just like it wasn't the big deal

2:47:19.840 --> 2:47:23.040
 that the Easter Island losers wiped themselves out.

2:47:23.040 --> 2:47:25.200
 That's a dangerous way of lulling yourself

2:47:25.200 --> 2:47:26.600
 into false sense of security.

2:47:27.760 --> 2:47:32.000
 If it's actually the case that it might be up to us

2:47:32.000 --> 2:47:35.000
 and only us, the whole future of intelligent life

2:47:35.000 --> 2:47:36.360
 in our observable universe,

2:47:37.680 --> 2:47:42.440
 then I think it really puts a lot of responsibility

2:47:42.440 --> 2:47:43.280
 on our shoulders.

2:47:43.280 --> 2:47:45.320
 It's inspiring, it's a little bit terrifying,

2:47:45.320 --> 2:47:46.480
 but it's also inspiring.

2:47:46.480 --> 2:47:48.600
 But it's empowering, I think, most of all,

2:47:48.600 --> 2:47:50.240
 because the biggest problem today is,

2:47:50.240 --> 2:47:51.960
 I see this even when I teach,

2:47:53.120 --> 2:47:56.360
 so many people feel that it doesn't matter what they do

2:47:56.360 --> 2:47:58.760
 or we do, we feel disempowered.

2:47:58.760 --> 2:48:00.120
 Oh, it makes no difference.

2:48:02.560 --> 2:48:05.080
 This is about as far from that as you can come.

2:48:05.080 --> 2:48:06.880
 But we realize that what we do

2:48:07.760 --> 2:48:12.200
 on our little spinning ball here in our lifetime

2:48:12.200 --> 2:48:15.440
 could make the difference for the entire future of life

2:48:15.440 --> 2:48:17.080
 in our universe.

2:48:17.080 --> 2:48:18.720
 How empowering is that?

2:48:18.720 --> 2:48:20.280
 Yeah, survival of consciousness.

2:48:20.280 --> 2:48:25.280
 I mean, a very similar kind of empowering aspect

2:48:25.840 --> 2:48:27.680
 of the Drake equation is,

2:48:27.680 --> 2:48:31.120
 say there is a huge number of intelligent civilizations

2:48:31.120 --> 2:48:32.920
 that spring up everywhere,

2:48:32.920 --> 2:48:34.760
 but because of the Drake equation,

2:48:34.760 --> 2:48:38.000
 which is the lifetime of a civilization,

2:48:38.000 --> 2:48:39.880
 maybe many of them hit a wall.

2:48:39.880 --> 2:48:43.360
 And just like you said, it's clear that that,

2:48:43.360 --> 2:48:45.920
 for us, the great filter,

2:48:45.920 --> 2:48:49.040
 the one possible great filter seems to be coming

2:48:49.040 --> 2:48:51.240
 in the next 100 years.

2:48:51.240 --> 2:48:53.720
 So it's also empowering to say,

2:48:53.720 --> 2:48:58.720
 okay, well, we have a chance to not,

2:48:58.720 --> 2:49:00.120
 I mean, the way great filters work,

2:49:00.120 --> 2:49:02.080
 they just get most of them.

2:49:02.080 --> 2:49:02.920
 Exactly.

2:49:02.920 --> 2:49:06.120
 Nick Bostrom has articulated this really beautifully too.

2:49:06.120 --> 2:49:09.480
 Every time yet another search for life on Mars

2:49:09.480 --> 2:49:11.120
 comes back negative or something,

2:49:11.120 --> 2:49:14.760
 I'm like, yes, yes.

2:49:14.760 --> 2:49:17.840
 Our odds for us surviving is the best.

2:49:17.840 --> 2:49:20.960
 You already made the argument in broad brush there, right?

2:49:20.960 --> 2:49:22.560
 But just to unpack it, right?

2:49:22.560 --> 2:49:24.520
 The point is we already know

2:49:26.880 --> 2:49:28.640
 there is a crap ton of planets out there

2:49:28.640 --> 2:49:29.640
 that are Earth like,

2:49:29.640 --> 2:49:33.160
 and we also know that most of them do not seem

2:49:33.160 --> 2:49:35.080
 to have anything like our kind of life on them.

2:49:35.080 --> 2:49:37.240
 So what went wrong?

2:49:37.240 --> 2:49:39.520
 There's clearly one step along the evolutionary,

2:49:39.520 --> 2:49:42.360
 at least one filter or roadblock

2:49:42.360 --> 2:49:45.600
 in going from no life to spacefaring life.

2:49:45.600 --> 2:49:48.160
 And where is it?

2:49:48.160 --> 2:49:50.720
 Is it in front of us or is it behind us, right?

2:49:51.640 --> 2:49:54.080
 If there's no filter behind us,

2:49:54.080 --> 2:49:59.080
 and we keep finding all sorts of little mice on Mars

2:50:00.520 --> 2:50:01.880
 or whatever, right?

2:50:01.880 --> 2:50:03.120
 That's actually very depressing

2:50:03.120 --> 2:50:04.440
 because that makes it much more likely

2:50:04.440 --> 2:50:06.280
 that the filter is in front of us.

2:50:06.280 --> 2:50:08.080
 And that what actually is going on

2:50:08.080 --> 2:50:11.080
 is like the ultimate dark joke

2:50:11.080 --> 2:50:13.800
 that whenever a civilization

2:50:13.800 --> 2:50:15.640
 invents sufficiently powerful tech,

2:50:15.640 --> 2:50:17.240
 it's just, you just set your clock.

2:50:17.240 --> 2:50:19.160
 And then after a little while it goes poof

2:50:19.160 --> 2:50:21.840
 for one reason or other and wipes itself out.

2:50:21.840 --> 2:50:24.240
 Now wouldn't that be like utterly depressing

2:50:24.240 --> 2:50:26.120
 if we're actually doomed?

2:50:26.120 --> 2:50:29.720
 Whereas if it turns out that there is a really,

2:50:29.720 --> 2:50:31.720
 there is a great filter early on

2:50:31.720 --> 2:50:33.960
 that for whatever reason seems to be really hard

2:50:33.960 --> 2:50:38.960
 to get to the stage of sexually reproducing organisms

2:50:39.160 --> 2:50:43.320
 or even the first ribosome or whatever, right?

2:50:43.320 --> 2:50:47.160
 Or maybe you have lots of planets with dinosaurs and cows,

2:50:47.160 --> 2:50:48.880
 but for some reason they tend to get stuck there

2:50:48.880 --> 2:50:50.840
 and never invent smartphones.

2:50:50.840 --> 2:50:55.200
 All of those are huge boosts for our own odds

2:50:55.200 --> 2:50:58.840
 because been there done that, you know?

2:50:58.840 --> 2:51:01.720
 It doesn't matter how hard or unlikely it was

2:51:01.720 --> 2:51:03.800
 that we got past that roadblock

2:51:03.800 --> 2:51:05.120
 because we already did.

2:51:05.120 --> 2:51:07.520
 And then that makes it likely

2:51:07.520 --> 2:51:11.440
 that the future is in our own hands, we're not doomed.

2:51:11.440 --> 2:51:14.800
 So that's why I think the fact

2:51:14.800 --> 2:51:18.280
 that life is rare in the universe,

2:51:18.280 --> 2:51:21.440
 it's not just something that there is some evidence for,

2:51:21.440 --> 2:51:24.560
 but also something we should actually hope for.

2:51:26.680 --> 2:51:29.920
 So that's the end, the mortality,

2:51:29.920 --> 2:51:31.520
 the death of human civilization

2:51:31.520 --> 2:51:33.120
 that we've been discussing in life,

2:51:33.120 --> 2:51:36.680
 maybe prospering beyond any kind of great filter.

2:51:36.680 --> 2:51:39.440
 Do you think about your own death?

2:51:39.440 --> 2:51:44.440
 Does it make you sad that you may not witness some of the,

2:51:45.760 --> 2:51:47.440
 you know, you lead a research group

2:51:47.440 --> 2:51:49.040
 on working some of the biggest questions

2:51:49.040 --> 2:51:51.080
 in the universe actually,

2:51:51.080 --> 2:51:53.720
 both on the physics and the AI side?

2:51:53.720 --> 2:51:55.560
 Does it make you sad that you may not be able

2:51:55.560 --> 2:51:58.840
 to see some of these exciting things come to fruition

2:51:58.840 --> 2:52:00.640
 that we've been talking about?

2:52:00.640 --> 2:52:03.920
 Of course, of course it sucks, the fact that I'm gonna die.

2:52:04.840 --> 2:52:07.200
 I remember once when I was much younger,

2:52:07.200 --> 2:52:10.800
 my dad made this remark that life is fundamentally tragic.

2:52:10.800 --> 2:52:13.080
 And I'm like, what are you talking about, daddy?

2:52:13.080 --> 2:52:15.640
 And then many years later, I felt,

2:52:15.640 --> 2:52:17.320
 now I feel I totally understand what he means.

2:52:17.320 --> 2:52:19.040
 You know, we grow up, we're little kids

2:52:19.040 --> 2:52:21.920
 and everything is infinite and it's so cool.

2:52:21.920 --> 2:52:24.720
 And then suddenly we find out that actually, you know,

2:52:25.800 --> 2:52:26.840
 you got to serve only,

2:52:26.840 --> 2:52:30.280
 this is the, you're gonna get game over at some point.

2:52:30.280 --> 2:52:35.280
 So of course it's something that's sad.

2:52:36.400 --> 2:52:37.240
 Are you afraid?

2:52:42.640 --> 2:52:46.000
 No, not in the sense that I think anything terrible

2:52:46.000 --> 2:52:48.240
 is gonna happen after I die or anything like that.

2:52:48.240 --> 2:52:50.960
 No, I think it's really gonna be a game over,

2:52:50.960 --> 2:52:55.960
 but it's more that it makes me very acutely aware

2:52:56.280 --> 2:52:57.920
 of what a wonderful gift this is

2:52:57.920 --> 2:53:00.200
 that I get to be alive right now.

2:53:00.200 --> 2:53:04.680
 And is a steady reminder to just live life to the fullest

2:53:04.680 --> 2:53:08.000
 and really enjoy it because it is finite, you know.

2:53:08.000 --> 2:53:11.240
 And I think actually, and we know we all get

2:53:11.240 --> 2:53:14.280
 the regular reminders when someone near and dear to us dies

2:53:14.280 --> 2:53:19.280
 that one day it's gonna be our turn.

2:53:19.560 --> 2:53:21.480
 It adds this kind of focus.

2:53:21.480 --> 2:53:23.680
 I wonder what it would feel like actually

2:53:23.680 --> 2:53:26.960
 to be an immortal being if they might even enjoy

2:53:26.960 --> 2:53:29.440
 some of the wonderful things of life a little bit less

2:53:29.440 --> 2:53:33.400
 just because there isn't that.

2:53:33.400 --> 2:53:34.320
 Finiteness?

2:53:34.320 --> 2:53:35.160
 Yeah.

2:53:35.160 --> 2:53:38.040
 Do you think that could be a feature, not a bug,

2:53:38.040 --> 2:53:42.040
 the fact that we beings are finite?

2:53:42.040 --> 2:53:44.320
 Maybe there's lessons for engineering

2:53:44.320 --> 2:53:46.940
 in artificial intelligence systems as well

2:53:46.940 --> 2:53:48.400
 that are conscious.

2:53:48.400 --> 2:53:53.400
 Like do you think it makes, is it possible

2:53:53.920 --> 2:53:56.960
 that the reason the pistachio ice cream is delicious

2:53:56.960 --> 2:53:59.920
 is the fact that you're going to die one day

2:53:59.920 --> 2:54:03.720
 and you will not have all the pistachio ice cream

2:54:03.720 --> 2:54:06.200
 that you could eat because of that fact?

2:54:06.200 --> 2:54:07.560
 Well, let me say two things.

2:54:07.560 --> 2:54:09.660
 First of all, it's actually quite profound

2:54:09.660 --> 2:54:10.500
 what you're saying.

2:54:10.500 --> 2:54:12.300
 I do think I appreciate the pistachio ice cream

2:54:12.300 --> 2:54:14.400
 a lot more knowing that I will,

2:54:14.400 --> 2:54:17.760
 there's only a finite number of times I get to enjoy that.

2:54:17.760 --> 2:54:19.900
 And I can only remember a finite number of times

2:54:19.900 --> 2:54:20.740
 in the past.

2:54:21.720 --> 2:54:25.120
 And moreover, my life is not so long

2:54:25.120 --> 2:54:26.800
 that it just starts to feel like things are repeating

2:54:26.800 --> 2:54:28.120
 themselves in general.

2:54:28.120 --> 2:54:30.520
 It's so new and fresh.

2:54:30.520 --> 2:54:35.520
 I also think though that death is a little bit overrated

2:54:36.400 --> 2:54:41.400
 in the sense that it comes from a sort of outdated view

2:54:42.020 --> 2:54:45.640
 of physics and what life actually is.

2:54:45.640 --> 2:54:49.120
 Because if you ask, okay, what is it that's gonna die

2:54:49.120 --> 2:54:52.040
 exactly, what am I really?

2:54:52.040 --> 2:54:56.000
 When I say I feel sad about the idea of myself dying,

2:54:56.000 --> 2:54:59.180
 am I really sad that this skin cell here is gonna die?

2:54:59.180 --> 2:55:01.600
 Of course not, because it's gonna die next week anyway

2:55:01.600 --> 2:55:04.020
 and I'll grow a new one, right?

2:55:04.020 --> 2:55:08.440
 And it's not any of my cells that I'm associating really

2:55:08.440 --> 2:55:11.000
 with who I really am.

2:55:11.000 --> 2:55:14.060
 Nor is it any of my atoms or quarks or electrons.

2:55:15.640 --> 2:55:19.380
 In fact, basically all of my atoms get replaced

2:55:19.380 --> 2:55:20.520
 on a regular basis, right?

2:55:20.520 --> 2:55:22.880
 So what is it that's really me

2:55:22.880 --> 2:55:24.320
 from a more modern physics perspective?

2:55:24.320 --> 2:55:28.800
 It's the information in processing me.

2:55:28.800 --> 2:55:31.520
 That's where my memory, that's my memories,

2:55:31.520 --> 2:55:36.520
 that's my values, my dreams, my passion, my love.

2:55:40.560 --> 2:55:43.580
 That's what's really fundamentally me.

2:55:43.580 --> 2:55:48.580
 And frankly, not all of that will die when my body dies.

2:55:48.580 --> 2:55:53.580
 Like Richard Feynman, for example, his body died of cancer,

2:55:55.100 --> 2:55:59.720
 but many of his ideas that he felt made him very him

2:55:59.720 --> 2:56:01.400
 actually live on.

2:56:01.400 --> 2:56:04.100
 This is my own little personal tribute to Richard Feynman.

2:56:04.100 --> 2:56:07.500
 I try to keep a little bit of him alive in myself.

2:56:07.500 --> 2:56:09.620
 I've even quoted him today, right?

2:56:09.620 --> 2:56:11.740
 Yeah, he almost came alive for a brief moment

2:56:11.740 --> 2:56:13.320
 in this conversation, yeah.

2:56:13.320 --> 2:56:17.500
 Yeah, and this honestly gives me some solace.

2:56:17.500 --> 2:56:20.780
 When I work as a teacher, I feel,

2:56:20.780 --> 2:56:25.780
 if I can actually share a bit about myself

2:56:25.820 --> 2:56:30.740
 that my students feel worthy enough to copy and adopt

2:56:30.740 --> 2:56:33.140
 as some part of things that they know

2:56:33.140 --> 2:56:36.140
 or they believe or aspire to,

2:56:36.140 --> 2:56:39.540
 now I live on also a little bit in them, right?

2:56:39.540 --> 2:56:44.540
 And so being a teacher is a little bit

2:56:44.540 --> 2:56:49.540
 of what I, that's something also that contributes

2:56:49.740 --> 2:56:53.740
 to making me a little teeny bit less mortal, right?

2:56:53.740 --> 2:56:56.740
 Because I'm not, at least not all gonna die all at once,

2:56:56.740 --> 2:56:57.580
 right?

2:56:57.580 --> 2:56:59.820
 And I find that a beautiful tribute to people

2:56:59.820 --> 2:57:01.020
 we do not respect.

2:57:01.020 --> 2:57:05.740
 If we can remember them and carry in us

2:57:05.740 --> 2:57:10.260
 the things that we felt was the most awesome about them,

2:57:10.260 --> 2:57:11.620
 right, then they live on.

2:57:11.620 --> 2:57:13.580
 And I'm getting a bit emotional here,

2:57:13.580 --> 2:57:16.140
 but it's a very beautiful idea you bring up there.

2:57:16.140 --> 2:57:19.620
 I think we should stop this old fashioned materialism

2:57:19.620 --> 2:57:24.620
 and just equate who we are with our quirks and electrons.

2:57:25.220 --> 2:57:27.820
 There's no scientific basis for that really.

2:57:27.820 --> 2:57:31.180
 And it's also very uninspiring.

2:57:33.180 --> 2:57:36.980
 Now, if you look a little bit towards the future, right?

2:57:36.980 --> 2:57:40.740
 One thing which really sucks about humans dying is that even

2:57:40.740 --> 2:57:43.300
 though some of their teachings and memories and stories

2:57:43.300 --> 2:57:47.540
 and ethics and so on will be copied by those around them,

2:57:47.540 --> 2:57:50.260
 hopefully, a lot of it can't be copied

2:57:50.260 --> 2:57:51.980
 and just dies with them, with their brain.

2:57:51.980 --> 2:57:53.140
 And that really sucks.

2:57:53.140 --> 2:57:56.860
 That's the fundamental reason why we find it so tragic

2:57:56.860 --> 2:57:59.660
 when someone goes from having all this information there

2:57:59.660 --> 2:58:03.460
 to the more just gone, ruined, right?

2:58:03.460 --> 2:58:07.460
 With more post biological intelligence,

2:58:07.460 --> 2:58:09.940
 that's going to shift a lot, right?

2:58:10.940 --> 2:58:13.980
 The only reason it's so hard to make a backup of your brain

2:58:13.980 --> 2:58:15.380
 in its entirety is exactly

2:58:15.380 --> 2:58:17.580
 because it wasn't built for that, right?

2:58:17.580 --> 2:58:21.540
 If you have a future machine intelligence,

2:58:21.540 --> 2:58:24.300
 there's no reason for why it has to die at all.

2:58:24.300 --> 2:58:28.300
 If you want to copy it, whatever it is,

2:58:28.300 --> 2:58:30.780
 into some other machine intelligence,

2:58:30.780 --> 2:58:35.780
 whatever it is, into some other quark blob, right?

2:58:36.660 --> 2:58:39.540
 You can copy not just some of it, but all of it, right?

2:58:39.540 --> 2:58:42.180
 And so in that sense,

2:58:45.020 --> 2:58:48.300
 you can get immortality because all the information

2:58:48.300 --> 2:58:51.180
 can be copied out of any individual entity.

2:58:51.940 --> 2:58:54.220
 And it's not just mortality that will change

2:58:54.220 --> 2:58:56.900
 if we get to more post biological life.

2:58:56.900 --> 2:59:01.900
 It's also with that, very much the whole individualism

2:59:03.180 --> 2:59:04.020
 we have now, right?

2:59:04.020 --> 2:59:05.740
 The reason that we make such a big difference

2:59:05.740 --> 2:59:09.100
 between me and you is exactly because

2:59:09.100 --> 2:59:10.940
 we're a little bit limited in how much we can copy.

2:59:10.940 --> 2:59:13.300
 Like I would just love to go like this

2:59:13.300 --> 2:59:17.780
 and copy your Russian skills, Russian speaking skills.

2:59:17.780 --> 2:59:18.820
 Wouldn't it be awesome?

2:59:18.820 --> 2:59:21.980
 But I can't, I have to actually work for years

2:59:21.980 --> 2:59:23.900
 if I want to get better on it.

2:59:23.900 --> 2:59:27.940
 But if we were robots.

2:59:27.940 --> 2:59:31.820
 Just copy and paste freely, then that loses completely.

2:59:31.820 --> 2:59:35.140
 It washes away the sense of what immortality is.

2:59:35.140 --> 2:59:37.460
 And also individuality a little bit, right?

2:59:37.460 --> 2:59:39.300
 We would start feeling much more,

2:59:40.620 --> 2:59:43.540
 maybe we would feel much more collaborative with each other

2:59:43.540 --> 2:59:45.620
 if we can just, hey, you know, I'll give you my Russian,

2:59:45.620 --> 2:59:46.540
 you can give me your Russian

2:59:46.540 --> 2:59:47.940
 and I'll give you whatever,

2:59:47.940 --> 2:59:50.220
 and suddenly you can speak Swedish.

2:59:50.220 --> 2:59:52.060
 Maybe that's less a bad trade for you,

2:59:52.060 --> 2:59:54.620
 but whatever else you want from my brain, right?

2:59:54.620 --> 2:59:58.060
 And there've been a lot of sci fi stories

2:59:58.060 --> 2:59:59.540
 about hive minds and so on,

2:59:59.540 --> 3:00:02.140
 where people, where experiences

3:00:02.140 --> 3:00:03.620
 can be more broadly shared.

3:00:05.500 --> 3:00:08.540
 And I think one, we don't,

3:00:08.540 --> 3:00:12.140
 I don't pretend to know what it would feel like

3:00:12.140 --> 3:00:16.940
 to be a super intelligent machine,

3:00:16.940 --> 3:00:20.420
 but I'm quite confident that however it feels

3:00:20.420 --> 3:00:22.420
 about mortality and individuality

3:00:22.420 --> 3:00:24.940
 will be very, very different from how it is for us.

3:00:26.660 --> 3:00:30.500
 Well, for us, mortality and finiteness

3:00:30.500 --> 3:00:34.100
 seems to be pretty important at this particular moment.

3:00:34.100 --> 3:00:37.460
 And so all good things must come to an end.

3:00:37.460 --> 3:00:39.100
 Just like this conversation, Max.

3:00:39.100 --> 3:00:40.660
 I saw that coming.

3:00:40.660 --> 3:00:44.660
 Sorry, this is the world's worst translation.

3:00:44.660 --> 3:00:45.820
 I could talk to you forever.

3:00:45.820 --> 3:00:49.100
 It's such a huge honor that you've spent time with me.

3:00:49.100 --> 3:00:50.140
 The honor is mine.

3:00:50.140 --> 3:00:53.380
 Thank you so much for getting me essentially

3:00:53.380 --> 3:00:55.980
 to start this podcast by doing the first conversation,

3:00:55.980 --> 3:00:58.500
 making me realize falling in love

3:00:58.500 --> 3:01:01.140
 with conversation in itself.

3:01:01.140 --> 3:01:03.220
 And thank you so much for inspiring

3:01:03.220 --> 3:01:05.380
 so many people in the world with your books,

3:01:05.380 --> 3:01:07.740
 with your research, with your talking,

3:01:07.740 --> 3:01:12.740
 and with the other, like this ripple effect of friends,

3:01:12.780 --> 3:01:15.460
 including Elon and everybody else that you inspire.

3:01:15.460 --> 3:01:18.140
 So thank you so much for talking today.

3:01:18.140 --> 3:01:21.540
 Thank you, I feel so fortunate

3:01:21.540 --> 3:01:23.620
 that you're doing this podcast

3:01:23.620 --> 3:01:27.780
 and getting so many interesting voices out there

3:01:27.780 --> 3:01:30.940
 into the ether and not just the five second sound bites,

3:01:30.940 --> 3:01:33.060
 but so many of the interviews I've watched you do.

3:01:33.060 --> 3:01:36.140
 You really let people go in into depth

3:01:36.140 --> 3:01:38.660
 in a way which we sorely need in this day and age.

3:01:38.660 --> 3:01:41.740
 That I got to be number one, I feel super honored.

3:01:41.740 --> 3:01:43.500
 Yeah, you started it.

3:01:43.500 --> 3:01:44.660
 Thank you so much, Max.

3:01:45.620 --> 3:01:47.180
 Thanks for listening to this conversation

3:01:47.180 --> 3:01:50.260
 with Max Tegmark, and thank you to our sponsors,

3:01:50.260 --> 3:01:54.860
 the Jordan Harbinger Show, For Sigmatic Mushroom Coffee,

3:01:54.860 --> 3:01:58.940
 BetterHelp Online Therapy, and ExpressVPN.

3:01:58.940 --> 3:02:03.940
 So the choice is wisdom, caffeine, sanity, or privacy.

3:02:04.420 --> 3:02:05.820
 Choose wisely, my friends.

3:02:05.820 --> 3:02:08.740
 And if you wish, click the sponsor links below

3:02:08.740 --> 3:02:11.860
 to get a discount and to support this podcast.

3:02:11.860 --> 3:02:15.100
 And now let me leave you with some words from Max Tegmark.

3:02:15.100 --> 3:02:18.860
 If consciousness is the way that information feels

3:02:18.860 --> 3:02:21.380
 when it's processed in certain ways,

3:02:21.380 --> 3:02:24.220
 then it must be substrate independent.

3:02:24.220 --> 3:02:26.660
 It's only the structure of information processing

3:02:26.660 --> 3:02:29.100
 that matters, not the structure of the matter

3:02:29.100 --> 3:02:31.900
 doing the information processing.

3:02:31.900 --> 3:02:46.900
 Thank you for listening, and hope to see you next time.

