WEBVTT

00:00.000 --> 00:05.260
 The following is a conversation with Wojciech Zaremba, cofounder of OpenAI,

00:05.520 --> 00:09.900
 which is one of the top organizations in the world doing artificial intelligence

00:09.900 --> 00:11.520
 research and development.

00:12.360 --> 00:17.820
 Wojciech is the head of language and cogeneration teams, building and doing

00:17.820 --> 00:27.080
 research on GitHub Copilot, OpenAI Codex, and GPT 3, and who knows, 4, 5, 6,

00:27.080 --> 00:33.800
 and, and, and plus one, and he also previously led OpenAI's robotic efforts.

00:34.400 --> 00:39.320
 These are incredibly exciting projects to me that deeply challenge and expand

00:39.600 --> 00:43.160
 our understanding of the structure and nature of intelligence.

00:43.720 --> 00:49.040
 The 21st century, I think, may very well be remembered for a handful of

00:49.040 --> 00:52.440
 revolutionary AI systems and their implementations.

00:52.440 --> 00:57.520
 GPT, Codex, and applications of language models and transformers in general

00:57.760 --> 01:03.680
 to the language and visual domains may very well be at the core of these AI

01:03.680 --> 01:08.160
 systems. To support this podcast, please check out our sponsors.

01:08.640 --> 01:10.360
 They're listed in the description.

01:11.000 --> 01:14.880
 This is the Lex Friedman podcast, and here is my conversation

01:15.040 --> 01:16.560
 with Wojciech Zaremba.

01:16.560 --> 01:22.960
 You mentioned that Sam Altman asked about the Fermi Paradox, and the people

01:22.960 --> 01:27.200
 at OpenAI had really sophisticated, interesting answers, so that's when you

01:27.200 --> 01:29.200
 knew this is the right team to be working with.

01:29.680 --> 01:33.280
 So let me ask you about the Fermi Paradox, about aliens.

01:34.600 --> 01:38.560
 Why have we not found overwhelming evidence for aliens visiting Earth?

01:39.520 --> 01:42.920
 I don't have a conviction in the answer, but rather kind of probabilistic

01:42.920 --> 01:46.160
 perspective on what might be, let's say, possible answers.

01:46.200 --> 01:51.480
 It's also interesting that the question itself even can touch on the, you

01:51.480 --> 01:54.800
 know, your typical question of what's the meaning of life, because if you

01:54.800 --> 01:58.280
 assume that, like, we don't see aliens because they destroy themselves, that

01:58.280 --> 02:04.160
 kind of upweights the focus on making sure that we won't destroy ourselves.

02:04.400 --> 02:10.160
 At the moment, the place where I am actually with my belief, and these

02:10.160 --> 02:15.200
 things also change over the time, is I think that we might be alone in

02:15.200 --> 02:19.880
 the universe, which actually makes life more, or let's say, consciousness

02:19.880 --> 02:24.360
 life, more kind of valuable, and that means that we should more appreciate it.

02:24.840 --> 02:26.000
 Have we always been alone?

02:26.000 --> 02:29.000
 So what's your intuition about our galaxy, our universe?

02:29.040 --> 02:34.160
 Is it just sprinkled with graveyards of intelligent civilizations, or are

02:34.160 --> 02:37.920
 we truly, is life, intelligent life, truly unique?

02:37.920 --> 02:41.880
 At the moment, my belief that it is unique, but I would say I could also,

02:42.320 --> 02:47.640
 you know, there was like some footage released with UFO objects, which makes

02:47.640 --> 02:49.480
 me actually doubt my own belief.

02:49.560 --> 02:50.040
 Yes.

02:51.040 --> 02:53.760
 Yeah, I can tell you one crazy answer that I have heard.

02:53.960 --> 02:54.280
 Yes.

02:55.280 --> 03:00.520
 So, apparently, when you look actually at the limits of computation, you

03:00.520 --> 03:06.080
 can compute more if the temperature of the universe would drop.

03:06.080 --> 03:08.360
 Temperature of the universe would drop down.

03:09.720 --> 03:15.160
 So one of the things that aliens might want to do if they are truly optimizing

03:15.160 --> 03:18.800
 to maximize amount of compute, which, you know, maybe can lead to, or let's

03:18.800 --> 03:24.120
 say simulations or so, it's instead of wasting current entropy of the

03:24.120 --> 03:27.080
 universe, because, you know, we, by living, we are actually somewhat

03:27.080 --> 03:32.080
 wasting entropy, then you can wait for the universe to cool down such that

03:32.080 --> 03:33.160
 you have more computation.

03:33.360 --> 03:34.560
 So that's kind of a funny answer.

03:34.560 --> 03:37.400
 I'm not sure if I believe in it, but that would be one of the

03:37.400 --> 03:39.520
 reasons why you don't see aliens.

03:39.760 --> 03:44.680
 It's also possible to some people say that maybe there is not that much

03:44.680 --> 03:48.600
 point in actually going to other galaxies if you can go inwards.

03:49.400 --> 03:54.440
 So there is no limits of what could be an experience if we could, you

03:54.440 --> 03:58.600
 know, connect machines to our brains while there are still some limits

03:58.600 --> 03:59.880
 if we want to explore the universe.

03:59.880 --> 04:04.280
 Yeah, there could be a lot of ways to go inwards too.

04:04.960 --> 04:08.720
 Once you figure out some aspect of physics, we haven't figured out yet.

04:08.960 --> 04:11.000
 Maybe you can travel to different dimensions.

04:11.000 --> 04:18.720
 I mean, travel in three dimensional space may not be the most fun kind of travel.

04:19.080 --> 04:22.680
 There may be like just a huge amount of different ways to travel and it

04:22.680 --> 04:28.200
 doesn't require a spaceship going slowly in 3d space to space time.

04:28.200 --> 04:31.920
 It also feels, you know, one of the problems is that speed of light

04:31.920 --> 04:34.200
 is low and the universe is vast.

04:34.760 --> 04:40.680
 And it seems that actually most likely if we want to travel very far, then

04:42.360 --> 04:46.640
 we would, instead of actually sending spaceships with humans that weight a

04:46.640 --> 04:50.720
 lot, we would send something similar to what Yuri Miller is working on.

04:51.000 --> 04:56.200
 These are like a huge sail, which is at first powered or there is a shot of

04:56.200 --> 05:02.400
 laser from an air and it can propel it to quarter of speed of light and sail

05:02.400 --> 05:06.720
 itself contains a few grams of equipment.

05:07.200 --> 05:12.120
 And that might be the way to actually transport matter through universe.

05:12.280 --> 05:16.160
 But then when you think what would it mean for humans, it means that we would

05:16.160 --> 05:20.080
 need to actually put their 3d printer and, you know, 3d print the human on

05:20.080 --> 05:24.400
 other planet, I don't know, play them YouTube or let's say, or like a 3d

05:24.400 --> 05:28.120
 print like huge human right away, or maybe a womb or so, um, yeah.

05:28.960 --> 05:34.080
 With our current techniques of archeology, if, if, if a civilization

05:34.080 --> 05:39.560
 was born and died, uh, long, long enough ago on earth, we wouldn't be able to

05:39.560 --> 05:42.440
 tell, and so that makes me really sad.

05:43.280 --> 05:45.480
 And so I think about earth in that same way.

05:45.520 --> 05:49.640
 How can we leave some remnants if we do destroy ourselves?

05:50.040 --> 05:53.320
 How can we leave remnants for aliens in the future to discover?

05:53.320 --> 05:57.400
 Like, here's some nice stuff we've done, like Wikipedia and YouTube.

05:57.600 --> 06:02.520
 Do we have it like in a satellite orbiting earth with a hard drive?

06:02.920 --> 06:07.080
 Like, how, how do we say, how do we back up human civilization?

06:07.560 --> 06:13.440
 Uh, the good parts or all of it is good parts so that, uh, it can be

06:13.440 --> 06:15.480
 preserved longer than our bodies can.

06:15.920 --> 06:19.720
 That's a, that's kind of, um, it's a difficult question.

06:19.720 --> 06:23.800
 It also requires the difficult acceptance of the fact that we may die.

06:24.040 --> 06:28.040
 And if we die, we may die suddenly as a civilization.

06:28.680 --> 06:32.160
 So let's see, I think it kind of depends on the cataclysm.

06:32.480 --> 06:37.680
 We have observed in other parts of the universe that birds of gamma rays, uh,

06:38.160 --> 06:42.720
 these are, uh, high energy, uh, rays of light that actually can

06:42.800 --> 06:44.440
 apparently kill entire galaxy.

06:44.440 --> 06:48.560
 So there might be actually nothing, even to, nothing to protect us from it.

06:48.960 --> 06:51.560
 I'm also, and I'm looking actually at the past civilizations.

06:51.560 --> 06:56.800
 So it's like Aztecs or so they disappear from the surface of the earth.

06:56.880 --> 06:59.800
 And one can ask, why is it the case?

07:00.240 --> 07:06.520
 And the way I'm thinking about it is, you know, that definitely they had some

07:06.520 --> 07:10.720
 problem that they couldn't solve and maybe there was a flood and all of a

07:10.720 --> 07:14.520
 sudden they couldn't drink, uh, there was no potable water and they all died.

07:15.040 --> 07:24.200
 And, um, I think that, uh, so far the best solution to such a problems is I

07:24.200 --> 07:27.960
 guess, technology, so, I mean, if they would know that you can just boil

07:27.960 --> 07:31.680
 water and then drink it after, then that would save their civilization.

07:31.920 --> 07:36.040
 And even now, when we look actually at the current pandemic, it seems

07:36.040 --> 07:38.680
 that there, once again, actually science comes to rest.

07:38.680 --> 07:42.040
 And somehow science increases size of the action space.

07:42.440 --> 07:44.000
 And I think that's a good thing.

07:44.600 --> 07:44.800
 Yeah.

07:44.800 --> 07:51.440
 But nature has a vastly larger action space, but still it might be a good thing

07:51.440 --> 07:53.200
 for us to keep on increasing action space.

07:55.080 --> 07:55.720
 Okay.

07:56.200 --> 07:58.000
 Uh, looking at past civilizations.

07:58.000 --> 07:58.520
 Yes.

07:58.920 --> 08:04.560
 But looking at the destruction of human civilization, perhaps expanding the

08:04.560 --> 08:12.880
 action space will add, um, actions that are easily acted upon, easily executed

08:12.920 --> 08:15.400
 and as a result, destroy us.

08:15.960 --> 08:21.800
 So let's see, I was pondering, uh, why actually even, uh, we have

08:21.800 --> 08:23.880
 negative impact on the, uh, globe.

08:24.200 --> 08:27.680
 Because, you know, if you ask every single individual, they

08:27.680 --> 08:28.880
 would like to have clean air.

08:29.720 --> 08:32.320
 They would like healthy planet, but somehow it's not.

08:32.320 --> 08:35.800
 It's not the case that as a collective, we are not going in this direction.

08:36.840 --> 08:41.040
 I think that there exists very powerful system to describe what we value.

08:41.080 --> 08:41.960
 That's capitalism.

08:42.000 --> 08:45.280
 It assigns actually monetary values to various activities.

08:45.760 --> 08:49.000
 At the moment, the problem in the current system is that there's

08:49.000 --> 08:50.360
 some things which we value.

08:50.680 --> 08:52.320
 There is no cost assigned to it.

08:52.320 --> 09:00.240
 So even though we value clean air, or maybe we also, uh, value, uh,

09:00.240 --> 09:06.000
 value lack of destruction on, let's say internet or so at the moment, these

09:06.000 --> 09:10.680
 quantities, you know, companies, corporations can pollute them, uh, for free.

09:11.680 --> 09:19.640
 So in some sense, I wished or like, and that's, I guess, purpose of politics

09:20.040 --> 09:22.960
 to, to align the incentive systems.

09:23.000 --> 09:25.680
 And we are kind of maybe even moving in this direction.

09:25.680 --> 09:28.920
 The first issue is even to be able to measure the things that we value.

09:28.920 --> 09:32.080
 Then we can actually assign the monetary value to them.

09:32.720 --> 09:32.840
 Yeah.

09:32.840 --> 09:38.000
 And that's, so it's getting the data and also probably through technology,

09:38.040 --> 09:44.640
 enabling people to vote and to move money around in a way that is aligned

09:44.640 --> 09:48.320
 with their values, and that's very much a technology question.

09:48.720 --> 09:55.880
 So like having one president and Congress and voting that happens every four years

09:55.880 --> 09:59.720
 or something like that, that's a very outdated idea that could be some

09:59.720 --> 10:01.840
 technological improvements to that kind of idea.

10:02.080 --> 10:06.400
 So I'm thinking from time to time about these topics, but it's also feels to me

10:06.400 --> 10:10.240
 that it's, it's a little bit like, uh, it's hard for me to actually make

10:10.240 --> 10:11.160
 correct predictions.

10:11.160 --> 10:12.920
 What is the appropriate thing to do?

10:13.120 --> 10:20.480
 I extremely trust, uh, Sam Altman, our CEO on these topics here, um, like, uh,

10:20.560 --> 10:24.200
 I'm more on the side of being, I guess, naive hippie.

10:24.200 --> 10:29.400
 That, uh, yeah, that's your life philosophy.

10:29.400 --> 10:37.200
 Um, well, like I think self doubt and, uh, I think hippie implies optimism.

10:37.480 --> 10:40.520
 Those, those two things are pretty, pretty good way to operate.

10:41.080 --> 10:46.920
 I mean, still, it is hard for me to actually understand how the politics

10:46.920 --> 10:51.440
 works or like, uh, how this, like, uh, exactly how the things would play out.

10:51.440 --> 10:53.720
 And Sam is, uh, really excellent with it.

10:54.120 --> 10:56.480
 What do you think is rarest in the universe?

10:56.560 --> 10:57.920
 You said we might be alone.

10:58.560 --> 11:02.880
 What's hardest to build is another engineering way to ask that life,

11:03.360 --> 11:05.480
 intelligence or consciousness.

11:05.680 --> 11:11.000
 So like you said that we might be alone, which is the thing that's hardest to get

11:11.000 --> 11:13.280
 to, is it just the origin of life?

11:13.680 --> 11:15.320
 Is it the origin of intelligence?

11:15.640 --> 11:17.800
 Is it the origin of consciousness?

11:17.800 --> 11:23.080
 So, um, let me at first explain to you my kind of mental model, what I think

11:23.080 --> 11:24.760
 is needed for life to appear.

11:25.560 --> 11:32.680
 Um, so I imagine that at some point there was this primordial, uh, soup of, uh,

11:32.840 --> 11:38.240
 amino acids and maybe some proteins in the ocean and, uh, you know, some

11:38.240 --> 11:42.640
 proteins were turning into some other proteins through reaction and, uh, you

11:42.640 --> 11:46.480
 can also, uh, you know, you can, you know, you can, you know, you can

11:46.480 --> 11:52.160
 and, uh, you can almost think about this, uh, cycle of what, uh, turns into what

11:52.200 --> 11:55.680
 as there is a graph essentially describing which substance turns into

11:55.840 --> 12:00.120
 some other substance and essentially life means that all of a sudden in the graph

12:00.480 --> 12:04.640
 has been created that cycle such that the same thing keeps on happening over

12:04.640 --> 12:07.280
 and over again, that's what is needed for life to happen.

12:07.280 --> 12:11.840
 And in some sense, you can think almost that you have this gigantic graph and it

12:12.000 --> 12:15.400
 needs like a sufficient number of edges for the cycle to appear.

12:15.400 --> 12:21.760
 Um, then, um, from perspective of intelligence and consciousness, uh, my

12:21.760 --> 12:25.960
 current intuition is that they might be quite intertwined.

12:26.280 --> 12:29.160
 First of all, it might not be that it's like a binary thing that you

12:29.160 --> 12:30.560
 have intelligence or consciousness.

12:30.720 --> 12:36.360
 It seems to be, uh, uh, more, uh, continuous component.

12:36.800 --> 12:41.320
 Let's see, if we look for instance on the event networks, uh, recognizing

12:41.320 --> 12:45.920
 images and people are able to show that the activations of these networks

12:46.120 --> 12:51.680
 correlate very strongly, uh, with activations in visual cortex, uh, of

12:51.760 --> 12:55.520
 some monkeys, the same seems to be true about language models.

12:56.080 --> 13:04.320
 Um, also if you, for instance, um, look, um, if you train agent in, um, 3d

13:04.320 --> 13:09.880
 world, um, at first, you know, it, it, it, it barely recognizes what is going

13:09.880 --> 13:14.640
 on over the time, it kind of recognizes foreground from a background over the

13:14.640 --> 13:18.520
 time, it kind of knows where there is a foot, uh, and it just follows it.

13:18.880 --> 13:22.640
 Um, over the time it actually starts having a 3d perception.

13:22.800 --> 13:27.480
 So it is possible for instance, to look inside of the head of an agent and ask,

13:27.480 --> 13:29.280
 what would it see if it looks to the right?

13:29.760 --> 13:33.600
 And the crazy thing is, you know, initially when the agents are barely

13:33.600 --> 13:37.200
 trained, that these predictions are pretty bad over the time they become

13:37.200 --> 13:42.840
 better and better, you can still see that if you ask what happens when the

13:42.840 --> 13:47.400
 head is turned by 360 degrees for some time, they think that the different

13:47.400 --> 13:51.440
 thing appears and then at some stage they understand actually that the same

13:51.440 --> 13:52.440
 thing supposed to appear.

13:52.640 --> 13:55.520
 So they get that understanding of 3d structure.

13:55.760 --> 14:01.960
 It's also, you know, very likely that they have inside some level of, of like

14:01.960 --> 14:06.720
 a symbolic reasoning, like a particular, these symbols for other agents.

14:06.720 --> 14:13.600
 So when you look at DOTA agents, they collaborate together and, uh, and, uh,

14:13.800 --> 14:17.880
 no, they, they, they, they have some anticipation of, uh, if, if they would

14:17.880 --> 14:21.720
 win battle, they have some, some expectations with respect to other

14:21.720 --> 14:22.120
 agents.

14:22.360 --> 14:26.160
 I might be, you know, too much anthropomorphizing, um, the, the, the,

14:26.160 --> 14:31.400
 how the things look, look, look for me, but then the fact that they have a

14:31.400 --> 14:37.160
 symbol for other agents, uh, makes me believe that, uh, at some stage as the,

14:37.440 --> 14:41.400
 uh, you know, as they are optimizing for skills, they would have also symbol to

14:41.400 --> 14:42.400
 describe themselves.

14:43.360 --> 14:46.040
 Uh, this is like a very useful symbol to have.

14:46.400 --> 14:50.280
 And this particularity, I would call it like a self consciousness or self

14:50.280 --> 14:55.120
 awareness, uh, and, uh, still it might be different from the consciousness.

14:55.280 --> 14:59.800
 So I guess the, the way how I'm understanding the word consciousness,

14:59.800 --> 15:03.120
 I'd say the experience of drinking a coffee or let's say experience of being

15:03.120 --> 15:06.280
 a bat, that's the meaning of the word consciousness.

15:06.280 --> 15:07.400
 It doesn't mean to be awake.

15:07.760 --> 15:13.480
 Uh, yeah, it feels, it might be also somewhat related to memory and

15:13.480 --> 15:14.560
 recurrent connections.

15:14.840 --> 15:21.480
 So, um, it's kind of like, if you look at anesthetic drugs, they might be, uh,

15:21.480 --> 15:30.200
 uh, like, uh, that they essentially, they, they disturb, uh, uh, brainwaves, uh, such

15:30.200 --> 15:33.320
 that, um, maybe memories, not, not form.

15:33.960 --> 15:37.160
 And so there's a lessening of consciousness when you do that.

15:37.280 --> 15:37.680
 Correct.

15:37.840 --> 15:40.720
 And so that's the one way to intuit what is consciousness.

15:41.040 --> 15:45.040
 There's also kind of another element here.

15:45.360 --> 15:49.480
 It could be that it's, you know, this kind of self awareness

15:49.480 --> 15:56.320
 module that you described, plus the actual subjective experience is a

15:56.360 --> 16:04.600
 storytelling module that tells us a story about, uh, what we're experiencing.

16:05.160 --> 16:06.960
 The crazy thing.

16:06.960 --> 16:11.200
 So let's say, I mean, in meditation, they teach people not to speak

16:11.200 --> 16:12.280
 story inside of their head.

16:12.800 --> 16:17.280
 And there is also some fraction of population who doesn't have actually

16:17.280 --> 16:22.120
 a narrator, I know people who don't have a narrator and, you know, they have

16:22.120 --> 16:27.760
 to use external people in order to, um, kind of solve tasks that

16:27.760 --> 16:29.920
 require internal narrator.

16:30.360 --> 16:35.040
 Um, so it seems that it's possible to have the experience without the talk.

16:37.440 --> 16:40.680
 What are we talking about when we talk about the internal narrator?

16:41.080 --> 16:44.000
 Is that the voice when you're like, yeah, I thought that that's what you are

16:44.000 --> 16:49.960
 referring to while I was referring more on the, like, not an actual voice.

16:51.120 --> 16:58.200
 I meant like, there's some kind of like subjective experience feels like it's.

17:00.400 --> 17:03.840
 It's fundamentally about storytelling to ourselves.

17:04.560 --> 17:13.760
 It feels like, like the feeling is a story that is much, uh, much

17:13.760 --> 17:16.800
 simpler abstraction than the raw sensory information.

17:17.400 --> 17:23.360
 So there feels like it's a very high level of abstraction that, uh, is useful

17:23.960 --> 17:27.000
 for me to feel like entity in this world.

17:27.280 --> 17:35.920
 M most useful aspect of it is that because I'm conscious, I think there's

17:35.920 --> 17:39.400
 an intricate connection to me, not wanting to die.

17:39.400 --> 17:46.160
 So like, it's a useful hack to really prioritize not dying, like those

17:46.160 --> 17:47.520
 seem to be somehow connected.

17:47.560 --> 17:50.440
 So I'm telling the story of like, it's rich.

17:50.440 --> 17:55.200
 He feels like something to be me and the fact that me exists in this world.

17:55.200 --> 17:56.280
 I want to preserve me.

17:56.920 --> 17:58.840
 And so that makes it a useful agent hack.

17:59.280 --> 18:03.720
 So I will just refer maybe to that first part, as you said, about that kind

18:03.720 --> 18:05.800
 of story of describing who you are.

18:05.800 --> 18:13.600
 Um, I was, uh, thinking about that even, so, you know, obviously I'm, I, I like

18:13.600 --> 18:18.280
 thinking about consciousness, uh, I like thinking about AI as well, and I'm trying

18:18.280 --> 18:21.840
 to see analogies of these things in AI, what would it correspond to?

18:22.520 --> 18:34.360
 So, um, um, you know, open AI train, uh, uh, a model called GPT, uh, which, uh,

18:34.360 --> 18:42.280
 can generate, uh, pretty, I'm using texts on arbitrary topic and, um, um, and one

18:42.280 --> 18:49.400
 way to control GPT is, uh, by putting into prefix at the beginning of the text, some

18:49.400 --> 18:55.120
 information, what would be the story about, uh, you can have even chat with, uh, uh,

18:55.400 --> 19:01.200
 you know, with GPT by saying that the chat is with Lex or Elon Musk or so, and, uh,

19:01.200 --> 19:08.720
 GPT would just pretend to be you or Elon Musk or so, and, uh, uh, it almost feels

19:08.720 --> 19:14.960
 that this, uh, story that we give ourselves to describe our life, it's almost like, uh,

19:15.360 --> 19:17.320
 things that you put into context of GPT.

19:17.360 --> 19:17.640
 Yeah.

19:17.680 --> 19:25.400
 The primary, it's the, and so, but the context we provide to GPT is, uh, is multimodal.

19:25.520 --> 19:27.760
 It's more so GPT itself is multimodal.

19:27.760 --> 19:33.120
 GPT itself, uh, hasn't learned actually from experience of single human, but from the

19:33.120 --> 19:35.360
 experience of humanity, it's a chameleon.

19:35.520 --> 19:42.000
 You can turn it into anything and in some sense, by providing context, um, it, you

19:42.000 --> 19:44.920
 know, behaves as the thing that you wanted it to be.

19:45.160 --> 19:49.840
 Um, it's interesting that the, you know, people have a stories of who they are.

19:50.520 --> 19:54.400
 And, uh, as you said, these stories, they help them to operate in the world.

19:54.400 --> 19:59.800
 Um, but it's also, you know, interesting, I guess, various people find it out through

19:59.800 --> 20:05.400
 meditation or so that, uh, there might be some patterns that you have learned when

20:05.400 --> 20:07.800
 you were a kid that actually are not serving you anymore.

20:08.600 --> 20:13.320
 And you also might be thinking that that's who you are and that's actually just a story.

20:13.320 --> 20:13.840
 Mm hmm.

20:15.040 --> 20:15.240
 Yeah.

20:15.240 --> 20:18.160
 So it's a useful hack, but sometimes it gets us into trouble.

20:18.240 --> 20:19.280
 It's a local optima.

20:19.360 --> 20:20.200
 It's a local optima.

20:20.200 --> 20:24.880
 You wrote that Stephen Hawking, he tweeted, Stephen Hawking asked what

20:24.880 --> 20:29.440
 breathes fire into equations, which meant what makes given mathematical

20:29.440 --> 20:32.720
 equations realize the physics of a universe.

20:33.120 --> 20:37.200
 Similarly, I wonder what breathes fire into computation.

20:37.520 --> 20:39.920
 What makes given computation conscious?

20:40.600 --> 20:41.000
 Okay.

20:41.240 --> 20:44.360
 So how do we engineer consciousness?

20:44.400 --> 20:47.280
 How do you breathe fire and magic?

20:47.280 --> 20:51.240
 How do you breathe fire and magic into the machine?

20:51.800 --> 20:57.200
 So, um, it seems clear to me that not every computation is conscious.

20:57.280 --> 21:01.520
 I mean, you can, let's say, just keep on multiplying one matrix over and over

21:01.520 --> 21:03.920
 again and might be gigantic matrix.

21:03.920 --> 21:05.400
 You can put a lot of computation.

21:05.480 --> 21:06.880
 I don't think it would be conscious.

21:07.080 --> 21:13.000
 So in some sense, the question is, uh, what are the computations which could be

21:13.000 --> 21:18.280
 conscious, uh, I mean, so, so one assumption is that it has to do purely

21:18.280 --> 21:21.960
 with computation that you can abstract away matter and other possibilities

21:22.160 --> 21:25.400
 that it's very important was the realization of computation that it has

21:25.400 --> 21:30.200
 to do with some, uh, uh, force fields or so, and they bring consciousness.

21:30.520 --> 21:33.440
 At the moment, my intuition is that it can be fully abstracted away.

21:33.680 --> 21:38.240
 So in case of computation, you can ask yourself, what are the mathematical

21:38.280 --> 21:41.440
 objects or so that could bring such a properties?

21:41.440 --> 21:49.000
 So for instance, if we think about the models, uh, AI models, the, what they

21:49.000 --> 21:57.000
 truly try to do, uh, or like a models like GPT is, uh, uh, you know, they try

21:57.000 --> 22:00.320
 to predict, uh, next word or so.

22:00.480 --> 22:05.920
 And this turns out to be equivalent to, uh, compressing, uh, text.

22:05.920 --> 22:11.120
 Um, and, uh, because in some sense, compression means that, uh, you learn

22:11.120 --> 22:16.040
 the model of reality and you have just to, uh, remember where are your mistakes.

22:16.320 --> 22:20.640
 The better you are in predicting the, and, and, and in some sense, when we

22:20.640 --> 22:24.040
 look at our experience, also, when you look, for instance, at the car driving,

22:24.080 --> 22:27.400
 you know, in which direction it will go, you are good like in prediction.

22:27.720 --> 22:32.880
 And, um, you know, it might be the case that the consciousness is intertwined

22:32.880 --> 22:38.080
 with, uh, compression, it might be also the case that self consciousness, uh,

22:38.400 --> 22:41.000
 has to do with compress or trying to compress itself.

22:41.280 --> 22:43.360
 So, um, okay.

22:43.600 --> 22:47.640
 I was just wondering, what are the objects in, you know, mathematics or

22:47.640 --> 22:52.360
 computer science, which are mysterious that could, uh, that, that, that could

22:52.360 --> 22:53.520
 have to do with consciousness.

22:53.520 --> 22:59.680
 And then I thought, um, you know, you, you see in mathematics, there is

22:59.680 --> 23:03.720
 something called Gadel theorem, uh, which means, okay, you have, if you have

23:03.720 --> 23:08.440
 sufficiently complicated mathematical system, it is possible to point the

23:08.440 --> 23:10.600
 mathematical system back on itself.

23:10.800 --> 23:14.040
 In computer science, there is, uh, something called helping problem.

23:14.280 --> 23:16.480
 It's, it's somewhat similar construction.

23:16.800 --> 23:22.960
 So I thought that, you know, if we believe that, uh, that, uh, that under

23:22.960 --> 23:28.320
 assumption that consciousness has to do with, uh, with compression, uh, then

23:28.320 --> 23:32.760
 you could imagine that the, that the, as you keep on compressing things, then

23:32.760 --> 23:36.640
 at some point, it actually makes sense for the compressor to compress itself.

23:36.720 --> 23:40.080
 Metacompression consciousness is metacompression.

23:40.760 --> 23:43.480
 That's a, that's an I, an, an, an idea.

23:44.360 --> 23:47.280
 And in some sense, you know, the crazy, thank you.

23:47.280 --> 23:52.280
 So, uh, but do you think if we think of a Turing machine, a universal

23:52.280 --> 23:55.880
 Turing machine, can that achieve consciousness?

23:55.880 --> 24:00.240
 So is there some thing beyond our traditional definition

24:00.240 --> 24:02.000
 of computation that's required?

24:02.200 --> 24:03.920
 So it's a specific computation.

24:03.920 --> 24:08.760
 And I said, this computation has to do with compression and, uh, the compression

24:08.760 --> 24:13.040
 itself, maybe other way of putting it is like, uh, you are internally creating

24:13.040 --> 24:18.040
 the model of reality in order, like, uh, it's like a, you try inside to simplify

24:18.040 --> 24:20.200
 reality in order to predict what's going to happen.

24:20.200 --> 24:25.080
 And, um, that also feels somewhat similar to how I think actually about my own

24:25.200 --> 24:28.680
 conscious experience, though clearly I don't have access to reality.

24:29.040 --> 24:33.240
 The only access to reality is through, you know, cable going to my brain and my

24:33.240 --> 24:37.320
 brain is creating a simulation of reality and I have access to the simulation of

24:37.320 --> 24:37.840
 reality.

24:38.400 --> 24:43.520
 Are you by any chance, uh, aware of, uh, the Hutter prize, Marcus Hutter?

24:44.200 --> 24:48.160
 He, uh, he made this prize for compression.

24:48.160 --> 24:53.080
 Uh, Wikipedia pages, and, uh, there's a few qualities to it.

24:53.560 --> 24:57.640
 One, I think has to be perfect compression, which makes, I think that

24:57.640 --> 25:03.520
 little cork makes it much less, um, applicable to the general task of

25:03.520 --> 25:06.920
 intelligence, because it feels like intelligence is always going to be messy.

25:07.720 --> 25:14.280
 Uh, like perfect compression is feels like it's not the right goal, but

25:14.280 --> 25:19.280
 it's nevertheless a very interesting goal.

25:19.280 --> 25:22.000
 So for him, intelligence equals compression.

25:22.680 --> 25:29.240
 And so the smaller you make the file, given a large Wikipedia page, the

25:29.240 --> 25:31.000
 more intelligent the system has to be.

25:31.200 --> 25:31.920
 Yeah, that makes sense.

25:31.920 --> 25:34.520
 So you can make perfect compression if you store errors.

25:34.920 --> 25:37.960
 And I think that actually what he meant is you have algorithm plus errors.

25:37.960 --> 25:44.720
 Uh, by the way, Hutter, Hutter is, uh, he was a PhD advisor of Sean

25:44.720 --> 25:48.360
 Leck, who is a DeepMind, uh, uh, DeepMind cofounder.

25:48.600 --> 25:48.920
 Yeah.

25:49.080 --> 25:49.360
 Yeah.

25:49.360 --> 25:53.600
 So there's an interesting, uh, and now he's a DeepMind, there's an

25:53.600 --> 25:55.720
 interesting, uh, network of people.

25:55.720 --> 26:02.360
 And he's one of the people that I think seriously took on the task of

26:02.680 --> 26:04.960
 what would an AGI system look like?

26:04.960 --> 26:11.760
 Uh, I think for a longest time, the question of AGI was not taken

26:12.680 --> 26:14.960
 seriously or rather rigorously.

26:15.640 --> 26:19.800
 And he did just that, like mathematically speaking, what

26:19.800 --> 26:23.440
 would the model look like if you remove the constraints of it, having to be,

26:23.440 --> 26:31.880
 uh, um, having to have a reasonable amount of memory, reasonable amount

26:31.880 --> 26:36.400
 of, uh, running time, complexity, uh, computation time, what would it look

26:36.400 --> 26:41.760
 like and essentially it's, it's a half math, half philosophical discussion

26:41.760 --> 26:45.000
 of, uh, how would it like a reinforcement learning type of

26:45.240 --> 26:47.360
 framework look like for an AGI?

26:47.520 --> 26:47.800
 Yeah.

26:47.800 --> 26:51.640
 So he developed the framework even to describe what's optimal with

26:51.640 --> 26:53.240
 respect to reinforcement learning.

26:53.240 --> 26:57.040
 Like there is a theoretical framework, which is, as you said, under assumption,

26:57.040 --> 26:59.000
 there is infinite amount of memory and compute.

26:59.000 --> 27:03.560
 Um, there was actually one person before his name is Solomonov, who

27:03.560 --> 27:07.840
 there extended, uh, Solomonov work to reinforcement learning, but there

27:07.840 --> 27:13.640
 exists the, uh, theoretical algorithm, which is optimal algorithm to build

27:13.840 --> 27:16.480
 intelligence and I can actually explain you the algorithm.

27:16.560 --> 27:16.880
 Yes.

27:18.080 --> 27:18.520
 Let's go.

27:18.960 --> 27:19.400
 Let's go.

27:19.880 --> 27:26.680
 So the task itself, can I just pause how absurd it is for brain in a

27:26.680 --> 27:30.800
 skull, trying to explain the algorithm for intelligence, just go ahead.

27:31.120 --> 27:32.160
 It is pretty crazy.

27:32.160 --> 27:34.640
 It is pretty crazy that, you know, the brain itself is actually so

27:34.640 --> 27:40.960
 small and it can ponder, uh, how to design algorithms that optimally

27:40.960 --> 27:42.520
 solve the problem of intelligence.

27:42.560 --> 27:42.840
 Okay.

27:43.440 --> 27:43.640
 All right.

27:43.640 --> 27:44.640
 So what's the algorithm?

27:44.920 --> 27:46.080
 So let's see.

27:46.120 --> 27:51.560
 So first of all, the task itself is, uh, described as, uh, you have infinite

27:51.560 --> 27:52.960
 sequence of zeros and ones.

27:53.560 --> 27:53.840
 Okay.

27:53.840 --> 27:58.600
 Okay. You read, uh, N bits and they are about to predict N plus one bit.

27:59.120 --> 28:00.160
 So that's the task.

28:00.160 --> 28:04.120
 And you could imagine that every task could be casted as such a task.

28:04.440 --> 28:08.800
 So if for instance, you have images and labels, you can just turn every image

28:08.800 --> 28:12.960
 into a sequence of zeros and ones, then label, you concatenate labels and

28:12.960 --> 28:16.680
 you, and that that's actually the, the, and you could, you could start by

28:16.680 --> 28:20.480
 having training data first, and then afterwards you have test data.

28:20.480 --> 28:25.480
 So theoretically any problem could be casted as a problem of predicting

28:25.480 --> 28:28.120
 zeros and ones on this, uh, infinite tape.

28:28.320 --> 28:35.240
 So, um, so let's say you read already N bits and you want to predict N plus

28:35.240 --> 28:42.160
 one bit, and I will ask you to write every possible program that generates

28:42.160 --> 28:42.880
 these N bits.

28:43.560 --> 28:43.760
 Okay.

28:43.760 --> 28:47.880
 So, um, and you can have, you, you choose programming language.

28:47.880 --> 28:49.720
 It can be Python or C plus plus.

28:49.720 --> 28:53.480
 And the difference between programming languages, uh, might be, there is

28:53.480 --> 28:57.760
 a difference by constant asymptotically, your predictions will be equivalent.

28:59.160 --> 29:04.080
 So you read N bits, you enumerate all the programs that produce

29:04.080 --> 29:06.040
 these N bits in their output.

29:06.680 --> 29:13.160
 And then in order to predict N plus one bit, you actually weight the programs

29:13.480 --> 29:14.600
 according to their length.

29:15.440 --> 29:18.480
 And there is like a, some specific formula, how you weight them.

29:18.480 --> 29:24.120
 And then the N plus, uh, one bit prediction is the prediction, uh, from each

29:24.120 --> 29:26.040
 of these program, according to that weight.

29:27.040 --> 29:31.880
 Like statistically, you pick, so the smaller the program, the more likely

29:31.880 --> 29:34.680
 you, you are to pick the, its output.

29:35.480 --> 29:42.280
 So, uh, that's, that algorithm is grounded in the hope or the intuition

29:42.280 --> 29:43.960
 that the simple answer is the right one.

29:44.600 --> 29:46.000
 It's a formalization of it.

29:46.000 --> 29:52.040
 Um, it also means like, if you would ask the question after how many years

29:52.600 --> 29:58.080
 would, you know, sun explode, uh, you can say, hmm, it's more likely

29:58.080 --> 30:01.560
 the answer is due to some power because they're shorter program.

30:02.160 --> 30:02.440
 Yeah.

30:02.920 --> 30:08.240
 Um, then other, well, I don't have a good intuition about, uh, how different

30:08.240 --> 30:12.120
 the space of short programs are from the space of large programs.

30:12.120 --> 30:17.600
 Like, what is the universe where short programs, uh, like run things?

30:18.520 --> 30:21.960
 Uh, so, so I said, the things have to agree with N bits.

30:22.160 --> 30:25.600
 So even if you have, you, you need to start, okay.

30:25.640 --> 30:29.920
 If, if you have very short program and they're like a steel, some has, if, if

30:29.920 --> 30:33.520
 it's not perfectly prediction of N bits, you have to start errors.

30:33.760 --> 30:34.520
 What are the errors?

30:34.520 --> 30:37.120
 And that gives you the full program that agrees on N bits.

30:38.200 --> 30:40.160
 Oh, so you don't agree with the N bits.

30:40.160 --> 30:44.280
 And you store, that's like a longer, a longer program, slightly longer program

30:45.000 --> 30:47.120
 because it can take these extra bits of errors.

30:47.440 --> 30:48.440
 That's fascinating.

30:48.480 --> 30:55.920
 What's what's your intuition about the, the programs that are able to do cool

30:55.920 --> 31:02.560
 stuff like intelligence and consciousness, are they, uh, perfectly like, is, is it,

31:02.560 --> 31:05.400
 uh, is there if then statements in them?

31:05.680 --> 31:08.920
 So like, is there a lot of a good, uh, if then statements in them?

31:08.920 --> 31:11.240
 So like, is there a lot of exceptions that they're storing?

31:11.480 --> 31:15.800
 So, um, you could imagine if there would be tremendous amount of if statements,

31:16.280 --> 31:17.560
 then they wouldn't be that short.

31:17.720 --> 31:23.520
 In case of neural networks, you could imagine that, um, what happens is, uh,

31:24.280 --> 31:29.840
 they, uh, when you start with an initialized neural network, uh, it stores

31:29.840 --> 31:34.840
 internally many possibilities, how the, uh, how the problem can be solved.

31:34.840 --> 31:42.320
 And SGD is kind of magnifying some, some, uh, some, uh, paths, which are slightly

31:42.720 --> 31:44.000
 similar to the correct answer.

31:44.000 --> 31:45.960
 So it's kind of magnifying correct programs.

31:46.280 --> 31:50.960
 And in some sense, SGD is a search algorithm in the program space and the

31:50.960 --> 31:56.440
 program space is represented by, uh, you know, kind of the wiring inside of the

31:56.440 --> 32:00.760
 neural network and there's like an insane number of ways how the features can be

32:00.760 --> 32:01.280
 computed.

32:01.280 --> 32:05.080
 Let me ask you the high level, basic question that's not so basic.

32:05.720 --> 32:07.760
 What is deep learning?

32:08.480 --> 32:11.960
 Is there a way you'd like to think of it that is different than like

32:11.960 --> 32:13.640
 a generic textbook definition?

32:14.360 --> 32:19.160
 The thing that I hinted just a second ago is maybe that, uh, closest to how I'm

32:19.160 --> 32:21.200
 thinking these days about deep learning.

32:21.600 --> 32:29.240
 So, uh, now the statement is, uh, neural networks can represent some programs.

32:29.240 --> 32:33.320
 Uh, it seems that various modules that we are actually adding up to, or like, uh,

32:33.600 --> 32:37.520
 you know, we, we want networks to be deep because we, we want multiple

32:37.520 --> 32:45.160
 steps of the computation and, uh, uh, and deep learning provides the way to

32:45.160 --> 32:48.920
 represent space of programs, which is searchable and it's searchable with,

32:48.920 --> 32:50.800
 uh, stochastic gradient descent.

32:50.840 --> 32:56.600
 So we have an algorithm to search over humongous number of programs and

32:56.600 --> 33:01.160
 gradient descent kind of bubbles up the things that are, uh, tend to give correct

33:01.160 --> 33:01.640
 answers.

33:01.800 --> 33:09.800
 So a neural network with a, with fixed weights that's optimized, do you think

33:09.800 --> 33:11.160
 of that as a single program?

33:11.400 --> 33:18.280
 Um, so there is a, uh, work by Christopher Olaj where he, uh, so he works on

33:18.360 --> 33:24.800
 interpretability of neural networks and he was able to, uh, to identify the

33:24.800 --> 33:29.920
 neural network, for instance, a detector of a wheel for a car, or the detector of

33:29.920 --> 33:35.120
 a mask for a car, and then he was able to separate them out and assemble them, uh,

33:35.280 --> 33:39.960
 together using a simple program, uh, for the detector, for a car detector.

33:40.400 --> 33:44.440
 That's like, uh, if you think of traditionally defined programs, that's

33:44.440 --> 33:48.240
 like a function within a program that this particular neural network was able

33:48.240 --> 33:53.000
 to find and you can tear that out, just like you can copy and paste it into a

33:53.000 --> 33:59.960
 stack overflow that, so, uh, any program is a composition of smaller programs.

34:00.520 --> 34:00.760
 Yeah.

34:00.760 --> 34:04.880
 I mean, the nice thing about the neural networks is that it allows the things

34:04.880 --> 34:07.040
 to be more fuzzy than in case of programs.

34:07.360 --> 34:11.440
 Uh, in case of programs, you have this, like a branching this way or that way.

34:11.760 --> 34:16.240
 And the neural networks, they, they have an easier way to, to be somewhere in

34:16.240 --> 34:18.080
 between or to share things.

34:18.080 --> 34:23.360
 What is the most beautiful or surprising idea in deep learning and the utilization

34:23.360 --> 34:27.080
 of these neural networks, which by the way, for people who are not familiar,

34:27.800 --> 34:32.840
 neural networks is a bunch of, uh, what would you say it's inspired by the human

34:32.840 --> 34:37.080
 brain, there's neurons, there's connection between those neurons, there's inputs and

34:37.080 --> 34:41.840
 there's outputs and there's millions or billions of those neurons and the

34:41.840 --> 34:44.800
 learning happens in the neural network.

34:44.800 --> 34:52.000
 Neurons and the learning happens, uh, by adjusting the weights on the

34:52.000 --> 34:53.800
 edges that connect these neurons.

34:54.160 --> 34:58.320
 Thank you for giving definition that I supposed to do it, but I guess you have

34:58.320 --> 35:02.680
 enough empathy to listeners to actually know that the, that that might be useful.

35:02.760 --> 35:07.480
 No, that's like, so I'm asking Plato of like, what is the meaning of life?

35:07.480 --> 35:08.800
 He's not going to answer.

35:09.320 --> 35:13.320
 You're being philosophical and deep and quite profound talking about the space

35:13.320 --> 35:17.120
 of programs, which is, which is very interesting, but also for people who

35:17.120 --> 35:20.360
 just not familiar with the hell we're talking about when we talk about deep

35:20.360 --> 35:25.840
 learning anyway, sorry, what is the most beautiful or surprising idea to you in,

35:25.920 --> 35:29.560
 in, um, in all the time you've worked at deep learning and you worked on a lot of.

35:30.040 --> 35:34.480
 Fascinating projects, applications of neural networks.

35:35.240 --> 35:36.920
 It doesn't have to be big and profound.

35:36.920 --> 35:38.000
 It can be a cool trick.

35:38.240 --> 35:38.760
 Yeah.

35:38.920 --> 35:42.520
 I mean, I'm thinking about the trick, but like, uh, it's still, uh, I'm using

35:42.520 --> 35:47.360
 to me that it works at all that let's say that the extremely simple algorithm

35:47.360 --> 35:52.120
 stochastic gradient descent, which is something that I would be able to derive

35:52.120 --> 35:58.120
 on the piece of paper to high school student, uh, when put at the, at the

35:58.120 --> 36:03.360
 scale of, you know, thousands of machines actually, uh, can create the.

36:03.880 --> 36:07.520
 Behaviors we, which we called kind of human like behaviors.

36:07.960 --> 36:11.760
 So in general, any application is stochastic gradient descent

36:11.760 --> 36:14.560
 to neural networks is, is amazing to you.

36:14.600 --> 36:19.920
 So that, or is there a particular application in natural language

36:20.320 --> 36:29.160
 reinforcement learning, uh, and also what do you attribute that success to?

36:29.200 --> 36:30.400
 Is it just scale?

36:31.320 --> 36:36.200
 What profound insight can we take from the fact that the thing works

36:36.200 --> 36:39.880
 for gigantic, uh, sets of variables?

36:39.880 --> 36:44.360
 I mean, the interesting thing is this algorithms, they were invented decades

36:44.360 --> 36:52.680
 ago and, uh, people actually, uh, gave up on the idea and, um, you know, back

36:52.680 --> 36:58.040
 then they thought that we need profoundly different algorithms and they spent a lot

36:58.040 --> 36:59.920
 of cycles on very different algorithms.

37:00.240 --> 37:05.040
 And I believe that, uh, you know, we have seen that various, uh, various innovations

37:05.040 --> 37:11.400
 that say like transformer or, or dropout or so they can, uh, you know, pass the

37:11.400 --> 37:17.880
 help, but it's also remarkable to me that this algorithm from sixties or so, uh, or,

37:18.000 --> 37:22.680
 I mean, you can even say that the gradient descent was invented by Leibniz in, I

37:22.680 --> 37:29.200
 guess, 18th century or so that actually is the core of learning in the past.

37:29.200 --> 37:35.400
 In the past people are, it's almost like a, out of the, maybe an ego, people are

37:35.400 --> 37:39.640
 saying that it cannot be the case that such a simple algorithm is there, you

37:39.640 --> 37:43.640
 know, uh, could solve complicated problems.

37:44.480 --> 37:48.280
 So they were in search for the other algorithms.

37:48.560 --> 37:51.560
 And as I'm saying, like, I believe that actually we are in the game where there

37:51.560 --> 37:54.000
 is, there are actually frankly three levers.

37:54.040 --> 37:56.960
 There is compute, there are algorithms and there is data.

37:56.960 --> 38:01.480
 And, uh, if we want to build intelligent systems, we have to pull, uh, all three

38:01.480 --> 38:04.520
 levers and they are actually multiplicative.

38:05.440 --> 38:06.800
 Um, it's also interesting.

38:06.800 --> 38:08.400
 So you ask, is it only compute?

38:08.920 --> 38:14.200
 Uh, people internally, they did the studies to determine how much gains they

38:14.200 --> 38:15.560
 were coming from different levers.

38:16.040 --> 38:20.520
 And so far we have seen that more gains came from compute than algorithms, but

38:20.520 --> 38:24.200
 also we are in the world that in case of compute, there is a kind of, you know,

38:24.200 --> 38:28.640
 exponential increase in funding and at some point it's impossible to, uh, invest

38:28.640 --> 38:32.800
 more, it's impossible to, you know, invest $10 trillion as we are speaking about

38:32.800 --> 38:35.840
 the, let's say all taxes in us.

38:36.600 --> 38:41.880
 Uh, but you're talking about money that could be innovation in the compute.

38:42.000 --> 38:42.960
 That's that's true as well.

38:43.680 --> 38:45.760
 Uh, so I mean, they're like a few pieces.

38:45.760 --> 38:51.800
 So one piece is human brain is an incredible supercomputer and they're like

38:51.800 --> 39:01.360
 a, it, it, it has a hundred trillion parameters or like a, if you try to count

39:01.360 --> 39:05.720
 the various quantities in the brain, they're like a neuron synapses that small

39:05.720 --> 39:10.600
 number of neurons, there is a lot of synapses it's unclear even how to map, uh,

39:10.760 --> 39:16.880
 synapses to, uh, to parameters of neural networks, but it's clear that there are

39:16.880 --> 39:17.400
 many more.

39:17.400 --> 39:22.400
 Yeah. Um, so it might be the case that our networks are still somewhat small.

39:22.880 --> 39:27.040
 Uh, it also might be the case that they are more efficient than brain or less

39:27.040 --> 39:29.280
 efficient by some, by some huge factor.

39:29.680 --> 39:33.960
 Um, I also believe that there will be like a, you know, at the moment we are at

39:33.960 --> 39:39.000
 the stage that the, these neural networks, they require thousand X or, or like a

39:39.000 --> 39:41.920
 huge factor of more data than humans do.

39:41.920 --> 39:48.560
 And it will be a matter of, uh, um, there will be algorithms that vastly decrease

39:48.560 --> 39:52.960
 sample complexity, I believe so, but that place where we are heading today is

39:53.280 --> 39:57.920
 there are domains which contains million X more data.

39:58.080 --> 40:02.320
 And even though computers might be 1000 times slower than humans in learning,

40:02.640 --> 40:03.520
 that's not a problem.

40:03.560 --> 40:09.640
 Like, uh, for instance, uh, I believe that, uh, it should be possible to create

40:09.640 --> 40:15.560
 super human therapist, uh, by, uh, and, and the, the, like, uh, even simple

40:15.560 --> 40:18.520
 steps of, of, of doing what, of, of doing it.

40:18.880 --> 40:23.560
 And, you know, the, the core reason is there is just machine will be able to

40:23.560 --> 40:27.760
 read way more transcripts of therapies, and then it should be able to speak

40:27.760 --> 40:31.720
 simultaneously with many more people and it should be possible to optimize it,

40:31.960 --> 40:33.160
 uh, all in parallel.

40:33.760 --> 40:37.760
 And, uh, well, there's now you're touching on something I deeply care about

40:37.760 --> 40:39.960
 and think is way harder than we imagine.

40:40.360 --> 40:43.360
 Um, what's the goal of a therapist?

40:43.480 --> 40:44.520
 What's the goal of therapies?

40:45.520 --> 40:50.600
 So, okay, so one goal now this is terrifying to me, but there's a lot of

40:50.600 --> 40:57.320
 people that, uh, contemplate suicide, suffer from depression, uh, and they

40:57.320 --> 41:03.640
 could significantly be helped with therapy and the idea that an AI algorithm

41:03.640 --> 41:08.240
 might be in charge of that, it's like a life and death task.

41:08.480 --> 41:11.440
 It's, uh, the stakes are high.

41:12.000 --> 41:19.400
 So one goal for a therapist, whether human or AI is to prevent suicide

41:19.400 --> 41:21.640
 ideation to prevent suicide.

41:21.960 --> 41:23.040
 How do you achieve that?

41:23.800 --> 41:25.240
 So let's see.

41:25.800 --> 41:31.160
 So to be clear, I don't think that the current models are good enough for such

41:31.160 --> 41:35.280
 a task because it requires insane amount of understanding, empathy, and the

41:35.280 --> 41:38.360
 models are far from this place, but it's.

41:38.640 --> 41:43.120
 But do you think that understanding empathy, that signal is in the data?

41:43.560 --> 41:45.520
 Um, I think there is some signal in the data.

41:45.520 --> 41:45.800
 Yes.

41:45.800 --> 41:50.840
 I mean, there are plenty of transcripts of conversations and it is possible to,

41:51.680 --> 41:54.320
 it is possible from it to understand personalities.

41:54.480 --> 41:59.720
 It is possible from it to understand, uh, if conversation is, uh,

41:59.720 --> 42:05.720
 friendly, uh, amicable, uh, uh, antagonistic, it is, I believe that the,

42:05.760 --> 42:12.440
 you know, given the fact that the models that we train now, they can, uh, they

42:12.440 --> 42:17.000
 can have, they are chameleons that they can have any personality, they might

42:17.000 --> 42:21.520
 turn out to be better in understanding, uh, personality of other people than

42:21.520 --> 42:24.720
 anyone else and they empathetic to be empathetic.

42:24.760 --> 42:25.040
 Yeah.

42:25.840 --> 42:26.520
 Interesting.

42:26.520 --> 42:34.960
 Yeah, interesting. Uh, but I wonder if there's some level of, uh, multiple

42:34.960 --> 42:41.560
 modalities required to be able to, um, be empathetic of the human experience,

42:42.000 --> 42:46.080
 whether language is not enough to understand death, to understand fear,

42:46.080 --> 42:54.240
 to understand, uh, childhood trauma, to understand, uh, wit and humor required

42:54.240 --> 42:59.040
 when you're dancing with a person who might be depressed or suffering both

42:59.040 --> 43:02.320
 humor and hope and love and all those kinds of things.

43:02.760 --> 43:07.480
 So there's another underlying question, which is self supervised versus

43:07.480 --> 43:08.320
 supervised.

43:09.440 --> 43:16.280
 So can you get that from the data by just reading a huge number of transcripts?

43:16.320 --> 43:20.400
 I actually, so I think that reading huge number of transcripts is a step one.

43:20.400 --> 43:25.200
 It's like at the same way as you cannot learn to dance if just from YouTube by

43:25.200 --> 43:27.720
 watching it, you have to actually try it out yourself.

43:28.160 --> 43:31.280
 And so I think that here that's a similar situation.

43:31.520 --> 43:36.400
 I also wouldn't deploy the system in the high stakes situations right away, but

43:36.400 --> 43:39.280
 kind of see gradually where it goes.

43:39.600 --> 43:45.680
 And, uh, obviously initially, uh, it would have to go hand in hand with humans.

43:45.680 --> 43:50.480
 But, uh, at the moment we are in the situation that actually there is many

43:50.480 --> 43:55.400
 more people who actually would like to have a therapy or, or speak with, with

43:55.400 --> 43:57.080
 someone than there are therapies out there.

43:57.400 --> 44:02.320
 I can, you know, I was so, so fundamentally I was thinking, what are

44:02.320 --> 44:08.760
 the things that, uh, can vastly increase people's well being therapy is one of

44:08.760 --> 44:13.160
 them being meditation is other one, I guess maybe human connection is a third

44:13.160 --> 44:17.840
 one, and I guess pharmacologically it's also possible, maybe direct brain

44:17.840 --> 44:19.160
 stimulation or something like that.

44:19.160 --> 44:21.120
 But these are pretty much options out there.

44:21.440 --> 44:26.040
 Then let's say the way I'm thinking about the AGI endeavor is by default,

44:26.040 --> 44:29.520
 that's an endeavor to, uh, increase amount of wealth.

44:29.960 --> 44:34.000
 And I believe that we can invest the increase amount of wealth for everyone

44:34.400 --> 44:35.840
 and simultaneously.

44:35.880 --> 44:39.040
 So, I mean, there are like a two endeavors that make sense to me.

44:39.320 --> 44:41.760
 One is like essentially increase amount of wealth.

44:41.760 --> 44:45.560
 And second one is, uh, increase overall human wellbeing.

44:46.200 --> 44:49.280
 And those are coupled together and they, they can, like, uh, I would

44:49.280 --> 44:51.080
 say these are different topics.

44:51.080 --> 44:56.760
 One can help another and, uh, you know, therapist is a, is a funny word

44:57.080 --> 44:59.520
 because I see friendship and love as therapy.

44:59.520 --> 45:04.000
 I mean, so therapist broadly defined as just friendship as a friend.

45:04.640 --> 45:10.160
 So like therapist is, has a very kind of clinical sense to it, but what

45:10.160 --> 45:17.800
 is human connection you're like, uh, not to get all Camus and Dostoevsky on you,

45:17.800 --> 45:23.880
 but you know, life is suffering and we draw, we seek connection with the

45:23.880 --> 45:29.760
 humans as we, uh, desperately try to make sense of this world in a deep

45:30.040 --> 45:33.080
 overwhelming loneliness that we feel inside.

45:34.040 --> 45:36.680
 So I think connection has to do with understanding.

45:36.680 --> 45:40.120
 And I think that almost like a lack of understanding causes suffering.

45:40.160 --> 45:45.440
 If you speak with someone and do you, do you feel ignored that actually causes pain?

45:45.480 --> 45:50.720
 If you are feeling deeply understood that actually they, they, they might

45:50.720 --> 45:54.720
 not even tell you what to do in life, but like a pure understanding

45:54.800 --> 45:59.240
 or just being heard, understanding is a kind of, that's a lot, you know,

45:59.480 --> 46:04.840
 just being heard, feel like you're being heard, like somehow that's a

46:04.840 --> 46:10.720
 alleviation temporarily of the loneliness that if somebody knows

46:10.720 --> 46:15.960
 you're here with their body language, with the way they are, with the way

46:15.960 --> 46:20.400
 they look at you, with the way they talk, do you feel less alone for a brief moment?

46:22.080 --> 46:23.200
 Yeah, very much agree.

46:23.320 --> 46:28.000
 So I thought in the past about, um, somewhat similar question to yours,

46:28.000 --> 46:31.320
 which is what is love, uh, rather what is connection.

46:31.320 --> 46:36.240
 Yes. And, um, and obviously I think about these things from AI perspective.

46:36.240 --> 46:36.960
 What would it mean?

46:37.480 --> 46:43.080
 Um, so I said that, um, you know, intelligence has to do with some compression,

46:43.120 --> 46:46.720
 which is more or less like I can say, almost understanding of what is going around.

46:47.200 --> 46:52.720
 It seems to me that, uh, other aspect is there seem to be reward functions and you

46:52.720 --> 46:59.040
 can have, uh, uh, you know, reward for, uh, food, for maybe human connection, for,

46:59.040 --> 47:02.840
 uh, let's say warmth, uh, sex and so on.

47:03.480 --> 47:09.880
 And, um, and it turns out that the various people might be optimizing slightly

47:09.880 --> 47:11.320
 different, uh, reward functions.

47:11.320 --> 47:13.520
 They essentially might care about different things.

47:14.120 --> 47:20.840
 And, uh, uh, in case of, uh, love at least the love between two people, you can say

47:20.840 --> 47:25.560
 that the, um, you know, boundary between people dissolves to such extent that, uh,

47:25.560 --> 47:33.160
 they end up optimizing each other reward functions and yeah, oh, that's interesting.

47:33.200 --> 47:36.800
 Um, celebrate the success of each other.

47:36.880 --> 47:37.160
 Yeah.

47:37.200 --> 47:42.760
 In some sense, I would say love means, uh, helping others to optimize their, uh,

47:42.800 --> 47:45.840
 reward functions, not your reward functions, not the things that you think are

47:45.840 --> 47:51.080
 important, but the things that the person cares about, you try to help them to,

47:51.120 --> 47:51.920
 uh, optimize it.

47:51.920 --> 47:56.840
 So love is, uh, if you think of two reward functions, you just, it's a condition.

47:56.880 --> 48:00.840
 You combine them together, pretty much maybe like with a weight and it depends

48:00.840 --> 48:02.640
 like the dynamic of the relationship.

48:02.760 --> 48:03.080
 Yeah.

48:03.080 --> 48:06.560
 I mean, you could imagine that if you're fully, uh, optimizing someone's reward

48:06.560 --> 48:10.360
 function without yours, then, then maybe are creating codependency or something

48:10.360 --> 48:14.600
 like that, but I'm not sure what's the appropriate weight, but the interesting

48:14.600 --> 48:19.920
 thing is I even, I even think that the, uh, individual reward function is

48:19.920 --> 48:27.480
 saying that the individual person, uh, uh, we ourselves, we are actually less

48:27.480 --> 48:29.640
 of a unified insight.

48:29.720 --> 48:33.520
 So for instance, if you look at, at the donut on the one level, you might think,

48:33.560 --> 48:35.080
 oh, this is like, it looks tasty.

48:35.080 --> 48:36.600
 I would like to eat it on other level.

48:36.840 --> 48:41.600
 You might tell yourself, I shouldn't be doing it because I want to gain muscles.

48:42.000 --> 48:45.920
 So, and you know, you might do it regardless kind of against yourself.

48:45.920 --> 48:50.520
 So it seems that even within ourselves, they're almost like a kind of intertwined

48:50.520 --> 48:57.440
 personas and, um, I believe that the self love means that, uh, the love between all

48:57.440 --> 49:03.880
 these personas, which also means being able to love, love yourself when we are

49:04.280 --> 49:08.400
 angry or stressed or so combining all those reward functions of the different

49:08.400 --> 49:12.000
 selves you have and accepting that they are there, like, uh, you know, often

49:12.000 --> 49:16.320
 people, they have a negative self talk or they say, I don't like when I'm angry.

49:16.720 --> 49:23.840
 And like, I try to imagine, try to imagine if there would be like a small

49:23.840 --> 49:29.640
 baby Lex, like a five years old, angry, and then they are like, you shouldn't

49:29.640 --> 49:30.080
 be angry.

49:30.080 --> 49:31.240
 Like stop being angry.

49:31.280 --> 49:31.640
 Yeah.

49:31.720 --> 49:35.920
 But like an instant, actually you want the Lex to come over, give him a hug and

49:35.920 --> 49:37.240
 just like, I say, it's fine.

49:37.560 --> 49:37.920
 Okay.

49:37.920 --> 49:39.960
 It's going to be angry as long as you want.

49:39.960 --> 49:45.240
 And then he would stop or, or maybe not, or maybe not, but you cannot expect it

49:45.240 --> 49:45.520
 even.

49:45.800 --> 49:46.200
 Yeah.

49:46.800 --> 49:49.280
 But still, that doesn't explain the why of love.

49:49.280 --> 49:51.720
 Like why is love part of the human condition?

49:51.720 --> 49:55.560
 Why is it useful to combine the reward functions?

49:56.160 --> 50:01.080
 It seems like that doesn't, I mean, I don't think reinforcement learning

50:01.080 --> 50:06.800
 frameworks can give us answers to why even, even the Hutter framework has

50:06.800 --> 50:08.920
 an objective function that's static.

50:08.920 --> 50:13.640
 So we came to existence as a consequence of evolutionary process.

50:13.960 --> 50:16.800
 And in some sense, the purpose of evolution is survival.

50:17.080 --> 50:23.720
 And then the, this complicated optimization objective baked into us, let's

50:23.720 --> 50:27.960
 say compression, which might help us operate in the real world and it baked

50:27.960 --> 50:29.600
 into us various reward functions.

50:29.680 --> 50:30.040
 Yeah.

50:31.080 --> 50:35.360
 Then to be clear at the moment we are operating in the regime, which is somewhat

50:35.360 --> 50:38.040
 out of distribution, where they even evolution optimized us.

50:38.040 --> 50:42.640
 It's almost like love is a consequence of a cooperation that we've discovered is

50:42.640 --> 50:43.160
 useful.

50:43.240 --> 50:43.640
 Correct.

50:43.880 --> 50:45.800
 In some way it's even the case.

50:45.800 --> 50:49.680
 If you, I just love the idea that love is like the out of distribution.

50:50.560 --> 50:51.720
 Or it's not out of distribution.

50:51.720 --> 50:53.960
 It's like, as you said, it evolved for cooperation.

50:54.600 --> 50:55.000
 Yes.

50:55.000 --> 50:58.960
 And I believe that the cop, like in some sense, cooperation ends up helping each

50:58.960 --> 51:03.400
 of us individually, so it makes sense evolutionary and there is a, in some

51:03.400 --> 51:08.000
 sense, and, you know, love means there is this dissolution of boundaries that you

51:08.000 --> 51:12.640
 have a shared reward function and we evolve to actually identify ourselves with

51:12.640 --> 51:18.160
 larger groups, so we can identify ourselves, you know, with a family, we can

51:18.160 --> 51:22.240
 identify ourselves with a country to such extent that people are willing to give

51:22.240 --> 51:23.520
 away their life for country.

51:24.880 --> 51:29.000
 So there is, we are wired actually even for love.

51:29.000 --> 51:36.440
 And at the moment, I guess, the, maybe it would be somewhat more beneficial if you

51:36.440 --> 51:40.200
 will, if we would identify ourselves with all the humanity as a whole.

51:40.520 --> 51:44.440
 So you can clearly see when people travel around the world, when they run into

51:44.440 --> 51:48.720
 person from the same country, they say, oh, which CPR and all this, like all the

51:48.720 --> 51:50.920
 sudden they find all these similarities.

51:50.920 --> 51:55.040
 They find some, they befriended those folks earlier than others.

51:55.040 --> 51:58.840
 So there is like a sense, some sense of the belonging. And I would say, I think

51:58.840 --> 52:05.720
 it would be overall good thing to the world for people to move towards, I think

52:05.720 --> 52:11.320
 it's even called open individualism, move toward the mindset of a larger and

52:11.320 --> 52:12.160
 larger groups.

52:12.400 --> 52:17.520
 So the challenge there, that's a beautiful vision and I share it to expand

52:17.520 --> 52:21.960
 that circle of empathy, that circle of love towards the entirety of humanity.

52:21.960 --> 52:24.520
 But then you start to ask, well, where do you draw the line?

52:25.120 --> 52:28.200
 Because why not expand it to other conscious beings?

52:28.520 --> 52:34.360
 And then finally, for our discussion, something I think about is why not

52:34.360 --> 52:36.320
 expand it to AI systems?

52:37.200 --> 52:42.080
 Like we, we start respecting each other when the, the person, the entity on the

52:42.080 --> 52:45.000
 other side has the capacity to suffer.

52:45.360 --> 52:49.320
 Cause then we develop a capacity to sort of empathize.

52:49.320 --> 52:54.640
 And so I could see AI systems that are interacting with humans more and more

52:54.640 --> 52:58.200
 having conscious, like displays.

52:58.480 --> 53:02.400
 So like they display consciousness through language and through other means.

53:02.880 --> 53:06.280
 And so then the question is like, well, is that consciousness?

53:06.840 --> 53:08.440
 Because they're acting conscious.

53:08.960 --> 53:15.920
 And so, you know, the reason we don't like torturing animals is because

53:15.920 --> 53:21.240
 they look like they're suffering when they're tortured and if AI looks like

53:21.240 --> 53:30.560
 it's suffering when it's tortured, how is that not requiring of the same kind

53:30.560 --> 53:35.880
 of empathy from us and respect and rights that animals do and other humans do?

53:35.920 --> 53:37.600
 I think it requires empathy as well.

53:37.600 --> 53:42.520
 I mean, I would like, I guess us or humanity or so make a progress in

53:42.520 --> 53:46.040
 understanding what consciousness is, because I don't want just to be speaking

53:46.040 --> 53:50.800
 about that, the philosophy, but rather actually make a scientific, uh, to have

53:50.800 --> 53:56.280
 a, like, you know, there was a time that people thought that there is a force of

53:56.280 --> 54:01.560
 life and, uh, the things that have this force, they are alive.

54:03.040 --> 54:08.280
 And, um, I think that there is actually a path to understand exactly what

54:08.280 --> 54:10.560
 consciousness is and how it works.

54:10.560 --> 54:13.160
 Understand exactly what consciousness is.

54:13.840 --> 54:19.440
 And, uh, um, in some sense, it might require essentially putting

54:19.440 --> 54:23.760
 probes inside of a human brain, uh, what Neuralink, uh, does.

54:23.800 --> 54:26.440
 So the goal there, I mean, there's several things with consciousness

54:26.440 --> 54:30.240
 that make it a real discipline, which is one is rigorous

54:30.240 --> 54:31.640
 measurement of consciousness.

54:32.480 --> 54:34.680
 And then the other is the engineering of consciousness,

54:34.680 --> 54:36.240
 which may or may not be related.

54:36.520 --> 54:38.840
 I mean, you could also run into trouble.

54:38.840 --> 54:43.200
 Like, for example, in the United States for the department, DOT,

54:43.200 --> 54:46.680
 department of transportation, and a lot of different places

54:46.720 --> 54:48.200
 put a value on human life.

54:48.720 --> 54:53.040
 I think DOT is, uh, values $9 million per person.

54:54.200 --> 54:56.760
 Sort of in that same way, you can get into trouble.

54:57.840 --> 55:01.960
 If you put a number on how conscious of being is, because then you can start

55:01.960 --> 55:11.960
 making policy, if a cow is a 0.1 or like, um, 10% as conscious as a human,

55:12.400 --> 55:15.360
 then you can start making calculations and it might get you into trouble.

55:15.360 --> 55:17.720
 But then again, that might be a very good way to do it.

55:18.920 --> 55:23.360
 I would like, uh, to move to that place that actually we have scientific

55:23.360 --> 55:24.720
 understanding what consciousness is.

55:25.160 --> 55:27.400
 And then we'll be able to actually assign value.

55:27.400 --> 55:32.000
 And I believe that there is even the path for the experimentation in it.

55:32.440 --> 55:37.760
 So, uh, you know, w we said that, you know, you could put the

55:37.800 --> 55:38.960
 probes inside of the brain.

55:39.280 --> 55:42.640
 There is actually a few other things that you could do with

55:42.640 --> 55:44.120
 devices like Neuralink.

55:44.400 --> 55:48.800
 So you could imagine that the way even to measure if AI system is conscious

55:49.360 --> 55:51.920
 is by literally just plugging into the brain.

55:52.760 --> 55:56.040
 Um, I mean, that, that seems like it's kind of easy, but the plugging

55:56.040 --> 55:59.240
 into the brain and asking person if they feel that their consciousness

55:59.240 --> 56:02.880
 expanded, um, this direction of course has some issues.

56:02.880 --> 56:05.880
 You can say, you know, if someone takes a psychedelic drug, they might

56:05.880 --> 56:08.920
 feel that their consciousness expanded, even though that drug

56:08.920 --> 56:09.960
 itself is not conscious.

56:10.840 --> 56:11.280
 Right.

56:11.520 --> 56:15.480
 So like, you can't fully trust the self report of a person saying their,

56:15.800 --> 56:18.040
 their consciousness is expanded or not.

56:20.280 --> 56:23.160
 Let me ask you a little bit about psychedelics is, uh, there've been

56:23.160 --> 56:26.960
 a lot of excellent research on, uh, different psychedelics, psilocybin,

56:26.960 --> 56:32.200
 MDMA, even DMT drugs in general, marijuana too.

56:33.280 --> 56:36.360
 Uh, what do you think psychedelics do to the human mind?

56:36.800 --> 56:40.840
 It seems they take the human mind to some interesting places.

56:41.760 --> 56:46.760
 Is that just a little, uh, hack, a visual hack, or is there some

56:46.760 --> 56:48.480
 profound expansion of the mind?

56:49.160 --> 56:52.120
 So let's see, I don't believe in magic.

56:52.120 --> 57:00.000
 I believe in, uh, I believe in, uh, in science in, in causality, um, still,

57:00.000 --> 57:06.000
 let's say, and then as I said, like, I think that the brain, that the, our

57:06.000 --> 57:12.120
 subjective experience of reality is, uh, we live in the simulation run by our

57:12.120 --> 57:17.200
 brain and the simulation that our brain runs, they can be very pleasant or very

57:17.200 --> 57:22.680
 hellish drugs, they are changing some hyper parameters of the simulation.

57:23.040 --> 57:27.920
 It is possible thanks to change of these hyper parameters to actually look back

57:27.920 --> 57:32.160
 on your experience and even see that the given things that we took for

57:32.160 --> 57:34.360
 granted, they are changeable.

57:35.320 --> 57:38.880
 So they allow to have a amazing perspective.

57:39.160 --> 57:44.280
 There is also, for instance, the fact that after DMT people can see the

57:44.280 --> 57:51.480
 full movie inside of their head, gives me further belief that the brain can generate

57:51.480 --> 57:57.000
 that full movie, that the brain is actually learning the model of reality

57:57.000 --> 57:59.840
 to such extent that it tries to predict what's going to happen next.

58:00.080 --> 58:00.280
 Yeah.

58:00.280 --> 58:01.560
 Very high resolution.

58:01.560 --> 58:02.880
 So it can replay reality.

58:03.400 --> 58:04.960
 Extremely high resolution.

58:05.640 --> 58:05.920
 Yeah.

58:05.960 --> 58:11.040
 It's also kind of interesting to me that somehow there seems to be some similarity

58:11.040 --> 58:15.920
 between these, uh, drugs and meditation itself.

58:16.440 --> 58:20.760
 And I actually started even these days to think about meditation as a psychedelic.

58:22.240 --> 58:23.600
 Do you practice meditation?

58:24.160 --> 58:25.800
 I practice meditation.

58:26.080 --> 58:31.520
 I mean, I went a few times on the retreats and it feels after like after

58:31.520 --> 58:39.080
 second or third day of meditation, uh, there is a, there is almost like a

58:39.080 --> 58:43.520
 sense of, you know, tripping what, what does the meditation retreat entail?

58:44.320 --> 58:50.520
 So you w you wake up early in the morning and you meditate for extended

58:50.520 --> 58:56.480
 period of time, uh, and yeah, so it's optimized, even though there are other

58:56.480 --> 58:59.440
 people, it's optimized for isolation.

58:59.600 --> 59:01.040
 So you don't speak with anyone.

59:01.040 --> 59:06.360
 You don't actually look into other people's eyes and, uh, you know, you sit

59:06.360 --> 59:13.160
 on the chair and say Vipassana meditation tells you, uh, to focus on the breath.

59:13.200 --> 59:18.600
 So you try to put, uh, all the, all attention into breathing and, uh,

59:18.640 --> 59:19.920
 breathing in and breathing out.

59:20.440 --> 59:26.760
 And the crazy thing is that as you focus attention like that, uh, after some

59:26.760 --> 59:33.080
 time, their stems starts coming back, like some memories that you completely

59:33.080 --> 59:39.320
 forgotten, it almost feels like, uh, that you'll have a mailbox and then you know,

59:39.320 --> 59:42.320
 you are just like a archiving email one by one.

59:43.080 --> 59:48.640
 And at some point, at some point there is this like a amazing feeling of getting

59:48.640 --> 59:50.960
 to mailbox zero, zero emails.

59:51.040 --> 59:52.880
 And, uh, it's very pleasant.

59:53.080 --> 1:00:02.400
 It's, it's kind of, it's, it's, it's crazy to me that, um, that once you

1:00:02.400 --> 1:00:08.960
 resolve these, uh, inner store stories or like inner traumas, then once there is

1:00:08.960 --> 1:00:16.040
 nothing, uh, left that default, uh, state of human mind is extremely peaceful and

1:00:16.040 --> 1:00:24.520
 happy, extreme, like, uh, some sense it, it feels that the, it feels at least to

1:00:24.520 --> 1:00:30.400
 me that way, how, when I was a child that I can look at any object and it's very

1:00:30.400 --> 1:00:34.680
 beautiful, I have a lot of curiosity about the simple things and that's where

1:00:34.680 --> 1:00:36.080
 the usual meditation takes me.

1:00:37.440 --> 1:00:40.000
 Are you, what are you experiencing?

1:00:40.040 --> 1:00:45.560
 Are you just taking in simple sensory information and they're just enjoying

1:00:45.560 --> 1:00:47.840
 the rawness of that sensory information?

1:00:48.120 --> 1:00:52.000
 So there's no, there's no memories or all that kind of stuff.

1:00:52.000 --> 1:00:54.400
 You're just enjoying being.

1:00:54.960 --> 1:00:55.920
 Yeah, pretty much.

1:00:55.920 --> 1:01:00.760
 I mean, still there is, uh, that it's, it's thoughts are slowing down.

1:01:00.880 --> 1:01:06.080
 Sometimes they pop up, but it's also somehow the extended meditation takes you

1:01:06.080 --> 1:01:11.080
 to the space that they are way more friendly, way more positive.

1:01:11.400 --> 1:01:19.240
 Um, there is also this, uh, this thing that, uh, we've, it almost feels that the.

1:01:19.240 --> 1:01:24.240
 It almost feels that the, we are constantly getting a little bit of a reward

1:01:24.240 --> 1:01:28.240
 function and we are just spreading this reward function on various activities.

1:01:28.560 --> 1:01:33.000
 But if you'll stay still for extended period of time, it kind of accumulates,

1:01:33.000 --> 1:01:38.800
 accumulates, accumulates, and, uh, there is a, there is a sense, there is a sense

1:01:38.800 --> 1:01:46.080
 that some point it passes some threshold and it feels as drop is falling into kind

1:01:46.080 --> 1:01:49.920
 of ocean of love and this, and that's like, uh, this is like a very pleasant.

1:01:49.920 --> 1:01:54.440
 And that's, I'm saying like, uh, that corresponds to the subjective experience.

1:01:54.920 --> 1:02:01.440
 Some people, uh, I guess in spiritual community, they describe it that that's

1:02:01.440 --> 1:02:04.840
 the reality, and I would say, I believe that they're like, uh, all sorts of

1:02:04.840 --> 1:02:06.920
 subjective experience that one can have.

1:02:07.320 --> 1:02:11.720
 And, uh, I believe that for instance, meditation might take you to the

1:02:11.720 --> 1:02:13.640
 subjective experiences with the subject.

1:02:13.640 --> 1:02:16.480
 Vision might take you to the subjective experiences, which are

1:02:16.480 --> 1:02:17.880
 very pleasant, collaborative.

1:02:18.080 --> 1:02:23.400
 And I would like a word to move toward a more collaborative, uh, uh, place.

1:02:24.880 --> 1:02:25.200
 Yeah.

1:02:25.240 --> 1:02:28.240
 I would say that's very pleasant and I enjoy doing stuff like that.

1:02:28.440 --> 1:02:34.800
 I, um, I wonder how that maps to your, uh, mathematical model of love with, uh,

1:02:35.040 --> 1:02:42.280
 the reward function, combining a bunch of things, it seems like our life then is

1:02:42.280 --> 1:02:46.120
 just, we have this reward function and we're accumulating a bunch of stuff

1:02:46.120 --> 1:02:55.000
 in it with weights, it's like, um, like multi objective and what meditation

1:02:55.000 --> 1:03:01.040
 is, is you just remove them, remove them until the weight on one, uh, or

1:03:01.040 --> 1:03:04.960
 just a few is very high and that's where the pleasure comes from.

1:03:05.200 --> 1:03:05.480
 Yeah.

1:03:05.480 --> 1:03:08.200
 So something similar, how I'm thinking about this.

1:03:08.200 --> 1:03:13.240
 So I told you that there is this like, uh, that there is a story of who you are.

1:03:14.120 --> 1:03:19.000
 And I think almost about it as a, you know, text prepended to GPT.

1:03:20.400 --> 1:03:20.720
 Yeah.

1:03:21.000 --> 1:03:23.720
 And, uh, some people refer to it as ego.

1:03:24.120 --> 1:03:24.480
 Okay.

1:03:24.600 --> 1:03:27.560
 There's like a story who, who, who you are.

1:03:27.560 --> 1:03:27.880
 Okay.

1:03:28.000 --> 1:03:31.320
 So ego is the prompt for GPT three or GPT.

1:03:31.360 --> 1:03:31.600
 Yes.

1:03:31.600 --> 1:03:31.760
 Yes.

1:03:31.760 --> 1:03:32.720
 And that's description of you.

1:03:32.960 --> 1:03:37.080
 And then with meditation, you can get to the point that actually you experience

1:03:37.080 --> 1:03:42.480
 things without the prompt and you experience things like as they are, you

1:03:42.480 --> 1:03:47.040
 are not biased over the description, how they supposed to be, uh, that's very

1:03:47.040 --> 1:03:47.480
 pleasant.

1:03:47.480 --> 1:03:49.560
 And then we've respected the reward function.

1:03:50.000 --> 1:03:54.960
 Uh, it's possible to get to the point that the, there is the solution of self.

1:03:55.480 --> 1:03:59.480
 And therefore you can say that the, or you're having a, your, or like a, your

1:03:59.480 --> 1:04:03.320
 brain attempts to simulate the reward function of everyone else or like

1:04:03.320 --> 1:04:07.120
 everything that's that there is this like a love, which feels like a oneness with

1:04:07.120 --> 1:04:07.560
 everything.

1:04:08.760 --> 1:04:11.440
 And that's also, you know, very beautiful, very pleasant.

1:04:11.440 --> 1:04:16.120
 At some point you might have a lot of altruistic thoughts during that moment.

1:04:16.120 --> 1:04:18.840
 And then the self, uh, always comes back.

1:04:19.240 --> 1:04:23.480
 How would you recommend if somebody is interested in meditation, like a big

1:04:23.480 --> 1:04:27.360
 thing to take on as a project, would you recommend a meditation retreat?

1:04:27.400 --> 1:04:29.840
 How many days, what kind of thing would you recommend?

1:04:30.160 --> 1:04:32.560
 I think that actually retreat is the way to go.

1:04:32.560 --> 1:04:38.800
 Um, it almost feels that, uh, um, as I said, like a meditation is a psychedelic,

1:04:39.000 --> 1:04:43.000
 but, uh, when you take it in the small dose, you might barely feel it.

1:04:43.280 --> 1:04:46.160
 Once you get the high dose, actually you're going to feel it.

1:04:46.880 --> 1:04:51.800
 Um, so even cold turkey, if you haven't really seriously meditated for a long

1:04:51.800 --> 1:04:53.800
 period of time, just go to a retreat.

1:04:53.920 --> 1:04:54.280
 Yeah.

1:04:54.280 --> 1:04:55.560
 How many days, how many days?

1:04:55.560 --> 1:04:57.600
 Start weekend one weekend.

1:04:57.600 --> 1:04:58.800
 So like two, three days.

1:04:58.800 --> 1:05:03.480
 And it's like, uh, it's interesting that first or second day, it's hard.

1:05:03.520 --> 1:05:05.040
 And at some point it becomes easy.

1:05:06.560 --> 1:05:08.200
 There's a lot of seconds in a day.

1:05:08.520 --> 1:05:12.360
 How hard is the meditation retreat just sitting there in a chair?

1:05:13.040 --> 1:05:20.480
 So the thing is actually, it literally just depends on your, uh, on the,

1:05:20.800 --> 1:05:24.560
 your own framing, like if you are in the mindset that you are waiting for it to

1:05:24.560 --> 1:05:28.720
 be over, or you are waiting for a Nirvana to happen, you are waiting

1:05:28.720 --> 1:05:30.200
 it will be very unpleasant.

1:05:30.680 --> 1:05:36.480
 And in some sense, even the difficulty, it's not even in the lack of being

1:05:36.480 --> 1:05:40.360
 able to speak with others, like, uh, you're sitting there, your legs

1:05:40.360 --> 1:05:44.440
 will hurt from sitting in terms of like the practical things.

1:05:44.480 --> 1:05:48.160
 Do you experience kind of discomfort, like physical discomfort of just

1:05:48.160 --> 1:05:53.720
 sitting, like your, your butt being numb, your legs being sore, all that kind of

1:05:53.720 --> 1:05:54.040
 stuff?

1:05:54.160 --> 1:05:54.520
 Yes.

1:05:54.520 --> 1:05:55.360
 You experience it.

1:05:55.360 --> 1:05:59.320
 And then the, the, they teach you to observe it rather.

1:05:59.320 --> 1:06:03.280
 And it's like, uh, the crazy thing is you at first might have a feeling

1:06:03.280 --> 1:06:07.520
 toward trying to escape it and that becomes very apparent that that's

1:06:07.560 --> 1:06:08.640
 extremely unpleasant.

1:06:09.120 --> 1:06:11.400
 And then you just, just observe it.

1:06:11.840 --> 1:06:18.720
 And then at some point it just becomes, uh, it just is, it's like, uh, I remember

1:06:18.720 --> 1:06:22.680
 that we've, Ilya told me some time ago that, uh, you know, he takes a cold

1:06:22.680 --> 1:06:28.200
 shower and he's the mindset of taking a cold shower was to embrace suffering.

1:06:28.360 --> 1:06:28.680
 Yeah.

1:06:28.960 --> 1:06:29.440
 Excellent.

1:06:29.680 --> 1:06:30.320
 I do the same.

1:06:30.320 --> 1:06:31.160
 This is your style?

1:06:31.240 --> 1:06:32.320
 Yeah, it's my style.

1:06:32.880 --> 1:06:33.480
 I like this.

1:06:34.200 --> 1:06:38.520
 So my style is actually, I also sometimes take cold showers.

1:06:38.960 --> 1:06:43.480
 It is purely observing how the water goes through my body, like a purely being

1:06:43.480 --> 1:06:45.000
 present, not trying to escape from there.

1:06:46.040 --> 1:06:46.360
 Yeah.

1:06:46.800 --> 1:06:49.360
 And I would say then it actually becomes pleasant.

1:06:49.360 --> 1:06:52.000
 It's not like, ah, well, that that's interesting.

1:06:52.200 --> 1:06:57.520
 Um, I I'm also that mean that's, that's the way to deal with anything really

1:06:57.520 --> 1:07:03.200
 difficult, especially in the physical space is to observe it to say it's pleasant.

1:07:04.880 --> 1:07:05.200
 Hmm.

1:07:05.600 --> 1:07:07.680
 It's a D I would use a different word.

1:07:08.480 --> 1:07:14.160
 You're, um, you're accepting of the full beauty of reality.

1:07:14.480 --> 1:07:16.600
 I would say, cause say pleasant.

1:07:16.600 --> 1:07:19.520
 But yeah, I mean, in some sense it is pleasant.

1:07:19.560 --> 1:07:24.200
 That's the only way to deal with a cold shower is to, to, uh, become an

1:07:24.200 --> 1:07:27.000
 observer and to find joy in it.

1:07:28.440 --> 1:07:32.920
 Um, same with like really difficult, physical, um, exercise or like running

1:07:32.920 --> 1:07:37.680
 for a really long time, endurance events, just anytime you're, any kind of pain.

1:07:38.040 --> 1:07:41.680
 I think the only way to survive it is not to resist it is to observe it.

1:07:43.120 --> 1:07:46.520
 You mentioned, you mentioned, um, you mentioned, um, you mentioned

1:07:46.520 --> 1:07:51.920
 Ilya, Ilya says, it's very, he's our chief scientist, but also

1:07:51.920 --> 1:07:53.160
 he's very close friend of mine.

1:07:53.600 --> 1:07:55.360
 He cofounded open air with you.

1:07:56.280 --> 1:07:58.400
 I've spoken with him a few times.

1:07:58.440 --> 1:07:59.160
 He's brilliant.

1:07:59.160 --> 1:08:00.440
 I really enjoy talking to him.

1:08:02.960 --> 1:08:06.040
 His mind, just like yours works in fascinating ways.

1:08:06.960 --> 1:08:10.000
 Now, both of you are not able to define deep learning simply.

1:08:10.000 --> 1:08:15.840
 Uh, what's it like having him as somebody you have technical discussions with on

1:08:15.880 --> 1:08:20.360
 in the space of machine learning, deep learning, AI, but also life.

1:08:21.200 --> 1:08:28.480
 What's it like when these two, um, agents get into a self play situation in a room?

1:08:29.000 --> 1:08:30.280
 What's it like collaborating with him?

1:08:30.840 --> 1:08:35.320
 So I believe that we have, uh, extreme, uh, respect to each other.

1:08:35.320 --> 1:08:43.400
 So, uh, in, I love Ilya's insight, both like, uh, I guess about

1:08:43.720 --> 1:08:49.480
 consciousness, uh, life AI, but, uh, in terms of the, it's interesting to

1:08:49.480 --> 1:08:56.080
 me, cause you're a brilliant, uh, Thinker in the space of machine

1:08:56.080 --> 1:09:01.840
 learning, like intuition, like digging deep in what works, what doesn't,

1:09:01.840 --> 1:09:04.720
 why it works, why it doesn't, and so is Ilya.

1:09:05.200 --> 1:09:09.600
 I'm wondering if there's interesting deep discussions you've had with him in the

1:09:09.600 --> 1:09:12.040
 past or disagreements that were very productive.

1:09:12.280 --> 1:09:18.000
 So I can say, I also understood over the time, where are my strengths?

1:09:18.000 --> 1:09:24.240
 So obviously we have plenty of AI discussions and, um, um, and do you

1:09:24.240 --> 1:09:29.440
 know, I myself have plenty of ideas, but like I consider Ilya, uh, what

1:09:29.440 --> 1:09:32.440
 of the most prolific AI scientists in the entire world.

1:09:33.160 --> 1:09:39.760
 And, uh, I think that, um, I realized that maybe my super skill, um, is, uh,

1:09:40.000 --> 1:09:43.640
 being able to bring people to collaborate together, that I have some level of

1:09:43.800 --> 1:09:46.320
 empathy that is unique in AI world.

1:09:46.760 --> 1:09:50.800
 And that might come, you know, from either meditation, psychedelics, or

1:09:50.800 --> 1:09:53.080
 let's say I read just hundreds of books on this topic.

1:09:53.080 --> 1:09:56.920
 So, and I also went through a journey of, you know, I developed a

1:09:56.920 --> 1:10:04.840
 lot of, uh, algorithms, so I think that maybe I can, that's my super human skill.

1:10:05.320 --> 1:10:11.200
 Uh, Ilya is, uh, one of the best AI scientists, but then I'm pretty

1:10:11.200 --> 1:10:14.920
 good in assembling teams and I'm also not holding to people.

1:10:14.920 --> 1:10:18.080
 Like I'm growing people and then people become managers at OpenAI.

1:10:18.400 --> 1:10:20.680
 I grew many of them, like a research managers.

1:10:20.680 --> 1:10:27.240
 So you, you find, you find places where you're excellent and he finds like his,

1:10:27.240 --> 1:10:31.800
 his, his deep scientific insights is where he is and you find ways you can,

1:10:31.840 --> 1:10:33.600
 the puzzle pieces fit together.

1:10:33.600 --> 1:10:33.920
 Correct.

1:10:33.920 --> 1:10:37.680
 Like, uh, you know, ultimately, for instance, let's say Ilya, he doesn't

1:10:37.680 --> 1:10:41.960
 manage people, uh, that's not what he likes or so.

1:10:42.280 --> 1:10:45.680
 Um, I like, I like hanging out with people.

1:10:45.680 --> 1:10:48.200
 By default, I'm an extrovert and I care about people.

1:10:48.200 --> 1:10:50.840
 Oh, interesting. Okay. All right. Okay, cool.

1:10:50.880 --> 1:10:52.880
 So that, that fits perfectly together.

1:10:52.920 --> 1:10:56.600
 But I mean, uh, I also just like your intuition about various

1:10:56.600 --> 1:10:57.920
 problems in machine learning.

1:10:58.160 --> 1:11:00.480
 He's definitely one I really enjoy.

1:11:01.440 --> 1:11:06.800
 I remember talking to him about something I was struggling with, which

1:11:06.800 --> 1:11:12.920
 is coming up with a good model for pedestrians, for human beings across

1:11:12.920 --> 1:11:16.800
 the street in the context of autonomous vehicles, and I was like, okay,

1:11:16.800 --> 1:11:18.600
 in the context of autonomous vehicles.

1:11:19.840 --> 1:11:24.400
 And he immediately started to like formulate a framework within which you

1:11:24.400 --> 1:11:29.040
 can evolve a model for pedestrians, like through self play, all that kind of

1:11:29.040 --> 1:11:35.040
 mechanisms, the depth of thought on a particular problem, especially problems

1:11:35.040 --> 1:11:38.120
 he doesn't know anything about is, is fascinating to watch.

1:11:38.560 --> 1:11:46.000
 It makes you realize like, um, yeah, the, the, the limits of the, that the human

1:11:46.000 --> 1:11:50.560
 intellect may be limitless, or it's just impressive to see a descendant of

1:11:50.560 --> 1:11:52.440
 ape come up with clever ideas.

1:11:52.640 --> 1:11:53.000
 Yeah.

1:11:53.000 --> 1:11:56.920
 I mean, so even in the space of deep learning, when you look at various

1:11:56.920 --> 1:12:03.680
 people, there are people now who invented some breakthroughs once, but

1:12:03.680 --> 1:12:06.120
 there are very few people who did it multiple times.

1:12:06.280 --> 1:12:10.880
 And you can think if someone invented it once, that might be just a sheer luck.

1:12:11.680 --> 1:12:15.160
 And if someone invented it multiple times, you know, if a probability of

1:12:15.160 --> 1:12:19.080
 inventing it once is one over a million, then probability of inventing it twice

1:12:19.080 --> 1:12:22.200
 or three times would be one over a million square or, or to the power of

1:12:22.200 --> 1:12:24.800
 three, which, which would be just impossible.

1:12:25.040 --> 1:12:30.280
 So it literally means that it's, it's given that, uh, it's not the luck.

1:12:30.680 --> 1:12:30.920
 Yeah.

1:12:30.920 --> 1:12:36.680
 And Ilya is one of these few people who, uh, uh, who have, uh, a lot of

1:12:36.680 --> 1:12:38.400
 these inventions in his arsenal.

1:12:38.640 --> 1:12:42.800
 It also feels that, um, you know, for instance, if you think about folks

1:12:42.800 --> 1:12:49.760
 like Gauss or Euler, uh, you know, at first they read a lot of books and then

1:12:49.760 --> 1:12:55.280
 they did thinking and then they figure out math and that's how it feels with

1:12:55.280 --> 1:13:00.320
 Ilya, you know, at first he read stuff and then like he spent his thinking cycles.

1:13:01.000 --> 1:13:04.120
 And that's a really good way to put it.

1:13:05.680 --> 1:13:11.320
 When I talk to him, I, I see thinking.

1:13:11.320 --> 1:13:15.960
 He's actually thinking, like, he makes me realize that there's like deep

1:13:15.960 --> 1:13:17.800
 thinking that the human mind can do.

1:13:18.280 --> 1:13:20.480
 Like most of us are not thinking deeply.

1:13:21.440 --> 1:13:24.560
 Uh, like you really have to put in a lot of effort to think deeply.

1:13:24.760 --> 1:13:29.040
 Like I have to really put myself in a place where I think deeply about a

1:13:29.040 --> 1:13:30.640
 problem, it takes a lot of effort.

1:13:30.960 --> 1:13:33.680
 It's like, uh, it's like an airplane taking off or something.

1:13:33.680 --> 1:13:35.320
 You have to achieve deep focus.

1:13:35.640 --> 1:13:38.560
 He he's just, uh, he's what is it?

1:13:38.560 --> 1:13:43.600
 He said, what does it, his brain is like a vertical takeoff in

1:13:43.600 --> 1:13:45.080
 terms of airplane analogy.

1:13:45.320 --> 1:13:49.520
 So it's interesting, but it, I mean, Cal Newport talks about

1:13:49.520 --> 1:13:51.080
 this as ideas of deep work.

1:13:51.880 --> 1:13:57.400
 It's, you know, most of us don't work much at all in terms of like, like deeply

1:13:57.400 --> 1:14:01.400
 think about particular problems, whether it's a math engineering, all that kind

1:14:01.400 --> 1:14:06.480
 of stuff, you want to go to that place often and that's real hard work.

1:14:06.480 --> 1:14:08.520
 And some of us are better than others at that.

1:14:08.760 --> 1:14:13.040
 So I think that the big piece has to do with actually even engineering

1:14:13.040 --> 1:14:15.640
 your environment that says that it's conducive to that.

1:14:15.840 --> 1:14:16.040
 Yeah.

1:14:16.040 --> 1:14:22.480
 So, um, see both Ilya and I, uh, on the frequent basis, we kind of disconnect

1:14:22.480 --> 1:14:26.760
 ourselves from the world in order to be able to do extensive amount of thinking.

1:14:26.920 --> 1:14:27.200
 Yes.

1:14:27.480 --> 1:14:33.320
 So Ilya usually, he just, uh, leaves iPad at hand.

1:14:33.400 --> 1:14:34.400
 He loves his iPad.

1:14:34.400 --> 1:14:39.320
 And, uh, for me, I'm even sometimes, you know, just going for a few days

1:14:39.320 --> 1:14:44.520
 to different location to Airbnb, I'm turning off my phone and there is no

1:14:44.520 --> 1:14:51.040
 access to me and, uh, that's extremely important for me to be able to actually

1:14:51.040 --> 1:14:55.040
 just formulate new thoughts, to do deep work rather than to be reactive.

1:14:55.400 --> 1:15:00.440
 And the, the, the older I am, the more of these random tasks are at hand.

1:15:00.440 --> 1:15:05.600
 Before I go on to that, uh, thread, let me return to our friend, GPT.

1:15:06.400 --> 1:15:08.720
 And let me ask you another ridiculously big question.

1:15:09.440 --> 1:15:13.840
 Can you give an overview of what GPT three is, or like you say in

1:15:13.840 --> 1:15:20.320
 your Twitter bio, GPT N plus one, how it works and why it works.

1:15:21.120 --> 1:15:25.640
 So, um, GPT three is a humongous neural network.

1:15:25.640 --> 1:15:30.760
 Um, let's assume that we know what is neural network, the definition, and it

1:15:30.760 --> 1:15:36.000
 is trained on the entire internet and just to predict next word.

1:15:36.000 --> 1:15:41.400
 So let's say it sees part of the, uh, article and it, the only task that it

1:15:41.400 --> 1:15:45.680
 has at hand, it is to say what would be the next word and what would be the next

1:15:45.680 --> 1:15:51.800
 word and it becomes a really exceptional at the task of figuring out what's the

1:15:51.800 --> 1:15:57.640
 next word. So you might ask, why would, uh, this be an important, uh, task?

1:15:57.640 --> 1:16:00.560
 Why would it be important to predict what's the next word?

1:16:01.280 --> 1:16:07.920
 And it turns out that a lot of problems, uh, can be formulated, uh, as a text

1:16:07.920 --> 1:16:08.840
 completion problem.

1:16:08.840 --> 1:16:12.560
 So GPT is purely, uh, learning to complete the text.

1:16:13.120 --> 1:16:17.240
 And you could imagine, for instance, if you are asking a question, uh, who is

1:16:17.240 --> 1:16:21.840
 the president of the United States, then GPT can give you an answer to it.

1:16:22.160 --> 1:16:25.720
 It turns out that many more things can be formulated this way.

1:16:25.720 --> 1:16:29.880
 You can format text in the way that you have sentence in English.

1:16:30.920 --> 1:16:35.600
 You make it even look like some content of a website, uh, elsewhere, which would

1:16:35.600 --> 1:16:38.440
 be teaching people how to translate things between languages.

1:16:38.440 --> 1:16:43.720
 So it would be EN colon, uh, text in English, FR colon, and then you'll

1:16:43.720 --> 1:16:48.560
 uh, uh, and then you'll ask people and then you ask model to, to continue.

1:16:48.560 --> 1:16:52.720
 And it turns out that the, such a model is predicting translation from English

1:16:52.720 --> 1:16:53.240
 to French.

1:16:53.640 --> 1:17:00.840
 The crazy thing is that this model can be used for way more sophisticated tasks.

1:17:00.840 --> 1:17:05.240
 So you can format text such that it looks like a conversation between two people.

1:17:05.640 --> 1:17:08.920
 And that might be a conversation between you and Elon Musk.

1:17:08.920 --> 1:17:13.960
 And because the model read all the texts about Elon Musk, it will be able to

1:17:13.960 --> 1:17:16.480
 predict Elon Musk words as it would be Elon Musk.

1:17:16.480 --> 1:17:22.160
 It will speak about colonization of Mars, about sustainable future and so on.

1:17:22.560 --> 1:17:29.200
 And it's also possible to, to even give arbitrary personality to the model.

1:17:29.200 --> 1:17:32.640
 You can say, here is a conversation that we've a friendly AI bot.

1:17:32.640 --> 1:17:37.480
 And the model, uh, will complete the text as a friendly AI bot.

1:17:37.520 --> 1:17:43.680
 So, I mean, how do I express how amazing this is?

1:17:43.920 --> 1:17:49.760
 So just to clarify, uh, a conversation, generating a conversation between me and

1:17:49.760 --> 1:17:56.320
 Elon Musk, it wouldn't just generate good examples of what Elon would say.

1:17:56.800 --> 1:18:01.080
 It would get the same results as the conversation between Elon Musk and me.

1:18:01.080 --> 1:18:03.960
 Say it would get the syntax all correct.

1:18:04.200 --> 1:18:09.000
 So like interview style, it would say like Elon call and Lex call, like it,

1:18:09.280 --> 1:18:17.240
 it's not just like, uh, inklings of, um, semantic correctness.

1:18:17.720 --> 1:18:25.520
 It's like the whole thing, grammatical, syntactic, semantic, it's just really,

1:18:25.520 --> 1:18:29.080
 really impressive, uh, generalization.

1:18:30.000 --> 1:18:30.280
 Yeah.

1:18:30.280 --> 1:18:34.680
 I mean, I also want to, you know, provide some caveats so it can generate

1:18:34.680 --> 1:18:38.880
 few paragraphs of coherent text, but as you go to, uh, longer pieces,

1:18:38.880 --> 1:18:41.120
 it, uh, it actually goes off the rails.

1:18:41.360 --> 1:18:41.480
 Okay.

1:18:41.480 --> 1:18:45.360
 If you try to write a book, it won't work out this way.

1:18:45.680 --> 1:18:47.840
 What way does it go off the rails, by the way?

1:18:47.840 --> 1:18:50.320
 Is there interesting ways in which it goes off the rails?

1:18:50.560 --> 1:18:53.160
 Like what falls apart first?

1:18:54.040 --> 1:18:58.720
 So the model is trained on the, all the existing data, uh, that is out there,

1:18:58.720 --> 1:19:01.920
 which means that it is not trained on its own mistakes.

1:19:02.040 --> 1:19:06.160
 So for instance, if it would make a mistake, then, uh, I kept,

1:19:06.360 --> 1:19:07.960
 so to give you, give you an example.

1:19:08.160 --> 1:19:13.880
 So let's say I have a conversation with a model pretending that is Elon Musk.

1:19:14.360 --> 1:19:18.960
 And then I start putting some, uh, I'm start actually making up

1:19:19.000 --> 1:19:20.320
 things which are not factual.

1:19:21.360 --> 1:19:25.680
 Um, I would say like Twitter, but I got you.

1:19:25.680 --> 1:19:26.040
 Sorry.

1:19:26.120 --> 1:19:26.440
 Yeah.

1:19:26.440 --> 1:19:28.960
 Um, like, uh, I don't know.

1:19:28.960 --> 1:19:35.440
 I would say that Elon is my wife and the model will just keep on carrying it on.

1:19:35.440 --> 1:19:36.560
 And as if it's true.

1:19:37.120 --> 1:19:37.600
 Yes.

1:19:38.000 --> 1:19:41.680
 And in some sense, if you would have a normal conversation with Elon,

1:19:41.720 --> 1:19:42.720
 he would be what the fuck.

1:19:43.160 --> 1:19:43.600
 Yeah.

1:19:43.760 --> 1:19:48.480
 There'll be some feedback between, so the model is trained on things

1:19:48.480 --> 1:19:52.280
 that humans have written, but through the generation process, there's

1:19:52.280 --> 1:19:54.000
 no human in the loop feedback.

1:19:54.200 --> 1:19:54.600
 Correct.

1:19:55.360 --> 1:19:56.240
 That's fascinating.

1:19:56.240 --> 1:19:56.920
 Makes sense.

1:19:57.000 --> 1:19:57.960
 So it's magnified.

1:19:57.960 --> 1:20:03.160
 It's like the errors get magnified and magnified and it's also interesting.

1:20:04.880 --> 1:20:06.760
 I mean, first of all, humans have the same problem.

1:20:06.760 --> 1:20:13.600
 It's just that we, uh, we'll make fewer errors and magnify the errors slower.

1:20:13.960 --> 1:20:17.400
 I think that actually what happens with humans is if you have a wrong

1:20:17.400 --> 1:20:21.720
 belief about the world as a kid, then very quickly we'll learn that it's

1:20:21.720 --> 1:20:25.320
 not correct because they are grounded in reality and they are learning

1:20:25.320 --> 1:20:26.400
 from your new experience.

1:20:26.400 --> 1:20:26.680
 Yes.

1:20:27.520 --> 1:20:30.240
 But do you think the model can correct itself too?

1:20:30.960 --> 1:20:34.040
 Won't it through the power of the representation.

1:20:34.840 --> 1:20:40.560
 And so the absence of Elon Musk being your wife information on the

1:20:40.560 --> 1:20:42.880
 internet, won't it correct itself?

1:20:43.720 --> 1:20:45.280
 There won't be examples like that.

1:20:45.760 --> 1:20:47.960
 So the errors will be subtle at first.

1:20:48.320 --> 1:20:49.200
 Subtle at first.

1:20:49.200 --> 1:20:54.440
 And in some sense, you can also say that the data that is not out there is

1:20:54.440 --> 1:21:00.400
 the data, which would represent how the human learns and maybe model would

1:21:00.400 --> 1:21:01.800
 be learned, trained on such a data.

1:21:01.800 --> 1:21:03.120
 Then it would be better off.

1:21:03.480 --> 1:21:06.440
 How intelligent is GPT3 do you think?

1:21:06.480 --> 1:21:10.080
 Like when you think about the nature of intelligence, it

1:21:10.080 --> 1:21:12.320
 seems exceptionally impressive.

1:21:14.440 --> 1:21:18.040
 But then if you think about the big AGI problem, is this

1:21:18.040 --> 1:21:20.120
 footsteps along the way to AGI?

1:21:20.120 --> 1:21:25.480
 So let's see, it seems that intelligence itself is, there are multiple axis of it.

1:21:25.920 --> 1:21:33.240
 And I would expect that the systems that we are building, they might end up being

1:21:33.280 --> 1:21:37.360
 superhuman on some axis and subhuman on some other axis.

1:21:37.360 --> 1:21:41.400
 It would be surprising to me on all axis simultaneously, they would become superhuman.

1:21:43.040 --> 1:21:48.560
 Of course, people ask this question, is GPT a spaceship that would take us to

1:21:48.560 --> 1:21:52.360
 the moon or are we putting a, building a ladder to heaven that we are just

1:21:52.360 --> 1:21:53.960
 building bigger and bigger ladder.

1:21:54.520 --> 1:21:58.880
 And we don't know in some sense, which one of these two.

1:21:59.080 --> 1:21:59.840
 Which one is better?

1:22:02.240 --> 1:22:04.120
 I'm trying to, I like stairway to heaven.

1:22:04.120 --> 1:22:04.840
 It's a good song.

1:22:04.840 --> 1:22:08.120
 So I'm not exactly sure which one is better, but you're saying like the

1:22:08.120 --> 1:22:10.040
 spaceship to the moon is actually effective.

1:22:10.680 --> 1:22:11.080
 Correct.

1:22:11.080 --> 1:22:17.960
 So people who criticize GPT, they say, you guys just building a

1:22:17.960 --> 1:22:20.960
 taller, a ladder, and it will never reach the moon.

1:22:22.320 --> 1:22:28.080
 And at the moment, I would say the way I'm thinking is, is like a scientific question.

1:22:28.480 --> 1:22:35.040
 And I'm also in heart, I'm a builder creator and like, I'm thinking, let's try out, let's

1:22:35.040 --> 1:22:36.200
 see how far it goes.

1:22:36.840 --> 1:22:40.480
 And so far we see constantly that there is a progress.

1:22:40.800 --> 1:22:41.320
 Yeah.

1:22:41.320 --> 1:22:52.320
 So do you think GPT four, GPT five, GPT N plus one will, um, there'll be a phase

1:22:52.320 --> 1:22:56.760
 shift, like a transition to a, to a place where we'll be truly surprised.

1:22:56.960 --> 1:23:00.360
 Then again, like GPT three is already very like truly surprising.

1:23:00.880 --> 1:23:04.600
 The people that criticize GPT three as a stair, as a, what is it?

1:23:04.600 --> 1:23:05.560
 Ladder to heaven.

1:23:06.240 --> 1:23:09.880
 I think too quickly get accustomed to how impressive it is that they're

1:23:09.880 --> 1:23:15.080
 impressive, it is that the prediction of the next word can achieve such depth of

1:23:15.080 --> 1:23:19.640
 semantics, accuracy of syntax, grammar, and semantics.

1:23:20.680 --> 1:23:26.720
 Um, do you, do you think GPT four and five and six will continue to surprise us?

1:23:28.120 --> 1:23:31.320
 I mean, definitely there will be more impressive models that there is a

1:23:31.320 --> 1:23:38.560
 question of course, if there will be a phase shift and, uh, the, also even the

1:23:38.560 --> 1:23:42.880
 way I'm thinking about the, about these models is that when we build these

1:23:42.880 --> 1:23:47.560
 models, you know, we see some level of the capabilities, but we don't even fully

1:23:47.560 --> 1:23:50.000
 understand everything that the model can do.

1:23:50.280 --> 1:23:55.880
 And actually one of the best things to do is to allow other people to probe the

1:23:55.880 --> 1:23:57.560
 model to even see what is possible.

1:23:58.880 --> 1:24:05.240
 Hence the, the using GPT as an API and opening it up to the world.

1:24:05.320 --> 1:24:05.600
 Yeah.

1:24:05.600 --> 1:24:10.680
 I mean, so when I'm thinking from perspective of like, uh, obviously

1:24:10.680 --> 1:24:14.280
 various people are, that have concerns about AGI, including myself.

1:24:14.840 --> 1:24:18.920
 Um, and then when I'm thinking from perspective, what's the strategy even to

1:24:18.960 --> 1:24:23.880
 deploy these things to the world, then the one strategy that I have seen many

1:24:23.880 --> 1:24:29.360
 times working is that iterative deployment that you deploy, um, slightly

1:24:29.360 --> 1:24:32.520
 better versions and you allow other people to criticize you.

1:24:32.520 --> 1:24:36.720
 So you actually, or try it out, you see where are their fundamental issues.

1:24:37.200 --> 1:24:42.280
 And it's almost, you don't want to be in that situation that you are holding

1:24:42.320 --> 1:24:48.320
 into powerful system and there's like a huge overhang, then you deploy it and it

1:24:48.320 --> 1:24:50.960
 might have a random chaotic impact on the world.

1:24:50.960 --> 1:24:53.800
 So you actually want to be in the situation that they are

1:24:53.800 --> 1:24:55.520
 gradually deploying systems.

1:24:56.560 --> 1:25:00.680
 I asked this question of Illya, let me ask you, uh, you this question.

1:25:00.680 --> 1:25:04.680
 I've been reading a lot about Stalin and power.

1:25:09.360 --> 1:25:14.480
 If you're in possession of a system that's like AGI, that's exceptionally

1:25:14.480 --> 1:25:20.640
 powerful, do you think your character and integrity might become corrupted?

1:25:21.040 --> 1:25:23.920
 Like famously power corrupts and absolute power corrupts.

1:25:23.920 --> 1:25:24.440
 Absolutely.

1:25:24.440 --> 1:25:30.640
 So I believe that the, you want at some point to work toward distributing the power.

1:25:31.440 --> 1:25:36.360
 I think that the, you want to be in the situation that actually AGI is not

1:25:36.360 --> 1:25:42.680
 controlled by a small number of people, uh, but, uh, essentially, uh, by a larger

1:25:42.680 --> 1:25:43.200
 collective.

1:25:43.560 --> 1:25:50.360
 So the thing is that requires a George Washington style move in the ascent to

1:25:50.360 --> 1:25:55.280
 power, there's always a moment when somebody gets a lot of power and they

1:25:55.280 --> 1:26:01.040
 have to have the integrity and, uh, the moral compass to give away that power.

1:26:01.920 --> 1:26:06.480
 That humans have been good and bad throughout history at this particular

1:26:06.480 --> 1:26:07.000
 step.

1:26:07.400 --> 1:26:13.120
 And I wonder, I wonder we like blind ourselves in a, for example, between

1:26:13.120 --> 1:26:20.440
 nations, a race, uh, towards, um, they, yeah, AI race between nations, we might

1:26:20.440 --> 1:26:25.240
 blind ourselves and justify to ourselves the development of AI without distributing

1:26:25.240 --> 1:26:29.440
 the power because we want to defend ourselves against China, against Russia,

1:26:29.920 --> 1:26:31.480
 that kind of, that kind of logic.

1:26:32.360 --> 1:26:40.160
 And, um, I wonder how we, um, how we design governance mechanisms that, um,

1:26:40.160 --> 1:26:45.360
 prevent us from becoming power hungry and in the process, destroying ourselves.

1:26:46.280 --> 1:26:50.600
 So let's see, I have been thinking about this topic quite a bit, but I also want

1:26:50.600 --> 1:26:55.800
 to admit that, uh, once again, I actually want to rely way more on Sam Altman on it.

1:26:55.840 --> 1:27:01.280
 He wrote an excellent blog on how even to distribute wealth.

1:27:01.280 --> 1:27:08.720
 Um, and he's proper, he proposed in his blog, uh, to tax, uh, equity of the companies

1:27:08.720 --> 1:27:10.680
 rather than profit and to distribute it.

1:27:11.000 --> 1:27:15.680
 And this is, this is an example of, uh, Washington move.

1:27:17.680 --> 1:27:24.040
 I guess I personally have insane trust in some here already spent plenty of money

1:27:24.320 --> 1:27:28.360
 running, uh, universal basic income, uh, project.

1:27:28.360 --> 1:27:33.280
 That like, uh, gives me, I guess, maybe some level of trust to him, but I also,

1:27:34.480 --> 1:27:37.480
 I guess love him as a friend.

1:27:37.720 --> 1:27:38.220
 Yeah.

1:27:38.920 --> 1:27:43.120
 I wonder because we're sort of summoning a new set of technologies.

1:27:44.280 --> 1:27:50.280
 I wonder if we'll be, um, cognizant, like you're describing the process of open AI,

1:27:50.680 --> 1:27:54.360
 but it could also be at other places like in the U S government, right?

1:27:54.360 --> 1:28:00.680
 Uh, both China and the U S are now full steam ahead on autonomous

1:28:00.680 --> 1:28:02.160
 weapons systems development.

1:28:03.200 --> 1:28:09.680
 And that's really worrying to me because in the framework of something being a

1:28:09.680 --> 1:28:14.880
 national security danger or military danger, you can do a lot of pretty dark

1:28:14.880 --> 1:28:18.720
 things that blind our moral compass.

1:28:18.720 --> 1:28:23.920
 And I think AI will be one of those things, um, in some sense, the, the mission

1:28:24.320 --> 1:28:28.440
 and the work you're doing in open AI is like the counterbalance to that.

1:28:28.840 --> 1:28:32.760
 So you want to have more open AI and less autonomous weapons systems.

1:28:33.200 --> 1:28:37.200
 I, I, I, I like these statements, like to be clear, like this interesting and I'm

1:28:37.200 --> 1:28:43.760
 thinking about it myself, but, uh, this is a place that I, I, I put my trust

1:28:43.760 --> 1:28:48.440
 actually in Sam's hands, because it's extremely hard for me to reason about it.

1:28:48.760 --> 1:28:49.080
 Yeah.

1:28:49.200 --> 1:28:54.200
 I mean, one important statement to make is, um, it's good to think about this.

1:28:54.280 --> 1:28:54.600
 Yeah.

1:28:54.640 --> 1:28:55.520
 No question about it.

1:28:55.520 --> 1:29:02.680
 No question, even like low level quote unquote engineer, like there's such a,

1:29:02.680 --> 1:29:10.080
 um, I remember I, I programmed a car, uh, our RC car, um, and it was, it was

1:29:10.080 --> 1:29:17.760
 programmed a car, uh, our RC car, they went really fast, like 30, 40 miles an hour.

1:29:18.480 --> 1:29:21.040
 And I remember I was like sleep deprived.

1:29:21.080 --> 1:29:26.440
 So I programmed it pretty crappily and it like, uh, the, the, the code froze.

1:29:26.440 --> 1:29:30.040
 So it's doing some basic computer vision and it's going around on track,

1:29:30.280 --> 1:29:31.600
 but it's going full speed.

1:29:32.640 --> 1:29:39.280
 And, uh, there was a bug in the code that, uh, the car just went, it didn't turn.

1:29:39.280 --> 1:29:42.040
 Went straight full speed and smash into the wall.

1:29:42.520 --> 1:29:49.480
 And I remember thinking the seriousness with which you need to approach the

1:29:49.480 --> 1:29:53.240
 design of artificial intelligence systems and the programming of artificial

1:29:53.240 --> 1:29:58.520
 intelligence systems is high because the consequences are high, like that

1:29:58.520 --> 1:30:00.320
 little car smashing into the wall.

1:30:00.880 --> 1:30:04.480
 For some reason, I immediately thought of like an algorithm that controls

1:30:04.480 --> 1:30:07.160
 nuclear weapons, having the same kind of bug.

1:30:07.160 --> 1:30:11.840
 And so like the lowest level engineer and the CEO of a company all need to

1:30:11.840 --> 1:30:15.240
 have the seriousness, uh, in approaching this problem and thinking

1:30:15.240 --> 1:30:16.840
 about the worst case consequences.

1:30:17.000 --> 1:30:18.560
 So I think that is true.

1:30:18.800 --> 1:30:24.840
 I mean, the, what I also recognize in myself and others even asking this

1:30:24.840 --> 1:30:29.680
 question is that it evokes a lot of fear and fear itself ends up being

1:30:29.680 --> 1:30:31.400
 actually quite debilitating.

1:30:31.400 --> 1:30:38.680
 The place where I arrived at the moment might sound cheesy or so, but it's

1:30:38.680 --> 1:30:48.680
 almost to build things out of love rather than fear, like a focus on how, uh, I can,

1:30:48.720 --> 1:30:54.000
 you know, maximize the value, how the systems that I'm building might be, uh,

1:30:54.280 --> 1:30:54.800
 useful.

1:30:55.800 --> 1:31:00.400
 I'm not saying that the fear doesn't exist out there and like it totally

1:31:00.400 --> 1:31:04.920
 makes sense to minimize it, but I don't want to be working because, uh, I'm

1:31:04.920 --> 1:31:10.320
 scared, I want to be working out of passion, out of curiosity, out of the,

1:31:10.640 --> 1:31:13.120
 you know, uh, looking forward for the positive future.

1:31:13.840 --> 1:31:19.480
 With, uh, the definition of love arising from a rigorous practice of empathy.

1:31:19.800 --> 1:31:23.560
 So not just like your own conception of what is good for the world, but

1:31:23.600 --> 1:31:24.840
 always listening to others.

1:31:25.160 --> 1:31:25.560
 Correct.

1:31:25.560 --> 1:31:29.160
 Like the love where I'm considering reward functions of others.

1:31:29.160 --> 1:31:35.280
 Others to limit to infinity is like a sum of like one to N where N is, uh,

1:31:35.280 --> 1:31:36.680
 7 billion or whatever it is.

1:31:36.680 --> 1:31:38.880
 Not, not projecting my reward functions on others.

1:31:38.920 --> 1:31:39.720
 Yeah, exactly.

1:31:40.440 --> 1:31:40.920
 Okay.

1:31:41.360 --> 1:31:43.760
 Can we just take a step back to something else?

1:31:43.760 --> 1:31:46.240
 Super cool, which is, uh, OpenAI Codex.

1:31:47.240 --> 1:31:53.200
 Can you give an overview of what OpenAI Codex and GitHub Copilot is, how it works

1:31:53.680 --> 1:31:55.280
 and why the hell it works so well?

1:31:55.280 --> 1:32:00.960
 So with GPT tree, we noticed that the system, uh, you know, that system train

1:32:00.960 --> 1:32:05.400
 on all the language out there started having some rudimentary coding capabilities.

1:32:05.440 --> 1:32:10.880
 So we're able to ask it, you know, to implement addition function between

1:32:10.880 --> 1:32:14.680
 two numbers and indeed it can write item or JavaScript code for that.

1:32:15.320 --> 1:32:20.520
 And then we thought, uh, we might as well just go full steam ahead and try to

1:32:20.520 --> 1:32:25.320
 create a system that is actually good at what we are doing every day ourselves,

1:32:25.800 --> 1:32:26.680
 which is programming.

1:32:27.320 --> 1:32:31.440
 We optimize models for proficiency in coding.

1:32:31.600 --> 1:32:38.040
 We actually even created models that both have a comprehension of language and code.

1:32:38.840 --> 1:32:42.200
 And Codex is API for these models.

1:32:42.600 --> 1:32:48.840
 So it's first pre trained on language and then codex.

1:32:48.840 --> 1:32:54.080
 Then I don't know if you can say fine tuned because there's a lot of code,

1:32:54.600 --> 1:32:56.320
 but it's language and code.

1:32:56.400 --> 1:32:57.320
 It's language and code.

1:32:58.320 --> 1:33:00.200
 It's also optimized for various things.

1:33:00.200 --> 1:33:01.960
 I can, let's say low latency and so on.

1:33:02.600 --> 1:33:05.520
 Codex is the API, the similar to GPT tree.

1:33:06.000 --> 1:33:10.560
 We expect that there will be proliferation of the potential products that can use

1:33:10.560 --> 1:33:14.640
 coding capabilities and I can, I can speak about it in a second.

1:33:14.920 --> 1:33:18.200
 Copilot is a first product and developed by GitHub.

1:33:18.200 --> 1:33:22.000
 So as we're building, uh, models, we wanted to make sure that these

1:33:22.000 --> 1:33:26.840
 models are useful and we work together with GitHub on building the first product.

1:33:27.320 --> 1:33:32.040
 Copilot is actually, as you code, it suggests you code completions.

1:33:32.240 --> 1:33:36.520
 And we have seen in the past, there are like a various tools that can suggest

1:33:36.760 --> 1:33:40.600
 how to like a few characters of the code or a line of code.

1:33:41.000 --> 1:33:44.600
 Then the thing about Copilot is it can generate 10 lines of code.

1:33:44.600 --> 1:33:49.160
 You, it's often the way how it works is you often write in the comment

1:33:49.480 --> 1:33:53.720
 what you want to happen because people in comments, they describe what happens next.

1:33:53.960 --> 1:34:00.200
 So, um, these days when I code, instead of going to Google to search, uh, for

1:34:00.200 --> 1:34:06.200
 the appropriate code to solve my problem, I say, Oh, for this area, could you

1:34:06.200 --> 1:34:10.520
 smooth it and then, you know, it imports some appropriate libraries and say it

1:34:10.520 --> 1:34:15.000
 uses NumPy convolution or so I, that I was not even aware that exists and

1:34:15.000 --> 1:34:16.240
 it does the appropriate thing.

1:34:16.840 --> 1:34:21.440
 Um, so you, uh, you write a comment, maybe the header of a function

1:34:21.440 --> 1:34:22.680
 and it completes the function.

1:34:23.320 --> 1:34:27.200
 Of course, you don't know what is the space of all the possible small

1:34:27.200 --> 1:34:28.440
 programs that can generate.

1:34:28.840 --> 1:34:30.360
 What are the failure cases?

1:34:30.360 --> 1:34:34.880
 How many edge cases, how many subtle errors there are, how many big errors

1:34:34.880 --> 1:34:38.840
 there are, it's hard to know, but the fact that it works at all in a large

1:34:38.840 --> 1:34:40.680
 number of cases is incredible.

1:34:41.000 --> 1:34:45.920
 It's like, uh, it's a kind of search engine into code that's

1:34:45.920 --> 1:34:47.120
 been written on the internet.

1:34:47.720 --> 1:34:48.120
 Correct.

1:34:48.120 --> 1:34:53.240
 So for instance, when you search things online, then usually you get to the,

1:34:53.720 --> 1:34:58.920
 some particular case, like if you go to stack overflow and people describe

1:34:58.920 --> 1:35:03.040
 that one particular situation, uh, and then they seek for a solution.

1:35:03.040 --> 1:35:08.040
 But in case of a copilot, it's aware of your entire context and in

1:35:08.040 --> 1:35:10.320
 context is, Oh, these are the libraries that they are using.

1:35:10.480 --> 1:35:13.640
 That's the set of the variables that is initialized.

1:35:14.120 --> 1:35:16.520
 And on the spot, it can actually tell you what to do.

1:35:17.280 --> 1:35:21.280
 So the interesting thing is, and we think that the copilot is one

1:35:21.280 --> 1:35:25.080
 possible product using codecs, but there is a place for many more.

1:35:25.080 --> 1:35:29.480
 So internally we tried out, you know, to create other fun products.

1:35:29.760 --> 1:35:33.880
 So it turns out that a lot of tools out there, let's say Google

1:35:33.880 --> 1:35:38.480
 calendar or Microsoft word or so, they all have a internal API

1:35:38.480 --> 1:35:40.480
 to build plugins around them.

1:35:41.240 --> 1:35:47.000
 So there is a way in the sophisticated way to control calendar or Microsoft word.

1:35:47.520 --> 1:35:51.160
 Today, if you want, if you want more complicated behaviors from these

1:35:51.160 --> 1:35:54.000
 programs, you have to add the new button for every behavior.

1:35:55.040 --> 1:36:00.440
 But it is possible to use codecs and tell for instance, to calendar, uh,

1:36:00.440 --> 1:36:06.200
 could you schedule an appointment with Lex next week after 2 PM and it

1:36:06.200 --> 1:36:07.800
 writes corresponding piece of code.

1:36:08.920 --> 1:36:10.760
 And that's the thing that actually you want.

1:36:10.800 --> 1:36:11.440
 So interesting.

1:36:11.440 --> 1:36:15.000
 So you figure out is there's a lot of programs with which

1:36:15.000 --> 1:36:16.440
 you can interact through code.

1:36:17.080 --> 1:36:21.960
 And so there you can generate that code from natural language.

1:36:22.480 --> 1:36:23.400
 That's fascinating.

1:36:23.440 --> 1:36:28.880
 And that's somewhat like also closest to what was the promise of Siri or Alexa.

1:36:28.880 --> 1:36:33.680
 So previously all these behaviors, they were hard coded and it seems

1:36:33.680 --> 1:36:39.000
 that codecs on the fly can pick up the API of let's say, given software.

1:36:39.360 --> 1:36:42.320
 And then it can turn language into use of this API.

1:36:42.320 --> 1:36:46.560
 So without hard coding, you can find, it can translate to machine language.

1:36:46.640 --> 1:36:47.040
 Correct.

1:36:47.040 --> 1:36:51.840
 To, uh, so for example, this would be really exciting for me, like for, um,

1:36:51.880 --> 1:36:57.320
 Adobe products, like Photoshop, uh, which I think action scripted, I think

1:36:57.320 --> 1:37:00.080
 there's a scripting language that communicates with them, same with Premier.

1:37:00.440 --> 1:37:05.960
 And do you could imagine that that allows even to do coding by voice on your phone?

1:37:06.480 --> 1:37:08.880
 So for instance, in the past, okay.

1:37:09.000 --> 1:37:13.760
 As of today, I'm not editing Word documents on my phone because it's

1:37:13.760 --> 1:37:15.360
 just the keyboard is too small.

1:37:15.480 --> 1:37:20.520
 But if I would be able to tell, uh, to my phone, you know, uh, make the

1:37:20.520 --> 1:37:25.040
 header large, then move the paragraphs around and that's actually what I want.

1:37:25.040 --> 1:37:29.080
 So I can tell you one more cool thing, or even how I'm thinking about codecs.

1:37:29.720 --> 1:37:36.280
 So if you look actually at the evolution of, uh, of computers, we started with

1:37:36.320 --> 1:37:40.160
 a very primitive interfaces, which is a punch card and punch card.

1:37:40.320 --> 1:37:46.280
 So Charlie, you make a holes in the, in the plastic card to indicate zeros and ones.

1:37:47.040 --> 1:37:50.720
 And, uh, during that time, there was a small number of specialists

1:37:50.720 --> 1:37:52.040
 who were able to use computers.

1:37:52.040 --> 1:37:55.000
 And by the way, people even suspected that there is no need for many

1:37:55.000 --> 1:37:56.320
 more people to use computers.

1:37:56.960 --> 1:38:03.880
 Um, but then we moved from punch cards to at first assembly and see, and

1:38:03.920 --> 1:38:07.040
 at these programming languages, they were slightly higher level.

1:38:07.200 --> 1:38:11.920
 They allowed many more people to code and they also, uh, led to more

1:38:11.920 --> 1:38:13.600
 of a proliferation of technology.

1:38:14.040 --> 1:38:19.960
 And, uh, you know, further on, there was a jump to say from C++ to Java and Python.

1:38:19.960 --> 1:38:23.560
 And every time it has happened, more people are able to code

1:38:23.600 --> 1:38:25.840
 and we build more technology.

1:38:26.200 --> 1:38:31.200
 And it's even, you know, hard to imagine now, if someone will tell you that you

1:38:31.200 --> 1:38:36.760
 should write code in assembly instead of let's say, Python or Java or JavaScript.

1:38:37.160 --> 1:38:41.520
 And codecs is yet another step toward kind of bringing computers closer to

1:38:41.520 --> 1:38:47.120
 humans such that you communicate with a computer with your own language rather

1:38:47.120 --> 1:38:52.600
 than with a specialized language, and, uh, I think that it will lead to an

1:38:52.600 --> 1:38:54.600
 increase of number of people who can code.

1:38:55.280 --> 1:38:55.440
 Yeah.

1:38:55.440 --> 1:39:00.160
 And then, and the kind of technologies that those people will create is it's

1:39:00.160 --> 1:39:03.760
 innumerable, it could, you know, it could be a huge number of technologies.

1:39:03.760 --> 1:39:07.560
 We're not predicting at all because that's less and less requirement

1:39:07.600 --> 1:39:13.480
 of having a technical mind, a programming mind, you're not opening it to the world

1:39:13.480 --> 1:39:19.360
 of, um, other kinds of minds, creative minds, artistic minds, all that kind of stuff.

1:39:19.400 --> 1:39:23.720
 I would like, for instance, biologists who work on DNA to be able to program

1:39:23.800 --> 1:39:26.720
 and not to need to spend a lot of time learning it.

1:39:26.720 --> 1:39:28.600
 And I, I believe that's a good thing to the world.

1:39:29.080 --> 1:39:33.720
 And I would actually add, I would add, so at the moment I'm a managing codecs

1:39:33.800 --> 1:39:37.800
 team and also language team, and I believe that there is like a plenty

1:39:37.800 --> 1:39:41.640
 of brilliant people out there and they should have a lot of experience.

1:39:41.640 --> 1:39:43.800
 There and they should apply.

1:39:44.360 --> 1:39:45.080
 Oh, okay.

1:39:45.080 --> 1:39:45.320
 Yeah.

1:39:45.320 --> 1:39:45.720
 Awesome.

1:39:45.880 --> 1:39:48.440
 So what's the language and the codecs is, so those are kind of,

1:39:48.960 --> 1:39:50.760
 they're overlapping teams.

1:39:50.760 --> 1:39:56.640
 It's like GPT, the raw language, and then the codecs is like applied to programming.

1:39:57.120 --> 1:39:57.480
 Correct.

1:39:57.480 --> 1:39:59.280
 And they are quite intertwined.

1:40:00.000 --> 1:40:03.880
 There are many more things involved making this, uh, models,

1:40:03.960 --> 1:40:06.240
 uh, extremely efficient and deployable.

1:40:06.480 --> 1:40:06.600
 Okay.

1:40:06.600 --> 1:40:10.800
 For instance, there are people who are working to, you know, make our data

1:40:10.800 --> 1:40:14.960
 centers, uh, amazing, or there are people who work on putting these

1:40:14.960 --> 1:40:20.120
 models into production or, uh, or even pushing it at the very limit of the scale.

1:40:21.640 --> 1:40:25.240
 So all aspects from, from the infrastructure to the actual machine.

1:40:25.240 --> 1:40:29.640
 So I'm just saying there are multiple teams while the, and the team working

1:40:29.640 --> 1:40:33.440
 on codecs and language, uh, I guess I'm, I'm directly managing them.

1:40:33.560 --> 1:40:37.560
 I would like, I would love to hire more interested in machine learning.

1:40:37.560 --> 1:40:41.960
 This is probably one of the most exciting problems and like systems

1:40:41.960 --> 1:40:45.520
 to be working on is it's actually, it's, it's, it's pretty cool.

1:40:45.560 --> 1:40:48.760
 Like what, what, uh, the program synthesis, like generating a

1:40:48.760 --> 1:40:53.480
 programs is very interesting, very interesting problem that has echoes

1:40:53.480 --> 1:40:56.520
 of reasoning and intelligence in it.

1:40:57.080 --> 1:41:00.520
 It's and I think there's a lot of fundamental questions that you might

1:41:00.520 --> 1:41:05.480
 be able to sneak, uh, sneak up to by generating programs.

1:41:05.480 --> 1:41:09.600
 Yeah, that one more exciting thing about the programs is that, so I said

1:41:09.600 --> 1:41:13.720
 that the, um, you know, the, in case of language, that one of the travels

1:41:13.720 --> 1:41:15.160
 is even evaluating language.

1:41:15.200 --> 1:41:20.600
 So when the things are made up, you, you need somehow either a human to,

1:41:20.840 --> 1:41:25.360
 to say that this doesn't make sense or so in case of program, there is one extra

1:41:25.360 --> 1:41:29.400
 lever that we can actually execute programs and see what they evaluate to.

1:41:29.400 --> 1:41:35.800
 So that process might be somewhat, uh, more automated in, in order to improve

1:41:35.800 --> 1:41:38.040
 the, uh, qualities of generations.

1:41:38.440 --> 1:41:39.160
 Oh, that's fascinating.

1:41:39.160 --> 1:41:41.800
 So like the, wow, that's really interesting.

1:41:42.120 --> 1:41:45.680
 So, so for the language, the, you know, the simulation to actually

1:41:45.680 --> 1:41:47.200
 execute it as a human mind.

1:41:47.440 --> 1:41:47.940
 Yeah.

1:41:48.280 --> 1:41:52.400
 For programs, there is a, there is a computer on which you can evaluate it.

1:41:53.760 --> 1:41:54.720
 Wow.

1:41:54.960 --> 1:41:58.400
 That's a brilliant little insight.

1:41:58.400 --> 1:42:04.640
 Insight that the thing compiles and runs that's first and second, you can evaluate

1:42:04.880 --> 1:42:11.000
 on a, like do automated unit testing and in some sense, it seems to me that we'll

1:42:11.000 --> 1:42:12.920
 be able to make a tremendous progress.

1:42:12.920 --> 1:42:17.320
 You know, we are in the paradigm that there is way more data.

1:42:17.320 --> 1:42:23.440
 There is like a transcription of millions of, uh, of, uh, software engineers.

1:42:23.520 --> 1:42:24.020
 Yeah.

1:42:24.320 --> 1:42:24.820
 Yeah.

1:42:24.820 --> 1:42:29.020
 So, uh, I mean, you just mean, cause I was going to ask you about reliability.

1:42:29.300 --> 1:42:35.260
 The thing about programs is you don't know if they're going to, like a program

1:42:35.260 --> 1:42:38.860
 that's controlling a nuclear power plant has to be very reliable.

1:42:39.140 --> 1:42:43.060
 So I wouldn't start with controlling nuclear power plant maybe one day,

1:42:43.140 --> 1:42:46.380
 but that's not actually, that's not on the current roadmap.

1:42:46.420 --> 1:42:48.060
 That's not the step one.

1:42:48.540 --> 1:42:50.460
 And you know, it's the Russian thing.

1:42:50.460 --> 1:42:53.500
 You just want to go to the most powerful, destructive, most powerful

1:42:53.500 --> 1:42:57.620
 the most powerful, destructive thing right away run by JavaScript.

1:42:57.660 --> 1:42:58.300
 But I got you.

1:42:58.300 --> 1:43:01.020
 So this is a lower impact, but nevertheless, when you make me

1:43:01.020 --> 1:43:05.380
 realize it is possible to achieve some levels of reliability by doing testing.

1:43:06.620 --> 1:43:09.820
 And you could, you could imagine that, you know, maybe there are ways for

1:43:09.820 --> 1:43:15.260
 model to write event code for testing itself and so on, and there exists

1:43:15.340 --> 1:43:19.260
 a ways to create the feedback loops that the model could keep on improving.

1:43:19.260 --> 1:43:25.420
 Yeah. By writing programs that generate tests for the instance, for instance.

1:43:26.940 --> 1:43:30.460
 And that's how we get consciousness, because it's metacompression.

1:43:30.660 --> 1:43:31.540
 That's what you're going to write.

1:43:31.540 --> 1:43:32.460
 That's the comment.

1:43:32.460 --> 1:43:34.380
 That's the prompt that generates consciousness.

1:43:34.900 --> 1:43:36.780
 Compressor of compressors.

1:43:36.780 --> 1:43:37.540
 You just write that.

1:43:38.500 --> 1:43:41.300
 Do you think the code that generates consciousness will be simple?

1:43:42.300 --> 1:43:44.140
 So let's see.

1:43:44.140 --> 1:43:48.060
 I mean, ultimately, the core idea behind will be simple,

1:43:48.060 --> 1:43:53.380
 but there will be also decent amount of engineering involved.

1:43:53.380 --> 1:43:58.580
 Like in some sense, it seems that, you know, spreading these models

1:43:58.580 --> 1:44:01.780
 on many machines, it's not that trivial.

1:44:01.860 --> 1:44:02.260
 Yeah.

1:44:02.260 --> 1:44:07.820
 And we find all sorts of innovations that make our models more efficient.

1:44:08.460 --> 1:44:14.460
 I believe that first models that I guess are conscious or like a truly intelligent,

1:44:14.460 --> 1:44:21.580
 they will have all sorts of tricks, but then again, there's a Richard Sutton

1:44:21.620 --> 1:44:25.780
 argument that maybe the tricks are temporary things that they might be

1:44:25.780 --> 1:44:32.300
 temporary things and in some sense, it's also even important to, to know

1:44:32.300 --> 1:44:33.780
 that even the cost of a trick.

1:44:33.780 --> 1:44:38.220
 So sometimes people are eager to put the trick while forgetting that

1:44:38.220 --> 1:44:43.300
 there is a cost of maintenance or like a long term cost, long term cost

1:44:43.300 --> 1:44:48.980
 or maintenance, or maybe even flexibility of code to actually implement new ideas.

1:44:48.980 --> 1:44:52.740
 So even if you have something that gives you 2x, but it requires, you know,

1:44:53.100 --> 1:44:55.860
 1000 lines of code, I'm not sure if it's actually worth it.

1:44:56.300 --> 1:45:00.980
 So in some sense, you know, if it's five lines of code and 2x, I would take it.

1:45:02.060 --> 1:45:07.620
 And we see many of this, but also, you know, that requires some level of,

1:45:07.620 --> 1:45:12.180
 I guess, lack of attachment to code that we are willing to remove it.

1:45:12.540 --> 1:45:13.020
 Yeah.

1:45:14.620 --> 1:45:17.340
 So you led the OpenAI robotics team.

1:45:17.580 --> 1:45:20.460
 Can you give an overview of the cool things you were able to

1:45:20.460 --> 1:45:21.980
 accomplish, what are you most proud of?

1:45:22.780 --> 1:45:26.060
 So when we started robotics, we knew that actually reinforcement learning works

1:45:26.060 --> 1:45:29.620
 and it is possible to solve fairly complicated problems.

1:45:29.940 --> 1:45:36.020
 Like for instance, AlphaGo is an evidence that it is possible to build superhuman

1:45:36.020 --> 1:45:44.060
 Go players, DOTA2 is an evidence that it's possible to build superhuman agents

1:45:44.060 --> 1:45:48.500
 playing DOTA, so I asked myself a question, you know, what about robots out there?

1:45:48.820 --> 1:45:52.940
 Could we train machines to solve arbitrary tasks in the physical world?

1:45:53.820 --> 1:45:59.620
 Our approach was, I guess, let's pick a complicated problem that if we would

1:45:59.620 --> 1:46:04.260
 solve it, that means that we made some significant progress in the domain.

1:46:04.260 --> 1:46:07.780
 And if can progress the domain, and then we went after the problem.

1:46:08.220 --> 1:46:13.740
 So we noticed that actually the robots out there, they are kind of at the moment

1:46:13.780 --> 1:46:18.420
 optimized per task, so you can have a robot that it's like, if you have a robot

1:46:18.420 --> 1:46:22.460
 opening a bottle, it's very likely that the end factor is that bottle opener.

1:46:24.060 --> 1:46:27.740
 And the, and in some sense, that's a hack to be able to solve a task,

1:46:27.780 --> 1:46:33.180
 which makes any task easier and ask myself, so what would be a robot that

1:46:33.180 --> 1:46:34.540
 can actually solve many tasks?

1:46:35.300 --> 1:46:42.900
 And we conclude that human hands have such a quality that indeed they are, you

1:46:42.900 --> 1:46:48.060
 know, you have five kind of tiny arms attached individually.

1:46:48.060 --> 1:46:51.420
 They can manipulate pretty broad spectrum of objects.

1:46:51.860 --> 1:46:57.140
 So we went after a single hand, like trying to solve Rubik's cube single handed.

1:46:57.420 --> 1:47:01.740
 We picked this task because we thought that there is no way to hard code it.

1:47:01.740 --> 1:47:05.380
 And it's also, we picked the robot on which it would be hard to hard code it.

1:47:05.700 --> 1:47:11.140
 And we went after the solution such that it could generalize to other problems.

1:47:11.180 --> 1:47:15.900
 And just to clarify, it's one robotic hand solving the Rubik's cube.

1:47:16.300 --> 1:47:20.580
 The hard part isn't the solution to the Rubik's cube is the manipulation of the,

1:47:21.180 --> 1:47:27.100
 of like having it not fall out of the hand, having it use the, uh, five baby

1:47:27.100 --> 1:47:32.020
 arms to, uh, what is it like rotate different parts of the Rubik's cube to

1:47:32.020 --> 1:47:32.980
 achieve the solution.

1:47:33.140 --> 1:47:33.540
 Correct.

1:47:33.940 --> 1:47:34.260
 Yeah.

1:47:34.660 --> 1:47:37.740
 So what, uh, what was the hardest part about that?

1:47:38.380 --> 1:47:39.980
 What was the approach taken there?

1:47:40.180 --> 1:47:41.260
 What are you most proud of?

1:47:41.460 --> 1:47:44.300
 Obviously we have like a strong belief in reinforcement learning.

1:47:44.980 --> 1:47:49.660
 And, uh, you know, one path it is to do reinforcement learning, the real

1:47:49.660 --> 1:47:55.860
 world other path is to, uh, uh, that simulation in some sense, the tricky

1:47:55.860 --> 1:47:59.620
 part about the real world is at the moment, our models, they require a lot

1:47:59.620 --> 1:48:01.660
 of data and there is essentially no data.

1:48:02.220 --> 1:48:07.060
 And, uh, I did, we decided to go through the path of the simulation.

1:48:07.060 --> 1:48:09.420
 And in simulation, you can have infinite amount of data.

1:48:09.780 --> 1:48:12.380
 The tricky part is the fidelity of the simulation.

1:48:12.740 --> 1:48:16.780
 And also can you in simulation represent everything that you represent

1:48:16.780 --> 1:48:18.140
 otherwise in the real world.

1:48:18.940 --> 1:48:22.900
 And, you know, it turned out that, uh, that, you know, because there is

1:48:22.900 --> 1:48:29.820
 lack of fidelity, it is possible to what we, what we arrived at is training

1:48:29.820 --> 1:48:33.820
 a model that doesn't solve one simulation, but it actually solves the

1:48:34.180 --> 1:48:39.260
 entire range of simulations, which, uh, uh, in terms of like, uh, what's

1:48:39.260 --> 1:48:45.260
 the, exactly the friction of the cube or the weight or so, and the single AI

1:48:45.260 --> 1:48:48.860
 that can solve all of them ends up working well with the reality.

1:48:49.220 --> 1:48:51.300
 How do you generate the different simulations?

1:48:51.300 --> 1:48:54.220
 So, uh, you know, there's plenty of parameters out there.

1:48:54.260 --> 1:48:55.540
 We just pick them randomly.

1:48:55.820 --> 1:49:01.740
 And, uh, and in simulation model just goes for thousands of years and keeps

1:49:01.740 --> 1:49:03.460
 on solving Rubik's cube in each of them.

1:49:03.780 --> 1:49:08.660
 And the thing is that neural network that we used, it has a memory.

1:49:09.260 --> 1:49:15.580
 And as it presses, for instance, the side of the, of the cube, it can sense,

1:49:15.620 --> 1:49:19.620
 oh, that's actually, this side was, uh, difficult to press.

1:49:19.620 --> 1:49:24.540
 I should press it stronger and throughout this process kind of, uh, learn it's even

1:49:24.540 --> 1:49:29.060
 how to, uh, how to solve this particular instance of the Rubik's cube, like even

1:49:29.060 --> 1:49:34.620
 mass, it's kind of like, uh, you know, sometimes when you go to a gym and after,

1:49:34.660 --> 1:49:44.020
 um, after bench press, you try to leave the class and you kind of forgot, uh, and,

1:49:44.060 --> 1:49:48.900
 and your head goes like up right away because kind of you got used to maybe

1:49:48.900 --> 1:49:54.940
 different weight and it takes a second to adjust and this kind of, of a memory,

1:49:54.940 --> 1:49:58.180
 the model gained through the process of interacting with the cube in the

1:49:58.180 --> 1:50:02.580
 simulation, I appreciate you speaking to the audience with the bench press,

1:50:02.660 --> 1:50:05.780
 all the bros in the audience, probably working out right now.

1:50:05.780 --> 1:50:08.540
 There's probably somebody listening to this actually doing bench press.

1:50:09.300 --> 1:50:13.900
 Um, so maybe, uh, put the bar down and pick up the water bottle and you'll

1:50:13.900 --> 1:50:17.020
 know exactly what, uh, what Jack is talking about.

1:50:17.060 --> 1:50:17.540
 Okay.

1:50:17.540 --> 1:50:18.500
 Okay.

1:50:18.500 --> 1:50:24.620
 So what, uh, what was the hardest part of getting the whole thing to work?

1:50:24.780 --> 1:50:31.660
 So the hardest part is at the moment when it comes to, uh, physical work, when it

1:50:31.660 --> 1:50:36.740
 comes to robots, uh, they require maintenance, it's hard to replicate a

1:50:36.740 --> 1:50:41.620
 million times it's, uh, it's also, it's hard to replay things exactly.

1:50:41.620 --> 1:50:48.460
 I remember this situation that one guy at our company, he had like a model that

1:50:48.460 --> 1:50:52.180
 performs way better than other models in solving Rubik's cube.

1:50:52.580 --> 1:50:57.540
 And, uh, you know, we kind of didn't know what's going on, why it's that.

1:50:58.420 --> 1:51:04.420
 And, uh, it turned out, uh, that, you know, he was running it from his laptop

1:51:04.420 --> 1:51:09.540
 that had better CPU or better, maybe local GPU as well.

1:51:09.540 --> 1:51:14.140
 And, uh, because of that, there was less of a latency and the model was the same.

1:51:14.780 --> 1:51:18.540
 And that actually made solving Rubik's cube more reliable.

1:51:18.820 --> 1:51:22.300
 So in some sense, there might be some subtle bugs like that when it comes

1:51:22.300 --> 1:51:24.060
 to running things in the real world.

1:51:24.700 --> 1:51:29.420
 Even hinting on that, you could imagine that the initial models you would like

1:51:29.420 --> 1:51:34.140
 to have models, which are insanely huge neural networks, and you would like to

1:51:34.140 --> 1:51:36.460
 give them even more time for thinking.

1:51:36.460 --> 1:51:41.980
 And when you have these real time systems, uh, then you might be constrained

1:51:41.980 --> 1:51:43.700
 actually by the amount of latency.

1:51:44.660 --> 1:51:50.940
 And, uh, ultimately I would like to build a system that it is worth for you to wait

1:51:50.940 --> 1:51:55.220
 five minutes because it gives you the answer that you're willing to wait for

1:51:55.220 --> 1:51:55.900
 five minutes.

1:51:56.260 --> 1:51:59.620
 So latency is a very unpleasant constraint under which to operate.

1:51:59.820 --> 1:52:00.320
 Correct.

1:52:00.620 --> 1:52:04.260
 And also there is actually one more thing, which is tricky about robots.

1:52:04.260 --> 1:52:08.060
 Uh, there is actually, uh, no, uh, not much data.

1:52:08.060 --> 1:52:13.380
 So the data that I'm speaking about would be a data of, uh, first person

1:52:13.380 --> 1:52:17.660
 experience from the robot and like a gigabytes of data like that, if we would

1:52:17.660 --> 1:52:21.900
 have gigabytes of data like that, of robots solving various problems, it would

1:52:21.900 --> 1:52:24.060
 be very easy to make a progress on robotics.

1:52:24.420 --> 1:52:28.660
 And you can see that in case of text or code, there is a lot of data, like a

1:52:28.660 --> 1:52:31.980
 first person perspective, they don't writing code.

1:52:31.980 --> 1:52:37.740
 Yeah. So you had this, you mentioned this really interesting idea that if you were

1:52:37.740 --> 1:52:42.100
 to build like a successful robotics company, so open as mission is much

1:52:42.100 --> 1:52:46.500
 bigger than robotics, this is one of the, one of the things you've worked on, but

1:52:46.500 --> 1:52:51.260
 if it was a robotics company, they, you wouldn't so quickly dismiss supervised

1:52:51.260 --> 1:52:58.300
 learning, uh, correct that you would build a robot that, uh, was perhaps what

1:52:58.300 --> 1:53:04.180
 like, um, an empty shell, like dumb, and they would operate under teleoperation.

1:53:04.660 --> 1:53:09.660
 So you would invest, that's just one way to do it, invest in human supervision,

1:53:09.700 --> 1:53:14.740
 like direct human control of the robots as it's learning and over time, add

1:53:14.740 --> 1:53:15.820
 more and more automation.

1:53:16.380 --> 1:53:16.860
 That's correct.

1:53:16.860 --> 1:53:20.220
 So let's say that's how I would build a robotics company today.

1:53:20.780 --> 1:53:23.620
 If I would be building a robotics company, which is, you know, spend 10

1:53:23.620 --> 1:53:28.660
 million dollars or so recording human trajectories, controlling a robot.

1:53:29.100 --> 1:53:34.860
 After you find a thing that the robot should be doing, that there's a market

1:53:34.860 --> 1:53:37.340
 fit for, like you can make a lot of money with that product.

1:53:37.380 --> 1:53:37.700
 Correct.

1:53:37.700 --> 1:53:38.100
 Correct.

1:53:38.100 --> 1:53:38.500
 Yeah.

1:53:38.500 --> 1:53:43.420
 Uh, so I would record data and then I would essentially train supervised

1:53:43.500 --> 1:53:44.460
 learning model on it.

1:53:45.020 --> 1:53:46.540
 That might be the path today.

1:53:47.220 --> 1:53:47.860
 Long term.

1:53:47.860 --> 1:53:52.340
 I think that actually what is needed is to have a robot that can

1:53:52.340 --> 1:53:54.780
 train powerful models over video.

1:53:55.580 --> 1:54:02.740
 So, um, you have seen maybe a models that can generate images like Dali and people

1:54:02.740 --> 1:54:06.260
 are looking into models, generating videos, they're like, uh, bodies,

1:54:06.300 --> 1:54:08.260
 algorithmic questions, even how to do it.

1:54:08.500 --> 1:54:13.220
 And it's unclear if there is enough compute for this purpose, but, uh, I, I

1:54:13.220 --> 1:54:19.300
 suspect that the models that which would have a level of understanding of video,

1:54:19.300 --> 1:54:25.620
 same as GPT has a level of understanding of text, could be used, uh, to train

1:54:25.620 --> 1:54:26.540
 robots to solve tasks.

1:54:26.580 --> 1:54:28.300
 They would have a lot of common sense.

1:54:29.780 --> 1:54:36.420
 If one day, I'm pretty sure one day there will be a robotics company by robotics

1:54:36.420 --> 1:54:42.740
 company, I mean, the primary source of income is, is from robots that is worth

1:54:42.740 --> 1:54:44.740
 over $1 trillion.

1:54:44.740 --> 1:54:49.940
 What do you think that company will do?

1:54:49.940 --> 1:54:51.620
 I think self driving cars.

1:54:51.620 --> 1:54:53.260
 No, it's interesting.

1:54:53.260 --> 1:54:56.500
 Cause my mind went to personal robotics, robots in the home.

1:54:57.220 --> 1:54:59.820
 It seems like there's much more market opportunity there.

1:55:00.300 --> 1:55:03.380
 I think it's very difficult to achieve.

1:55:04.420 --> 1:55:09.460
 I mean, this, this, this might speak to something important, which is I understand

1:55:09.460 --> 1:55:12.180
 self driving much better than understand robotics in the home.

1:55:12.180 --> 1:55:17.500
 So I understand how difficult it is to actually solve self driving to a, to a

1:55:17.500 --> 1:55:22.060
 level, not just the actual computer vision and the control problem and just the

1:55:22.060 --> 1:55:27.540
 basic problem of self driving, but creating a product that would undeniably

1:55:28.100 --> 1:55:31.260
 be, um, that will cost less money.

1:55:31.300 --> 1:55:34.020
 Like it will save you a lot of money, like orders of magnitude, less money

1:55:34.220 --> 1:55:36.300
 that could replace Uber drivers, for example.

1:55:36.780 --> 1:55:41.380
 So car sharing that's autonomous, that creates a similar or better

1:55:41.380 --> 1:55:46.220
 experience in terms of how quickly you get from A to B or just whatever, the

1:55:46.220 --> 1:55:50.260
 pleasantness of the experience, the efficiency of the experience, the value

1:55:50.260 --> 1:55:54.580
 of the experience, and at the same time, the car itself costs cheaper.

1:55:55.300 --> 1:55:57.300
 I think that's very difficult to achieve.

1:55:57.340 --> 1:56:02.140
 I think there's a lot more, um, low hanging fruit in the home.

1:56:03.780 --> 1:56:08.340
 That, that, that could be, I also want to give you a perspective on like how

1:56:08.340 --> 1:56:12.900
 challenging it would be at home or like it maybe kind of depends on that exact

1:56:12.900 --> 1:56:14.100
 problem that you'd be solving.

1:56:14.100 --> 1:56:20.220
 Like if we're speaking about these robotic arms and hands, these things,

1:56:20.220 --> 1:56:27.540
 they cost tens of thousands of dollars or maybe a hundred K and, um, you know,

1:56:27.580 --> 1:56:30.260
 maybe, obviously, maybe there would be economy of scale.

1:56:30.260 --> 1:56:34.460
 These things would be cheaper, but actually for any household to buy it,

1:56:34.540 --> 1:56:37.340
 the price would have to go down to maybe a thousand bucks.

1:56:37.340 --> 1:56:38.340
 Yeah.

1:56:38.340 --> 1:56:44.220
 I personally think that, uh, so self driving car, it provides a clear service.

1:56:44.500 --> 1:56:48.180
 I don't think robots in the home, there'll be a trillion dollar company

1:56:48.180 --> 1:56:53.260
 will just be all about service, meaning it will not necessarily be about like

1:56:53.260 --> 1:56:56.060
 a robotic arm that's helps you.

1:56:56.100 --> 1:57:02.580
 I don't know, open a bottle or wash the dishes or, uh, any of that kind of stuff.

1:57:02.580 --> 1:57:05.940
 It has to be able to take care of that whole, the therapist thing.

1:57:05.940 --> 1:57:10.700
 You mentioned, I think that's, um, of course there's a line between what

1:57:10.700 --> 1:57:14.420
 is a robot and what is not like, does it really need a body?

1:57:14.460 --> 1:57:19.780
 But you know, some, um, uh, AI system with some embodiment, I think.

1:57:20.340 --> 1:57:23.860
 So the tricky part when you think actually what's the difficult part is,

1:57:24.260 --> 1:57:29.940
 um, when the robot has like, when there is a diversity of the environment

1:57:29.940 --> 1:57:31.980
 with which the robot has to interact, that becomes hard.

1:57:31.980 --> 1:57:36.740
 So, you know, on the one spectrum, you have, uh, industrial robots as they

1:57:36.740 --> 1:57:40.900
 are doing over and over the same thing, it is possible to some extent to

1:57:40.900 --> 1:57:46.220
 prescribe the movements and we've very small amount of intelligence, the, the

1:57:46.300 --> 1:57:48.100
 movement can be repeated millions of times.

1:57:48.100 --> 1:57:52.700
 Um, the, it, there are also, you know, various pieces of industrial robots

1:57:52.700 --> 1:57:54.340
 where it becomes harder and harder.

1:57:54.500 --> 1:57:59.460
 I can, for instance, in case of Tesla, it might be a matter of putting a, a

1:57:59.460 --> 1:58:03.860
 rack inside of a car and, you know, because the rack kind of moves around,

1:58:03.860 --> 1:58:05.580
 it's, uh, it's not that easy.

1:58:05.580 --> 1:58:07.940
 It's not exactly the same every time.

1:58:08.100 --> 1:58:10.460
 That's not being the case that you need actually humans to do it.

1:58:11.500 --> 1:58:15.580
 Uh, while, you know, welding cars together, it's a very repetitive process.

1:58:16.100 --> 1:58:23.300
 Um, then in case of self driving itself, uh, that difficulty has to do with the

1:58:23.460 --> 1:58:27.860
 diversity of the environment, but still the car itself, um, the problem

1:58:27.860 --> 1:58:32.300
 that they are solving is you try to avoid even interacting with things.

1:58:32.540 --> 1:58:35.900
 You are not touching anything around because touching itself is hard.

1:58:36.140 --> 1:58:40.580
 And then if you would have in the home, uh, robot that, you know, has to

1:58:40.580 --> 1:58:44.140
 touch things and like if these things, they change the shape, if there is a huge

1:58:44.140 --> 1:58:46.660
 variety of things to be touched, then that's difficult.

1:58:46.860 --> 1:58:50.300
 If you are speaking about the robot, which there is, you know, head that

1:58:50.300 --> 1:58:54.380
 is smiling in some way with cameras that either doesn't, you know, touch things.

1:58:54.660 --> 1:58:55.900
 That's relatively simple.

1:58:55.900 --> 1:58:59.260
 Okay. So to both agree and to push back.

1:59:00.060 --> 1:59:07.260
 So you're referring to touch, like soft robotics, like the actual touch, but.

1:59:08.060 --> 1:59:13.860
 I would argue that you could formulate just basic interaction between, um, like

1:59:13.900 --> 1:59:18.660
 non contact interaction is also a kind of touch and that might be very difficult

1:59:18.660 --> 1:59:22.620
 to solve that's the basic, this not disagreement, but that's the basic open

1:59:22.620 --> 1:59:27.540
 question to me with self driving cars and this agreement with Elon, which

1:59:27.540 --> 1:59:31.260
 is how much interaction is required to solve self driving cars.

1:59:31.260 --> 1:59:32.820
 How much touch is required?

1:59:33.180 --> 1:59:36.900
 You said that in your intuition, touch is not required.

1:59:37.380 --> 1:59:41.820
 And my intuition to create a product that's compelling to use, you're going

1:59:41.820 --> 1:59:46.740
 to have to, uh, interact with pedestrians, not just avoid pedestrians,

1:59:46.740 --> 1:59:49.980
 but interact with them when we drive around.

1:59:49.980 --> 1:59:54.100
 In major cities, we're constantly threatening everybody's life with

1:59:54.100 --> 1:59:57.740
 our movements, um, and that's how they respect us.

1:59:57.740 --> 2:00:02.940
 There's a game to ready going out with pedestrians and I'm afraid you can't

2:00:02.940 --> 2:00:08.460
 just formulate autonomous driving as a collision avoidance problem.

2:00:08.820 --> 2:00:12.380
 So I think it goes beyond like a collision avoidance is the

2:00:12.380 --> 2:00:13.700
 first order approximation.

2:00:14.180 --> 2:00:18.420
 Uh, but then at least in case of Tesla, you can't just

2:00:18.420 --> 2:00:22.500
 at least in case of Tesla, they are gathering data from people driving their

2:00:22.500 --> 2:00:27.220
 cars and I believe that's an example of supervised data that they can train

2:00:27.220 --> 2:00:32.900
 their models, uh, on, and they are doing it, uh, which, you know, can give

2:00:32.900 --> 2:00:38.900
 a model dislike, uh, another level of, uh, of, uh, behavior that is needed

2:00:38.900 --> 2:00:40.900
 to actually interact with the real world.

2:00:41.140 --> 2:00:41.340
 Yeah.

2:00:41.340 --> 2:00:45.340
 It's interesting how much data is required to achieve that.

2:00:45.340 --> 2:00:49.380
 Um, w what do you think of the whole Tesla autopilot approach, the computer

2:00:49.380 --> 2:00:53.380
 vision based approach with multiple cameras and there's a data engine.

2:00:53.380 --> 2:00:57.820
 It's a multitask, multiheaded neural network, and it's this fascinating

2:00:57.820 --> 2:01:02.740
 process of, uh, similar to what you're talking about with the robotics

2:01:02.780 --> 2:01:06.540
 approach, uh, which is, you know, you deploy in your own network and

2:01:06.540 --> 2:01:10.940
 then there's humans that use it and then it runs into trouble in a bunch

2:01:10.940 --> 2:01:12.780
 of places and that stuff is sent back.

2:01:12.780 --> 2:01:17.740
 So like the deployment discovers a bunch of edge cases and those edge

2:01:17.740 --> 2:01:22.140
 cases are sent back for supervised annotation, thereby improving the

2:01:22.140 --> 2:01:23.980
 neural network and that's deployed again.

2:01:24.540 --> 2:01:29.340
 It goes over and over until the network becomes really good at the task of

2:01:29.340 --> 2:01:31.340
 driving becomes safer and safer.

2:01:31.580 --> 2:01:34.460
 What do you think of that kind of approach to robotics?

2:01:34.700 --> 2:01:36.060
 I believe that's the way to go.

2:01:36.100 --> 2:01:39.660
 So in some sense, even when I was speaking about, you know, collecting

2:01:39.660 --> 2:01:43.180
 trajectories from humans, that's like a first step and then you deploy

2:01:43.180 --> 2:01:46.620
 the system and then you have humans revising the, all the issues.

2:01:46.620 --> 2:01:51.580
 And in some sense, like at this approach converges to system that doesn't make

2:01:51.580 --> 2:01:54.700
 mistakes because for the cases where there are mistakes, you got their

2:01:54.700 --> 2:01:57.660
 data, how to fix them and the system will keep on improving.

2:01:58.220 --> 2:02:02.380
 So there's a very, to me, difficult question of how hard that, you know,

2:02:02.460 --> 2:02:04.940
 how long that converging takes, how hard it is.

2:02:04.940 --> 2:02:09.180
 The other aspect of autonomous vehicles, this probably applies to certain

2:02:09.180 --> 2:02:12.620
 robotics applications is society, right?

2:02:12.700 --> 2:02:17.580
 They put as, as the quality of the system converges.

2:02:18.220 --> 2:02:21.820
 So one, there's a human factors perspective of psychology of humans being

2:02:21.820 --> 2:02:25.420
 able to supervise those even with teleoperation, those robots.

2:02:25.740 --> 2:02:28.380
 And the other is society willing to accept robots.

2:02:29.100 --> 2:02:32.540
 Currently society is much harsher on self driving cars than it is on human

2:02:32.540 --> 2:02:35.340
 driven cars in terms of the expectation of safety.

2:02:35.660 --> 2:02:38.620
 So the bar is set much higher than for humans.

2:02:39.100 --> 2:02:43.740
 And so if there's a death in an autonomous vehicle, that's seen as a much more,

2:02:47.180 --> 2:02:50.220
 much more dramatic than a death in the human driven vehicle.

2:02:50.940 --> 2:02:55.260
 Part of the success of deployment of robots is figuring out how to make robots

2:02:55.260 --> 2:03:01.100
 part of society, both on the, just the human side, on the media side, on the

2:03:01.100 --> 2:03:04.460
 media journalist side, and also on the policy government side.

2:03:04.780 --> 2:03:08.620
 And that seems to be, maybe you can put that into the objective function to

2:03:08.620 --> 2:03:14.860
 optimize, but that is, that is definitely a tricky one.

2:03:14.860 --> 2:03:18.460
 And I wonder if that is actually the trickiest part for self driving cars or

2:03:18.460 --> 2:03:20.300
 any system that's safety critical.

2:03:21.340 --> 2:03:24.460
 It's not the algorithm, it's the society accepting it.

2:03:24.460 --> 2:03:31.020
 Yeah, I would say, I believe that the part of the process of deployment is actually

2:03:31.020 --> 2:03:36.860
 showing people that the given things can be trusted and, you know, trust is also

2:03:36.860 --> 2:03:42.620
 like a glass that is actually really easy to crack it and damage it.

2:03:43.100 --> 2:03:52.300
 And I think that's actually very common with, with innovation, that there's

2:03:52.300 --> 2:03:56.620
 some resistance toward it and it's just the natural progression.

2:03:56.620 --> 2:04:00.140
 So in some sense, people will have to keep on proving that indeed these

2:04:00.140 --> 2:04:02.300
 systems are worth being used.

2:04:02.780 --> 2:04:08.940
 And I would say, I also found out that often the best way to convince people

2:04:09.420 --> 2:04:11.420
 is by letting them experience it.

2:04:11.660 --> 2:04:12.540
 Yeah, absolutely.

2:04:12.540 --> 2:04:17.180
 That's the case with Tesla autopilot, for example, that's the case with, yeah,

2:04:17.180 --> 2:04:18.940
 with basically robots in general.

2:04:18.940 --> 2:04:22.220
 It's kind of funny to hear people talk about robots.

2:04:22.220 --> 2:04:27.420
 Like there's a lot of fear, even with like legged robots, but when they

2:04:27.420 --> 2:04:30.780
 actually interact with them, there's joy.

2:04:31.420 --> 2:04:32.700
 I love interacting with them.

2:04:32.780 --> 2:04:38.860
 And the same with the car, with a robot, if it starts being useful, I think

2:04:38.860 --> 2:04:40.380
 people immediately understand.

2:04:40.460 --> 2:04:43.100
 And if the product is designed well, they fall in love.

2:04:43.340 --> 2:04:43.820
 You're right.

2:04:44.300 --> 2:04:46.940
 It's actually even similar when I'm thinking about the car.

2:04:46.940 --> 2:04:51.180
 It's actually even similar when I'm thinking about Copilot, the GitHub Copilot.

2:04:51.260 --> 2:04:53.980
 There was a spectrum of responses that people had.

2:04:54.460 --> 2:04:59.900
 And ultimately the important piece was to let people try it out.

2:05:00.140 --> 2:05:02.300
 And then many people just loved it.

2:05:02.620 --> 2:05:04.700
 Especially like programmers.

2:05:05.020 --> 2:05:07.820
 Yeah, programmers, but like some of them, you know, they came with a fear.

2:05:08.300 --> 2:05:08.540
 Yeah.

2:05:08.860 --> 2:05:11.260
 But then you try it out and you think, actually, that's cool.

2:05:11.820 --> 2:05:15.180
 And, you know, you can try to resist the same way as, you know, you could

2:05:15.180 --> 2:05:20.060
 resist moving from punch cards to, let's say, C++ or so.

2:05:20.860 --> 2:05:22.860
 And it's a little bit futile.

2:05:23.980 --> 2:05:30.060
 So we talked about generation of program, generation of language, even

2:05:30.540 --> 2:05:33.820
 self supervised learning in the visual space for robotics and then

2:05:33.820 --> 2:05:34.780
 reinforcement learning.

2:05:35.100 --> 2:05:40.700
 What do you, in like this whole beautiful spectrum of AI, do you think is a

2:05:40.700 --> 2:05:47.340
 good benchmark, a good test to strive for to achieve intelligence?

2:05:47.740 --> 2:05:49.580
 That's a strong test of intelligence.

2:05:49.820 --> 2:05:52.380
 You know, it started with Alan Turing and the Turing test.

2:05:53.260 --> 2:05:56.700
 Maybe you think natural language conversation is a good test.

2:05:57.100 --> 2:06:01.260
 So, you know, it would be nice if, for instance, machine would be able to

2:06:01.340 --> 2:06:03.180
 solve Riemann hypothesis in math.

2:06:04.540 --> 2:06:07.420
 That would be, I think that would be very impressive.

2:06:07.420 --> 2:06:12.940
 So theorem proving, is that to you, proving theorems is a good, oh, oh,

2:06:12.940 --> 2:06:15.820
 like one thing that the machine did, you would say, damn.

2:06:16.460 --> 2:06:17.020
 Exactly.

2:06:18.460 --> 2:06:18.780
 Okay.

2:06:19.420 --> 2:06:22.380
 That would be quite, quite impressive.

2:06:22.460 --> 2:06:26.940
 I mean, the tricky part about the benchmarks is, you know, as we are

2:06:26.940 --> 2:06:29.340
 getting closer with them, we have to invent new benchmarks.

2:06:29.340 --> 2:06:31.180
 There is actually no ultimate benchmark out there.

2:06:31.660 --> 2:06:31.820
 Yeah.

2:06:31.820 --> 2:06:36.140
 See, my thought with the Riemann hypothesis would be the moment the

2:06:36.140 --> 2:06:39.820
 machine proves it, we would say, okay, well then the problem was easy.

2:06:40.860 --> 2:06:41.660
 That's what happens.

2:06:42.060 --> 2:06:46.140
 And I mean, in some sense, that's actually what happens over the years

2:06:46.140 --> 2:06:49.660
 in AI that like, we get used to things very quickly.

2:06:50.380 --> 2:06:52.300
 You know something, I talked to Rodney Brooks.

2:06:52.300 --> 2:06:53.340
 I don't know if you know who that is.

2:06:54.380 --> 2:06:57.020
 He called AlphaZero homework problem.

2:06:57.020 --> 2:06:59.740
 Cause he was saying like, there's nothing special about it.

2:06:59.740 --> 2:07:00.780
 It's not a big leap.

2:07:00.780 --> 2:07:05.260
 And I didn't, well, he's coming from one of the aspects that we referred

2:07:05.260 --> 2:07:10.140
 to is he was part of the founding of iRobot, which deployed now tens

2:07:10.140 --> 2:07:11.900
 of millions of robot in the home.

2:07:11.900 --> 2:07:18.540
 So if you see robots that are actually in the homes of people as the

2:07:18.540 --> 2:07:23.340
 legitimate instantiation of artificial intelligence, then yes, maybe an AI

2:07:23.340 --> 2:07:26.460
 that plays a silly game like go and chess is not a real accomplishment,

2:07:26.460 --> 2:07:29.180
 but to me it's a fundamental leap.

2:07:29.180 --> 2:07:33.740
 But I think we as humans then say, okay, well then that that game of

2:07:33.740 --> 2:07:37.660
 chess or go wasn't that difficult compared to the thing that's currently

2:07:37.660 --> 2:07:38.220
 unsolved.

2:07:38.220 --> 2:07:44.940
 So my intuition is that from perspective of the evolution of these AI

2:07:44.940 --> 2:07:49.820
 systems will at first seen the tremendous progress in digital space.

2:07:49.820 --> 2:07:52.700
 And the, you know, the main thing about digital space is also that you

2:07:52.700 --> 2:07:56.300
 can, everything is that there is a lot of recorded data.

2:07:56.300 --> 2:07:59.900
 Plus you can very rapidly deploy things to billions of people.

2:07:59.900 --> 2:08:05.260
 While in case of a physical space, the deployment part takes multiple

2:08:05.260 --> 2:08:05.500
 years.

2:08:05.500 --> 2:08:10.300
 You have to manufacture things and, you know, delivering it to actual

2:08:10.300 --> 2:08:11.900
 people, it's very hard.

2:08:13.580 --> 2:08:19.980
 So I'm expecting that the first and that prices in digital space of

2:08:19.980 --> 2:08:24.220
 goods, they would go, you know, down to the, let's say marginal costs

2:08:24.220 --> 2:08:25.020
 are two zero.

2:08:25.020 --> 2:08:28.780
 And also the question is how much of our life will be in digital because

2:08:28.780 --> 2:08:31.980
 it seems like we're heading towards more and more of our lives being in

2:08:31.980 --> 2:08:33.260
 the digital space.

2:08:33.260 --> 2:08:37.100
 So like innovation in the physical space might become less and less

2:08:37.100 --> 2:08:38.060
 significant.

2:08:38.060 --> 2:08:42.700
 Like why do you need to drive anywhere if most of your life is spent in

2:08:42.700 --> 2:08:44.060
 virtual reality?

2:08:44.060 --> 2:08:47.980
 I still would like, you know, to at least at the moment, my impression

2:08:47.980 --> 2:08:51.020
 is that I would like to have a physical contact with other people.

2:08:51.020 --> 2:08:52.220
 And that's very important to me.

2:08:52.940 --> 2:08:55.180
 We don't have a way to replicate it in the computer.

2:08:55.180 --> 2:08:57.260
 It might be the case that over the time it will change.

2:08:57.260 --> 2:09:02.380
 Like in 10 years from now, why not have like an arbitrary infinite number

2:09:02.380 --> 2:09:04.060
 of people you can interact with?

2:09:04.060 --> 2:09:09.740
 Some of them are real, some are not with arbitrary characteristics that

2:09:09.740 --> 2:09:12.700
 you can define based on your own preferences.

2:09:12.700 --> 2:09:15.900
 I think that's maybe where we are heading and maybe I'm resisting the

2:09:15.900 --> 2:09:16.460
 future.

2:09:16.460 --> 2:09:25.100
 Yeah, I'm telling you, if I got to choose, if I could live in Elder

2:09:25.100 --> 2:09:29.820
 Scrolls Skyrim versus the real world, I'm not so sure I would stay with

2:09:29.820 --> 2:09:30.380
 the real world.

2:09:31.420 --> 2:09:35.900
 Yeah, I mean, the question is, so will VR be sufficient to get us there

2:09:35.900 --> 2:09:38.940
 or do you need to, you know, plug electrodes in the brain?

2:09:40.140 --> 2:09:43.500
 And it would be nice if these electrodes wouldn't be invasive.

2:09:45.020 --> 2:09:47.500
 Or at least like provably non destructive.

2:09:49.020 --> 2:09:53.420
 But in the digital space, do you think we'll be able to solve the

2:09:53.420 --> 2:09:57.020
 Turing test, the spirit of the Turing test, which is, do you think we'll

2:09:57.020 --> 2:10:02.380
 be able to achieve compelling natural language conversation between

2:10:02.380 --> 2:10:06.300
 people, like have friends that are AI systems on the internet?

2:10:07.100 --> 2:10:08.780
 I totally think it's doable.

2:10:08.780 --> 2:10:12.460
 Do you think the current approach of GPT will take us there?

2:10:12.460 --> 2:10:16.700
 So there is, you know, the part of at first learning all the content

2:10:16.700 --> 2:10:20.060
 out there and I think that Steel System should keep on learning as

2:10:20.060 --> 2:10:21.260
 it speaks with you.

2:10:21.260 --> 2:10:21.500
 Yeah.

2:10:21.500 --> 2:10:23.900
 Yeah, and I think that should work.

2:10:23.900 --> 2:10:25.660
 The question is how exactly to do it.

2:10:25.660 --> 2:10:29.740
 And, you know, obviously we have people at OpenAI asking these

2:10:29.740 --> 2:10:35.100
 questions and kind of at first pre training on all existing content

2:10:35.100 --> 2:10:37.580
 is like a backbone and is a decent backbone.

2:10:39.340 --> 2:10:45.820
 Do you think AI needs a body connecting to our robotics question to

2:10:45.820 --> 2:10:49.100
 truly connect with humans or can most of the connection be in the

2:10:49.100 --> 2:10:49.820
 digital space?

2:10:49.820 --> 2:10:55.260
 So let's see, we know that there are people who met each other online

2:10:55.260 --> 2:10:56.300
 and they fell in love.

2:10:57.740 --> 2:10:57.980
 Yeah.

2:10:58.620 --> 2:11:03.740
 So it seems that it's conceivable to establish connection, which is

2:11:03.740 --> 2:11:06.060
 purely through internet.

2:11:07.340 --> 2:11:10.540
 Of course, it might be more compelling the more modalities you add.

2:11:12.140 --> 2:11:16.620
 So it would be like you're proposing like a Tinder, but for AI, you

2:11:16.620 --> 2:11:20.220
 like swipe right and left and half the systems are AI and the other is

2:11:21.100 --> 2:11:22.700
 humans and you don't know which is which.

2:11:24.380 --> 2:11:27.180
 That would be our formulation of Turing test.

2:11:27.980 --> 2:11:32.460
 The moment AI is able to achieve more swipe right or left, whatever,

2:11:33.260 --> 2:11:36.940
 the moment it's able to be more attractive than other humans, it

2:11:36.940 --> 2:11:38.060
 passes the Turing test.

2:11:38.060 --> 2:11:40.620
 Then you would pass the Turing test in attractiveness.

2:11:40.620 --> 2:11:41.100
 That's right.

2:11:41.100 --> 2:11:42.940
 Well, no, like attractiveness just to clarify.

2:11:42.940 --> 2:11:44.060
 There will be conversation.

2:11:44.060 --> 2:11:44.780
 Not just visual.

2:11:44.780 --> 2:11:45.260
 Right, right.

2:11:45.260 --> 2:11:51.660
 It's also attractiveness with wit and humor and whatever makes

2:11:51.660 --> 2:11:53.340
 conversation is pleasant for humans.

2:11:56.060 --> 2:11:56.700
 Okay.

2:11:56.700 --> 2:11:57.100
 All right.

2:11:58.780 --> 2:12:02.620
 So you're saying it's possible to achieve in the digital space.

2:12:02.620 --> 2:12:04.620
 In some sense, I would almost ask that question.

2:12:05.180 --> 2:12:06.620
 Why wouldn't that be possible?

2:12:07.980 --> 2:12:11.180
 Well, I have this argument with my dad all the time.

2:12:11.180 --> 2:12:13.820
 He thinks that touch and smell are really important.

2:12:13.820 --> 2:12:16.700
 So they can be very important.

2:12:16.700 --> 2:12:19.260
 And I'm saying the initial systems, they won't have it.

2:12:20.380 --> 2:12:28.380
 Still, there are people being born without these senses and I believe

2:12:28.380 --> 2:12:31.580
 that they can still fall in love and have meaningful life.

2:12:32.140 --> 2:12:32.460
 Yeah.

2:12:32.460 --> 2:12:37.500
 I wonder if it's possible to go close to all the way by just training

2:12:37.500 --> 2:12:39.740
 on transcripts of conversations.

2:12:40.620 --> 2:12:42.220
 I wonder how far that takes us.

2:12:42.220 --> 2:12:45.980
 So I think that actually still you want images like I would like.

2:12:45.980 --> 2:12:50.620
 So I don't have kids, but like I could imagine having AI Tutor.

2:12:50.620 --> 2:12:55.100
 It has to see, you know, kids drawing some pictures on the paper.

2:12:56.300 --> 2:12:58.460
 And also facial expressions, all that kind of stuff.

2:12:58.460 --> 2:13:04.060
 We use dogs and humans use their eyes to communicate with each other.

2:13:04.060 --> 2:13:07.500
 I think that's a really powerful mechanism of communication.

2:13:07.500 --> 2:13:12.540
 Body language too, that words are much lower bandwidth.

2:13:12.540 --> 2:13:15.340
 And for body language, we still, you know, we kind of have a system

2:13:15.340 --> 2:13:19.420
 that displays an image of its or facial expression on the computer.

2:13:19.980 --> 2:13:23.420
 Doesn't have to move, you know, mechanical pieces or so.

2:13:23.420 --> 2:13:27.420
 So I think that, you know, that there is like kind of a progression.

2:13:27.420 --> 2:13:31.100
 You can imagine that text might be the simplest to tackle.

2:13:31.660 --> 2:13:36.700
 But this is not a complete human experience at all.

2:13:36.700 --> 2:13:41.260
 You expand it to, let's say images, both for input and output.

2:13:41.260 --> 2:13:45.900
 And what you describe is actually the final, I guess, frontier.

2:13:45.900 --> 2:13:50.060
 What makes us human, the fact that we can touch each other or smell or so.

2:13:50.060 --> 2:13:53.420
 And it's the hardest from perspective of data and deployment.

2:13:54.140 --> 2:13:58.380
 And I believe that these things might happen gradually.

2:13:59.660 --> 2:14:01.340
 Are you excited by that possibility?

2:14:01.340 --> 2:14:07.820
 This particular application of human to AI friendship and interaction?

2:14:07.820 --> 2:14:08.700
 So let's see.

2:14:09.660 --> 2:14:11.500
 Like would you, do you look forward to a world?

2:14:12.380 --> 2:14:15.340
 You said you're living with a few folks and you're very close friends with them.

2:14:16.060 --> 2:14:19.580
 Do you look forward to a day where one or two of those friends are AI systems?

2:14:19.580 --> 2:14:25.180
 So if the system would be truly wishing me well, rather than being in the situation

2:14:25.180 --> 2:14:28.460
 that it optimizes for my time to interact with the system.

2:14:28.460 --> 2:14:33.500
 The line between those is, it's a gray area.

2:14:33.500 --> 2:14:38.060
 I think that's the distinction between love and possession.

2:14:39.340 --> 2:14:46.620
 And these things, they might be often correlated for humans, but you might find that there are

2:14:46.620 --> 2:14:48.860
 some friends with whom you haven't spoke for months.

2:14:49.660 --> 2:14:50.060
 Yeah.

2:14:50.060 --> 2:14:54.620
 And then you pick up the phone, it's as the time hasn't passed.

2:14:54.620 --> 2:14:55.820
 They are not holding to you.

2:14:55.820 --> 2:15:02.300
 And I will, I wouldn't like to have AI system that, you know, it's trying to convince me

2:15:02.300 --> 2:15:03.420
 to spend time with it.

2:15:03.420 --> 2:15:10.700
 I would like the system to optimize for what I care about and help me in achieving my own goals.

2:15:12.300 --> 2:15:17.900
 But there's some, I mean, I don't know, there's some manipulation, there's some possessiveness,

2:15:17.900 --> 2:15:23.340
 there's some insecurities, this fragility, all those things are necessary to form a close

2:15:23.340 --> 2:15:29.260
 friendship over time, to go through some dark shit together, some bliss and happiness together.

2:15:29.740 --> 2:15:33.900
 I feel like there's a lot of greedy self centered behavior within that process.

2:15:35.020 --> 2:15:41.340
 My intuition, but I might be wrong, is that human computer interaction doesn't have to

2:15:41.340 --> 2:15:46.140
 go through a computer being greedy, possessive, and so on.

2:15:46.140 --> 2:15:50.700
 It is possible to train systems, maybe, that they actually

2:15:50.700 --> 2:15:57.020
 they are, I guess, prompted or fine tuned or so to truly optimize for what you care about.

2:15:57.020 --> 2:16:01.980
 And you could imagine that, you know, the way how the process would look like is at

2:16:01.980 --> 2:16:08.860
 some point, we as humans, we look at the transcript of the conversation or like an entire

2:16:08.860 --> 2:16:14.700
 interaction and we say, actually here, there was more loving way to go about it.

2:16:14.700 --> 2:16:20.540
 And we supervise system toward being more loving, or maybe we train the system such

2:16:20.540 --> 2:16:23.180
 that it has a reward function toward being more loving.

2:16:23.180 --> 2:16:23.740
 Yeah.

2:16:23.740 --> 2:16:29.820
 Or maybe the possibility of the system being an asshole and manipulative and possessive

2:16:29.820 --> 2:16:32.940
 every once in a while is a feature, not a bug.

2:16:33.580 --> 2:16:40.860
 Because some of the happiness that we experience when two souls meet each other, when two humans

2:16:40.860 --> 2:16:44.620
 meet each other, is a kind of break from the assholes in the world.

2:16:45.420 --> 2:16:52.060
 And so you need assholes in AI as well, because, like, it'll be like a breath of fresh air

2:16:52.060 --> 2:17:00.540
 to discover an AI that the three previous AIs you had are too friendly or no, or cruel

2:17:00.540 --> 2:17:01.340
 or whatever.

2:17:01.340 --> 2:17:03.020
 It's like some kind of mix.

2:17:03.020 --> 2:17:07.420
 And then this one is just right, but you need to experience the full spectrum.

2:17:07.420 --> 2:17:10.940
 Like, I think you need to be able to engineer assholes.

2:17:11.500 --> 2:17:12.860
 So let's see.

2:17:14.380 --> 2:17:20.220
 Because there's some level to us being appreciated to appreciate the human experience.

2:17:21.180 --> 2:17:24.300
 We need the dark and the light.

2:17:24.300 --> 2:17:25.820
 So that kind of reminds me.

2:17:27.100 --> 2:17:35.820
 I met a while ago at the meditation retreat, one woman, and she told me, you know,

2:17:35.820 --> 2:17:41.260
 beautiful, beautiful woman, and she had a she had a crutch.

2:17:41.260 --> 2:17:41.980
 Okay.

2:17:41.980 --> 2:17:44.940
 She had the trouble walking on one leg.

2:17:44.940 --> 2:17:46.460
 I asked her what has happened.

2:17:47.340 --> 2:17:55.820
 And she said that five years ago she was in Maui, Hawaii, and she was eating a salad and

2:17:55.820 --> 2:17:57.980
 some snail fell into the salad.

2:17:57.980 --> 2:18:01.820
 And apparently there are neurotoxic snails over there.

2:18:02.380 --> 2:18:04.380
 And she got into coma for a year.

2:18:04.380 --> 2:18:05.740
 Okay.

2:18:05.740 --> 2:18:09.660
 And apparently there is, you know, high chance of even just dying.

2:18:09.660 --> 2:18:10.860
 But she was in the coma.

2:18:10.860 --> 2:18:14.860
 At some point, she regained partially consciousness.

2:18:14.860 --> 2:18:16.780
 She was able to hear people in the room.

2:18:18.380 --> 2:18:19.900
 People behave as she wouldn't be there.

2:18:21.100 --> 2:18:25.900
 You know, at some point she started being able to speak, but she was mumbling like a

2:18:25.900 --> 2:18:28.460
 barely able to express herself.

2:18:28.460 --> 2:18:30.700
 Then at some point she got into wheelchair.

2:18:30.700 --> 2:18:38.140
 Then at some point she actually noticed that she can move her toe and then she knew that

2:18:38.140 --> 2:18:39.180
 she will be able to walk.

2:18:40.220 --> 2:18:42.620
 And then, you know, that's where she was five years after.

2:18:42.620 --> 2:18:47.100
 And she said that since then she appreciates the fact that she can move her toe.

2:18:48.460 --> 2:18:53.580
 And I was thinking, hmm, do I need to go through such experience to appreciate that I have

2:18:53.580 --> 2:18:55.020
 I can move my toe?

2:18:55.020 --> 2:18:58.300
 Wow, that's a really good story and really deep example.

2:18:58.300 --> 2:18:58.780
 Yeah.

2:18:58.780 --> 2:19:05.420
 And in some sense, it might be the case that we don't see light if we haven't went through

2:19:05.420 --> 2:19:06.380
 the darkness.

2:19:06.380 --> 2:19:07.900
 But I wouldn't say that we should.

2:19:08.780 --> 2:19:14.460
 We shouldn't assume that that's the case, which it may be able to engineer shortcuts.

2:19:14.460 --> 2:19:15.180
 Yeah.

2:19:15.180 --> 2:19:22.220
 Ilya had this, you know, belief that maybe one has to go for a week or six months to

2:19:22.220 --> 2:19:29.660
 do some challenging camp to just experience, you know, a lot of difficulties and then comes

2:19:29.660 --> 2:19:33.500
 back and actually everything is bright, everything is beautiful.

2:19:33.500 --> 2:19:34.460
 I'm with Ilya on this.

2:19:34.460 --> 2:19:35.500
 It must be a Russian thing.

2:19:35.500 --> 2:19:36.940
 Where are you from originally?

2:19:36.940 --> 2:19:37.900
 I'm Polish.

2:19:37.900 --> 2:19:38.400
 Polish.

2:19:39.740 --> 2:19:40.240
 Okay.

2:19:41.500 --> 2:19:43.500
 I'm tempted to say that explains a lot.

2:19:43.500 --> 2:19:47.820
 But yeah, there's something about the Russian, the necessity of suffering.

2:19:47.820 --> 2:19:52.700
 I believe suffering or rather struggle is necessary.

2:19:52.700 --> 2:19:54.300
 I believe that struggle is necessary.

2:19:54.300 --> 2:20:00.380
 I mean, in some sense, you even look at the story of any superhero in the movie.

2:20:00.380 --> 2:20:03.340
 It's not that it was like everything goes easy, easy, easy, easy.

2:20:03.340 --> 2:20:07.820
 I like how that's your ground truth is the story of superheroes.

2:20:07.820 --> 2:20:08.320
 Okay.

2:20:09.260 --> 2:20:13.420
 You mentioned that you used to do research at night and go to bed at like 6 a.m.

2:20:13.420 --> 2:20:14.140
 or 7 a.m.

2:20:14.140 --> 2:20:17.260
 I still do that often.

2:20:18.860 --> 2:20:23.180
 What sleep schedules have you tried to make for a productive and happy life?

2:20:23.180 --> 2:20:29.500
 Like, is there is there some interesting wild sleeping patterns that you engaged that you

2:20:29.500 --> 2:20:30.860
 found that works really well for you?

2:20:31.420 --> 2:20:37.180
 I tried at some point decreasing number of hours of sleep like a gradually like a half

2:20:37.180 --> 2:20:38.540
 an hour every few days to this.

2:20:39.100 --> 2:20:41.260
 You know, I was hoping to just save time.

2:20:41.980 --> 2:20:43.500
 That clearly didn't work for me.

2:20:43.500 --> 2:20:48.380
 Like at some point, there's like a phase shift and I felt tired all the time.

2:20:50.380 --> 2:20:53.980
 You know, there was a time that I used to work during the nights.

2:20:53.980 --> 2:20:57.100
 The nice thing about the nights is that no one disturbs you.

2:20:57.740 --> 2:21:04.620
 And even I remember when I was meeting for the first time with Greg Brockman, his

2:21:04.620 --> 2:21:08.860
 CTO and chairman of OpenAI, our meeting was scheduled to 5 p.m.

2:21:09.660 --> 2:21:11.740
 And I overstepped for the meeting.

2:21:11.740 --> 2:21:14.060
 Over slept for the meeting at 5 p.m.

2:21:14.060 --> 2:21:15.740
 Yeah, now you sound like me.

2:21:15.740 --> 2:21:16.540
 That's hilarious.

2:21:16.540 --> 2:21:17.660
 OK, yeah.

2:21:17.660 --> 2:21:23.820
 And at the moment, in some sense, my sleeping schedule also has to do with the fact that

2:21:23.820 --> 2:21:26.060
 I'm interacting with people.

2:21:26.780 --> 2:21:28.140
 I sleep without an alarm.

2:21:28.620 --> 2:21:35.900
 So, yeah, the the team thing you mentioned, the extrovert thing, because most humans operate

2:21:35.900 --> 2:21:41.660
 during a certain set of hours, you're forced to then operate at the same set of hours.

2:21:42.220 --> 2:21:45.660
 But I'm not quite there yet.

2:21:46.460 --> 2:21:51.420
 I found a lot of joy, just like you said, working through the night because it's quiet

2:21:51.900 --> 2:21:53.660
 because the world doesn't disturb you.

2:21:53.660 --> 2:21:57.580
 And there's some aspect counter to everything you're saying.

2:21:57.580 --> 2:22:03.660
 There's some joyful aspect to sleeping through the mess of the day because people are having

2:22:03.660 --> 2:22:07.500
 people are having meetings and sending emails and there's drama meetings.

2:22:08.060 --> 2:22:09.980
 I can sleep through all the meetings.

2:22:09.980 --> 2:22:14.140
 You know, I have meetings every day and they prevent me from having sufficient amount of

2:22:14.140 --> 2:22:15.820
 time for focused work.

2:22:16.780 --> 2:22:23.980
 And then I modified my calendar and I said that I'm out of office Wednesday, Thursday

2:22:23.980 --> 2:22:27.500
 and Friday every day and I'm having meetings only Monday and Tuesday.

2:22:27.500 --> 2:22:33.420
 And that busty positively influenced my mood that I have literally like at three days for

2:22:33.420 --> 2:22:34.380
 fully focused work.

2:22:34.380 --> 2:22:34.940
 Yeah.

2:22:35.580 --> 2:22:39.580
 So there's better solutions to this problem than staying awake all night.

2:22:39.980 --> 2:22:45.420
 OK, you've been part of development of some of the greatest ideas in artificial intelligence.

2:22:45.420 --> 2:22:48.860
 What would you say is your process for developing good novel ideas?

2:22:49.820 --> 2:22:53.820
 You have to be aware that clearly there are many other brilliant people around.

2:22:53.820 --> 2:23:02.780
 So you have to ask yourself a question, why the given idea, let's say, wasn't tried by

2:23:02.780 --> 2:23:10.140
 someone else and in some sense, it has to do with, you know, kind of simple.

2:23:10.140 --> 2:23:12.940
 It might sound simple, but like a thinking outside of the box.

2:23:12.940 --> 2:23:14.300
 And what do I mean here?

2:23:14.780 --> 2:23:23.260
 So, for instance, for a while, people in academia, they assumed that you have a feeling that

2:23:23.260 --> 2:23:30.220
 you have a fixed data set and then you optimize the algorithms in order to get the best performance.

2:23:31.500 --> 2:23:39.580
 And that was so in great assumption that no one thought about training models on

2:23:39.580 --> 2:23:42.700
 anti internet or like that.

2:23:42.700 --> 2:23:48.540
 Maybe some people thought about it, but it felt to many as unfair.

2:23:48.540 --> 2:23:53.180
 And in some sense, that's almost like a it's not my idea or so, but that's an example of

2:23:53.180 --> 2:23:54.940
 breaking at the typical assumption.

2:23:55.740 --> 2:23:59.740
 So you want to be in the paradigm that you're breaking at the typical assumption.

2:24:00.540 --> 2:24:06.540
 In the context of the community, getting to pick your data set is cheating.

2:24:06.540 --> 2:24:07.020
 Correct.

2:24:07.020 --> 2:24:11.260
 And in some sense, so that was that was assumption that many people had out there.

2:24:11.260 --> 2:24:19.020
 And then if you free yourself from assumptions, then they are likely to achieve something

2:24:19.020 --> 2:24:20.380
 that others cannot do.

2:24:20.380 --> 2:24:24.940
 And in some sense, if you are trying to do exactly the same things as others, it's very

2:24:24.940 --> 2:24:26.940
 likely that you're going to have the same results.

2:24:26.940 --> 2:24:33.660
 Yeah, I but there's also that kind of tension, which is asking yourself the question, why

2:24:34.220 --> 2:24:35.660
 haven't others done this?

2:24:35.660 --> 2:24:44.620
 Because, I mean, I get a lot of good ideas, but I think probably most of them suck when

2:24:44.620 --> 2:24:45.900
 they meet reality.

2:24:45.900 --> 2:24:53.500
 So so actually, I think the other big piece is getting into habit of generating ideas,

2:24:53.500 --> 2:25:00.860
 training your brain towards generating ideas and not even suspending judgment of the ideas.

2:25:00.860 --> 2:25:06.380
 So in some sense, I noticed myself that even if I'm in the process of generating ideas,

2:25:06.380 --> 2:25:12.860
 if I tell myself, oh, that was a bad idea, then that actually interrupts the process

2:25:12.860 --> 2:25:17.180
 and I cannot generate more ideas because I'm actually focused on the negative part, why

2:25:17.180 --> 2:25:17.980
 it won't work.

2:25:17.980 --> 2:25:18.480
 Yes.

2:25:19.020 --> 2:25:25.020
 But I created also environment in the way that it's very easy for me to store new ideas.

2:25:25.020 --> 2:25:31.900
 So, for instance, next to my bed, I have a voice recorder and it happens to me often

2:25:31.900 --> 2:25:35.020
 like I wake up during the night and I have some idea.

2:25:35.020 --> 2:25:40.380
 In the past, I was writing them down on my phone, but that means, you know, turning on

2:25:40.380 --> 2:25:45.500
 the screen and that wakes me up or like pulling a paper, which requires, you know, turning

2:25:45.500 --> 2:25:47.500
 on the light.

2:25:47.500 --> 2:25:49.660
 These days, I just start recording it.

2:25:49.660 --> 2:25:55.740
 What do you think, I don't know if you know who Jim Keller is.

2:25:55.740 --> 2:25:57.740
 I know Jim Keller.

2:25:57.740 --> 2:26:03.580
 He's a big proponent of thinking harder on a problem right before sleep so that he can

2:26:03.580 --> 2:26:08.700
 sleep through it and solve it in his sleep or like come up with radical stuff in his

2:26:08.700 --> 2:26:11.180
 sleep that's trying to get me to do this.

2:26:11.180 --> 2:26:19.020
 So it happened from my experience perspective, it happened to me many times during the high

2:26:19.020 --> 2:26:25.180
 school days when I was doing mathematics that I had a solution to my problem as I woke up.

2:26:27.260 --> 2:26:33.420
 At the moment, regarding thinking hard about the given problem is I'm trying to actually

2:26:33.420 --> 2:26:37.500
 devote substantial amount of time to think about important problems, not just before

2:26:37.500 --> 2:26:37.900
 the sleep.

2:26:39.020 --> 2:26:44.060
 I'm organizing amount of the huge chunks of time such that I'm not constantly working

2:26:44.060 --> 2:26:48.220
 on the urgent problems, but I actually have time to think about the important one.

2:26:48.220 --> 2:26:49.740
 So you do it naturally.

2:26:49.740 --> 2:26:56.060
 But his idea is that you kind of prime your brain to make sure that that's the focus.

2:26:56.060 --> 2:27:00.700
 Oftentimes people have other worries in their life that's not fundamentally deep problems

2:27:00.700 --> 2:27:06.860
 like I don't know, just stupid drama in your life and even at work, all that kind of stuff.

2:27:06.860 --> 2:27:12.620
 He wants to kind of pick the most important problem that you're thinking about and go

2:27:12.620 --> 2:27:13.820
 to bed on that.

2:27:13.820 --> 2:27:14.940
 I think that's wise.

2:27:14.940 --> 2:27:19.820
 I mean, the other thing that comes to my mind is also I feel the most fresh in the morning.

2:27:20.380 --> 2:27:25.900
 So during the morning, I try to work on the most important things rather than just being

2:27:25.900 --> 2:27:28.540
 pulled by urgent things or checking email or so.

2:27:29.740 --> 2:27:30.620
 What do you do with the...

2:27:30.620 --> 2:27:35.020
 Because I've been doing the voice recorder thing too, but I end up recording so many

2:27:35.020 --> 2:27:36.700
 messages it's hard to organize.

2:27:37.260 --> 2:27:38.540
 I have the same problem.

2:27:38.540 --> 2:27:44.380
 Now I have heard that Google Pixel is really good in transcribing text and I might get

2:27:44.380 --> 2:27:47.020
 a Google Pixel just for the sake of transcribing text.

2:27:47.020 --> 2:27:50.940
 Yeah, people listening to this, if you have a good voice recorder suggestion that transcribe,

2:27:50.940 --> 2:27:51.580
 please let me know.

2:27:52.780 --> 2:27:57.900
 Some of it has to do with OpenAI codecs too.

2:27:57.900 --> 2:28:01.180
 Like some of it is simply like the friction.

2:28:01.900 --> 2:28:08.940
 I need apps that remove that friction between voice and the organization of the resulting

2:28:08.940 --> 2:28:10.300
 transcripts and all that kind of stuff.

2:28:11.980 --> 2:28:12.940
 But yes, you're right.

2:28:12.940 --> 2:28:20.460
 Absolutely, like during, for me it's walking, sleep too, but walking and running, especially

2:28:20.460 --> 2:28:25.500
 running, get a lot of thoughts during running and there's no good mechanism for recording

2:28:25.500 --> 2:28:25.980
 thoughts.

2:28:25.980 --> 2:28:32.780
 So one more thing that I do, I have a separate phone which has no apps.

2:28:33.660 --> 2:28:37.180
 Maybe it has like audible or let's say Kindle.

2:28:37.180 --> 2:28:40.060
 No one has this phone number, this kind of my meditation phone.

2:28:40.060 --> 2:28:40.620
 Yeah.

2:28:40.620 --> 2:28:46.380
 And I try to expand the amount of time that that's the phone that I'm having.

2:28:47.180 --> 2:28:52.220
 It has also Google Maps if I need to go somewhere and I also use this phone to write down ideas.

2:28:52.860 --> 2:28:54.860
 Ah, that's a really good idea.

2:28:55.660 --> 2:28:57.020
 That's a really good idea.

2:28:57.020 --> 2:29:01.740
 Often actually what I end up doing is even sending a message from that phone to the other

2:29:01.740 --> 2:29:02.380
 phone.

2:29:02.380 --> 2:29:06.780
 So that's actually my way of recording messages or I just put them into notes.

2:29:06.780 --> 2:29:07.340
 I love it.

2:29:07.340 --> 2:29:15.660
 What advice would you give to a young person, high school, college, about how to be successful?

2:29:15.660 --> 2:29:20.940
 You've done a lot of incredible things in the past decade, so maybe, maybe have some.

2:29:20.940 --> 2:29:22.540
 There's something, there might be something.

2:29:22.540 --> 2:29:23.580
 There might be something.

2:29:25.020 --> 2:29:33.020
 I mean, might sound like a simplistic or so, but I would say literally just follow your

2:29:33.020 --> 2:29:34.140
 passion, double down on it.

2:29:34.140 --> 2:29:38.460
 And if you don't know what's your passion, just figure out what could be a, what could

2:29:38.460 --> 2:29:39.100
 be a passion.

2:29:39.100 --> 2:29:40.940
 So that might be an exploration.

2:29:41.900 --> 2:29:45.500
 When I was in elementary school was math and chemistry.

2:29:46.300 --> 2:29:52.300
 And I remember for some time I gave up on math because my school teacher, she told me

2:29:52.300 --> 2:29:53.180
 that I'm dumb.

2:29:54.940 --> 2:30:00.140
 And I guess maybe an advice would be just ignore people if they tell you that you're

2:30:00.140 --> 2:30:00.860
 dumb.

2:30:00.860 --> 2:30:01.420
 You're dumb.

2:30:01.420 --> 2:30:06.300
 You're dumb. You mentioned something offline about chemistry and explosives.

2:30:08.540 --> 2:30:09.660
 What was that about?

2:30:09.660 --> 2:30:10.460
 So let's see.

2:30:11.900 --> 2:30:13.420
 So a story goes like that.

2:30:16.860 --> 2:30:18.300
 I got into chemistry.

2:30:18.300 --> 2:30:22.620
 Maybe I was like a second grade of my elementary school, third grade.

2:30:23.500 --> 2:30:25.900
 I started going to chemistry classes.

2:30:27.740 --> 2:30:30.060
 I really love building stuff.

2:30:30.060 --> 2:30:35.740
 And I did all the experiments that they describe in the book, like, you know, how to create

2:30:35.740 --> 2:30:39.660
 oxygen with vinegar and baking soda or so.

2:30:39.660 --> 2:30:40.160
 Okay.

2:30:40.780 --> 2:30:45.740
 So I did all the experiments and at some point I was, you know, so what's next?

2:30:45.740 --> 2:30:46.460
 What can I do?

2:30:47.260 --> 2:30:53.180
 And explosives, they also, it's like a, you have a clear reward signal, you know, if the

2:30:53.180 --> 2:30:54.140
 thing worked or not.

2:30:54.140 --> 2:31:00.780
 So I remember at first I got interested in producing hydrogen.

2:31:00.780 --> 2:31:03.260
 That was kind of funny experiment from school.

2:31:03.260 --> 2:31:04.380
 You can just burn it.

2:31:04.380 --> 2:31:07.420
 And then I moved to nitroglycerin.

2:31:07.420 --> 2:31:10.220
 So that's also relatively easy to synthesize.

2:31:11.260 --> 2:31:16.540
 I started producing essentially dynamite and detonating it with a friend.

2:31:16.540 --> 2:31:20.860
 I remember there was a, you know, there was at first like maybe two attempts that I went

2:31:20.860 --> 2:31:25.020
 with a friend to detonate what we built and it didn't work out.

2:31:25.020 --> 2:31:27.660
 And like a third time he was like, ah, it won't work.

2:31:27.660 --> 2:31:29.740
 Like, let's don't waste time.

2:31:30.220 --> 2:31:38.700
 And, you know, we were, I was carrying this, this, you know, that tube with dynamite, I

2:31:38.700 --> 2:31:45.260
 don't know, pound or so, dynamite in my backpack, we're like riding on the bike to the edges

2:31:45.260 --> 2:31:45.820
 of the city.

2:31:45.820 --> 2:31:51.340
 Yeah, and attempt number three, this was be attempt number three.

2:31:51.340 --> 2:31:52.300
 Attempt number three.

2:31:52.860 --> 2:31:57.420
 And now we dig a hole to put it inside.

2:31:57.420 --> 2:32:01.020
 It actually had the, you know, electrical detonator.

2:32:02.220 --> 2:32:04.860
 We draw a cable behind the tree.

2:32:05.660 --> 2:32:10.140
 I even, I never had, I haven't ever seen like a explosion before.

2:32:10.140 --> 2:32:15.580
 So I thought that there would be a lot of, you know, a lot of, you know, a lot of, you

2:32:15.580 --> 2:32:17.100
 know, there will be a lot of sound.

2:32:17.980 --> 2:32:22.380
 But, you know, we're like laying down and I'm holding the cable and the battery.

2:32:22.380 --> 2:32:28.380
 At some point, you know, we kind of like a three to one and I just connected it and it

2:32:28.380 --> 2:32:30.300
 felt like the ground shake.

2:32:30.300 --> 2:32:32.860
 It was like more like a sound.

2:32:32.860 --> 2:32:37.740
 And then the soil started kind of lifting up and started falling on us.

2:32:37.740 --> 2:32:38.380
 Yeah.

2:32:38.380 --> 2:32:39.180
 Wow.

2:32:39.180 --> 2:32:45.580
 And then, you know, the friend said, let's make sure the next time we have helmets.

2:32:45.580 --> 2:32:48.940
 But also, you know, I'm happy that nothing happened to me.

2:32:48.940 --> 2:32:52.300
 It could have been the case that I lost the limbo or so.

2:32:52.300 --> 2:33:01.900
 Yeah, but that's childhood of an engineering mind with a strong reward signal of an

2:33:01.900 --> 2:33:02.460
 explosion.

2:33:03.660 --> 2:33:04.140
 I love it.

2:33:04.140 --> 2:33:10.140
 My there's some aspect of chemists that the chemists I know, like my dad with plasma

2:33:10.140 --> 2:33:13.740
 chemistry, plasma physics, he was very much into explosives, too.

2:33:13.740 --> 2:33:18.300
 It's a worrying quality of people that work in chemistry that they love.

2:33:18.300 --> 2:33:23.500
 I think it is that exactly is the strong signal that the thing worked.

2:33:23.500 --> 2:33:24.620
 There is no doubt.

2:33:24.620 --> 2:33:25.660
 There's no doubt.

2:33:25.660 --> 2:33:26.860
 There's some magic.

2:33:26.860 --> 2:33:31.420
 It's almost like a reminder that physics works, that chemistry works.

2:33:31.420 --> 2:33:32.220
 It's cool.

2:33:32.220 --> 2:33:36.540
 It's almost like a little glimpse at nature that you yourself engineer.

2:33:36.540 --> 2:33:43.420
 I that's why I really like artificial intelligence, especially robotics, is you create a little

2:33:43.420 --> 2:33:49.020
 piece of nature and in some sense, even for me with explosives, the motivation was creation

2:33:49.020 --> 2:33:50.060
 rather than destruction.

2:33:50.060 --> 2:33:50.860
 Yes, exactly.

2:33:51.740 --> 2:33:57.180
 In terms of advice, I forgot to ask about just machine learning and deep learning for

2:33:57.180 --> 2:34:01.980
 people who are specifically interested in machine learning, how would you recommend

2:34:01.980 --> 2:34:03.020
 they get into the field?

2:34:03.580 --> 2:34:07.900
 So I would say re implement everything and also there is plenty of courses.

2:34:08.620 --> 2:34:09.660
 So like from scratch?

2:34:10.380 --> 2:34:14.780
 So on different levels of abstraction in some sense, but I would say re implement something

2:34:14.780 --> 2:34:19.100
 from scratch, re implement something from a paper, re implement something, you know,

2:34:19.100 --> 2:34:20.860
 from podcasts that you have heard about.

2:34:21.420 --> 2:34:23.820
 I would say that's a powerful way to understand things.

2:34:23.820 --> 2:34:30.220
 So it's often the case that you read the description and you think you understand, but you truly

2:34:30.220 --> 2:34:35.500
 understand once you build it, then you actually know what really matter in the description.

2:34:36.220 --> 2:34:40.300
 Is there a particular topics that you find people just fall in love with?

2:34:41.020 --> 2:34:41.980
 I've seen.

2:34:44.220 --> 2:34:51.500
 I tend to really enjoy reinforcement learning because it's much more, it's much easier

2:34:51.500 --> 2:34:57.260
 to get to a point where you feel like you created something special, like fun games

2:34:57.260 --> 2:34:58.620
 kind of things that are rewarding.

2:34:58.620 --> 2:34:59.100
 It's rewarding.

2:34:59.100 --> 2:34:59.600
 Yeah.

2:35:01.100 --> 2:35:07.740
 As opposed to like re implementing from scratch, more like supervised learning kind of things.

2:35:07.740 --> 2:35:08.940
 It's yeah.

2:35:08.940 --> 2:35:15.260
 So, you know, if someone would optimize for things to be rewarding, then it feels that

2:35:15.260 --> 2:35:18.460
 the things that are somewhat generative, they have such a property.

2:35:18.460 --> 2:35:23.580
 So you have, for instance, adversarial networks, or do you have just even generative language

2:35:23.580 --> 2:35:24.080
 models?

2:35:24.700 --> 2:35:30.780
 And you can even see, internally, we have seen this thing with our releases.

2:35:30.780 --> 2:35:33.820
 So we have, we released recently two models.

2:35:33.820 --> 2:35:39.340
 There is one model called Dali that generates images, and there is other model called Clip

2:35:39.340 --> 2:35:45.500
 that actually you provide various possibilities, what could be the answer to what is on the

2:35:45.500 --> 2:35:48.700
 picture, and it can tell you which one is the most likely.

2:35:48.700 --> 2:35:56.220
 And in some sense, in case of the first one, Dali, it is very easy for you to understand

2:35:56.220 --> 2:35:58.220
 that actually there is magic going on.

2:35:59.740 --> 2:36:04.860
 And in the case of the second one, even though it is insanely powerful, and you know, people

2:36:04.860 --> 2:36:10.540
 from a vision community, they, as they started probing it inside, they actually understood

2:36:12.540 --> 2:36:13.740
 how far it goes.

2:36:13.740 --> 2:36:19.980
 How far it goes, it's difficult for a person at first to see how well it works.

2:36:21.500 --> 2:36:25.260
 And that's the same, as you said, that in case of supervised learning models, you might

2:36:25.260 --> 2:36:30.300
 not kind of see, or it's not that easy for you to understand the strength.

2:36:31.180 --> 2:36:33.820
 Even though you don't believe in magic, to see the magic.

2:36:33.820 --> 2:36:35.020
 To see the magic, yeah.

2:36:35.020 --> 2:36:36.220
 It's a generative.

2:36:36.220 --> 2:36:37.340
 That's really brilliant.

2:36:37.340 --> 2:36:42.860
 So anything that's generative, because then you are at the core of the creation.

2:36:42.860 --> 2:36:46.620
 You get to experience creation without much effort.

2:36:46.620 --> 2:36:48.540
 Unless you have to do it from scratch, but.

2:36:48.540 --> 2:36:51.900
 And it feels that, you know, humans are wired.

2:36:51.900 --> 2:36:54.700
 There is some level of reward for creating stuff.

2:36:54.700 --> 2:36:54.940
 Yeah.

2:36:56.380 --> 2:36:59.100
 Of course, different people have a different weight on this reward.

2:36:59.100 --> 2:36:59.340
 Yeah.

2:37:00.460 --> 2:37:01.740
 In the big objective function.

2:37:01.740 --> 2:37:03.980
 In the big objective function of a person.

2:37:03.980 --> 2:37:04.620
 Of a person.

2:37:05.420 --> 2:37:10.860
 You wrote that beautiful is what you intensely pay attention to.

2:37:10.860 --> 2:37:12.380
 Even a cockroach is beautiful.

2:37:12.380 --> 2:37:16.300
 If you look very closely, can you expand on this?

2:37:16.300 --> 2:37:17.980
 What is beauty?

2:37:18.620 --> 2:37:26.060
 So what I'm, I wrote here actually corresponds to my subjective experience that I had through

2:37:26.060 --> 2:37:27.740
 extended periods of meditation.

2:37:28.540 --> 2:37:34.380
 It's, it's pretty crazy that at some point the meditation gets you to the place that

2:37:34.380 --> 2:37:39.820
 you have really increased focus, increased attention.

2:37:39.820 --> 2:37:40.940
 Increased attention.

2:37:40.940 --> 2:37:45.580
 And then you look at the very simple objects that were all the time around you can look

2:37:45.580 --> 2:37:48.540
 at the table or on the pen or at the nature.

2:37:49.260 --> 2:37:55.500
 And you notice more and more details and it becomes very pleasant to look at it.

2:37:56.780 --> 2:37:59.660
 And it, once again, it kind of reminds me of my childhood.

2:38:01.260 --> 2:38:03.900
 Like I just pure joy of being.

2:38:03.900 --> 2:38:11.580
 It's also, I have seen even the reverse effect that by default, regardless of what we possess,

2:38:11.580 --> 2:38:13.260
 we very quickly get used to it.

2:38:14.300 --> 2:38:20.700
 And you know, you can have a very beautiful house and if you don't put sufficient effort,

2:38:21.500 --> 2:38:25.500
 you're just going to get used to it and it doesn't bring any more joy,

2:38:25.500 --> 2:38:26.700
 regardless of what you have.

2:38:27.180 --> 2:38:27.680
 Yeah.

2:38:27.680 --> 2:38:36.960
 Well, I actually, I find that material possessions get in the way of that experience of pure

2:38:36.960 --> 2:38:37.460
 joy.

2:38:38.720 --> 2:38:45.360
 So I've always, I've been very fortunate to just find joy in simple things.

2:38:45.360 --> 2:38:50.800
 Just, just like you're saying, just like, I don't know, objects in my life, just stupid

2:38:50.800 --> 2:38:55.440
 objects like this cup, like thing, you know, just objects sounds okay.

2:38:55.440 --> 2:39:00.880
 I'm not being eloquent, but literally objects in the world, they're just full of joy.

2:39:00.880 --> 2:39:07.360
 Cause it's like, I can't believe when I can't believe that I'm fortunate enough to be alive

2:39:07.360 --> 2:39:09.200
 to experience these objects.

2:39:09.680 --> 2:39:14.320
 And then two, I can't believe humans are clever enough to have built these objects.

2:39:15.120 --> 2:39:19.520
 The hierarchy of pleasure that that provides is infinite.

2:39:19.520 --> 2:39:24.000
 I mean, even if you look at the cup of water, so, you know, you see first like a level of

2:39:24.000 --> 2:39:28.320
 like a reflection of light, but then you think, you know, man, there's like a trillions upon

2:39:28.320 --> 2:39:32.000
 of trillions of particles bouncing against each other.

2:39:32.000 --> 2:39:38.560
 There is also the tension on the surface that, you know, if the back, back could like a stand

2:39:38.560 --> 2:39:40.000
 on it and move around.

2:39:40.000 --> 2:39:44.800
 And you think it also has this like a magical property that as you decrease temperature,

2:39:45.440 --> 2:39:51.680
 it actually expands in volume, which allows for the, you know, legs to freeze on the,

2:39:51.680 --> 2:39:58.080
 on the surface and at the bottom to have actually not freeze, which allows for life like a crazy.

2:39:58.080 --> 2:39:58.560
 Yeah.

2:39:58.560 --> 2:40:03.520
 You look in detail at some object and you think actually, you know, this table, that

2:40:03.520 --> 2:40:06.400
 was just a figment of someone's imagination at some point.

2:40:06.400 --> 2:40:10.560
 And then there was like a thousands of people involved to actually manufacture it and put

2:40:10.560 --> 2:40:11.120
 it here.

2:40:11.120 --> 2:40:13.280
 And by default, no one cares.

2:40:15.280 --> 2:40:19.360
 And then you can start thinking about evolution, how it all started from single cell organisms

2:40:19.360 --> 2:40:21.280
 that led to this table.

2:40:21.280 --> 2:40:27.360
 And these thoughts, they give me life appreciation and even lack of thoughts, just the pure raw

2:40:27.360 --> 2:40:29.920
 signal also gives the life appreciation.

2:40:29.920 --> 2:40:37.360
 See, the thing is, and then that's coupled for me with the sadness that the whole ride

2:40:37.360 --> 2:40:43.440
 ends and perhaps is deeply coupled in that the fact that this experience, this moment

2:40:43.440 --> 2:40:49.680
 ends, gives it, gives it an intensity that I'm not sure I would otherwise have.

2:40:50.160 --> 2:40:53.600
 So in that same way, I tried to meditate on my own death.

2:40:53.600 --> 2:40:54.160
 Often.

2:40:54.880 --> 2:40:56.720
 Do you think about your mortality?

2:40:58.160 --> 2:40:59.120
 Are you afraid of death?

2:41:01.840 --> 2:41:07.840
 So fear of death is like one of the most fundamental fears that each of us has.

2:41:07.840 --> 2:41:09.680
 We might be not even aware of it.

2:41:09.680 --> 2:41:15.440
 It requires to look inside, to even recognize that it's out there and there is still, let's

2:41:15.440 --> 2:41:22.960
 say, this property of nature that if things would last forever, then they would be also

2:41:22.960 --> 2:41:23.760
 boring to us.

2:41:24.880 --> 2:41:29.520
 The fact that the things change in some way gives any meaning to them.

2:41:29.520 --> 2:41:40.800
 I also, you know, found out that it seems to be very healing to people to have these

2:41:40.800 --> 2:41:49.440
 short experiences, like, I guess, psychedelic experiences in which they experience death

2:41:49.440 --> 2:41:58.160
 of self in which they let go of this fear and then maybe can even increase the intensity

2:41:58.160 --> 2:42:00.720
 can even increase the appreciation of the moment.

2:42:01.520 --> 2:42:11.440
 It seems that many people, they can easily comprehend the fact that the money is finite

2:42:12.160 --> 2:42:14.720
 while they don't see that time is finite.

2:42:15.680 --> 2:42:18.640
 I have this like a discussion with Ilya from time to time.

2:42:18.640 --> 2:42:23.520
 He's like, you know, man, like the life will pass very fast.

2:42:23.520 --> 2:42:26.640
 At some point I will be 40, 50, 60, 70 and then it's over.

2:42:26.640 --> 2:42:33.120
 This is true, which also makes me believe that, you know, that every single moment it

2:42:33.120 --> 2:42:37.600
 is so unique that should be appreciated.

2:42:37.600 --> 2:42:44.560
 And this also makes me think that I should be acting on my life because otherwise it

2:42:44.560 --> 2:42:45.200
 will pass.

2:42:46.240 --> 2:42:53.280
 I also like this framework of thinking from Jeff Bezos on regret minimization that like

2:42:53.280 --> 2:43:01.520
 I would like if I will be at that deathbed to look back on my life and not regret that

2:43:01.520 --> 2:43:03.280
 I haven't done something.

2:43:03.280 --> 2:43:07.680
 It's usually you might regret that you haven't tried.

2:43:07.680 --> 2:43:08.880
 I'm fine with failing.

2:43:10.640 --> 2:43:11.440
 I haven't tried.

2:43:13.120 --> 2:43:15.360
 What's the Nietzsche eternal occurrence?

2:43:15.360 --> 2:43:20.480
 Try to live a life that if you had to live it infinitely many times, that would be the

2:43:20.480 --> 2:43:23.920
 you'd be okay with that kind of life.

2:43:24.640 --> 2:43:26.160
 So try to live it optimally.

2:43:27.120 --> 2:43:30.800
 I can say that it's almost like I'm.

2:43:33.280 --> 2:43:36.640
 I'm available to me where I am in my life.

2:43:36.640 --> 2:43:40.480
 I'm extremely grateful for actually people whom I met.

2:43:40.480 --> 2:43:44.320
 I would say I think that I'm decently smart and so on.

2:43:44.320 --> 2:43:50.160
 But I think that actually to a great extent where I am has to do with the people who I

2:43:50.160 --> 2:43:50.720
 met.

2:43:52.320 --> 2:43:55.600
 Would you be okay if after this conversation you died?

2:43:56.320 --> 2:44:00.800
 So if I'm dead, then it kind of I don't have a choice anymore.

2:44:01.600 --> 2:44:05.440
 So in some sense, there's like plenty of things that I would like to try out in my life.

2:44:07.040 --> 2:44:10.480
 I feel that I'm gradually going one by one and I'm just doing them.

2:44:10.480 --> 2:44:13.120
 I think that the list will be always infinite.

2:44:13.120 --> 2:44:15.680
 Yeah, so might as well go today.

2:44:16.800 --> 2:44:20.000
 Yeah, I mean, to be clear, I'm not looking forward to die.

2:44:20.800 --> 2:44:23.520
 I would say if there is no choice, I would accept it.

2:44:24.320 --> 2:44:30.480
 But like in some sense, I'm if there would be a choice, if there would be a possibility

2:44:30.480 --> 2:44:32.080
 to leave, I would fight for leaving.

2:44:33.680 --> 2:44:37.120
 I find it's more.

2:44:37.120 --> 2:44:44.560
 I find it's more honest and real to think about, you know, dying today at the end of

2:44:44.560 --> 2:44:45.040
 the day.

2:44:46.080 --> 2:44:52.960
 That seems to me, at least to my brain, more honest slap in the face as opposed to I still

2:44:52.960 --> 2:44:59.520
 have 10 years like today, then I'm much more about appreciating the cup and the table and

2:44:59.520 --> 2:45:04.960
 so on and less about like silly worldly accomplishments and all those kinds of things.

2:45:04.960 --> 2:45:11.760
 But we have in the company a person who say at some point found out that they have cancer

2:45:11.760 --> 2:45:16.000
 and that also gives, you know, huge perspective with respect to what matters now.

2:45:16.000 --> 2:45:16.560
 Yeah.

2:45:16.560 --> 2:45:20.320
 And, you know, often people in situations like that, they conclude that actually what

2:45:20.320 --> 2:45:21.680
 matters is human connection.

2:45:22.720 --> 2:45:28.720
 And love and that's people conclude also if you have kids, kids as family.

2:45:28.720 --> 2:45:35.120
 You, I think, tweeted, we don't assign the minus infinity reward to our death.

2:45:35.440 --> 2:45:38.640
 Such a reward would prevent us from taking any risk.

2:45:38.640 --> 2:45:42.480
 We wouldn't be able to cross the road in fear of being hit by a car.

2:45:42.480 --> 2:45:46.400
 So in the objective function, you mentioned fear of death might be fundamental to the

2:45:46.400 --> 2:45:47.440
 human condition.

2:45:48.400 --> 2:45:52.640
 So, as I said, let's assume that they're like a reward functions in our brain.

2:45:52.640 --> 2:46:01.840
 And the interesting thing is even realization, how different reward functions can play with

2:46:01.840 --> 2:46:02.640
 your behavior.

2:46:03.440 --> 2:46:09.280
 As a matter of fact, I wouldn't say that you should assign infinite negative reward to

2:46:09.280 --> 2:46:11.360
 anything because that messes up the math.

2:46:12.400 --> 2:46:13.600
 The math doesn't work out.

2:46:13.600 --> 2:46:14.320
 It doesn't work out.

2:46:14.320 --> 2:46:19.440
 And as you said, even, you know, government or some insurance companies, you said they

2:46:19.440 --> 2:46:22.720
 assign $9 million to human life.

2:46:22.720 --> 2:46:29.600
 And I'm just saying it with respect to, that might be a hard statement to ourselves, but

2:46:29.600 --> 2:46:32.480
 in some sense that there is a finite value of our own life.

2:46:34.640 --> 2:46:43.440
 I'm trying to put it from perspective of being less, of being more egoless and realizing

2:46:43.440 --> 2:46:44.800
 fragility of my own life.

2:46:44.800 --> 2:46:53.760
 And in some sense, the fear of death might prevent you from acting because anything can

2:46:53.760 --> 2:46:54.560
 cause death.

2:46:56.080 --> 2:46:56.560
 Yeah.

2:46:56.560 --> 2:47:00.800
 And I'm sure actually, if you were to put death in the objective function, there's probably

2:47:00.800 --> 2:47:06.960
 so many aspects to death and fear of death and realization of death and mortality.

2:47:06.960 --> 2:47:13.600
 There's just whole components of finiteness of not just your life, but every experience

2:47:13.600 --> 2:47:17.840
 and so on that you're going to have to formalize mathematically.

2:47:18.320 --> 2:47:27.040
 And also, you know, that might lead to you spending a lot of compute cycles on this like

2:47:27.040 --> 2:47:32.480
 a deliberating this terrible future instead of experiencing now.

2:47:32.480 --> 2:47:36.480
 And then in some sense, it's also kind of unpleasant simulation to run in your head.

2:47:36.480 --> 2:47:37.040
 Yeah.

2:47:39.040 --> 2:47:45.120
 Do you think there's an objective function that describes the entirety of human life?

2:47:45.920 --> 2:47:49.680
 So, you know, usually the way you ask that is what is the meaning of life?

2:47:50.560 --> 2:47:55.760
 Is there a universal objective functions that captures the why of life?

2:47:55.760 --> 2:48:03.440
 So, yeah, I mean, I suspect that they will ask this question, but it's also a question

2:48:03.440 --> 2:48:05.280
 that I ask myself many, many times.

2:48:06.320 --> 2:48:10.320
 See, I can tell you a framework that I have these days to think about this question.

2:48:10.320 --> 2:48:16.480
 So I think that fundamentally, meaning of life has to do with some of our reward actions

2:48:16.480 --> 2:48:21.680
 that we have in brain and they might have to do with, let's say, for instance, curiosity

2:48:21.680 --> 2:48:25.760
 or human connection, which might mean understanding others.

2:48:27.760 --> 2:48:32.080
 It's also possible for a person to slightly modify their reward function.

2:48:32.080 --> 2:48:37.280
 Usually they mostly stay fixed, but it's possible to modify reward function and you can pretty

2:48:37.280 --> 2:48:38.080
 much choose.

2:48:38.080 --> 2:48:42.480
 So in some sense, the reward functions, optimizing reward functions, they will give you a life

2:48:42.480 --> 2:48:43.120
 satisfaction.

2:48:44.000 --> 2:48:45.920
 Is there some randomness in the function?

2:48:45.920 --> 2:48:48.000
 I think when you are born, there is some randomness.

2:48:48.000 --> 2:48:53.840
 You can see that some people, for instance, they care more about building stuff.

2:48:53.840 --> 2:48:56.160
 Some people care more about caring for others.

2:48:56.880 --> 2:49:00.880
 Some people, there are all sorts of default reward functions.

2:49:00.880 --> 2:49:08.400
 And then in some sense, you can ask yourself, what is the satisfying way for you to go after

2:49:08.400 --> 2:49:09.680
 this reward function?

2:49:09.680 --> 2:49:11.280
 And you just go after this reward function.

2:49:11.280 --> 2:49:15.120
 And, you know, some people also ask, are you satisfied with your life?

2:49:15.120 --> 2:49:19.120
 And, you know, some people also ask, are these reward functions real?

2:49:19.840 --> 2:49:27.680
 I almost think about it as, let's say, if you would have to discover mathematics, in

2:49:27.680 --> 2:49:34.640
 mathematics, you are likely to run into various objects like complex numbers or differentiation,

2:49:34.640 --> 2:49:35.680
 some other objects.

2:49:35.680 --> 2:49:38.320
 And these are very natural objects that arise.

2:49:38.320 --> 2:49:42.480
 And similarly, the reward functions that we are having in our brain, they are somewhat

2:49:42.480 --> 2:49:50.240
 very natural, that, you know, there is a reward function for understanding, like a comprehension,

2:49:52.080 --> 2:49:53.280
 curiosity, and so on.

2:49:53.280 --> 2:49:59.040
 So in some sense, they are in the same way natural as their natural objects in mathematics.

2:49:59.040 --> 2:49:59.680
 Interesting.

2:49:59.680 --> 2:50:05.600
 So, you know, there's the old sort of debate, is mathematics invented or discovered?

2:50:05.600 --> 2:50:07.840
 You're saying reward functions are discovered.

2:50:07.840 --> 2:50:08.880
 So nature.

2:50:08.880 --> 2:50:12.960
 So nature provided some, you can still, let's say, expand it throughout the life.

2:50:12.960 --> 2:50:15.360
 Some of the reward functions, they might be futile.

2:50:15.360 --> 2:50:19.600
 Like, for instance, there might be a reward function, maximize amount of wealth.

2:50:20.320 --> 2:50:20.800
 Yeah.

2:50:20.800 --> 2:50:24.160
 And this is more like a learned reward function.

2:50:25.520 --> 2:50:30.560
 But we know also that some reward functions, if you optimize them, you won't be quite satisfied.

2:50:32.240 --> 2:50:37.040
 Well, I don't know which part of your reward function resulted in you coming today, but

2:50:37.040 --> 2:50:40.960
 I am deeply appreciative that you did spend your valuable time with me.

2:50:40.960 --> 2:50:43.360
 Wojtek is really fun talking to you.

2:50:43.920 --> 2:50:45.200
 You're brilliant.

2:50:45.200 --> 2:50:46.320
 You're a good human being.

2:50:46.320 --> 2:50:48.880
 And it's an honor to meet you and an honor to talk to you.

2:50:48.880 --> 2:50:50.080
 Thanks for talking today, brother.

2:50:50.720 --> 2:50:51.600
 Thank you, Lex a lot.

2:50:51.600 --> 2:50:54.240
 I appreciated your questions, curiosity.

2:50:54.240 --> 2:50:55.680
 I had a lot of time being here.

2:50:57.120 --> 2:51:00.480
 Thanks for listening to this conversation with Wojtek Zaremba.

2:51:00.480 --> 2:51:04.480
 To support this podcast, please check out our sponsors in the description.

2:51:04.480 --> 2:51:10.000
 And now, let me leave you with some words from Arthur C. Clarke, who is the author of

2:51:10.000 --> 2:51:11.840
 2001 A Space Odyssey.

2:51:12.800 --> 2:51:18.800
 It may be that our role on this planet is not to worship God, but to create him.

2:51:18.800 --> 2:51:34.160
 Thank you for listening, and I hope to see you next time.

