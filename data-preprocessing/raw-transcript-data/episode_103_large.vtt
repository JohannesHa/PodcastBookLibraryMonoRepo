WEBVTT

00:00.000 --> 00:03.000
 The following is a conversation with Ben Goertzel,

00:03.000 --> 00:04.560
 one of the most interesting minds

00:04.560 --> 00:06.680
 in the artificial intelligence community.

00:06.680 --> 00:08.920
 He's the founder of SingularityNet,

00:08.920 --> 00:11.520
 designer of OpenCog AI Framework,

00:11.520 --> 00:13.220
 formerly a director of research

00:13.220 --> 00:15.720
 at the Machine Intelligence Research Institute,

00:15.720 --> 00:18.440
 and chief scientist of Hanson Robotics,

00:18.440 --> 00:21.000
 the company that created the Sophia robot.

00:21.000 --> 00:23.680
 He has been a central figure in the AGI community

00:23.680 --> 00:26.960
 for many years, including in his organizing

00:26.960 --> 00:28.720
 and contributing to the conference

00:28.720 --> 00:30.920
 on artificial general intelligence,

00:30.920 --> 00:34.440
 the 2020 version of which is actually happening this week,

00:34.440 --> 00:36.480
 Wednesday, Thursday, and Friday.

00:36.480 --> 00:38.480
 It's virtual and free.

00:38.480 --> 00:40.040
 I encourage you to check out the talks,

00:40.040 --> 00:45.040
 including by Yosha Bach from episode 101 of this podcast.

00:45.160 --> 00:46.600
 Quick summary of the ads.

00:46.600 --> 00:51.040
 Two sponsors, The Jordan Harbinger Show and Masterclass.

00:51.040 --> 00:52.800
 Please consider supporting this podcast

00:52.800 --> 00:56.500
 by going to jordanharbinger.com slash lex

00:56.500 --> 01:00.380
 and signing up at masterclass.com slash lex.

01:00.380 --> 01:02.840
 Click the links, buy all the stuff.

01:02.840 --> 01:04.640
 It's the best way to support this podcast

01:04.640 --> 01:08.840
 and the journey I'm on in my research and startup.

01:08.840 --> 01:11.480
 This is the Artificial Intelligence Podcast.

01:11.480 --> 01:13.680
 If you enjoy it, subscribe on YouTube,

01:13.680 --> 01:15.940
 review it with five stars on Apple Podcast,

01:15.940 --> 01:18.920
 support it on Patreon, or connect with me on Twitter

01:18.920 --> 01:23.920
 at lexfriedman, spelled without the E, just F R I D M A N.

01:23.920 --> 01:25.980
 As usual, I'll do a few minutes of ads now

01:25.980 --> 01:27.340
 and never any ads in the middle

01:27.340 --> 01:29.980
 that can break the flow of the conversation.

01:29.980 --> 01:33.340
 This episode is supported by The Jordan Harbinger Show.

01:33.340 --> 01:35.900
 Go to jordanharbinger.com slash lex.

01:35.900 --> 01:37.740
 It's how he knows I sent you.

01:37.740 --> 01:40.140
 On that page, there's links to subscribe to it

01:40.140 --> 01:43.300
 on Apple Podcast, Spotify, and everywhere else.

01:43.300 --> 01:45.100
 I've been binging on his podcast.

01:45.100 --> 01:46.220
 Jordan is great.

01:46.220 --> 01:47.900
 He gets the best out of his guests,

01:47.900 --> 01:50.100
 dives deep, calls them out when it's needed,

01:50.100 --> 01:52.260
 and makes the whole thing fun to listen to.

01:52.260 --> 01:55.340
 He's interviewed Kobe Bryant, Mark Cuban,

01:55.340 --> 01:59.060
 Neil deGrasse Tyson, Keira Kasparov, and many more.

01:59.060 --> 02:01.540
 His conversation with Kobe is a reminder

02:01.540 --> 02:06.060
 how much focus and hard work is required for greatness

02:06.060 --> 02:09.540
 in sport, business, and life.

02:09.540 --> 02:12.420
 I highly recommend the episode if you want to be inspired.

02:12.420 --> 02:15.940
 Again, go to jordanharbinger.com slash lex.

02:15.940 --> 02:17.900
 It's how Jordan knows I sent you.

02:18.900 --> 02:21.300
 This show is sponsored by Master Class.

02:21.300 --> 02:24.300
 Sign up at masterclass.com slash lex

02:24.300 --> 02:27.660
 to get a discount and to support this podcast.

02:27.660 --> 02:29.220
 When I first heard about Master Class,

02:29.220 --> 02:31.060
 I thought it was too good to be true.

02:31.060 --> 02:34.220
 For 180 bucks a year, you get an all access pass

02:34.220 --> 02:37.540
 to watch courses from to list some of my favorites.

02:37.540 --> 02:39.780
 Chris Hadfield on Space Exploration,

02:39.780 --> 02:41.780
 Neil deGrasse Tyson on Scientific Thinking

02:41.780 --> 02:44.980
 and Communication, Will Wright, creator of

02:44.980 --> 02:47.940
 the greatest city building game ever, Sim City,

02:47.940 --> 02:50.700
 and Sims on Space Exploration.

02:50.700 --> 02:54.860
 Ben Sims on Game Design, Carlos Santana on Guitar,

02:54.860 --> 02:59.460
 Keira Kasparov, the greatest chess player ever on chess,

02:59.460 --> 03:01.980
 Daniel Negrano on Poker, and many more.

03:01.980 --> 03:04.660
 Chris Hadfield explaining how rockets work

03:04.660 --> 03:07.300
 and the experience of being launched into space alone

03:07.300 --> 03:08.700
 is worth the money.

03:08.700 --> 03:12.020
 Once again, sign up at masterclass.com slash lex

03:12.020 --> 03:15.820
 to get a discount and to support this podcast.

03:15.820 --> 03:20.780
 Now, here's my conversation with Ben Kurtzell.

03:20.780 --> 03:25.100
 What books, authors, ideas had a lot of impact on you

03:25.100 --> 03:26.780
 in your life in the early days?

03:27.900 --> 03:32.180
 You know, what got me into AI and science fiction

03:32.180 --> 03:34.580
 and such in the first place wasn't a book,

03:34.580 --> 03:37.020
 but the original Star Trek TV show,

03:37.020 --> 03:39.860
 which my dad watched with me like in its first run.

03:39.860 --> 03:42.700
 It would have been 1968, 69 or something,

03:42.700 --> 03:45.340
 and that was incredible because every show

03:45.340 --> 03:49.140
 they visited a different alien civilization

03:49.140 --> 03:51.300
 with different culture and weird mechanisms.

03:51.300 --> 03:55.020
 But that got me into science fiction,

03:55.020 --> 03:57.180
 and there wasn't that much science fiction

03:57.180 --> 03:58.660
 to watch on TV at that stage,

03:58.660 --> 04:01.420
 so that got me into reading the whole literature

04:01.420 --> 04:03.340
 of science fiction, you know,

04:03.340 --> 04:07.500
 from the beginning of the previous century until that time.

04:07.500 --> 04:10.860
 And I mean, there was so many science fiction writers

04:10.860 --> 04:12.420
 who were inspirational to me.

04:12.420 --> 04:14.820
 I'd say if I had to pick two,

04:14.820 --> 04:18.820
 it would have been Stanisław Lem, the Polish writer.

04:18.820 --> 04:22.020
 Yeah, Solaris, and then he had a bunch

04:22.020 --> 04:25.780
 of more obscure writings on superhuman AIs

04:25.780 --> 04:26.620
 that were engineered.

04:26.620 --> 04:28.660
 Solaris was sort of a superhuman,

04:28.660 --> 04:31.540
 naturally occurring intelligence.

04:31.540 --> 04:34.780
 Then Philip K. Dick, who, you know,

04:34.780 --> 04:37.340
 ultimately my fandom for Philip K. Dick

04:37.340 --> 04:39.100
 is one of the things that brought me together

04:39.100 --> 04:43.740
 with David Hansen, my collaborator on robotics projects.

04:43.740 --> 04:47.620
 So, you know, Stanisław Lem was very much an intellectual,

04:47.620 --> 04:51.020
 right, so he had a very broad view of intelligence

04:51.020 --> 04:54.420
 going beyond the human and into what I would call,

04:54.420 --> 04:56.900
 you know, open ended superintelligence.

04:56.900 --> 05:01.900
 The Solaris superintelligent ocean was intelligent,

05:01.900 --> 05:04.420
 in some ways more generally intelligent than people,

05:04.420 --> 05:07.340
 but in a complex and confusing way

05:07.340 --> 05:10.180
 so that human beings could never quite connect to it,

05:10.180 --> 05:13.260
 but it was still probably very, very smart.

05:13.260 --> 05:16.620
 And then the Golem 4 supercomputer

05:16.620 --> 05:20.420
 in one of Lem's books, this was engineered by people,

05:20.420 --> 05:24.420
 but eventually it became very intelligent

05:24.420 --> 05:26.020
 in a different direction than humans

05:26.020 --> 05:29.260
 and decided that humans were kind of trivial,

05:29.260 --> 05:30.260
 not that interesting.

05:30.260 --> 05:35.260
 So it put some impenetrable shield around itself,

05:35.300 --> 05:36.700
 shut itself off from humanity,

05:36.700 --> 05:40.060
 and then issued some philosophical screed

05:40.060 --> 05:44.540
 about the pathetic and hopeless nature of humanity

05:44.540 --> 05:48.380
 and all human thought, and then disappeared.

05:48.380 --> 05:51.140
 Now, Philip K. Dick, he was a bit different.

05:51.140 --> 05:52.460
 He was human focused, right?

05:52.460 --> 05:55.860
 His main thing was, you know, human compassion

05:55.860 --> 05:59.540
 and the human heart and soul are going to be the constant

05:59.540 --> 06:03.620
 that will keep us going through whatever aliens we discover

06:03.620 --> 06:08.620
 or telepathy machines or super AIs or whatever it might be.

06:08.620 --> 06:10.660
 So he didn't believe in reality,

06:10.660 --> 06:13.740
 like the reality that we see may be a simulation

06:13.740 --> 06:17.100
 or a dream or something else we can't even comprehend,

06:17.100 --> 06:19.100
 but he believed in love and compassion

06:19.100 --> 06:20.660
 as something persistent

06:20.660 --> 06:22.460
 through the various simulated realities.

06:22.460 --> 06:26.740
 So those two science fiction writers had a huge impact on me.

06:26.740 --> 06:30.300
 Then a little older than that, I got into Dostoevsky

06:30.300 --> 06:33.620
 and Friedrich Nietzsche and Rimbaud

06:33.620 --> 06:36.900
 and a bunch of more literary type writing.

06:36.900 --> 06:38.620
 Can we talk about some of those things?

06:38.620 --> 06:41.660
 So on the Solaris side, Stanislaw Lem,

06:43.180 --> 06:47.020
 this kind of idea of there being intelligences out there

06:47.020 --> 06:48.700
 that are different than our own,

06:49.540 --> 06:53.020
 do you think there are intelligences maybe all around us

06:53.020 --> 06:56.420
 that we're not able to even detect?

06:56.420 --> 06:58.700
 So this kind of idea of,

06:58.700 --> 07:01.580
 maybe you can comment also on Stephen Wolfram

07:01.580 --> 07:04.340
 thinking that there's computations all around us

07:04.340 --> 07:07.460
 and we're just not smart enough to kind of detect

07:07.460 --> 07:10.380
 their intelligence or appreciate their intelligence.

07:10.380 --> 07:13.540
 Yeah, so my friend Hugo de Gares,

07:13.540 --> 07:15.780
 who I've been talking to about these things

07:15.780 --> 07:19.300
 for many decades, since the early 90s,

07:19.300 --> 07:21.740
 he had an idea he called SIPI,

07:21.740 --> 07:25.100
 the Search for Intraparticulate Intelligence.

07:25.100 --> 07:28.140
 So the concept there was as AIs get smarter

07:28.140 --> 07:29.460
 and smarter and smarter,

07:30.820 --> 07:33.660
 assuming the laws of physics as we know them now

07:33.660 --> 07:37.420
 are still what these super intelligences

07:37.420 --> 07:39.220
 perceived to hold and are bound by,

07:39.220 --> 07:40.420
 as they get smarter and smarter,

07:40.420 --> 07:42.980
 they're gonna shrink themselves littler and littler

07:42.980 --> 07:45.380
 because special relativity makes it

07:45.380 --> 07:47.220
 so they can communicate

07:47.220 --> 07:49.300
 between two spatially distant points.

07:49.300 --> 07:50.780
 So they're gonna get smaller and smaller,

07:50.780 --> 07:53.220
 but then ultimately, what does that mean?

07:53.220 --> 07:56.500
 The minds of the super, super, super intelligences,

07:56.500 --> 07:59.020
 they're gonna be packed into the interaction

07:59.020 --> 08:01.940
 of elementary particles or quarks

08:01.940 --> 08:04.580
 or the partons inside quarks or whatever it is.

08:04.580 --> 08:07.620
 So what we perceive as random fluctuations

08:07.620 --> 08:09.740
 on the quantum or sub quantum level

08:09.740 --> 08:11.500
 may actually be the thoughts

08:11.500 --> 08:16.300
 of the micro, micro, micro miniaturized super intelligences

08:16.300 --> 08:19.140
 because there's no way we can tell random

08:19.140 --> 08:21.620
 from structured but within algorithmic information

08:21.620 --> 08:23.100
 more complex than our brains, right?

08:23.100 --> 08:24.300
 We can't tell the difference.

08:24.300 --> 08:27.020
 So what we think is random could be the thought processes

08:27.020 --> 08:29.980
 of some really tiny super minds.

08:29.980 --> 08:34.020
 And if so, there is not a damn thing we can do about it,

08:34.020 --> 08:37.180
 except try to upgrade our intelligences

08:37.180 --> 08:40.060
 and expand our minds so that we can perceive

08:40.060 --> 08:41.300
 more of what's around us.

08:41.300 --> 08:43.980
 But if those random fluctuations,

08:43.980 --> 08:46.540
 like even if we go to like quantum mechanics,

08:46.540 --> 08:51.220
 if that's actually super intelligent systems,

08:51.220 --> 08:54.620
 aren't we then part of the super of super intelligence?

08:54.620 --> 08:58.340
 Aren't we just like a finger of the entirety

08:58.340 --> 09:01.300
 of the body of the super intelligent system?

09:01.300 --> 09:05.940
 It could be, I mean, a finger is a strange metaphor.

09:05.940 --> 09:08.060
 I mean, we...

09:08.060 --> 09:10.700
 A finger is dumb is what I mean.

09:10.700 --> 09:12.260
 But the finger is also useful

09:12.260 --> 09:14.780
 and is controlled with intent by the brain

09:14.780 --> 09:16.700
 whereas we may be much less than that, right?

09:16.700 --> 09:21.340
 I mean, yeah, we may be just some random epiphenomenon

09:21.340 --> 09:23.300
 that they don't care about too much.

09:23.300 --> 09:26.380
 Like think about the shape of the crowd emanating

09:26.380 --> 09:28.260
 from a sports stadium or something, right?

09:28.260 --> 09:31.580
 There's some emergent shape to the crowd, it's there.

09:31.580 --> 09:33.700
 You could take a picture of it, it's kind of cool.

09:33.700 --> 09:36.300
 It's irrelevant to the main point of the sports event

09:36.300 --> 09:37.860
 or where the people are going

09:37.860 --> 09:40.220
 or what's on the minds of the people

09:40.220 --> 09:41.860
 making that shape in the crowd, right?

09:41.860 --> 09:46.860
 So we may just be some semi arbitrary higher level pattern

09:47.660 --> 09:49.700
 popping out of a lower level

09:49.700 --> 09:52.260
 hyper intelligent self organization.

09:52.260 --> 09:55.860
 And I mean, so be it, right?

09:55.860 --> 09:57.060
 I mean, that's one thing that...

09:57.060 --> 09:59.500
 Yeah, I mean, the older I've gotten,

09:59.500 --> 10:04.220
 the more respect I've achieved for our fundamental ignorance.

10:04.220 --> 10:06.260
 I mean, mine and everybody else's.

10:06.260 --> 10:08.820
 I mean, I look at my two dogs,

10:08.820 --> 10:10.940
 two beautiful little toy poodles

10:10.940 --> 10:14.780
 and they watch me sitting at the computer typing.

10:14.780 --> 10:16.980
 They just think I'm sitting there wiggling my fingers

10:16.980 --> 10:19.980
 to exercise them maybe or guarding the monitor on the desk

10:19.980 --> 10:22.340
 that they have no idea that I'm communicating

10:22.340 --> 10:24.420
 with other people halfway around the world,

10:24.420 --> 10:27.660
 let alone creating complex algorithms

10:27.660 --> 10:30.220
 running in RAM on some computer server

10:30.220 --> 10:32.540
 in St. Petersburg or something, right?

10:32.540 --> 10:35.100
 Although they're right there in the room with me.

10:35.100 --> 10:37.780
 So what things are there right around us

10:37.780 --> 10:40.780
 that we're just too stupid or close minded to comprehend?

10:40.780 --> 10:42.140
 Probably quite a lot.

10:42.140 --> 10:46.220
 Your very poodle could also be communicating

10:46.220 --> 10:49.980
 across multiple dimensions with other beings

10:49.980 --> 10:53.180
 and you're too unintelligent to understand

10:53.180 --> 10:55.700
 the kind of communication mechanism they're going through.

10:55.700 --> 10:59.820
 There have been various TV shows and science fiction novels,

10:59.820 --> 11:03.220
 poisoning cats, dolphins, mice and whatnot

11:03.220 --> 11:07.220
 are actually super intelligences here to observe that.

11:07.220 --> 11:12.220
 I would guess as one or the other quantum physics founders

11:12.580 --> 11:15.500
 said, those theories are not crazy enough to be true.

11:15.500 --> 11:17.660
 The reality is probably crazier than that.

11:17.660 --> 11:18.500
 Beautifully put.

11:18.500 --> 11:22.020
 So on the human side, with Philip K. Dick

11:22.020 --> 11:27.020
 and in general, where do you fall on this idea

11:27.260 --> 11:30.580
 that love and just the basic spirit of human nature

11:30.580 --> 11:34.980
 persists throughout these multiple realities?

11:34.980 --> 11:38.420
 Are you on the side, like the thing that inspires you

11:38.420 --> 11:40.020
 about artificial intelligence,

11:40.980 --> 11:45.980
 is it the human side of somehow persisting

11:46.740 --> 11:49.820
 through all of the different systems we engineer

11:49.820 --> 11:53.340
 or is AI inspire you to create something

11:53.340 --> 11:55.500
 that's greater than human, that's beyond human,

11:55.500 --> 11:57.460
 that's almost nonhuman?

11:59.140 --> 12:02.820
 I would say my motivation to create AGI

12:02.820 --> 12:05.220
 comes from both of those directions actually.

12:05.220 --> 12:08.620
 So when I first became passionate about AGI

12:08.620 --> 12:11.420
 when I was, it would have been two or three years old

12:11.420 --> 12:14.700
 after watching robots on Star Trek.

12:14.700 --> 12:18.180
 I mean, then it was really a combination

12:18.180 --> 12:21.460
 of intellectual curiosity, like can a machine really think,

12:21.460 --> 12:22.860
 how would you do that?

12:22.860 --> 12:27.180
 And yeah, just ambition to create something much better

12:27.180 --> 12:28.660
 than all the clearly limited

12:28.660 --> 12:31.900
 and fundamentally defective humans I saw around me.

12:31.900 --> 12:35.340
 Then as I got older and got more enmeshed

12:35.340 --> 12:38.780
 in the human world and got married, had children,

12:38.780 --> 12:41.900
 saw my parents begin to age, I started to realize,

12:41.900 --> 12:45.300
 well, not only will AGI let you go far beyond

12:45.300 --> 12:46.860
 the limitations of the human,

12:46.860 --> 12:50.860
 but it could also stop us from dying and suffering

12:50.860 --> 12:54.980
 and feeling pain and tormenting ourselves mentally.

12:54.980 --> 12:58.060
 So you can see AGI has amazing capability

12:58.060 --> 13:01.380
 to do good for humans, as humans,

13:01.380 --> 13:03.420
 alongside with its capability

13:03.420 --> 13:06.620
 to go far, far beyond the human level.

13:06.620 --> 13:09.980
 So I mean, both aspects are there,

13:09.980 --> 13:13.220
 which makes it even more exciting and important.

13:13.220 --> 13:15.500
 So you mentioned Dostoevsky and Nietzsche.

13:15.500 --> 13:17.060
 Where did you pick up from those guys?

13:17.060 --> 13:17.900
 I mean.

13:18.980 --> 13:21.500
 That would probably go beyond the scope

13:21.500 --> 13:24.340
 of a brief interview, certainly.

13:24.340 --> 13:26.780
 I mean, both of those are amazing thinkers

13:26.780 --> 13:29.020
 who one, will necessarily have

13:29.020 --> 13:32.060
 a complex relationship with, right?

13:32.060 --> 13:36.460
 So, I mean, Dostoevsky on the minus side,

13:36.460 --> 13:38.460
 he's kind of a religious fanatic

13:38.460 --> 13:42.020
 and he sort of helped squash the Russian nihilist movement,

13:42.020 --> 13:43.140
 which was very interesting.

13:43.140 --> 13:45.820
 Because what nihilism meant originally

13:45.820 --> 13:48.660
 in that period of the mid, late 1800s in Russia

13:48.660 --> 13:52.180
 was not taking anything fully 100% for granted.

13:52.180 --> 13:54.420
 It was really more like what we'd call Bayesianism now,

13:54.420 --> 13:56.900
 where you don't wanna adopt anything

13:56.900 --> 14:01.060
 as a dogmatic certitude and always leave your mind open.

14:01.060 --> 14:04.420
 And how Dostoevsky parodied nihilism

14:04.420 --> 14:06.660
 was a bit different, right?

14:06.660 --> 14:10.340
 He parodied as people who believe absolutely nothing.

14:10.340 --> 14:13.020
 So they must assign an equal probability weight

14:13.020 --> 14:17.780
 to every proposition, which doesn't really work.

14:17.780 --> 14:22.540
 So on the one hand, I didn't really agree with Dostoevsky

14:22.540 --> 14:26.140
 on his sort of religious point of view.

14:26.140 --> 14:29.660
 On the other hand, if you look at his understanding

14:29.660 --> 14:32.660
 of human nature and sort of the human mind

14:32.660 --> 14:37.100
 and heart and soul, it's really unparalleled.

14:37.100 --> 14:42.100
 He had an amazing view of how human beings construct a world

14:42.100 --> 14:45.380
 for themselves based on their own understanding

14:45.380 --> 14:47.500
 and their own mental predisposition.

14:47.500 --> 14:50.100
 And I think if you look in the brothers Karamazov

14:50.100 --> 14:55.100
 in particular, the Russian literary theorist Mikhail Bakhtin

14:56.140 --> 14:59.580
 wrote about this as a polyphonic mode of fiction,

14:59.580 --> 15:02.300
 which means it's not third person,

15:02.300 --> 15:05.020
 but it's not first person from any one person really.

15:05.020 --> 15:07.020
 There are many different characters in the novel

15:07.020 --> 15:10.020
 and each of them is sort of telling part of the story

15:10.020 --> 15:11.580
 from their own point of view.

15:11.580 --> 15:15.900
 So the reality of the whole story is an intersection

15:15.900 --> 15:19.020
 like synergetically of the many different characters

15:19.020 --> 15:19.860
 world views.

15:19.860 --> 15:23.220
 And that really, it's a beautiful metaphor

15:23.220 --> 15:26.100
 and even a reflection I think of how all of us

15:26.100 --> 15:27.700
 socially create our reality.

15:27.700 --> 15:31.060
 Like each of us sees the world in a certain way.

15:31.060 --> 15:34.780
 Each of us in a sense is making the world as we see it

15:34.780 --> 15:37.620
 based on our own minds and understanding,

15:37.620 --> 15:40.980
 but it's polyphony like in music

15:40.980 --> 15:43.300
 where multiple instruments are coming together

15:43.300 --> 15:44.620
 to create the sound.

15:44.620 --> 15:46.700
 The ultimate reality that's created

15:46.700 --> 15:50.220
 comes out of each of our subjective understandings,

15:50.220 --> 15:51.340
 intersecting with each other.

15:51.340 --> 15:55.660
 And that was one of the many beautiful things in Dostoevsky.

15:55.660 --> 15:57.980
 So maybe a little bit to mention,

15:57.980 --> 16:02.260
 you have a connection to Russia and the Soviet culture.

16:02.260 --> 16:03.860
 I mean, I'm not sure exactly what the nature

16:03.860 --> 16:06.180
 of the connection is, but at least the spirit

16:06.180 --> 16:07.380
 of your thinking is in there.

16:07.380 --> 16:12.380
 Well, my ancestry is three quarters Eastern European Jewish.

16:12.740 --> 16:16.740
 So I mean, my three of my great grandparents

16:16.740 --> 16:20.340
 emigrated to New York from Lithuania

16:20.340 --> 16:23.060
 and sort of border regions of Poland,

16:23.060 --> 16:24.980
 which are in and out of Poland

16:24.980 --> 16:28.020
 in around the time of World War I.

16:28.020 --> 16:33.020
 And they were socialists and communists as well as Jews,

16:33.700 --> 16:35.940
 mostly Menshevik, not Bolshevik.

16:35.940 --> 16:39.260
 And they sort of, they fled at just the right time

16:39.260 --> 16:41.260
 to the US for their own personal reasons.

16:41.260 --> 16:45.580
 And then almost all, or maybe all of my extended family

16:45.580 --> 16:47.220
 that remained in Eastern Europe was killed

16:47.220 --> 16:50.380
 either by Hitlands or Stalin's minions at some point.

16:50.380 --> 16:53.580
 So the branch of the family that emigrated to the US

16:53.580 --> 16:56.740
 was pretty much the only one.

16:56.740 --> 16:58.700
 So how much of the spirit of the people

16:58.700 --> 16:59.900
 is in your blood still?

16:59.900 --> 17:02.820
 Like, when you look in the mirror, do you see,

17:03.900 --> 17:04.860
 what do you see?

17:04.860 --> 17:08.460
 Meat, I see a bag of meat that I want to transcend

17:08.460 --> 17:12.180
 by uploading into some sort of superior reality.

17:12.180 --> 17:17.180
 But very, I mean, yeah, very clearly,

17:18.340 --> 17:22.260
 I mean, I'm not religious in a traditional sense,

17:22.260 --> 17:27.260
 but clearly the Eastern European Jewish tradition

17:27.260 --> 17:28.780
 was what I was raised in.

17:28.780 --> 17:32.700
 I mean, there was, my grandfather, Leo Zwell,

17:32.700 --> 17:35.380
 was a physical chemist who worked with Linus Pauling

17:35.380 --> 17:38.100
 and a bunch of the other early greats in quantum mechanics.

17:38.100 --> 17:41.220
 I mean, he was into X ray diffraction.

17:41.220 --> 17:42.940
 He was on the material science side,

17:42.940 --> 17:45.420
 an experimentalist rather than a theorist.

17:45.420 --> 17:47.700
 His sister was also a physicist.

17:47.700 --> 17:51.100
 And my father's father, Victor Gertzel,

17:51.100 --> 17:56.100
 was a PhD in psychology who had the unenviable job

17:57.100 --> 17:59.260
 of giving Soka therapy to the Japanese

17:59.260 --> 18:03.100
 in internment camps in the US in World War II,

18:03.100 --> 18:05.820
 like to counsel them why they shouldn't kill themselves,

18:05.820 --> 18:08.420
 even though they'd had all their stuff taken away

18:08.420 --> 18:10.300
 and been imprisoned for no good reason.

18:10.300 --> 18:15.300
 So, I mean, yeah, there's a lot of Eastern European

18:15.780 --> 18:18.060
 Jewishness in my background.

18:18.060 --> 18:20.180
 One of my great uncles was, I guess,

18:20.180 --> 18:22.420
 conductor of San Francisco Orchestra.

18:22.420 --> 18:25.620
 So there's a lot of Mickey Salkind,

18:25.620 --> 18:27.660
 bunch of music in there also.

18:27.660 --> 18:31.540
 And clearly this culture was all about learning

18:31.540 --> 18:34.860
 and understanding the world,

18:34.860 --> 18:38.820
 and also not quite taking yourself too seriously

18:38.820 --> 18:39.900
 while you do it, right?

18:39.900 --> 18:42.060
 There's a lot of Yiddish humor in there.

18:42.060 --> 18:45.220
 So I do appreciate that culture,

18:45.220 --> 18:47.580
 although the whole idea that like the Jews

18:47.580 --> 18:49.020
 are the chosen people of God

18:49.020 --> 18:51.740
 never resonated with me too much.

18:51.740 --> 18:55.100
 The graph of the Gertzel family,

18:55.100 --> 18:56.940
 I mean, just the people I've encountered

18:56.940 --> 18:59.540
 just doing some research and just knowing your work

18:59.540 --> 19:02.700
 through the decades, it's kind of fascinating.

19:03.580 --> 19:06.380
 Just the number of PhDs.

19:06.380 --> 19:10.740
 Yeah, yeah, I mean, my dad is a sociology professor

19:10.740 --> 19:15.060
 who recently retired from Rutgers University,

19:15.060 --> 19:18.540
 but clearly that gave me a head start in life.

19:18.540 --> 19:20.260
 I mean, my grandfather gave me

19:20.260 --> 19:21.620
 all those quantum mechanics books

19:21.620 --> 19:24.220
 when I was like seven or eight years old.

19:24.220 --> 19:26.060
 I remember going through them,

19:26.060 --> 19:28.020
 and it was all the old quantum mechanics

19:28.020 --> 19:30.420
 like Rutherford Adams and stuff.

19:30.420 --> 19:32.860
 So I got to the part of wave functions,

19:32.860 --> 19:36.140
 which I didn't understand, although I was very bright kid.

19:36.140 --> 19:38.660
 And I realized he didn't quite understand it either,

19:38.660 --> 19:41.980
 but at least like he pointed me to some professor

19:41.980 --> 19:45.340
 he knew at UPenn nearby who understood these things, right?

19:45.340 --> 19:49.620
 So that's an unusual opportunity for a kid to have, right?

19:49.620 --> 19:52.380
 My dad, he was programming Fortran

19:52.380 --> 19:53.900
 when I was 10 or 11 years old

19:53.900 --> 19:57.660
 on like HP 3000 mainframes at Rutgers University.

19:57.660 --> 20:00.900
 So I got to do linear regression in Fortran

20:00.900 --> 20:04.220
 on punch cards when I was in middle school, right?

20:04.220 --> 20:07.460
 Because he was doing, I guess, analysis of demographic

20:07.460 --> 20:09.580
 and sociology data.

20:09.580 --> 20:14.580
 So yes, certainly that gave me a head start

20:14.780 --> 20:17.220
 and a push towards science beyond what would have been

20:17.220 --> 20:19.700
 the case with many, many different situations.

20:19.700 --> 20:22.220
 When did you first fall in love with AI?

20:22.220 --> 20:24.700
 Is it the programming side of Fortran?

20:24.700 --> 20:27.260
 Is it maybe the sociology psychology

20:27.260 --> 20:28.300
 that you picked up from your dad?

20:28.300 --> 20:29.140
 Or is it the quantum mechanics?

20:29.140 --> 20:30.660
 I fell in love with AI when I was probably three years old

20:30.660 --> 20:32.580
 when I saw a robot on Star Trek.

20:32.580 --> 20:34.620
 It was turning around in a circle going,

20:34.620 --> 20:36.660
 error, error, error, error,

20:36.660 --> 20:39.540
 because Spock and Kirk had tricked it

20:39.540 --> 20:41.300
 into a mechanical breakdown by presenting it

20:41.300 --> 20:42.900
 with a logical paradox.

20:42.900 --> 20:45.660
 And I was just like, well, this makes no sense.

20:45.660 --> 20:47.540
 This AI is very, very smart.

20:47.540 --> 20:49.620
 It's been traveling all around the universe,

20:49.620 --> 20:50.980
 but these people could trick it

20:50.980 --> 20:52.660
 with a simple logical paradox.

20:52.660 --> 20:57.020
 Like why, if the human brain can get beyond that paradox,

20:57.020 --> 20:59.460
 why can't this AI?

20:59.460 --> 21:03.140
 So I felt the screenwriters of Star Trek

21:03.140 --> 21:06.060
 had misunderstood the nature of intelligence.

21:06.060 --> 21:07.580
 And I complained to my dad about it,

21:07.580 --> 21:12.220
 and he wasn't gonna say anything one way or the other.

21:12.220 --> 21:17.220
 But before I was born, when my dad was at Antioch College

21:18.460 --> 21:20.860
 in the middle of the US,

21:20.860 --> 21:25.860
 he led a protest movement called SLAM,

21:25.860 --> 21:27.460
 Student League Against Mortality.

21:27.460 --> 21:28.980
 They were protesting against death,

21:28.980 --> 21:31.500
 wandering across the campus.

21:31.500 --> 21:35.900
 So he was into some futuristic things even back then,

21:35.900 --> 21:40.220
 but whether AI could confront logical paradoxes or not,

21:40.220 --> 21:41.220
 he didn't know.

21:41.220 --> 21:44.780
 But when I, 10 years after that or something,

21:44.780 --> 21:46.980
 I discovered Douglas Hofstadter's book,

21:46.980 --> 21:51.100
 Gordalesh or Bach, and that was sort of to the same point of AI

21:51.100 --> 21:52.620
 and paradox and logic, right?

21:52.620 --> 21:54.460
 Because he was over and over

21:54.460 --> 21:56.180
 with Gordal's incompleteness theorem,

21:56.180 --> 22:00.500
 and can an AI really fully model itself reflexively

22:00.500 --> 22:02.820
 or does that lead you into some paradox?

22:02.820 --> 22:05.260
 Can the human mind truly model itself reflexively

22:05.260 --> 22:07.500
 or does that lead you into some paradox?

22:07.500 --> 22:10.660
 So I think that book, Gordalesh or Bach,

22:10.660 --> 22:13.460
 which I think I read when it first came out,

22:13.460 --> 22:14.980
 I would have been 12 years old or something.

22:14.980 --> 22:17.100
 I remember it was like 16 hour day.

22:17.100 --> 22:19.780
 I read it cover to cover and then reread it.

22:19.780 --> 22:21.260
 I reread it after that,

22:21.260 --> 22:22.380
 because there was a lot of weird things

22:22.380 --> 22:24.380
 with little formal systems in there

22:24.380 --> 22:25.660
 that were hard for me at the time.

22:25.660 --> 22:27.980
 But that was the first book I read

22:27.980 --> 22:32.980
 that gave me a feeling for AI as like a practical academic

22:34.420 --> 22:37.380
 or engineering discipline that people were working in.

22:37.380 --> 22:40.060
 Because before I read Gordalesh or Bach,

22:40.060 --> 22:43.980
 I was into AI from the point of view of a science fiction fan.

22:43.980 --> 22:47.460
 And I had the idea, well, it may be a long time

22:47.460 --> 22:50.420
 before we can achieve immortality in superhuman AGI.

22:50.420 --> 22:54.380
 So I should figure out how to build a spacecraft

22:54.380 --> 22:57.060
 traveling close to the speed of light, go far away,

22:57.060 --> 22:58.780
 then come back to the earth in a million years

22:58.780 --> 23:00.220
 when technology is more advanced

23:00.220 --> 23:01.700
 and we can build these things.

23:01.700 --> 23:03.580
 Reading Gordalesh or Bach,

23:03.580 --> 23:06.580
 while it didn't all ring true to me, a lot of it did,

23:06.580 --> 23:09.860
 but I could see like there are smart people right now

23:09.860 --> 23:11.580
 at various universities around me

23:11.580 --> 23:15.420
 who are actually trying to work on building

23:15.420 --> 23:16.980
 what I would now call AGI,

23:16.980 --> 23:19.020
 although Hofstadter didn't call it that.

23:19.020 --> 23:21.100
 So really it was when I read that book,

23:21.100 --> 23:23.540
 which would have been probably middle school,

23:23.540 --> 23:24.820
 that then I started to think,

23:24.820 --> 23:29.020
 well, this is something that I could practically work on.

23:29.020 --> 23:31.660
 Yeah, as opposed to flying away and waiting it out,

23:31.660 --> 23:33.500
 you can actually be one of the people

23:33.500 --> 23:34.580
 that actually builds the system.

23:34.580 --> 23:35.420
 Yeah, exactly.

23:35.420 --> 23:36.740
 And if you think about, I mean,

23:36.740 --> 23:40.700
 I was interested in what we'd now call nanotechnology

23:40.700 --> 23:44.820
 and in the human immortality and time travel,

23:44.820 --> 23:46.940
 all the same cool things as every other,

23:46.940 --> 23:49.260
 like science fiction loving kid.

23:49.260 --> 23:52.700
 But AI seemed like if Hofstadter was right,

23:52.700 --> 23:54.180
 you just figure out the right program,

23:54.180 --> 23:55.060
 sit there and type it.

23:55.060 --> 23:59.620
 Like you don't need to spin stars into weird configurations

23:59.620 --> 24:02.620
 or get government approval to cut people up

24:02.620 --> 24:05.020
 and fiddle with their DNA or something, right?

24:05.020 --> 24:06.180
 It's just programming.

24:06.180 --> 24:10.700
 And then of course that can achieve anything else.

24:10.700 --> 24:12.220
 There's another book from back then,

24:12.220 --> 24:17.060
 which was by Gerald Feinbaum,

24:17.060 --> 24:21.580
 who was a physicist at Princeton.

24:21.580 --> 24:24.580
 And that was the Prometheus Project.

24:24.580 --> 24:26.700
 And this book was written in the late 1960s,

24:26.700 --> 24:28.780
 though I encountered it in the mid 70s.

24:28.780 --> 24:30.940
 But what this book said is in the next few decades,

24:30.940 --> 24:34.500
 humanity is gonna create superhuman thinking machines,

24:34.500 --> 24:37.460
 molecular nanotechnology and human immortality.

24:37.460 --> 24:41.140
 And then the challenge we'll have is what to do with it.

24:41.140 --> 24:43.020
 Do we use it to expand human consciousness

24:43.020 --> 24:44.500
 in a positive direction?

24:44.500 --> 24:49.500
 Or do we use it just to further vapid consumerism?

24:49.860 --> 24:51.820
 And what he proposed was that the UN

24:51.820 --> 24:53.460
 should do a survey on this.

24:53.460 --> 24:56.460
 And the UN should send people out to every little village

24:56.460 --> 24:58.940
 in remotest Africa or South America

24:58.940 --> 25:01.300
 and explain to everyone what technology

25:01.300 --> 25:03.020
 was gonna bring the next few decades

25:03.020 --> 25:05.020
 and the choice that we had about how to use it.

25:05.020 --> 25:07.780
 And let everyone on the whole planet vote

25:07.780 --> 25:11.740
 about whether we should develop super AI nanotechnology

25:11.740 --> 25:15.900
 and immortality for expanded consciousness

25:15.900 --> 25:18.220
 or for rampant consumerism.

25:18.220 --> 25:22.060
 And needless to say, that didn't quite happen.

25:22.060 --> 25:24.180
 And I think this guy died in the mid 80s,

25:24.180 --> 25:25.900
 so we didn't even see his ideas start

25:25.900 --> 25:28.220
 to become more mainstream.

25:28.220 --> 25:31.620
 But it's interesting, many of the themes I'm engaged with now

25:31.620 --> 25:33.340
 from AGI and immortality,

25:33.340 --> 25:36.140
 even to trying to democratize technology

25:36.140 --> 25:38.100
 as I've been pushing forward with Singularity,

25:38.100 --> 25:40.020
 my work in the blockchain world,

25:40.020 --> 25:43.620
 many of these themes were there in Feinbaum's book

25:43.620 --> 25:47.940
 in the late 60s even.

25:47.940 --> 25:52.220
 And of course, Valentin Turchin, a Russian writer

25:52.220 --> 25:55.860
 and a great Russian physicist who I got to know

25:55.860 --> 25:59.060
 when we both lived in New York in the late 90s

25:59.060 --> 25:59.900
 and early aughts.

25:59.900 --> 26:03.380
 I mean, he had a book in the late 60s in Russia,

26:03.380 --> 26:05.780
 which was the phenomenon of science,

26:05.780 --> 26:10.220
 which laid out all these same things as well.

26:10.220 --> 26:12.740
 And Val died in, I don't remember,

26:12.740 --> 26:15.420
 2004 or five or something of Parkinson'sism.

26:15.420 --> 26:20.420
 So yeah, it's easy for people to lose track now

26:20.780 --> 26:25.780
 of the fact that the futurist and Singularitarian

26:25.940 --> 26:29.740
 advanced technology ideas that are now almost mainstream

26:29.740 --> 26:30.900
 are on TV all the time.

26:30.900 --> 26:34.100
 I mean, these are not that new, right?

26:34.100 --> 26:37.100
 They're sort of new in the history of the human species,

26:37.100 --> 26:41.100
 but I mean, these were all around in fairly mature form

26:41.100 --> 26:43.660
 in the middle of the last century,

26:43.660 --> 26:45.500
 were written about quite articulately

26:45.500 --> 26:47.340
 by fairly mainstream people

26:47.340 --> 26:50.140
 who were professors at top universities.

26:50.140 --> 26:52.940
 It's just until the enabling technologies

26:52.940 --> 26:57.940
 got to a certain point, then you couldn't make it real.

26:57.940 --> 27:02.820
 And even in the 70s, I was sort of seeing that

27:02.820 --> 27:04.740
 and living through it, right?

27:04.740 --> 27:07.900
 From Star Trek to Douglas Hofstadter,

27:07.900 --> 27:09.660
 things were getting very, very practical

27:09.660 --> 27:11.980
 from the late 60s to the late 70s.

27:11.980 --> 27:15.020
 And the first computer I bought,

27:15.020 --> 27:17.580
 you could only program with hexadecimal machine code

27:17.580 --> 27:19.380
 and you had to solder it together.

27:19.380 --> 27:23.420
 And then like a few years later, there's punch cards.

27:23.420 --> 27:27.220
 And a few years later, you could get like Atari 400

27:27.220 --> 27:30.300
 and Commodore VIC 20, and you could type on the keyboard

27:30.300 --> 27:32.820
 and program in higher level languages

27:32.820 --> 27:34.660
 alongside the assembly language.

27:34.660 --> 27:38.700
 So these ideas have been building up a while.

27:38.700 --> 27:42.980
 And I guess my generation got to feel them build up,

27:42.980 --> 27:46.380
 which is different than people coming into the field now

27:46.380 --> 27:50.300
 for whom these things have just been part of the ambience

27:50.300 --> 27:52.180
 of culture for their whole career

27:52.180 --> 27:54.140
 or even their whole life.

27:54.140 --> 27:57.260
 Well, it's fascinating to think about there being all

27:57.260 --> 28:01.540
 of these ideas kind of swimming, almost with the noise

28:01.540 --> 28:04.380
 all around the world, all the different generations,

28:04.380 --> 28:07.900
 and then some kind of nonlinear thing happens

28:07.900 --> 28:09.380
 where they percolate up

28:09.380 --> 28:12.420
 and capture the imagination of the mainstream.

28:12.420 --> 28:14.780
 And that seems to be what's happening with AI now.

28:14.780 --> 28:16.580
 I mean, Nietzsche, who you mentioned had the idea

28:16.580 --> 28:18.260
 of the Superman, right?

28:18.260 --> 28:21.580
 But he didn't understand enough about technology

28:21.580 --> 28:24.860
 to think you could physically engineer a Superman

28:24.860 --> 28:28.180
 by piecing together molecules in a certain way.

28:28.180 --> 28:33.180
 He was a bit vague about how the Superman would appear,

28:33.620 --> 28:35.820
 but he was quite deep at thinking

28:35.820 --> 28:37.780
 about what the state of consciousness

28:37.780 --> 28:42.420
 and the mode of cognition of a Superman would be.

28:42.420 --> 28:47.420
 He was a very astute analyst of how the human mind

28:47.820 --> 28:49.420
 constructs the illusion of a self,

28:49.420 --> 28:52.140
 how it constructs the illusion of free will,

28:52.140 --> 28:56.660
 how it constructs values like good and evil

28:56.660 --> 28:59.780
 out of its own desire to maintain

28:59.780 --> 29:01.420
 and advance its own organism.

29:01.420 --> 29:04.020
 He understood a lot about how human minds work.

29:04.020 --> 29:05.660
 Then he understood a lot

29:05.660 --> 29:07.620
 about how post human minds would work.

29:07.620 --> 29:10.260
 I mean, the Superman was supposed to be a mind

29:10.260 --> 29:13.300
 that would basically have complete root access

29:13.300 --> 29:16.060
 to its own brain and consciousness

29:16.060 --> 29:19.620
 and be able to architect its own value system

29:19.620 --> 29:24.300
 and inspect and fine tune all of its own biases.

29:24.300 --> 29:27.340
 So that's a lot of powerful thinking there,

29:27.340 --> 29:29.340
 which then fed in and sort of seeded

29:29.340 --> 29:32.180
 all of postmodern continental philosophy

29:32.180 --> 29:35.540
 and all sorts of things have been very valuable

29:35.540 --> 29:39.740
 in development of culture and indirectly even of technology.

29:39.740 --> 29:42.140
 But of course, without the technology there,

29:42.140 --> 29:44.860
 it was all some quite abstract thinking.

29:44.860 --> 29:46.940
 So now we're at a time in history

29:46.940 --> 29:51.740
 when a lot of these ideas can be made real,

29:51.740 --> 29:54.300
 which is amazing and scary, right?

29:54.300 --> 29:56.020
 It's kind of interesting to think,

29:56.020 --> 29:57.180
 what do you think Nietzsche would do

29:57.180 --> 30:00.900
 if he was born a century later or transported through time?

30:00.900 --> 30:02.980
 What do you think he would say about AI?

30:02.980 --> 30:04.180
 I mean. Well, those are quite different.

30:04.180 --> 30:07.260
 If he's born a century later or transported through time.

30:07.260 --> 30:09.580
 Well, he'd be on like TikTok and Instagram

30:09.580 --> 30:11.940
 and he would never write the great works he's written.

30:11.940 --> 30:13.540
 So let's transport him through time.

30:13.540 --> 30:16.460
 Maybe also Sprach Zarathustra would be a music video,

30:16.460 --> 30:19.660
 right? I mean, who knows?

30:19.660 --> 30:21.700
 Yeah, but if he was transported through time,

30:21.700 --> 30:26.260
 do you think, that'd be interesting actually to go back.

30:26.260 --> 30:29.380
 You just made me realize that it's possible to go back

30:29.380 --> 30:31.220
 and read Nietzsche with an eye of,

30:31.220 --> 30:34.700
 is there some thinking about artificial beings?

30:34.700 --> 30:37.780
 I'm sure there he had inklings.

30:37.780 --> 30:40.500
 I mean, with Frankenstein before him,

30:40.500 --> 30:42.900
 I'm sure he had inklings of artificial beings

30:42.900 --> 30:44.060
 somewhere in the text.

30:44.060 --> 30:46.900
 It'd be interesting to try to read his work

30:46.900 --> 30:51.900
 to see if Superman was actually an AGI system.

30:55.820 --> 30:57.940
 Like if he had inklings of that kind of thinking.

30:57.940 --> 30:58.780
 He didn't.

30:58.780 --> 30:59.620
 He didn't.

30:59.620 --> 31:01.100
 No, I would say not.

31:01.100 --> 31:06.100
 I mean, he had a lot of inklings of modern cognitive science,

31:06.460 --> 31:07.420
 which are very interesting.

31:07.420 --> 31:11.820
 If you look in like the third part of the collection

31:11.820 --> 31:13.540
 that's been titled The Will to Power.

31:13.540 --> 31:15.660
 I mean, in book three there,

31:15.660 --> 31:20.620
 there's very deep analysis of thinking processes,

31:20.620 --> 31:25.620
 but he wasn't so much of a physical tinkerer type guy,

31:27.140 --> 31:29.620
 right? He was very abstract.

31:29.620 --> 31:32.780
 Do you think, what do you think about the will to power?

31:32.780 --> 31:36.100
 Do you think human, what do you think drives humans?

31:36.100 --> 31:37.460
 Is it?

31:37.460 --> 31:39.500
 Oh, an unholy mix of things.

31:39.500 --> 31:42.380
 I don't think there's one pure, simple,

31:42.380 --> 31:47.380
 and elegant objective function driving humans by any means.

31:47.380 --> 31:50.700
 What do you think, if we look at,

31:50.700 --> 31:53.260
 I know it's hard to look at humans in an aggregate,

31:53.260 --> 31:56.220
 but do you think overall humans are good?

31:57.540 --> 32:01.580
 Or do we have both good and evil within us

32:01.580 --> 32:03.540
 that depending on the circumstances,

32:03.540 --> 32:08.220
 depending on whatever can percolate to the top?

32:08.220 --> 32:13.220
 Good and evil are very ambiguous, complicated

32:13.900 --> 32:15.900
 and in some ways silly concepts.

32:15.900 --> 32:18.540
 But if we could dig into your question

32:18.540 --> 32:19.700
 from a couple of directions.

32:19.700 --> 32:23.420
 So I think if you look in evolution,

32:23.420 --> 32:28.220
 humanity is shaped both by individual selection

32:28.220 --> 32:30.940
 and what biologists would call group selection,

32:30.940 --> 32:32.740
 like tribe level selection, right?

32:32.740 --> 32:36.500
 So individual selection has driven us

32:36.500 --> 32:38.780
 in a selfish DNA sort of way.

32:38.780 --> 32:43.260
 So that each of us does to a certain approximation

32:43.260 --> 32:47.420
 what will help us propagate our DNA to future generations.

32:47.420 --> 32:50.700
 I mean, that's why I've got four kids so far

32:50.700 --> 32:53.900
 and probably that's not the last one.

32:53.900 --> 32:55.020
 On the other hand.

32:55.020 --> 32:56.780
 I like the ambition.

32:56.780 --> 33:00.740
 Tribal, like group selection means humans in a way

33:00.740 --> 33:04.380
 will do what will advocate for the persistence of the DNA

33:04.380 --> 33:08.100
 of their whole tribe or their social group.

33:08.100 --> 33:11.740
 And in biology, you have both of these, right?

33:11.740 --> 33:14.420
 And you can see, say an ant colony or a beehive,

33:14.420 --> 33:15.940
 there's a lot of group selection

33:15.940 --> 33:18.940
 in the evolution of those social animals.

33:18.940 --> 33:21.460
 On the other hand, say a big cat

33:21.460 --> 33:23.260
 or some very solitary animal,

33:23.260 --> 33:26.540
 it's a lot more biased toward individual selection.

33:26.540 --> 33:28.660
 Humans are an interesting balance.

33:28.660 --> 33:31.540
 And I think this reflects itself

33:31.540 --> 33:35.060
 in what we would view as selfishness versus altruism

33:35.060 --> 33:36.780
 to some extent.

33:36.780 --> 33:40.580
 So we just have both of those objective functions

33:40.580 --> 33:43.780
 contributing to the makeup of our brains.

33:43.780 --> 33:47.300
 And then as Nietzsche analyzed in his own way

33:47.300 --> 33:49.060
 and others have analyzed in different ways,

33:49.060 --> 33:51.500
 I mean, we abstract this as well,

33:51.500 --> 33:55.380
 we have both good and evil within us, right?

33:55.380 --> 33:57.820
 Because a lot of what we view as evil

33:57.820 --> 34:00.460
 is really just selfishness.

34:00.460 --> 34:03.740
 A lot of what we view as good is altruism,

34:03.740 --> 34:07.220
 which means doing what's good for the tribe.

34:07.220 --> 34:08.060
 And on that level,

34:08.060 --> 34:11.380
 we have both of those just baked into us

34:11.380 --> 34:13.180
 and that's how it is.

34:13.180 --> 34:17.020
 Of course, there are psychopaths and sociopaths

34:17.020 --> 34:21.340
 and people who get gratified by the suffering of others.

34:21.340 --> 34:25.260
 And that's a different thing.

34:25.260 --> 34:27.500
 Yeah, those are exceptions on the whole.

34:27.500 --> 34:31.540
 But I think at core, we're not purely selfish,

34:31.540 --> 34:35.180
 we're not purely altruistic, we are a mix

34:35.180 --> 34:38.020
 and that's the nature of it.

34:38.020 --> 34:43.020
 And we also have a complex constellation of values

34:43.380 --> 34:48.380
 that are just very specific to our evolutionary history.

34:49.180 --> 34:52.500
 Like we love waterways and mountains

34:52.500 --> 34:54.460
 and the ideal place to put a house

34:54.460 --> 34:56.340
 is in a mountain overlooking the water, right?

34:56.340 --> 35:00.580
 And we care a lot about our kids

35:00.580 --> 35:02.820
 and we care a little less about our cousins

35:02.820 --> 35:04.420
 and even less about our fifth cousins.

35:04.420 --> 35:09.420
 I mean, there are many particularities to human values,

35:09.460 --> 35:11.900
 which whether they're good or evil

35:11.900 --> 35:15.820
 depends on your perspective.

35:15.820 --> 35:19.660
 Say, I spent a lot of time in Ethiopia in Addis Ababa

35:19.660 --> 35:22.460
 where we have one of our AI development offices

35:22.460 --> 35:24.420
 for my SingularityNet project.

35:24.420 --> 35:27.540
 And when I walk through the streets in Addis,

35:27.540 --> 35:31.460
 you know, there's people lying by the side of the road,

35:31.460 --> 35:33.940
 like just living there by the side of the road,

35:33.940 --> 35:35.820
 dying probably of curable diseases

35:35.820 --> 35:37.940
 without enough food or medicine.

35:37.940 --> 35:39.980
 And when I walk by them, you know, I feel terrible,

35:39.980 --> 35:41.460
 I give them money.

35:41.460 --> 35:43.900
 When I come back home to the developed world,

35:45.100 --> 35:46.620
 they're not on my mind that much.

35:46.620 --> 35:48.620
 I do donate some, but I mean,

35:48.620 --> 35:52.860
 I also spend some of the limited money I have

35:52.860 --> 35:54.700
 enjoying myself in frivolous ways

35:54.700 --> 35:58.100
 rather than donating it to those people who are right now,

35:58.100 --> 36:01.020
 like starving, dying and suffering on the roadside.

36:01.020 --> 36:03.180
 So does that make me evil?

36:03.180 --> 36:05.500
 I mean, it makes me somewhat selfish

36:05.500 --> 36:06.740
 and somewhat altruistic.

36:06.740 --> 36:10.940
 And we each balance that in our own way, right?

36:10.940 --> 36:15.940
 So whether that will be true of all possible AGI's

36:17.060 --> 36:19.300
 is a subtler question.

36:19.300 --> 36:21.340
 So that's how humans are.

36:21.340 --> 36:23.100
 So you have a sense, you kind of mentioned

36:23.100 --> 36:25.500
 that there's a selfish,

36:25.500 --> 36:28.300
 I'm not gonna bring up the whole Ayn Rand idea

36:28.300 --> 36:31.140
 of selfishness being the core virtue.

36:31.140 --> 36:33.980
 That's a whole interesting kind of tangent

36:33.980 --> 36:36.420
 that I think we'll just distract ourselves on.

36:36.420 --> 36:38.460
 I have to make one amusing comment.

36:38.460 --> 36:39.300
 Sure.

36:39.300 --> 36:41.260
 A comment that has amused me anyway.

36:41.260 --> 36:46.260
 So the, yeah, I have extraordinary negative respect

36:46.340 --> 36:47.820
 for Ayn Rand.

36:47.820 --> 36:50.220
 Negative, what's a negative respect?

36:50.220 --> 36:54.740
 But when I worked with a company called Genescient,

36:54.740 --> 36:59.180
 which was evolving flies to have extraordinary long lives

36:59.180 --> 37:01.220
 in Southern California.

37:01.220 --> 37:04.980
 So we had flies that were evolved by artificial selection

37:04.980 --> 37:07.660
 to have five times the lifespan of normal fruit flies.

37:07.660 --> 37:11.780
 But the population of super long lived flies

37:11.780 --> 37:14.060
 was physically sitting in a spare room

37:14.060 --> 37:18.100
 at an Ayn Rand elementary school in Southern California.

37:18.100 --> 37:19.460
 So that was just like,

37:19.460 --> 37:22.620
 well, if I saw this in a movie, I wouldn't believe it.

37:23.980 --> 37:26.020
 Well, yeah, the universe has a sense of humor

37:26.020 --> 37:26.860
 in that kind of way.

37:26.860 --> 37:28.900
 That fits in, humor fits in somehow

37:28.900 --> 37:30.620
 into this whole absurd existence.

37:30.620 --> 37:33.820
 But you mentioned the balance between selfishness

37:33.820 --> 37:37.220
 and altruism as kind of being innate.

37:37.220 --> 37:38.140
 Do you think it's possible

37:38.140 --> 37:42.380
 that's kind of an emergent phenomena,

37:42.380 --> 37:45.420
 those peculiarities of our value system?

37:45.420 --> 37:47.180
 How much of it is innate?

37:47.180 --> 37:49.780
 How much of it is something we collectively

37:49.780 --> 37:51.460
 kind of like a Dostoevsky novel

37:52.300 --> 37:54.540
 bring to life together as a civilization?

37:54.540 --> 37:57.740
 I mean, the answer to nature versus nurture

37:57.740 --> 37:58.860
 is usually both.

37:58.860 --> 38:01.820
 And of course it's nature versus nurture

38:01.820 --> 38:04.780
 versus self organization, as you mentioned.

38:04.780 --> 38:08.460
 So clearly there are evolutionary roots

38:08.460 --> 38:11.460
 to individual and group selection

38:11.460 --> 38:13.900
 leading to a mix of selfishness and altruism.

38:13.900 --> 38:15.380
 On the other hand,

38:15.380 --> 38:19.780
 different cultures manifest that in different ways.

38:19.780 --> 38:22.540
 Well, we all have basically the same biology.

38:22.540 --> 38:26.660
 And if you look at sort of precivilized cultures,

38:26.660 --> 38:29.340
 you have tribes like the Yanomamo in Venezuela,

38:29.340 --> 38:34.340
 which their culture is focused on killing other tribes.

38:35.340 --> 38:37.620
 And you have other Stone Age tribes

38:37.620 --> 38:40.460
 that are mostly peaceful and have big taboos

38:40.460 --> 38:41.420
 against violence.

38:41.420 --> 38:43.900
 So you can certainly have a big difference

38:43.900 --> 38:46.860
 in how culture manifests

38:46.860 --> 38:50.820
 these innate biological characteristics,

38:50.820 --> 38:54.740
 but still, there's probably limits

38:54.740 --> 38:56.740
 that are given by our biology.

38:56.740 --> 39:00.060
 I used to argue this with my great grandparents

39:00.060 --> 39:01.500
 who were Marxists actually,

39:01.500 --> 39:04.540
 because they believed in the withering away of the state.

39:04.540 --> 39:06.900
 Like they believe that,

39:06.900 --> 39:10.660
 as you move from capitalism to socialism to communism,

39:10.660 --> 39:13.420
 people would just become more social minded

39:13.420 --> 39:15.940
 so that a state would be unnecessary

39:15.940 --> 39:20.940
 and everyone would give everyone else what they needed.

39:20.940 --> 39:23.140
 Now, setting aside that

39:23.140 --> 39:25.740
 that's not what the various Marxist experiments

39:25.740 --> 39:29.900
 on the planet seem to be heading toward in practice.

39:29.900 --> 39:32.740
 Just as a theoretical point,

39:32.740 --> 39:37.540
 I was very dubious that human nature could go there.

39:37.540 --> 39:39.900
 Like at that time when my great grandparents are alive,

39:39.900 --> 39:43.300
 I was just like, you know, I'm a cynical teenager.

39:43.300 --> 39:45.980
 I think humans are just jerks.

39:45.980 --> 39:48.020
 The state is not gonna wither away.

39:48.020 --> 39:49.980
 If you don't have some structure

39:49.980 --> 39:51.980
 keeping people from screwing each other over,

39:51.980 --> 39:52.900
 they're gonna do it.

39:52.900 --> 39:56.220
 So now I actually don't quite see things that way.

39:56.220 --> 39:59.900
 I mean, I think my feeling now subjectively

39:59.900 --> 40:02.580
 is the culture aspect is more significant

40:02.580 --> 40:04.620
 than I thought it was when I was a teenager.

40:04.620 --> 40:08.260
 And I think you could have a human society

40:08.260 --> 40:11.420
 that was dialed dramatically further toward,

40:11.420 --> 40:13.700
 you know, self awareness, other awareness,

40:13.700 --> 40:16.980
 compassion and sharing than our current society.

40:16.980 --> 40:20.580
 And of course, greater material abundance helps,

40:20.580 --> 40:23.480
 but to some extent material abundance

40:23.480 --> 40:25.380
 is a subjective perception also

40:25.380 --> 40:28.260
 because many Stone Age cultures perceive themselves

40:28.260 --> 40:30.540
 as living in great material abundance

40:30.540 --> 40:32.100
 that they had all the food and water they wanted,

40:32.100 --> 40:33.500
 they lived in a beautiful place,

40:33.500 --> 40:37.460
 that they had sex lives, that they had children.

40:37.460 --> 40:42.460
 I mean, they had abundance without any factories, right?

40:42.940 --> 40:46.460
 So I think humanity probably would be capable

40:46.460 --> 40:51.140
 of fundamentally more positive and joy filled mode

40:51.140 --> 40:55.560
 of social existence than what we have now.

40:57.320 --> 40:59.500
 Clearly Marx didn't quite have the right idea

40:59.500 --> 41:01.800
 about how to get there.

41:01.800 --> 41:05.660
 I mean, he missed a number of key aspects

41:05.660 --> 41:09.500
 of human society and its evolution.

41:09.500 --> 41:11.960
 And if we look at where we are in society now,

41:13.140 --> 41:15.760
 how to get there is a quite different question

41:15.760 --> 41:18.100
 because there are very powerful forces

41:18.100 --> 41:21.080
 pushing people in different directions

41:21.080 --> 41:26.080
 than a positive, joyous, compassionate existence, right?

41:26.380 --> 41:28.820
 So if we were tried to, you know,

41:28.820 --> 41:32.820
 Elon Musk is dreams of colonizing Mars at the moment,

41:32.820 --> 41:36.880
 so we maybe will have a chance to start a new civilization

41:36.880 --> 41:38.400
 with a new governmental system.

41:38.400 --> 41:41.580
 And certainly there's quite a bit of chaos.

41:41.580 --> 41:44.320
 We're sitting now, I don't know what the date is,

41:44.320 --> 41:46.860
 but this is June.

41:46.860 --> 41:49.260
 There's quite a bit of chaos in all different forms

41:49.260 --> 41:52.060
 going on in the United States and all over the world.

41:52.060 --> 41:55.560
 So there's a hunger for new types of governments,

41:55.560 --> 41:58.260
 new types of leadership, new types of systems.

41:59.860 --> 42:01.980
 And so what are the forces at play

42:01.980 --> 42:04.140
 and how do we move forward?

42:04.140 --> 42:06.780
 Yeah, I mean, colonizing Mars, first of all,

42:06.780 --> 42:08.980
 it's a super cool thing to do.

42:08.980 --> 42:10.060
 We should be doing it.

42:10.060 --> 42:11.540
 So you love the idea.

42:11.540 --> 42:14.780
 Yeah, I mean, it's more important than making

42:14.780 --> 42:18.540
 chocolatey or chocolates and sexier lingerie

42:18.540 --> 42:21.020
 and many of the things that we spend

42:21.020 --> 42:24.120
 a lot more resources on as a species, right?

42:24.120 --> 42:26.480
 So I mean, we certainly should do it.

42:26.480 --> 42:31.480
 I think the possible futures in which a Mars colony

42:33.180 --> 42:38.040
 makes a critical difference for humanity are very few.

42:38.040 --> 42:42.220
 I mean, I think, I mean, assuming we make a Mars colony

42:42.220 --> 42:44.000
 and people go live there in a couple of decades,

42:44.000 --> 42:46.380
 I mean, their supplies are gonna come from Earth.

42:46.380 --> 42:48.820
 The money to make the colony came from Earth

42:48.820 --> 42:53.740
 and whatever powers are supplying the goods there

42:53.740 --> 42:56.820
 from Earth are gonna, in effect, be in control

42:56.820 --> 42:58.700
 of that Mars colony.

42:58.700 --> 43:02.060
 Of course, there are outlier situations

43:02.060 --> 43:06.460
 where Earth gets nuked into oblivion

43:06.460 --> 43:10.780
 and somehow Mars has been made self sustaining by that point

43:10.780 --> 43:14.220
 and then Mars is what allows humanity to persist.

43:14.220 --> 43:19.220
 But I think that those are very, very, very unlikely.

43:19.740 --> 43:23.020
 You don't think it could be a first step on a long journey?

43:23.020 --> 43:24.740
 Of course it's a first step on a long journey,

43:24.740 --> 43:27.140
 which is awesome.

43:27.140 --> 43:30.980
 I'm guessing the colonization of the rest

43:30.980 --> 43:33.260
 of the physical universe will probably be done

43:33.260 --> 43:38.140
 by AGI's that are better designed to live in space

43:38.140 --> 43:41.840
 than by the meat machines that we are.

43:41.840 --> 43:43.020
 But I mean, who knows?

43:43.020 --> 43:45.860
 We may cryopreserve ourselves in some superior way

43:45.860 --> 43:48.700
 to what we know now and like shoot ourselves out

43:48.700 --> 43:50.720
 to Alpha Centauri and beyond.

43:50.720 --> 43:52.660
 I mean, that's all cool.

43:52.660 --> 43:55.140
 It's very interesting and it's much more valuable

43:55.140 --> 43:58.860
 than most things that humanity is spending its resources on.

43:58.860 --> 44:03.540
 On the other hand, with AGI, we can get to a singularity

44:03.540 --> 44:07.780
 before the Mars colony becomes sustaining for sure,

44:07.780 --> 44:10.100
 possibly before it's even operational.

44:10.100 --> 44:12.400
 So your intuition is that that's the problem

44:12.400 --> 44:14.940
 if we really invest resources and we can get to faster

44:14.940 --> 44:19.700
 than a legitimate full self sustaining colonization of Mars.

44:19.700 --> 44:23.160
 Yeah, and it's very clear that we will to me

44:23.160 --> 44:26.020
 because there's so much economic value

44:26.020 --> 44:29.460
 in getting from narrow AI toward AGI,

44:29.460 --> 44:33.380
 whereas the Mars colony, there's less economic value

44:33.380 --> 44:37.380
 until you get quite far out into the future.

44:37.380 --> 44:40.260
 So I think that's very interesting.

44:40.260 --> 44:44.380
 I just think it's somewhat off to the side.

44:44.380 --> 44:48.020
 I mean, just as I think, say, art and music

44:48.020 --> 44:51.860
 are very, very interesting and I wanna see resources

44:51.860 --> 44:55.460
 go into amazing art and music being created.

44:55.460 --> 44:59.580
 And I'd rather see that than a lot of the garbage

44:59.580 --> 45:01.760
 that the society spends their money on.

45:01.760 --> 45:04.620
 On the other hand, I don't think Mars colonization

45:04.620 --> 45:07.780
 or inventing amazing new genres of music

45:07.780 --> 45:11.000
 is not one of the things that is most likely

45:11.000 --> 45:13.900
 to make a critical difference in the evolution

45:13.900 --> 45:18.340
 of human or nonhuman life in this part of the universe

45:18.340 --> 45:19.820
 over the next decade.

45:19.820 --> 45:21.620
 Do you think AGI is really?

45:21.620 --> 45:25.820
 AGI is by far the most important thing

45:25.820 --> 45:27.500
 that's on the horizon.

45:27.500 --> 45:31.620
 And then technologies that have direct ability

45:31.620 --> 45:36.620
 to enable AGI or to accelerate AGI are also very important.

45:37.260 --> 45:40.540
 For example, say, quantum computing.

45:40.540 --> 45:42.740
 I don't think that's critical to achieve AGI,

45:42.740 --> 45:44.360
 but certainly you could see how

45:44.360 --> 45:46.700
 the right quantum computing architecture

45:46.700 --> 45:49.280
 could massively accelerate AGI,

45:49.280 --> 45:52.260
 similar other types of nanotechnology.

45:52.260 --> 45:57.260
 Right now, the quest to cure aging and end disease

45:57.860 --> 46:02.100
 while not in the big picture as important as AGI,

46:02.100 --> 46:07.100
 of course, it's important to all of us as individual humans.

46:07.380 --> 46:11.600
 And if someone made a super longevity pill

46:11.600 --> 46:14.260
 and distributed it tomorrow, I mean,

46:14.260 --> 46:17.220
 that would be huge and a much larger impact

46:17.220 --> 46:20.460
 than a Mars colony is gonna have for quite some time.

46:20.460 --> 46:23.300
 But perhaps not as much as an AGI system.

46:23.300 --> 46:27.060
 No, because if you can make a benevolent AGI,

46:27.060 --> 46:28.700
 then all the other problems are solved.

46:28.700 --> 46:31.940
 I mean, if then the AGI can be,

46:31.940 --> 46:34.260
 once it's as generally intelligent as humans,

46:34.260 --> 46:37.420
 it can rapidly become massively more generally intelligent

46:37.420 --> 46:38.620
 than humans.

46:38.620 --> 46:42.540
 And then that AGI should be able to solve science

46:42.540 --> 46:46.840
 and engineering problems much better than human beings,

46:46.840 --> 46:49.700
 as long as it is in fact motivated to do so.

46:49.700 --> 46:52.740
 That's why I said a benevolent AGI.

46:52.740 --> 46:54.020
 There could be other kinds.

46:54.020 --> 46:56.020
 Maybe it's good to step back a little bit.

46:56.020 --> 46:57.980
 I mean, we've been using the term AGI.

46:58.860 --> 47:00.860
 People often cite you as the creator,

47:00.860 --> 47:03.060
 or at least the popularizer of the term AGI,

47:03.060 --> 47:05.700
 artificial general intelligence.

47:05.700 --> 47:09.100
 Can you tell the origin story of the term maybe?

47:09.100 --> 47:14.100
 So yeah, I would say I launched the term AGI upon the world

47:14.860 --> 47:19.860
 for what it's worth without ever fully being in love

47:19.940 --> 47:21.660
 with the term.

47:21.660 --> 47:25.380
 What happened is I was editing a book,

47:25.380 --> 47:27.860
 and this process started around 2001 or two.

47:27.860 --> 47:30.500
 I think the book came out 2005, finally.

47:30.500 --> 47:33.140
 I was editing a book which I provisionally

47:33.140 --> 47:35.860
 was titling Real AI.

47:35.860 --> 47:38.840
 And I mean, the goal was to gather together

47:38.840 --> 47:41.700
 fairly serious academicish papers

47:41.700 --> 47:43.940
 on the topic of making thinking machines

47:43.940 --> 47:46.780
 that could really think in the sense like people can,

47:46.780 --> 47:49.240
 or even more broadly than people can, right?

47:49.240 --> 47:52.740
 So then I was reaching out to other folks

47:52.740 --> 47:54.060
 that I had encountered here or there

47:54.060 --> 47:57.380
 who were interested in that,

47:57.380 --> 48:01.700
 which included some other folks who I knew

48:01.700 --> 48:04.340
 from the transhumist and singularitarian world,

48:04.340 --> 48:07.660
 like Peter Vos, who has a company, AGI Incorporated,

48:07.660 --> 48:12.660
 still in California, and included Shane Legge,

48:13.100 --> 48:15.700
 who had worked for me at my company, WebMind,

48:15.700 --> 48:17.580
 in New York in the late 90s,

48:17.580 --> 48:20.500
 who by now has become rich and famous.

48:20.500 --> 48:22.780
 He was one of the cofounders of Google DeepMind.

48:22.780 --> 48:25.320
 But at that time, Shane was,

48:25.320 --> 48:30.320
 I think he may have just started doing his PhD

48:31.800 --> 48:35.900
 with Marcus Hooter, who at that time

48:35.900 --> 48:38.680
 hadn't yet published his book, Universal AI,

48:38.680 --> 48:41.040
 which sort of gives a mathematical foundation

48:41.040 --> 48:43.400
 for artificial general intelligence.

48:43.400 --> 48:46.140
 So I reached out to Shane and Marcus and Peter Vos

48:46.140 --> 48:49.480
 and Pei Wang, who was another former employee of mine

48:49.480 --> 48:51.880
 who had been Douglas Hofstadter's PhD student

48:51.880 --> 48:53.280
 who had his own approach to AGI,

48:53.280 --> 48:58.040
 and a bunch of some Russian folks reached out to these guys

48:58.040 --> 49:01.360
 and they contributed papers for the book.

49:01.360 --> 49:04.440
 But that was my provisional title, but I never loved it

49:04.440 --> 49:09.320
 because in the end, I was doing some,

49:09.320 --> 49:12.120
 what we would now call narrow AI as well,

49:12.120 --> 49:14.640
 like applying machine learning to genomics data

49:14.640 --> 49:17.920
 or chat data for sentiment analysis.

49:17.920 --> 49:19.240
 I mean, that work is real.

49:19.240 --> 49:22.760
 And in a sense, it's really AI.

49:22.760 --> 49:26.000
 It's just a different kind of AI.

49:26.000 --> 49:31.000
 Ray Kurzweil wrote about narrow AI versus strong AI,

49:31.160 --> 49:35.040
 but that seemed weird to me because first of all,

49:35.040 --> 49:36.680
 narrow and strong are not antennas.

49:36.680 --> 49:38.720
 That's right.

49:38.720 --> 49:41.940
 But secondly, strong AI was used

49:41.940 --> 49:43.360
 in the cognitive science literature

49:43.360 --> 49:46.640
 to mean the hypothesis that digital computer AIs

49:46.640 --> 49:50.140
 could have true consciousness like human beings.

49:50.140 --> 49:52.540
 So there was already a meaning to strong AI,

49:52.540 --> 49:56.440
 which was complexly different, but related, right?

49:56.440 --> 50:00.520
 So we were tossing around on an email list

50:00.520 --> 50:03.200
 whether what title it should be.

50:03.200 --> 50:07.560
 And so we talked about narrow AI, broad AI, wide AI,

50:07.560 --> 50:09.760
 narrow AI, general AI.

50:09.760 --> 50:14.760
 And I think it was either Shane Legge or Peter Vos

50:15.880 --> 50:18.120
 on the private email discussion we had.

50:18.120 --> 50:18.960
 He said, but why don't we go

50:18.960 --> 50:21.800
 with AGI, artificial general intelligence?

50:21.800 --> 50:24.280
 And Pei Wang wanted to do GAI,

50:24.280 --> 50:25.760
 general artificial intelligence,

50:25.760 --> 50:27.880
 because in Chinese it goes in that order.

50:27.880 --> 50:30.200
 But we figured gay wouldn't work

50:30.200 --> 50:33.240
 in US culture at that time, right?

50:33.240 --> 50:37.360
 So we went with the AGI.

50:37.360 --> 50:39.520
 We used it for the title of that book.

50:39.520 --> 50:43.460
 And part of Peter and Shane's reasoning

50:43.460 --> 50:45.460
 was you have the G factor in psychology,

50:45.460 --> 50:47.480
 which is IQ, general intelligence, right?

50:47.480 --> 50:51.160
 So you have a meaning of GI, general intelligence,

50:51.160 --> 50:55.360
 in psychology, so then you're looking like artificial GI.

50:55.360 --> 51:00.360
 So then we use that for the title of the book.

51:00.400 --> 51:04.040
 And so I think maybe both Shane and Peter

51:04.040 --> 51:05.200
 think they invented the term,

51:05.200 --> 51:08.320
 but then later after the book was published,

51:08.320 --> 51:11.640
 this guy, Mark Guberd, came up to me and he's like,

51:11.640 --> 51:14.800
 well, I published an essay with the term AGI

51:14.800 --> 51:17.120
 in like 1997 or something.

51:17.120 --> 51:20.520
 And so I'm just waiting for some Russian to come out

51:20.520 --> 51:23.400
 and say they published that in 1953, right?

51:23.400 --> 51:27.800
 I mean, that term is not dramatically innovative

51:27.800 --> 51:28.640
 or anything.

51:28.640 --> 51:31.560
 It's one of these obvious in hindsight things,

51:31.560 --> 51:34.880
 which is also annoying in a way,

51:34.880 --> 51:39.500
 because Joshua Bach, who you interviewed,

51:39.500 --> 51:40.400
 is a close friend of mine.

51:40.400 --> 51:43.240
 He likes the term synthetic intelligence,

51:43.240 --> 51:44.300
 which I like much better,

51:44.300 --> 51:47.080
 but it hasn't actually caught on, right?

51:47.080 --> 51:51.800
 Because I mean, artificial is a bit off to me

51:51.800 --> 51:54.640
 because artifice is like a tool or something,

51:54.640 --> 51:57.760
 but not all AGI's are gonna be tools.

51:57.760 --> 51:58.700
 I mean, they may be now,

51:58.700 --> 52:00.600
 but we're aiming toward making them agents

52:00.600 --> 52:02.800
 rather than tools.

52:02.800 --> 52:04.840
 And in a way, I don't like the distinction

52:04.840 --> 52:07.200
 between artificial and natural,

52:07.200 --> 52:09.360
 because I mean, we're part of nature also

52:09.360 --> 52:12.160
 and machines are part of nature.

52:12.160 --> 52:14.840
 I mean, you can look at evolved versus engineered,

52:14.840 --> 52:17.160
 but that's a different distinction.

52:17.160 --> 52:20.000
 Then it should be engineered general intelligence, right?

52:20.000 --> 52:21.920
 And then general, well,

52:21.920 --> 52:24.600
 if you look at Marcus Hooter's book,

52:24.600 --> 52:26.860
 universally, what he argues there is,

52:28.240 --> 52:30.520
 within the domain of computation theory,

52:30.520 --> 52:31.920
 which is limited, but interesting.

52:31.920 --> 52:33.680
 So if you assume computable environments

52:33.680 --> 52:35.600
 or computable reward functions,

52:35.600 --> 52:37.560
 then he articulates what would be

52:37.560 --> 52:40.040
 a truly general intelligence,

52:40.040 --> 52:43.160
 a system called AIXI, which is quite beautiful.

52:43.160 --> 52:46.280
 AIXI, and that's the middle name

52:46.280 --> 52:49.360
 of my latest child, actually, is it?

52:49.360 --> 52:50.200
 What's the first name?

52:50.200 --> 52:52.400
 First name is QORXI, Q O R X I,

52:52.400 --> 52:53.780
 which my wife came up with,

52:53.780 --> 52:57.320
 but that's an acronym for quantum organized rational

52:57.320 --> 53:02.320
 expanding intelligence, and his middle name is Xiphonies,

53:03.120 --> 53:08.120
 actually, which means the former principal underlying AIXI.

53:08.340 --> 53:09.480
 But in any case.

53:09.480 --> 53:12.160
 You're giving Elon Musk's new child a run for his money.

53:12.160 --> 53:13.800
 Well, I did it first.

53:13.800 --> 53:17.320
 He copied me with this new freakish name,

53:17.320 --> 53:18.600
 but now if I have another baby,

53:18.600 --> 53:20.600
 I'm gonna have to outdo him.

53:20.600 --> 53:24.560
 It's becoming an arms race of weird, geeky baby names.

53:24.560 --> 53:26.840
 We'll see what the babies think about it, right?

53:26.840 --> 53:30.220
 But I mean, my oldest son, Zarathustra, loves his name,

53:30.220 --> 53:33.800
 and my daughter, Sharazad, loves her name.

53:33.800 --> 53:36.960
 So far, basically, if you give your kids weird names.

53:36.960 --> 53:37.840
 They live up to it.

53:37.840 --> 53:39.800
 Well, you're obliged to make the kids weird enough

53:39.800 --> 53:42.000
 that they like the names, right?

53:42.000 --> 53:43.920
 It directs their upbringing in a certain way.

53:43.920 --> 53:47.680
 But yeah, anyway, I mean, what Marcus showed in that book

53:47.680 --> 53:50.560
 is that a truly general intelligence

53:50.560 --> 53:51.800
 theoretically is possible,

53:51.800 --> 53:53.840
 but would take infinite computing power.

53:53.840 --> 53:56.360
 So then the artificial is a little off.

53:56.360 --> 53:59.800
 The general is not really achievable within physics

53:59.800 --> 54:01.280
 as we know it.

54:01.280 --> 54:03.520
 And I mean, physics as we know it may be limited,

54:03.520 --> 54:05.300
 but that's what we have to work with now.

54:05.300 --> 54:06.140
 Intelligence.

54:06.140 --> 54:07.360
 Infinitely general, you mean,

54:07.360 --> 54:10.440
 like information processing perspective, yeah.

54:10.440 --> 54:14.760
 Yeah, intelligence is not very well defined either, right?

54:14.760 --> 54:16.760
 I mean, what does it mean?

54:16.760 --> 54:19.560
 I mean, in AI now, it's fashionable to look at it

54:19.560 --> 54:23.320
 as maximizing an expected reward over the future.

54:23.320 --> 54:27.800
 But that sort of definition is pathological in various ways.

54:27.800 --> 54:31.320
 And my friend David Weinbaum, AKA Weaver,

54:31.320 --> 54:34.840
 he had a beautiful PhD thesis on open ended intelligence,

54:34.840 --> 54:36.880
 trying to conceive intelligence in a...

54:36.880 --> 54:38.240
 Without a reward.

54:38.240 --> 54:40.120
 Yeah, he's just looking at it differently.

54:40.120 --> 54:42.680
 He's looking at complex self organizing systems

54:42.680 --> 54:44.640
 and looking at an intelligent system

54:44.640 --> 54:47.600
 as being one that revises and grows

54:47.600 --> 54:51.740
 and improves itself in conjunction with its environment

54:51.740 --> 54:54.880
 without necessarily there being one objective function

54:54.880 --> 54:56.080
 it's trying to maximize.

54:56.080 --> 54:58.520
 Although over certain intervals of time,

54:58.520 --> 54:59.960
 it may act as if it's optimizing

54:59.960 --> 55:01.360
 a certain objective function.

55:01.360 --> 55:04.580
 Very much Solaris from Stanislav Lem's novels, right?

55:04.580 --> 55:07.880
 So yeah, the point is artificial, general and intelligence.

55:07.880 --> 55:08.720
 Don't work.

55:08.720 --> 55:09.540
 They're all bad.

55:09.540 --> 55:12.040
 On the other hand, everyone knows what AI is.

55:12.040 --> 55:15.880
 And AGI seems immediately comprehensible

55:15.880 --> 55:17.520
 to people with a technical background.

55:17.520 --> 55:19.360
 So I think that the term has served

55:19.360 --> 55:20.720
 as sociological function.

55:20.720 --> 55:24.720
 And now it's out there everywhere, which baffles me.

55:24.720 --> 55:25.800
 It's like KFC.

55:25.800 --> 55:27.080
 I mean, that's it.

55:27.080 --> 55:30.200
 We're stuck with AGI probably for a very long time

55:30.200 --> 55:33.640
 until AGI systems take over and rename themselves.

55:33.640 --> 55:34.480
 Yeah.

55:34.480 --> 55:36.160
 And then we'll be biological.

55:36.160 --> 55:37.560
 We're stuck with GPUs too,

55:37.560 --> 55:39.320
 which mostly have nothing to do with graphics.

55:39.320 --> 55:40.520
 Any more, right?

55:40.520 --> 55:43.260
 I wonder what the AGI system will call us humans.

55:43.260 --> 55:44.280
 That was maybe.

55:44.280 --> 55:45.120
 Grandpa.

55:45.120 --> 55:45.960
 Yeah.

55:45.960 --> 55:46.800
 Yeah.

55:46.800 --> 55:47.620
 GPs.

55:47.620 --> 55:48.460
 Yeah.

55:48.460 --> 55:50.320
 Grandpa processing unit, yeah.

55:50.320 --> 55:52.120
 Biological grandpa processing units.

55:52.120 --> 55:52.960
 Yeah.

55:54.280 --> 55:59.280
 Okay, so maybe also just a comment on AGI representing

56:00.580 --> 56:02.160
 before even the term existed,

56:02.160 --> 56:04.640
 representing a kind of community.

56:04.640 --> 56:06.240
 You've talked about this in the past,

56:06.240 --> 56:08.340
 sort of AI is coming in waves,

56:08.340 --> 56:10.440
 but there's always been this community of people

56:10.440 --> 56:15.160
 who dream about creating general human level

56:15.160 --> 56:16.840
 super intelligence systems.

56:19.000 --> 56:21.880
 Can you maybe give your sense of the history

56:21.880 --> 56:24.280
 of this community as it exists today,

56:24.280 --> 56:26.720
 as it existed before this deep learning revolution

56:26.720 --> 56:29.520
 all throughout the winters and the summers of AI?

56:29.520 --> 56:30.340
 Sure.

56:30.340 --> 56:33.500
 First, I would say as a side point,

56:33.500 --> 56:37.840
 the winters and summers of AI are greatly exaggerated

56:37.840 --> 56:40.960
 by Americans and in that,

56:40.960 --> 56:43.600
 if you look at the publication record

56:43.600 --> 56:46.400
 of the artificial intelligence community

56:46.400 --> 56:48.480
 since say the 1950s,

56:48.480 --> 56:51.360
 you would find a pretty steady growth

56:51.360 --> 56:53.980
 in advance of ideas and papers.

56:53.980 --> 56:57.720
 And what's thought of as an AI winter or summer

56:57.720 --> 57:00.480
 was sort of how much money is the US military

57:00.480 --> 57:04.640
 pumping into AI, which was meaningful.

57:04.640 --> 57:06.960
 On the other hand, there was AI going on in Germany,

57:06.960 --> 57:10.960
 UK and in Japan and in Russia, all over the place,

57:10.960 --> 57:15.960
 while US military got more and less enthused about AI.

57:16.300 --> 57:17.560
 So, I mean.

57:17.560 --> 57:20.200
 That happened to be, just for people who don't know,

57:20.200 --> 57:22.840
 the US military happened to be the main source

57:22.840 --> 57:24.500
 of funding for AI research.

57:24.500 --> 57:27.480
 So another way to phrase that is it's up and down

57:27.480 --> 57:31.080
 of funding for artificial intelligence research.

57:31.080 --> 57:34.600
 And I would say the correlation between funding

57:34.600 --> 57:38.120
 and intellectual advance was not 100%, right?

57:38.120 --> 57:42.120
 Because I mean, in Russia, as an example, or in Germany,

57:42.120 --> 57:44.840
 there was less dollar funding than in the US,

57:44.840 --> 57:48.160
 but many foundational ideas were laid out,

57:48.160 --> 57:50.880
 but it was more theory than implementation, right?

57:50.880 --> 57:54.600
 And US really excelled at sort of breaking through

57:54.600 --> 57:59.600
 from theoretical papers to working implementations,

58:00.200 --> 58:03.020
 which did go up and down somewhat

58:03.020 --> 58:04.320
 with US military funding,

58:04.320 --> 58:07.440
 but still, I mean, you can look in the 1980s,

58:07.440 --> 58:10.400
 Dietrich Derner in Germany had self driving cars

58:10.400 --> 58:11.440
 on the Autobahn, right?

58:11.440 --> 58:15.600
 And I mean, it was a little early

58:15.600 --> 58:16.920
 with regard to the car industry,

58:16.920 --> 58:20.200
 so it didn't catch on such as has happened now.

58:20.200 --> 58:22.960
 But I mean, that whole advancement

58:22.960 --> 58:25.900
 of self driving car technology in Germany

58:25.900 --> 58:29.720
 was pretty much independent of AI military summers

58:29.720 --> 58:31.040
 and winters in the US.

58:31.040 --> 58:34.480
 So there's been more going on in AI globally

58:34.480 --> 58:37.120
 than not only most people on the planet realize,

58:37.120 --> 58:40.080
 but then most new AI PhDs realize

58:40.080 --> 58:44.600
 because they've come up within a certain sub field of AI

58:44.600 --> 58:47.680
 and haven't had to look so much beyond that.

58:47.680 --> 58:52.680
 But I would say when I got my PhD in 1989 in mathematics,

58:54.300 --> 58:56.000
 I was interested in AI already.

58:56.000 --> 58:56.840
 In Philadelphia.

58:56.840 --> 59:00.920
 Yeah, I started at NYU, then I transferred to Philadelphia

59:00.920 --> 59:03.960
 to Temple University, good old North Philly.

59:03.960 --> 59:04.800
 North Philly.

59:04.800 --> 59:07.920
 Yeah, yeah, yeah, the pearl of the US.

59:09.280 --> 59:10.920
 You never stopped at a red light then

59:10.920 --> 59:12.760
 because you were afraid if you stopped at a red light,

59:12.760 --> 59:13.760
 someone will carjack you.

59:13.760 --> 59:15.960
 So you just drive through every red light.

59:15.960 --> 59:16.800
 Yeah.

59:18.200 --> 59:20.940
 Every day driving or bicycling to Temple from my house

59:20.940 --> 59:24.280
 was like a new adventure.

59:24.280 --> 59:27.520
 But yeah, the reason I didn't do a PhD in AI

59:27.520 --> 59:30.860
 was what people were doing in the academic AI field then,

59:30.860 --> 59:34.880
 was just astoundingly boring and seemed wrong headed to me.

59:34.880 --> 59:38.060
 It was really like rule based expert systems

59:38.060 --> 59:39.360
 and production systems.

59:39.360 --> 59:42.080
 And actually I loved mathematical logic.

59:42.080 --> 59:45.840
 I had nothing against logic as the cognitive engine for an AI,

59:45.840 --> 59:48.920
 but the idea that you could type in the knowledge

59:48.920 --> 59:52.720
 that AI would need to think seemed just completely stupid

59:52.720 --> 59:55.380
 and wrong headed to me.

59:55.380 --> 59:57.400
 I mean, you can use logic if you want,

59:57.400 --> 1:00:00.160
 but somehow the system has got to be...

1:00:00.160 --> 1:00:01.000
 Automated.

1:00:01.000 --> 1:00:01.840
 Learning, right?

1:00:01.840 --> 1:00:03.800
 It should be learning from experience.

1:00:03.800 --> 1:00:06.120
 And the AI field then was not interested

1:00:06.120 --> 1:00:08.320
 in learning from experience.

1:00:08.320 --> 1:00:11.020
 I mean, some researchers certainly were.

1:00:11.020 --> 1:00:13.960
 I mean, I remember in mid eighties,

1:00:13.960 --> 1:00:17.160
 I discovered a book by John Andreas,

1:00:17.160 --> 1:00:21.920
 which was, it was about a reinforcement learning system

1:00:21.920 --> 1:00:26.920
 called PURRDASHPUSS, which was an acronym

1:00:27.080 --> 1:00:28.640
 that I can't even remember what it was for,

1:00:28.640 --> 1:00:30.400
 but purpose anyway.

1:00:30.400 --> 1:00:32.000
 But he, I mean, that was a system

1:00:32.000 --> 1:00:34.360
 that was supposed to be an AGI

1:00:34.360 --> 1:00:38.120
 and basically by some sort of fancy

1:00:38.120 --> 1:00:41.000
 like Markov decision process learning,

1:00:41.000 --> 1:00:43.440
 it was supposed to learn everything

1:00:43.440 --> 1:00:44.880
 just from the bits coming into it

1:00:44.880 --> 1:00:46.720
 and learn to maximize its reward

1:00:46.720 --> 1:00:49.080
 and become intelligent, right?

1:00:49.080 --> 1:00:51.800
 So that was there in academia back then,

1:00:51.800 --> 1:00:55.240
 but it was like isolated, scattered, weird people.

1:00:55.240 --> 1:00:57.440
 But all these isolated, scattered, weird people

1:00:57.440 --> 1:01:01.280
 in that period, I mean, they laid the intellectual grounds

1:01:01.280 --> 1:01:02.120
 for what happened later.

1:01:02.120 --> 1:01:05.300
 So you look at John Andreas at University of Canterbury

1:01:05.300 --> 1:01:09.720
 with his PURRDASHPUSS reinforcement learning Markov system.

1:01:09.720 --> 1:01:14.080
 He was the PhD supervisor for John Cleary in New Zealand.

1:01:14.080 --> 1:01:17.080
 Now, John Cleary worked with me

1:01:17.080 --> 1:01:21.680
 when I was at Waikato University in 1993 in New Zealand.

1:01:21.680 --> 1:01:23.900
 And he worked with Ian Whitten there

1:01:23.900 --> 1:01:25.940
 and they launched WEKA,

1:01:25.940 --> 1:01:29.840
 which was the first open source machine learning toolkit,

1:01:29.840 --> 1:01:33.520
 which was launched in, I guess, 93 or 94

1:01:33.520 --> 1:01:35.160
 when I was at Waikato University.

1:01:35.160 --> 1:01:36.480
 Written in Java, unfortunately.

1:01:36.480 --> 1:01:39.620
 Written in Java, which was a cool language back then.

1:01:39.620 --> 1:01:41.720
 I guess it's still, well, it's not cool anymore,

1:01:41.720 --> 1:01:43.280
 but it's powerful.

1:01:43.280 --> 1:01:45.760
 I find, like most programmers now,

1:01:45.760 --> 1:01:48.820
 I find Java unnecessarily bloated,

1:01:48.820 --> 1:01:52.020
 but back then it was like Java or C++ basically.

1:01:52.020 --> 1:01:55.760
 And Java was easier for students.

1:01:55.760 --> 1:01:57.760
 Amusingly, a lot of the work on WEKA

1:01:57.760 --> 1:02:01.200
 when we were in New Zealand was funded by a US,

1:02:01.200 --> 1:02:03.880
 sorry, a New Zealand government grant

1:02:03.880 --> 1:02:05.440
 to use machine learning

1:02:05.440 --> 1:02:08.240
 to predict the menstrual cycles of cows.

1:02:08.240 --> 1:02:10.440
 So in the US, all the grant funding for AI

1:02:10.440 --> 1:02:13.600
 was about how to kill people or spy on people.

1:02:13.600 --> 1:02:16.400
 In New Zealand, it's all about cows or kiwi fruits, right?

1:02:16.400 --> 1:02:17.560
 Yeah.

1:02:17.560 --> 1:02:20.560
 So yeah, anyway, I mean, John Andreas

1:02:20.560 --> 1:02:24.320
 had his probability theory based reinforcement learning,

1:02:24.320 --> 1:02:25.780
 proto AGI.

1:02:25.780 --> 1:02:29.400
 John Cleary was trying to do much more ambitious,

1:02:29.400 --> 1:02:31.820
 probabilistic AGI systems.

1:02:31.820 --> 1:02:36.160
 Now, John Cleary helped do WEKA,

1:02:36.160 --> 1:02:39.360
 which is the first open source machine learning toolkit.

1:02:39.360 --> 1:02:41.520
 So the predecessor for TensorFlow and Torch

1:02:41.520 --> 1:02:43.040
 and all these things.

1:02:43.040 --> 1:02:46.800
 Also, Shane Legg was at Waikato

1:02:46.800 --> 1:02:50.240
 working with John Cleary and Ian Witten

1:02:50.240 --> 1:02:51.500
 and this whole group.

1:02:51.500 --> 1:02:55.800
 And then working with my own companies,

1:02:55.800 --> 1:02:59.840
 my company, WebMind, an AI company I had in the late 90s

1:02:59.840 --> 1:03:02.320
 with a team there at Waikato University,

1:03:02.320 --> 1:03:05.360
 which is how Shane got his head full of AGI,

1:03:05.360 --> 1:03:06.440
 which led him to go on

1:03:06.440 --> 1:03:08.660
 and with Demis Hassabis found DeepMind.

1:03:08.660 --> 1:03:11.060
 So what you can see through that lineage is,

1:03:11.060 --> 1:03:12.580
 you know, in the 80s and 70s,

1:03:12.580 --> 1:03:14.800
 John Andreas was trying to build probabilistic

1:03:14.800 --> 1:03:17.200
 reinforcement learning AGI systems.

1:03:17.200 --> 1:03:19.680
 The technology, the computers just weren't there to support

1:03:19.680 --> 1:03:23.920
 his ideas were very similar to what people are doing now.

1:03:23.920 --> 1:03:27.720
 But, you know, although he's long since passed away

1:03:27.720 --> 1:03:30.940
 and didn't become that famous outside of Canterbury,

1:03:30.940 --> 1:03:33.720
 I mean, the lineage of ideas passed on from him

1:03:33.720 --> 1:03:35.140
 to his students, to their students,

1:03:35.140 --> 1:03:37.920
 you can go trace directly from there to me

1:03:37.920 --> 1:03:39.480
 and to DeepMind, right?

1:03:39.480 --> 1:03:42.180
 So that there was a lot going on in AGI

1:03:42.180 --> 1:03:46.460
 that did ultimately lay the groundwork

1:03:46.460 --> 1:03:48.560
 for what we have today, but there wasn't a community, right?

1:03:48.560 --> 1:03:53.520
 And so when I started trying to pull together

1:03:53.520 --> 1:03:56.920
 an AGI community, it was in the, I guess,

1:03:56.920 --> 1:04:00.400
 the early aughts when I was living in Washington, D.C.

1:04:00.400 --> 1:04:03.440
 and making a living doing AI consulting

1:04:03.440 --> 1:04:07.080
 for various U.S. government agencies.

1:04:07.080 --> 1:04:12.080
 And I organized the first AGI workshop in 2006.

1:04:13.200 --> 1:04:15.780
 And I mean, it wasn't like it was literally

1:04:15.780 --> 1:04:17.000
 in my basement or something.

1:04:17.000 --> 1:04:19.320
 I mean, it was in the conference room at the Marriott

1:04:19.320 --> 1:04:23.200
 in Bethesda, it's not that edgy or underground,

1:04:23.200 --> 1:04:25.000
 unfortunately, but still.

1:04:25.000 --> 1:04:25.840
 How many people attended?

1:04:25.840 --> 1:04:27.600
 About 60 or something.

1:04:27.600 --> 1:04:28.480
 That's not bad.

1:04:28.480 --> 1:04:30.780
 I mean, D.C. has a lot of AI going on,

1:04:30.780 --> 1:04:34.200
 probably until the last five or 10 years,

1:04:34.200 --> 1:04:37.800
 much more than Silicon Valley, although it's just quiet

1:04:37.800 --> 1:04:41.280
 because of the nature of what happens in D.C.

1:04:41.280 --> 1:04:43.600
 Their business isn't driven by PR.

1:04:43.600 --> 1:04:46.140
 Mostly when something starts to work really well,

1:04:46.140 --> 1:04:49.640
 it's taken black and becomes even more quiet, right?

1:04:49.640 --> 1:04:52.880
 But yeah, the thing is that really had the feeling

1:04:52.880 --> 1:04:57.880
 of a group of starry eyed mavericks huddled in a basement,

1:04:58.400 --> 1:05:02.520
 like plotting how to overthrow the narrow AI establishment.

1:05:02.520 --> 1:05:05.760
 And for the first time, in some cases,

1:05:05.760 --> 1:05:08.680
 coming together with others who shared their passion

1:05:08.680 --> 1:05:13.200
 for AGI and the technical seriousness about working on it.

1:05:13.200 --> 1:05:18.200
 And that's very, very different than what we have today.

1:05:19.160 --> 1:05:22.320
 I mean, now it's a little bit different.

1:05:22.320 --> 1:05:24.640
 We have AGI conference every year

1:05:24.640 --> 1:05:27.800
 and there's several hundred people rather than 50.

1:05:29.300 --> 1:05:32.760
 Now it's more like this is the main gathering

1:05:32.760 --> 1:05:35.020
 of people who want to achieve AGI

1:05:35.020 --> 1:05:39.220
 and think that large scale nonlinear regression

1:05:39.220 --> 1:05:42.480
 is not the golden path to AGI.

1:05:42.480 --> 1:05:43.320
 So I mean it's...

1:05:43.320 --> 1:05:44.160
 AKA neural networks.

1:05:44.160 --> 1:05:44.980
 Yeah, yeah, yeah.

1:05:44.980 --> 1:05:49.980
 Well, certain architectures for learning using neural networks.

1:05:51.840 --> 1:05:54.440
 So yeah, the AGI conferences are sort of now

1:05:54.440 --> 1:05:57.960
 the main concentration of people not obsessed

1:05:57.960 --> 1:06:00.880
 with deep neural nets and deep reinforcement learning,

1:06:00.880 --> 1:06:05.880
 but still interested in AGI, not the only ones.

1:06:06.460 --> 1:06:10.200
 I mean, there's other little conferences and groupings

1:06:10.200 --> 1:06:13.280
 interested in human level AI

1:06:13.280 --> 1:06:16.040
 and cognitive architectures and so forth.

1:06:16.040 --> 1:06:17.880
 But yeah, it's been a big shift.

1:06:17.880 --> 1:06:21.960
 Like back then, you couldn't really...

1:06:21.960 --> 1:06:23.540
 It'll be very, very edgy then

1:06:23.540 --> 1:06:26.220
 to give a university department seminar

1:06:26.220 --> 1:06:28.440
 that mentioned AGI or human level AI.

1:06:28.440 --> 1:06:30.640
 It was more like you had to talk about

1:06:30.640 --> 1:06:34.360
 something more short term and immediately practical

1:06:34.360 --> 1:06:36.600
 than in the bar after the seminar,

1:06:36.600 --> 1:06:39.540
 you could bullshit about AGI in the same breath

1:06:39.540 --> 1:06:44.200
 as time travel or the simulation hypothesis or something.

1:06:44.200 --> 1:06:48.360
 Whereas now, AGI is not only in the academic seminar room,

1:06:48.360 --> 1:06:51.960
 like you have Vladimir Putin knows what AGI is.

1:06:51.960 --> 1:06:55.480
 And he's like, Russia needs to become the leader in AGI.

1:06:55.480 --> 1:07:00.480
 So national leaders and CEOs of large corporations.

1:07:01.080 --> 1:07:04.240
 I mean, the CTO of Intel, Justin Ratner,

1:07:04.240 --> 1:07:06.840
 this was years ago, Singularity Summit Conference,

1:07:06.840 --> 1:07:07.780
 2008 or something.

1:07:07.780 --> 1:07:10.080
 He's like, we believe Ray Kurzweil,

1:07:10.080 --> 1:07:12.000
 the singularity will happen in 2045

1:07:12.000 --> 1:07:13.640
 and it will have Intel inside.

1:07:13.640 --> 1:07:18.640
 So, I mean, it's gone from being something

1:07:18.840 --> 1:07:21.700
 which is the pursuit of like crazed mavericks,

1:07:21.700 --> 1:07:24.540
 crackpots and science fiction fanatics

1:07:24.540 --> 1:07:29.540
 to being a marketing term for large corporations

1:07:30.120 --> 1:07:31.480
 and the national leaders,

1:07:31.480 --> 1:07:35.160
 which is a astounding transition.

1:07:35.160 --> 1:07:40.160
 But yeah, in the course of this transition,

1:07:40.160 --> 1:07:42.260
 I think a bunch of sub communities have formed

1:07:42.260 --> 1:07:44.920
 and the community around the AGI conference series

1:07:45.800 --> 1:07:47.640
 is certainly one of them.

1:07:47.640 --> 1:07:51.940
 It hasn't grown as big as I might've liked it to.

1:07:51.940 --> 1:07:56.320
 On the other hand, sometimes a modest size community

1:07:56.320 --> 1:07:59.080
 can be better for making intellectual progress also.

1:07:59.080 --> 1:08:02.160
 Like you go to a society for neuroscience conference,

1:08:02.160 --> 1:08:05.400
 you have 35 or 40,000 neuroscientists.

1:08:05.400 --> 1:08:07.480
 On the one hand, it's amazing.

1:08:07.480 --> 1:08:10.920
 On the other hand, you're not gonna talk to the leaders

1:08:10.920 --> 1:08:14.160
 of the field there if you're an outsider.

1:08:14.160 --> 1:08:17.920
 Yeah, in the same sense, the AAAI,

1:08:17.920 --> 1:08:19.280
 the artificial intelligence,

1:08:20.160 --> 1:08:23.640
 the main kind of generic artificial intelligence

1:08:23.640 --> 1:08:26.920
 conference is too big.

1:08:26.920 --> 1:08:28.280
 It's too amorphous.

1:08:28.280 --> 1:08:30.240
 Like it doesn't make sense.

1:08:30.240 --> 1:08:35.240
 Well, yeah, and NIPS has become a company advertising outlet

1:08:35.240 --> 1:08:37.000
 in the whole of it.

1:08:37.000 --> 1:08:40.240
 So, I mean, to comment on the role of AGI

1:08:40.240 --> 1:08:42.680
 in the research community, I'd still,

1:08:42.680 --> 1:08:45.200
 if you look at NeurIPS, if you look at CVPR,

1:08:45.200 --> 1:08:47.360
 if you look at these iClear,

1:08:49.240 --> 1:08:51.860
 AGI is still seen as the outcast.

1:08:51.860 --> 1:08:55.020
 I would say in these main machine learning,

1:08:55.020 --> 1:08:59.040
 in these main artificial intelligence conferences

1:08:59.040 --> 1:09:00.880
 amongst the researchers,

1:09:00.880 --> 1:09:03.880
 I don't know if it's an accepted term yet.

1:09:03.880 --> 1:09:08.280
 What I've seen bravely, you mentioned Shane Legg's

1:09:08.280 --> 1:09:13.000
 DeepMind and then OpenAI are the two places that are,

1:09:13.000 --> 1:09:15.580
 I would say unapologetically so far,

1:09:15.580 --> 1:09:17.440
 I think it's actually changing unfortunately,

1:09:17.440 --> 1:09:19.640
 but so far they've been pushing the idea

1:09:19.640 --> 1:09:22.760
 that the goal is to create an AGI.

1:09:22.760 --> 1:09:24.360
 Well, they have billions of dollars behind them.

1:09:24.360 --> 1:09:27.220
 So, I mean, they're in the public mind

1:09:27.220 --> 1:09:30.120
 that certainly carries some oomph, right?

1:09:30.120 --> 1:09:30.960
 I mean, I mean.

1:09:30.960 --> 1:09:33.160
 But they also have really strong researchers, right?

1:09:33.160 --> 1:09:34.260
 They do, they're great teams.

1:09:34.260 --> 1:09:36.660
 I mean, DeepMind in particular, yeah.

1:09:36.660 --> 1:09:39.280
 And they have, I mean, DeepMind has Marcus Hutter

1:09:39.280 --> 1:09:40.120
 walking around.

1:09:40.120 --> 1:09:43.480
 I mean, there's all these folks who basically

1:09:43.480 --> 1:09:46.400
 their full time position involves dreaming

1:09:46.400 --> 1:09:47.800
 about creating AGI.

1:09:47.800 --> 1:09:51.320
 I mean, Google Brain has a lot of amazing

1:09:51.320 --> 1:09:53.240
 AGI oriented people also.

1:09:53.240 --> 1:09:58.240
 And I mean, so I'd say from a public marketing view,

1:09:59.840 --> 1:10:03.820
 DeepMind and OpenAI are the two large well funded

1:10:03.820 --> 1:10:08.360
 organizations that have put the term and concept AGI

1:10:08.360 --> 1:10:12.720
 out there sort of as part of their public image.

1:10:12.720 --> 1:10:15.200
 But I mean, they're certainly not,

1:10:15.200 --> 1:10:17.160
 there are other groups that are doing research

1:10:17.160 --> 1:10:20.660
 that seems just as AGI is to me.

1:10:20.660 --> 1:10:23.320
 I mean, including a bunch of groups in Google's

1:10:23.320 --> 1:10:26.000
 main Mountain View office.

1:10:26.000 --> 1:10:27.960
 So yeah, it's true.

1:10:27.960 --> 1:10:32.960
 AGI is somewhat away from the mainstream now.

1:10:33.880 --> 1:10:38.040
 But if you compare it to where it was 15 years ago,

1:10:38.040 --> 1:10:41.960
 there's been an amazing mainstreaming.

1:10:41.960 --> 1:10:45.520
 You could say the same thing about super longevity research,

1:10:45.520 --> 1:10:49.120
 which is one of my application areas that I'm excited about.

1:10:49.120 --> 1:10:52.880
 I mean, I've been talking about this since the 90s,

1:10:52.880 --> 1:10:54.560
 but working on this since 2001.

1:10:54.560 --> 1:10:57.280
 And back then, really to say,

1:10:57.280 --> 1:10:59.440
 you're trying to create therapies to allow people

1:10:59.440 --> 1:11:02.360
 to live hundreds of thousands of years,

1:11:02.360 --> 1:11:05.520
 you were way, way, way, way out of the industry,

1:11:05.520 --> 1:11:06.720
 academic mainstream.

1:11:06.720 --> 1:11:11.540
 But now, Google had Project Calico,

1:11:11.540 --> 1:11:14.080
 Craig Venter had Human Longevity Incorporated.

1:11:14.080 --> 1:11:17.160
 And then once the suits come marching in, right?

1:11:17.160 --> 1:11:20.200
 I mean, once there's big money in it,

1:11:20.200 --> 1:11:22.720
 then people are forced to take it seriously

1:11:22.720 --> 1:11:24.880
 because that's the way modern society works.

1:11:24.880 --> 1:11:28.400
 So it's still not as mainstream as cancer research,

1:11:28.400 --> 1:11:31.060
 just as AGI is not as mainstream

1:11:31.060 --> 1:11:32.960
 as automated driving or something.

1:11:32.960 --> 1:11:36.020
 But the degree of mainstreaming that's happened

1:11:36.020 --> 1:11:40.120
 in the last 10 to 15 years is astounding

1:11:40.120 --> 1:11:42.080
 to those of us who've been at it for a while.

1:11:42.080 --> 1:11:45.360
 Yeah, but there's a marketing aspect to the term,

1:11:45.360 --> 1:11:48.800
 but in terms of actual full force research

1:11:48.800 --> 1:11:51.280
 that's going on under the header of AGI,

1:11:51.280 --> 1:11:54.280
 it's currently, I would say dominated,

1:11:54.280 --> 1:11:55.960
 maybe you can disagree,

1:11:55.960 --> 1:11:57.740
 dominated by neural networks research,

1:11:57.740 --> 1:12:01.140
 that the nonlinear regression, as you mentioned.

1:12:02.740 --> 1:12:06.520
 Like what's your sense with OpenCog, with your work,

1:12:06.520 --> 1:12:10.920
 but in general, I was logic based systems

1:12:10.920 --> 1:12:12.000
 and expert systems.

1:12:12.000 --> 1:12:17.000
 For me, always seemed to capture a deep element

1:12:18.440 --> 1:12:21.400
 of intelligence that needs to be there.

1:12:21.400 --> 1:12:23.020
 Like you said, it needs to learn,

1:12:23.020 --> 1:12:24.900
 it needs to be automated somehow,

1:12:24.900 --> 1:12:29.900
 but that seems to be missing from a lot of research currently.

1:12:31.360 --> 1:12:32.780
 So what's your sense?

1:12:34.360 --> 1:12:36.280
 I guess one way to ask this question,

1:12:36.280 --> 1:12:39.200
 what's your sense of what kind of things

1:12:39.200 --> 1:12:42.140
 will an AGI system need to have?

1:12:43.480 --> 1:12:45.960
 Yeah, that's a very interesting topic

1:12:45.960 --> 1:12:47.900
 that I've thought about for a long time.

1:12:47.900 --> 1:12:52.900
 And I think there are many, many different approaches

1:12:53.840 --> 1:12:56.920
 that can work for getting to human level AI.

1:12:56.920 --> 1:13:01.920
 So I don't think there's like one golden algorithm,

1:13:02.600 --> 1:13:05.840
 or one golden design that can work.

1:13:05.840 --> 1:13:10.720
 And I mean, flying machines is the much worn

1:13:10.720 --> 1:13:11.680
 analogy here, right?

1:13:11.680 --> 1:13:13.760
 Like, I mean, you have airplanes, you have helicopters,

1:13:13.760 --> 1:13:17.160
 you have balloons, you have stealth bombers

1:13:17.160 --> 1:13:18.760
 that don't look like regular airplanes.

1:13:18.760 --> 1:13:21.040
 You've got all blimps.

1:13:21.040 --> 1:13:21.880
 Birds too.

1:13:21.880 --> 1:13:24.280
 Birds, yeah, and bugs, right?

1:13:24.280 --> 1:13:25.120
 Yeah.

1:13:25.120 --> 1:13:29.920
 And there are certainly many kinds of flying machines that.

1:13:29.920 --> 1:13:32.360
 And there's a catapult that you can just launch.

1:13:32.360 --> 1:13:36.160
 And there's bicycle powered like flying machines, right?

1:13:36.160 --> 1:13:37.000
 Nice, yeah.

1:13:37.000 --> 1:13:40.920
 Yeah, so now these are all analyzable

1:13:40.920 --> 1:13:43.800
 by a basic theory of aerodynamics, right?

1:13:43.800 --> 1:13:48.800
 Now, so one issue with AGI is we don't yet have the analog

1:13:48.920 --> 1:13:50.800
 of the theory of aerodynamics.

1:13:50.800 --> 1:13:54.640
 And that's what Marcus Hutter was trying to make

1:13:54.640 --> 1:13:58.820
 with the AXI and his general theory of general intelligence.

1:13:58.820 --> 1:14:03.360
 But that theory in its most clearly articulated parts

1:14:03.360 --> 1:14:07.120
 really only works for either infinitely powerful machines

1:14:07.120 --> 1:14:11.840
 or almost, or insanely impractically powerful machines.

1:14:11.840 --> 1:14:14.880
 So I mean, if you were gonna take a theory based approach

1:14:14.880 --> 1:14:19.880
 to AGI, what you would do is say, well, let's take

1:14:20.040 --> 1:14:25.040
 what's called say AXE TL, which is Hutter's AXE machine

1:14:25.040 --> 1:14:29.000
 that can work on merely insanely much processing power

1:14:29.000 --> 1:14:30.200
 rather than infinitely much.

1:14:30.200 --> 1:14:32.240
 What does TL stand for?

1:14:32.240 --> 1:14:33.560
 Time and length.

1:14:33.560 --> 1:14:34.400
 Okay.

1:14:34.400 --> 1:14:35.600
 So you're basically how it.

1:14:35.600 --> 1:14:36.480
 Like constrained somehow.

1:14:36.480 --> 1:14:37.320
 Yeah, yeah, yeah.

1:14:37.320 --> 1:14:42.320
 So how AXE works basically is each action

1:14:42.420 --> 1:14:45.040
 that it wants to take, before taking that action,

1:14:45.040 --> 1:14:47.080
 it looks at all its history.

1:14:47.080 --> 1:14:49.880
 And then it looks at all possible programs

1:14:49.880 --> 1:14:51.760
 that it could use to make a decision.

1:14:51.760 --> 1:14:54.320
 And it decides like which decision program

1:14:54.320 --> 1:14:56.120
 would have let it make the best decisions

1:14:56.120 --> 1:14:58.400
 according to its reward function over its history.

1:14:58.400 --> 1:15:00.000
 And it uses that decision program

1:15:00.000 --> 1:15:02.080
 to make the next decision, right?

1:15:02.080 --> 1:15:04.760
 It's not afraid of infinite resources.

1:15:04.760 --> 1:15:06.360
 It's searching through the space

1:15:06.360 --> 1:15:08.440
 of all possible computer programs

1:15:08.440 --> 1:15:10.720
 in between each action and each next action.

1:15:10.720 --> 1:15:15.320
 Now, AXE TL searches through all possible computer programs

1:15:15.320 --> 1:15:18.160
 that have runtime less than T and length less than L.

1:15:18.160 --> 1:15:22.680
 So it's, which is still an impractically humongous space,

1:15:22.680 --> 1:15:23.520
 right?

1:15:23.520 --> 1:15:27.960
 So what you would like to do to make an AGI

1:15:27.960 --> 1:15:29.840
 and what will probably be done 50 years from now

1:15:29.840 --> 1:15:34.840
 to make an AGI is say, okay, well, we have some constraints.

1:15:34.840 --> 1:15:37.480
 We have these processing power constraints

1:15:37.480 --> 1:15:42.480
 and we have the space and time constraints on the program.

1:15:42.700 --> 1:15:45.360
 We have energy utilization constraints

1:15:45.360 --> 1:15:48.160
 and we have this particular class environments,

1:15:48.160 --> 1:15:50.320
 class of environments that we care about,

1:15:50.320 --> 1:15:54.400
 which may be say, you know, manipulating physical objects

1:15:54.400 --> 1:15:55.400
 on the surface of the earth,

1:15:55.400 --> 1:15:57.360
 communicating in human language.

1:15:57.360 --> 1:16:02.240
 I mean, whatever our particular, not annihilating humanity,

1:16:02.240 --> 1:16:05.440
 whatever our particular requirements happen to be.

1:16:05.440 --> 1:16:07.280
 If you formalize those requirements

1:16:07.280 --> 1:16:10.300
 in some formal specification language,

1:16:10.300 --> 1:16:12.320
 you should then be able to run

1:16:13.320 --> 1:16:17.040
 automated program specializer on AXE TL,

1:16:17.040 --> 1:16:21.400
 specialize it to the computing resource constraints

1:16:21.400 --> 1:16:23.600
 and the particular environment and goal.

1:16:23.600 --> 1:16:27.600
 And then it will spit out like the specialized version

1:16:27.600 --> 1:16:30.620
 of AXE TL to your resource restrictions

1:16:30.620 --> 1:16:32.700
 and your environment, which will be your AGI, right?

1:16:32.700 --> 1:16:36.160
 And that I think is how our super AGI

1:16:36.160 --> 1:16:38.560
 will create new AGI systems, right?

1:16:38.560 --> 1:16:40.600
 But that's a very rush.

1:16:40.600 --> 1:16:41.600
 It seems really inefficient.

1:16:41.600 --> 1:16:43.160
 It's a very Russian approach by the way,

1:16:43.160 --> 1:16:45.240
 like the whole field of program specialization

1:16:45.240 --> 1:16:47.280
 came out of Russia.

1:16:47.280 --> 1:16:48.120
 Can you backtrack?

1:16:48.120 --> 1:16:49.680
 So what is program specialization?

1:16:49.680 --> 1:16:51.120
 So it's basically...

1:16:51.120 --> 1:16:53.640
 Well, take sorting, for example.

1:16:53.640 --> 1:16:56.640
 You can have a generic program for sorting lists,

1:16:56.640 --> 1:16:58.280
 but what if all your lists you care about

1:16:58.280 --> 1:16:59.920
 are length 10,000 or less?

1:16:59.920 --> 1:17:00.760
 Got it.

1:17:00.760 --> 1:17:02.560
 You can run an automated program specializer

1:17:02.560 --> 1:17:04.080
 on your sorting algorithm,

1:17:04.080 --> 1:17:05.400
 and it will come up with the algorithm

1:17:05.400 --> 1:17:08.400
 that's optimal for sorting lists of length 1,000 or less,

1:17:08.400 --> 1:17:09.800
 or 10,000 or less, right?

1:17:09.800 --> 1:17:12.200
 That's kind of like, isn't that the kind of the process

1:17:12.200 --> 1:17:17.200
 of evolution as a program specializer to the environment?

1:17:17.440 --> 1:17:20.000
 So you're kind of evolving human beings,

1:17:20.000 --> 1:17:21.840
 or you're living creatures.

1:17:21.840 --> 1:17:24.320
 Your Russian heritage is showing there.

1:17:24.320 --> 1:17:28.480
 So with Alexander Vityaev and Peter Anokhin and so on,

1:17:28.480 --> 1:17:31.800
 I mean, there's a long history

1:17:31.800 --> 1:17:36.760
 of thinking about evolution that way also, right?

1:17:36.760 --> 1:17:40.120
 So, well, my point is that what we're thinking of

1:17:40.120 --> 1:17:44.160
 as a human level general intelligence,

1:17:44.160 --> 1:17:46.680
 if you start from narrow AIs,

1:17:46.680 --> 1:17:50.320
 like are being used in the commercial AI field now,

1:17:50.320 --> 1:17:51.440
 then you're thinking,

1:17:51.440 --> 1:17:53.400
 okay, how do we make it more and more general?

1:17:53.400 --> 1:17:54.400
 On the other hand,

1:17:54.400 --> 1:17:58.080
 if you start from AICSI or Schmidhuber's Gödel machine,

1:17:58.080 --> 1:18:01.120
 or these infinitely powerful,

1:18:01.120 --> 1:18:04.000
 but practically infeasible AIs,

1:18:04.000 --> 1:18:06.440
 then getting to a human level AGI

1:18:06.440 --> 1:18:08.240
 is a matter of specialization.

1:18:08.240 --> 1:18:10.200
 It's like, how do you take these

1:18:10.200 --> 1:18:12.880
 maximally general learning processes

1:18:12.880 --> 1:18:15.760
 and how do you specialize them

1:18:15.760 --> 1:18:17.600
 so that they can operate

1:18:17.600 --> 1:18:20.520
 within the resource constraints that you have,

1:18:20.520 --> 1:18:24.360
 but will achieve the particular things that you care about?

1:18:24.360 --> 1:18:28.200
 Because we humans are not maximally general intelligence.

1:18:28.200 --> 1:18:31.400
 If I ask you to run a maze in 750 dimensions,

1:18:31.400 --> 1:18:33.040
 you'd probably be very slow.

1:18:33.040 --> 1:18:34.600
 Whereas at two dimensions,

1:18:34.600 --> 1:18:37.080
 you're probably way better, right?

1:18:37.080 --> 1:18:40.800
 So, I mean, we're special because our hippocampus

1:18:40.800 --> 1:18:43.080
 has a two dimensional map in it, right?

1:18:43.080 --> 1:18:46.000
 And it does not have a 750 dimensional map in it.

1:18:46.000 --> 1:18:51.000
 So, I mean, we're a peculiar mix

1:18:51.440 --> 1:18:56.000
 of generality and specialization, right?

1:18:56.000 --> 1:18:58.240
 We'll probably start quite general at birth.

1:18:59.200 --> 1:19:00.760
 Not obviously still narrow,

1:19:00.760 --> 1:19:03.200
 but like more general than we are

1:19:03.200 --> 1:19:07.520
 at age 20 and 30 and 40 and 50 and 60.

1:19:07.520 --> 1:19:10.240
 I don't think that, I think it's more complex than that

1:19:10.240 --> 1:19:13.800
 because I mean, in some sense,

1:19:13.800 --> 1:19:17.520
 a young child is less biased

1:19:17.520 --> 1:19:20.000
 and the brain has yet to sort of crystallize

1:19:20.000 --> 1:19:22.360
 into appropriate structures

1:19:22.360 --> 1:19:25.360
 for processing aspects of the physical and social world.

1:19:25.360 --> 1:19:26.560
 On the other hand,

1:19:26.560 --> 1:19:30.120
 the young child is very tied to their sensorium.

1:19:30.120 --> 1:19:33.880
 Whereas we can deal with abstract mathematics,

1:19:33.880 --> 1:19:37.600
 like 750 dimensions and the young child cannot

1:19:37.600 --> 1:19:40.920
 because they haven't grown what Piaget

1:19:40.920 --> 1:19:44.000
 called the formal capabilities.

1:19:44.000 --> 1:19:46.240
 They haven't learned to abstract yet, right?

1:19:46.240 --> 1:19:48.120
 And the ability to abstract

1:19:48.120 --> 1:19:49.720
 gives you a different kind of generality

1:19:49.720 --> 1:19:51.680
 than what the baby has.

1:19:51.680 --> 1:19:55.400
 So, there's both more specialization

1:19:55.400 --> 1:19:57.240
 and more generalization that comes

1:19:57.240 --> 1:19:59.760
 with the development process actually.

1:19:59.760 --> 1:20:02.320
 I mean, I guess just the trajectories

1:20:02.320 --> 1:20:06.320
 of the specialization are most controllable

1:20:06.320 --> 1:20:09.720
 at the young age, I guess is one way to put it.

1:20:09.720 --> 1:20:10.720
 Do you have kids?

1:20:10.720 --> 1:20:11.680
 No.

1:20:11.680 --> 1:20:13.600
 They're not as controllable as you think.

1:20:13.600 --> 1:20:15.880
 So, you think it's interesting.

1:20:15.880 --> 1:20:19.040
 I think, honestly, I think a human adult

1:20:19.040 --> 1:20:23.240
 is much more generally intelligent than a human baby.

1:20:23.240 --> 1:20:25.800
 Babies are very stupid, you know what I mean?

1:20:25.800 --> 1:20:29.480
 I mean, they're cute, which is why we put up

1:20:29.480 --> 1:20:33.080
 with their repetitiveness and stupidity.

1:20:33.080 --> 1:20:35.040
 And they have what the Zen guys would call

1:20:35.040 --> 1:20:38.200
 a beginner's mind, which is a beautiful thing,

1:20:38.200 --> 1:20:40.760
 but that doesn't necessarily correlate

1:20:40.760 --> 1:20:43.320
 with a high level of intelligence.

1:20:43.320 --> 1:20:46.120
 On the plot of cuteness and stupidity,

1:20:46.120 --> 1:20:48.720
 there's a process that allows us to put up

1:20:48.720 --> 1:20:50.880
 with their stupidity as they become more intelligent.

1:20:50.880 --> 1:20:52.400
 So, by the time you're an ugly old man like me,

1:20:52.400 --> 1:20:54.720
 you gotta get really, really smart to compensate.

1:20:54.720 --> 1:20:56.160
 To compensate, okay, cool.

1:20:56.160 --> 1:20:59.160
 But yeah, going back to your original question,

1:20:59.160 --> 1:21:04.160
 so the way I look at human level AGI

1:21:05.280 --> 1:21:08.640
 is how do you specialize, you know,

1:21:08.640 --> 1:21:12.160
 unrealistically inefficient, superhuman,

1:21:12.160 --> 1:21:14.600
 brute force learning processes

1:21:14.600 --> 1:21:18.320
 to the specific goals that humans need to achieve

1:21:18.320 --> 1:21:21.920
 and the specific resources that we have.

1:21:21.920 --> 1:21:24.600
 And both of these, the goals and the resources

1:21:24.600 --> 1:21:27.120
 and the environments, I mean, all this is important.

1:21:27.120 --> 1:21:31.320
 And on the resources side, it's important

1:21:31.320 --> 1:21:34.680
 that the hardware resources we're bringing to bear

1:21:35.600 --> 1:21:38.240
 are very different than the human brain.

1:21:38.240 --> 1:21:42.680
 So the way I would want to implement AGI

1:21:42.680 --> 1:21:45.960
 on a bunch of neurons in a vat

1:21:45.960 --> 1:21:48.880
 that I could rewire arbitrarily is quite different

1:21:48.880 --> 1:21:51.760
 than the way I would want to create AGI

1:21:51.760 --> 1:21:55.760
 on say a modern server farm of CPUs and GPUs,

1:21:55.760 --> 1:21:57.440
 which in turn may be quite different

1:21:57.440 --> 1:22:00.200
 than the way I would want to implement AGI

1:22:00.200 --> 1:22:03.760
 on whatever quantum computer we'll have in 10 years,

1:22:03.760 --> 1:22:06.680
 supposing someone makes a robust quantum turing machine

1:22:06.680 --> 1:22:08.240
 or something, right?

1:22:08.240 --> 1:22:12.640
 So I think there's been coevolution

1:22:12.640 --> 1:22:16.960
 of the patterns of organization in the human brain

1:22:16.960 --> 1:22:19.960
 and the physiological particulars

1:22:19.960 --> 1:22:23.240
 of the human brain over time.

1:22:23.240 --> 1:22:25.240
 And when you look at neural networks,

1:22:25.240 --> 1:22:28.040
 that is one powerful class of learning algorithms,

1:22:28.040 --> 1:22:30.040
 but it's also a class of learning algorithms

1:22:30.040 --> 1:22:33.400
 that evolve to exploit the particulars of the human brain

1:22:33.400 --> 1:22:36.320
 as a computational substrate.

1:22:36.320 --> 1:22:38.880
 If you're looking at the computational substrate

1:22:38.880 --> 1:22:41.040
 of a modern server farm,

1:22:41.040 --> 1:22:43.200
 you won't necessarily want the same algorithms

1:22:43.200 --> 1:22:45.760
 that you want on the human brain.

1:22:45.760 --> 1:22:48.920
 And from the right level of abstraction,

1:22:48.920 --> 1:22:51.760
 you could look at maybe the best algorithms on the brain

1:22:51.760 --> 1:22:54.480
 and the best algorithms on a modern computer network

1:22:54.480 --> 1:22:56.480
 as implementing the same abstract learning

1:22:56.480 --> 1:22:59.080
 and representation processes,

1:22:59.080 --> 1:23:01.680
 but finding that level of abstraction

1:23:01.680 --> 1:23:04.960
 is its own AGI research project then, right?

1:23:04.960 --> 1:23:07.800
 So that's about the hardware side

1:23:07.800 --> 1:23:10.880
 and the software side, which follows from that.

1:23:10.880 --> 1:23:14.200
 Then regarding what are the requirements,

1:23:14.200 --> 1:23:16.440
 I wrote the paper years ago

1:23:16.440 --> 1:23:20.360
 on what I called the embodied communication prior,

1:23:20.360 --> 1:23:22.960
 which was quite similar in intent

1:23:22.960 --> 1:23:26.760
 to Yoshua Bengio's recent paper on the consciousness prior,

1:23:26.760 --> 1:23:30.440
 except I didn't wanna wrap up consciousness in it

1:23:30.440 --> 1:23:34.240
 because to me, the qualia problem and subjective experience

1:23:34.240 --> 1:23:35.880
 is a very interesting issue also,

1:23:35.880 --> 1:23:37.880
 which we can chat about,

1:23:37.880 --> 1:23:42.880
 but I would rather keep that philosophical debate distinct

1:23:43.200 --> 1:23:45.240
 from the debate of what kind of biases

1:23:45.240 --> 1:23:47.040
 do you wanna put in a general intelligence

1:23:47.040 --> 1:23:49.800
 to give it human like general intelligence.

1:23:49.800 --> 1:23:53.320
 And I'm not sure Yoshua Bengio is really addressing

1:23:53.320 --> 1:23:55.080
 that kind of consciousness.

1:23:55.080 --> 1:23:56.560
 He's just using the term.

1:23:56.560 --> 1:23:58.600
 I love Yoshua to pieces.

1:23:58.600 --> 1:24:02.960
 Like he's by far my favorite of the lines of deep learning.

1:24:02.960 --> 1:24:03.800
 Yeah.

1:24:03.800 --> 1:24:05.800
 He's such a good hearted guy.

1:24:05.800 --> 1:24:07.000
 He's a good human being.

1:24:07.000 --> 1:24:07.840
 Yeah, for sure.

1:24:07.840 --> 1:24:11.200
 I am not sure he has plumbed to the depths

1:24:11.200 --> 1:24:13.520
 of the philosophy of consciousness.

1:24:13.520 --> 1:24:15.040
 No, he's using it as a sexy term.

1:24:15.040 --> 1:24:15.880
 Yeah, yeah, yeah.

1:24:15.880 --> 1:24:20.880
 So what I called it was the embodied communication prior.

1:24:21.160 --> 1:24:22.520
 Can you maybe explain it a little bit?

1:24:22.520 --> 1:24:23.360
 Yeah, yeah.

1:24:23.360 --> 1:24:26.640
 What I meant was, what are we humans evolved for?

1:24:26.640 --> 1:24:29.720
 You can say being human, but that's very abstract, right?

1:24:29.720 --> 1:24:32.960
 I mean, our minds control individual bodies,

1:24:32.960 --> 1:24:36.920
 which are autonomous agents moving around in a world

1:24:36.920 --> 1:24:41.280
 that's composed largely of solid objects, right?

1:24:41.280 --> 1:24:46.240
 And we've also evolved to communicate via language

1:24:46.240 --> 1:24:49.960
 with other solid object agents that are going around

1:24:49.960 --> 1:24:52.200
 doing things collectively with us

1:24:52.200 --> 1:24:54.400
 in a world of solid objects.

1:24:54.400 --> 1:24:56.920
 And these things are very obvious,

1:24:56.920 --> 1:24:58.400
 but if you compare them to the scope

1:24:58.400 --> 1:25:01.400
 of all possible intelligences

1:25:01.400 --> 1:25:03.120
 or even all possible intelligences

1:25:03.120 --> 1:25:05.400
 that are physically realizable,

1:25:05.400 --> 1:25:07.400
 that actually constrains things a lot.

1:25:07.400 --> 1:25:12.400
 So if you start to look at how would you realize

1:25:13.000 --> 1:25:15.880
 some specialized or constrained version

1:25:15.880 --> 1:25:18.360
 of universal general intelligence

1:25:18.360 --> 1:25:21.160
 in a system that has limited memory

1:25:21.160 --> 1:25:23.160
 and limited speed of processing,

1:25:23.160 --> 1:25:26.200
 but whose general intelligence will be biased

1:25:26.200 --> 1:25:28.840
 toward controlling a solid object agent,

1:25:28.840 --> 1:25:31.360
 which is mobile in a solid object world

1:25:31.360 --> 1:25:33.480
 for manipulating solid objects

1:25:33.480 --> 1:25:38.480
 and communicating via language with other similar agents

1:25:38.560 --> 1:25:39.920
 in that same world, right?

1:25:39.920 --> 1:25:41.560
 Then starting from that,

1:25:41.560 --> 1:25:43.640
 you're starting to get a requirements analysis

1:25:43.640 --> 1:25:48.120
 for human level general intelligence.

1:25:48.120 --> 1:25:50.920
 And then that leads you into cognitive science

1:25:50.920 --> 1:25:53.080
 and you can look at, say, what are the different types

1:25:53.080 --> 1:25:56.960
 of memory that the human mind and brain has?

1:25:56.960 --> 1:26:00.840
 And this has matured over the last decades

1:26:00.840 --> 1:26:02.920
 and I got into this a lot.

1:26:02.920 --> 1:26:04.600
 So after getting my PhD in math,

1:26:04.600 --> 1:26:06.080
 I was an academic for eight years.

1:26:06.080 --> 1:26:08.720
 I was in departments of mathematics,

1:26:08.720 --> 1:26:11.320
 computer science, and psychology.

1:26:11.320 --> 1:26:12.760
 When I was in the psychology department

1:26:12.760 --> 1:26:14.240
 at the University of Western Australia,

1:26:14.240 --> 1:26:18.720
 I was focused on cognitive science of memory and perception.

1:26:18.720 --> 1:26:21.280
 Actually, I was teaching neural nets and deep neural nets

1:26:21.280 --> 1:26:23.600
 and it was multi layer perceptrons, right?

1:26:23.600 --> 1:26:24.640
 Psychology?

1:26:24.640 --> 1:26:25.800
 Yeah.

1:26:25.800 --> 1:26:27.880
 Cognitive science, it was cross disciplinary

1:26:27.880 --> 1:26:31.280
 among engineering, math, psychology, philosophy,

1:26:31.280 --> 1:26:33.280
 linguistics, computer science.

1:26:33.280 --> 1:26:35.960
 But yeah, we were teaching psychology students

1:26:35.960 --> 1:26:40.040
 to try to model the data from human cognition experiments

1:26:40.040 --> 1:26:42.080
 using multi layer perceptrons,

1:26:42.080 --> 1:26:45.040
 which was the early version of a deep neural network.

1:26:45.040 --> 1:26:47.880
 Very, very, yeah, recurrent back prop

1:26:47.880 --> 1:26:51.200
 was very, very slow to train back then, right?

1:26:51.200 --> 1:26:53.920
 So this is the study of these constraint systems

1:26:53.920 --> 1:26:55.640
 that are supposed to deal with physical objects.

1:26:55.640 --> 1:27:01.480
 So if you look at cognitive psychology,

1:27:01.480 --> 1:27:04.520
 you can see there's multiple types of memory,

1:27:04.520 --> 1:27:06.560
 which are to some extent represented

1:27:06.560 --> 1:27:08.480
 by different subsystems in the human brain.

1:27:08.480 --> 1:27:10.360
 So we have episodic memory,

1:27:10.360 --> 1:27:12.560
 which takes into account our life history

1:27:13.520 --> 1:27:15.240
 and everything that's happened to us.

1:27:15.240 --> 1:27:17.320
 We have declarative or semantic memory,

1:27:17.320 --> 1:27:20.080
 which is like facts and beliefs abstracted

1:27:20.080 --> 1:27:22.840
 from the particular situations that they occurred in.

1:27:22.840 --> 1:27:26.120
 There's sensory memory, which to some extent

1:27:26.120 --> 1:27:27.600
 is sense modality specific,

1:27:27.600 --> 1:27:32.600
 and then to some extent is unified across sense modalities.

1:27:33.360 --> 1:27:36.120
 There's procedural memory, memory of how to do stuff,

1:27:36.120 --> 1:27:38.160
 like how to swing the tennis racket, right?

1:27:38.160 --> 1:27:39.920
 Which is, there's motor memory,

1:27:39.920 --> 1:27:43.640
 but it's also a little more abstract than motor memory.

1:27:43.640 --> 1:27:47.520
 It involves cerebellum and cortex working together.

1:27:47.520 --> 1:27:51.600
 Then there's memory linkage with emotion

1:27:51.600 --> 1:27:55.920
 which has to do with linkages of cortex and limbic system.

1:27:55.920 --> 1:27:59.160
 There's specifics of spatial and temporal modeling

1:27:59.160 --> 1:28:02.760
 connected with memory, which has to do with hippocampus

1:28:02.760 --> 1:28:05.360
 and thalamus connecting to cortex.

1:28:05.360 --> 1:28:08.160
 And the basal ganglia, which influences goals.

1:28:08.160 --> 1:28:10.960
 So we have specific memory of what goals,

1:28:10.960 --> 1:28:13.160
 subgoals and sub subgoals we want to perceive

1:28:13.160 --> 1:28:15.040
 in which context in the past.

1:28:15.040 --> 1:28:18.240
 Human brain has substantially different subsystems

1:28:18.240 --> 1:28:21.040
 for these different types of memory

1:28:21.040 --> 1:28:24.240
 and substantially differently tuned learning,

1:28:24.240 --> 1:28:27.280
 like differently tuned modes of longterm potentiation

1:28:27.280 --> 1:28:29.720
 to do with the types of neurons and neurotransmitters

1:28:29.720 --> 1:28:31.280
 in the different parts of the brain

1:28:31.280 --> 1:28:33.040
 corresponding to these different types of knowledge.

1:28:33.040 --> 1:28:35.880
 And these different types of memory and learning

1:28:35.880 --> 1:28:38.520
 in the human brain, I mean, you can back these all

1:28:38.520 --> 1:28:41.920
 into embodied communication for controlling agents

1:28:41.920 --> 1:28:44.680
 in worlds of solid objects.

1:28:44.680 --> 1:28:47.720
 Now, so if you look at building an AGI system,

1:28:47.720 --> 1:28:50.440
 one way to do it, which starts more from cognitive science

1:28:50.440 --> 1:28:52.680
 than neuroscience is to say,

1:28:52.680 --> 1:28:55.240
 okay, what are the types of memory

1:28:55.240 --> 1:28:57.360
 that are necessary for this kind of world?

1:28:57.360 --> 1:29:00.720
 Yeah, yeah, necessary for this sort of intelligence.

1:29:00.720 --> 1:29:02.760
 What types of learning work well

1:29:02.760 --> 1:29:04.600
 with these different types of memory?

1:29:04.600 --> 1:29:07.800
 And then how do you connect all these things together, right?

1:29:07.800 --> 1:29:10.800
 And of course the human brain did it incrementally

1:29:10.800 --> 1:29:14.360
 through evolution because each of the sub networks

1:29:14.360 --> 1:29:16.680
 of the brain, I mean, it's not really the lobes

1:29:16.680 --> 1:29:18.240
 of the brain, it's the sub networks,

1:29:18.240 --> 1:29:20.800
 each of which is widely distributed,

1:29:20.800 --> 1:29:23.680
 which of the, each of the sub networks of the brain

1:29:23.680 --> 1:29:27.160
 co evolves with the other sub networks of the brain,

1:29:27.160 --> 1:29:29.480
 both in terms of its patterns of organization

1:29:29.480 --> 1:29:31.840
 and the particulars of the neurophysiology.

1:29:31.840 --> 1:29:33.440
 So they all grew up communicating

1:29:33.440 --> 1:29:34.440
 and adapting to each other.

1:29:34.440 --> 1:29:36.720
 It's not like they were separate black boxes

1:29:36.720 --> 1:29:40.200
 that were then glommed together, right?

1:29:40.200 --> 1:29:43.320
 Whereas as engineers, we would tend to say,

1:29:43.320 --> 1:29:46.680
 let's make the declarative memory box here

1:29:46.680 --> 1:29:48.440
 and the procedural memory box here

1:29:48.440 --> 1:29:51.400
 and the perception box here and wire them together.

1:29:51.400 --> 1:29:54.120
 And when you can do that, it's interesting.

1:29:54.120 --> 1:29:55.680
 I mean, that's how a car is built, right?

1:29:55.680 --> 1:29:58.560
 But on the other hand, that's clearly not

1:29:58.560 --> 1:30:01.400
 how biological systems are made.

1:30:01.400 --> 1:30:05.360
 The parts co evolve so as to adapt and work together.

1:30:05.360 --> 1:30:09.240
 That's by the way, how every human engineered system

1:30:09.240 --> 1:30:11.640
 that flies, that was, we were using that analogy

1:30:11.640 --> 1:30:13.000
 before it's built as well.

1:30:13.000 --> 1:30:14.440
 So do you find this at all appealing?

1:30:14.440 --> 1:30:16.680
 Like there's been a lot of really exciting,

1:30:16.680 --> 1:30:20.160
 which I find strange that it's ignored work

1:30:20.160 --> 1:30:21.880
 in cognitive architectures, for example,

1:30:21.880 --> 1:30:23.320
 throughout the last few decades.

1:30:23.320 --> 1:30:24.320
 Do you find that?

1:30:24.320 --> 1:30:27.960
 Yeah, I mean, I had a lot to do with that community

1:30:27.960 --> 1:30:31.000
 and you know, Paul Rosenbloom, who was one of the,

1:30:31.000 --> 1:30:33.480
 and John Laird who built the SOAR architecture,

1:30:33.480 --> 1:30:34.640
 are friends of mine.

1:30:34.640 --> 1:30:37.160
 And I learned SOAR quite well

1:30:37.160 --> 1:30:39.440
 and ACTAR and these different cognitive architectures.

1:30:39.440 --> 1:30:44.440
 And how I was looking at the AI world about 10 years ago

1:30:44.520 --> 1:30:47.840
 before this whole commercial deep learning explosion was,

1:30:47.840 --> 1:30:51.560
 on the one hand, you had these cognitive architecture guys

1:30:51.560 --> 1:30:53.480
 who were working closely with psychologists

1:30:53.480 --> 1:30:55.760
 and cognitive scientists who had thought a lot

1:30:55.760 --> 1:30:58.840
 about how the different parts of a human like mind

1:30:58.840 --> 1:31:00.400
 should work together.

1:31:00.400 --> 1:31:03.600
 On the other hand, you had these learning theory guys

1:31:03.600 --> 1:31:06.040
 who didn't care at all about the architecture,

1:31:06.040 --> 1:31:07.360
 but we're just thinking about like,

1:31:07.360 --> 1:31:10.280
 how do you recognize patterns in large amounts of data?

1:31:10.280 --> 1:31:14.560
 And in some sense, what you needed to do

1:31:14.560 --> 1:31:18.440
 was to get the learning that the learning theory guys

1:31:18.440 --> 1:31:21.440
 were doing and put it together with the architecture

1:31:21.440 --> 1:31:24.240
 that the cognitive architecture guys were doing.

1:31:24.240 --> 1:31:25.960
 And then you would have what you needed.

1:31:25.960 --> 1:31:30.760
 Now, you can't, unfortunately, when you look at the details,

1:31:31.600 --> 1:31:34.960
 you can't just do that without totally rebuilding

1:31:34.960 --> 1:31:37.840
 what is happening on both the cognitive architecture

1:31:37.840 --> 1:31:38.760
 and the learning side.

1:31:38.760 --> 1:31:41.760
 So, I mean, they tried to do that in SOAR,

1:31:41.760 --> 1:31:43.960
 but what they ultimately did is like,

1:31:43.960 --> 1:31:46.560
 take a deep neural net or something for perception

1:31:46.560 --> 1:31:50.800
 and you include it as one of the black boxes.

1:31:50.800 --> 1:31:51.960
 It becomes one of the boxes.

1:31:51.960 --> 1:31:53.800
 The learning mechanism becomes one of the boxes

1:31:53.800 --> 1:31:57.440
 as opposed to fundamental part of the system.

1:31:57.440 --> 1:32:00.400
 You could look at some of the stuff DeepMind has done,

1:32:00.400 --> 1:32:03.240
 like the differential neural computer or something

1:32:03.240 --> 1:32:07.080
 that sort of has a neural net for deep learning perception.

1:32:07.080 --> 1:32:10.640
 It has another neural net, which is like a memory matrix

1:32:10.640 --> 1:32:13.080
 that stores, say, the map of the London subway or something.

1:32:13.080 --> 1:32:16.440
 So probably Demis Tsabas was thinking about this

1:32:16.440 --> 1:32:18.520
 like part of cortex and part of hippocampus

1:32:18.520 --> 1:32:20.440
 because hippocampus has a spatial map.

1:32:20.440 --> 1:32:21.720
 And when he was a neuroscientist,

1:32:21.720 --> 1:32:24.600
 he was doing a bunch on cortex hippocampus interconnection.

1:32:24.600 --> 1:32:27.320
 So there, the DNC would be an example of folks

1:32:27.320 --> 1:32:30.160
 from the deep neural net world trying to take a step

1:32:30.160 --> 1:32:32.200
 in the cognitive architecture direction

1:32:32.200 --> 1:32:35.000
 by having two neural modules that correspond roughly

1:32:35.000 --> 1:32:36.720
 to two different parts of the human brain

1:32:36.720 --> 1:32:38.920
 that deal with different kinds of memory and learning.

1:32:38.920 --> 1:32:42.000
 But on the other hand, it's super, super, super crude

1:32:42.000 --> 1:32:44.360
 from the cognitive architecture view, right?

1:32:44.360 --> 1:32:48.080
 Just as what John Laird and Soar did with neural nets

1:32:48.080 --> 1:32:51.200
 was super, super crude from a learning point of view

1:32:51.200 --> 1:32:53.360
 because the learning was like off to the side,

1:32:53.360 --> 1:32:55.880
 not affecting the core representations, right?

1:32:55.880 --> 1:32:57.880
 I mean, you weren't learning the representation.

1:32:57.880 --> 1:33:00.080
 You were learning the data that feeds into the...

1:33:00.080 --> 1:33:02.600
 You were learning abstractions of perceptual data

1:33:02.600 --> 1:33:06.560
 to feed into the representation that was not learned, right?

1:33:06.560 --> 1:33:11.000
 So yeah, this was clear to me a while ago.

1:33:11.000 --> 1:33:14.240
 And one of my hopes with the AGI community

1:33:14.240 --> 1:33:15.960
 was to sort of bring people

1:33:15.960 --> 1:33:18.440
 from those two directions together.

1:33:19.320 --> 1:33:21.920
 That didn't happen much in terms of...

1:33:21.920 --> 1:33:22.760
 Not yet.

1:33:22.760 --> 1:33:24.520
 And what I was gonna say is it didn't happen

1:33:24.520 --> 1:33:26.360
 in terms of bringing like the lions

1:33:26.360 --> 1:33:28.560
 of cognitive architecture together

1:33:28.560 --> 1:33:30.480
 with the lions of deep learning.

1:33:30.480 --> 1:33:33.760
 It did work in the sense that a bunch of younger researchers

1:33:33.760 --> 1:33:35.760
 have had their heads filled with both of those ideas.

1:33:35.760 --> 1:33:38.840
 This comes back to a saying my dad,

1:33:38.840 --> 1:33:41.360
 who was a university professor, often quoted to me,

1:33:41.360 --> 1:33:44.960
 which was, science advances one funeral at a time,

1:33:45.840 --> 1:33:47.840
 which I'm trying to avoid.

1:33:47.840 --> 1:33:51.320
 Like I'm 53 years old and I'm trying to invent

1:33:51.320 --> 1:33:53.480
 amazing, weird ass new things

1:33:53.480 --> 1:33:56.160
 that nobody ever thought about,

1:33:56.160 --> 1:33:59.240
 which we'll talk about in a few minutes.

1:33:59.240 --> 1:34:02.280
 But there is that aspect, right?

1:34:02.280 --> 1:34:05.680
 Like the people who've been at AI a long time

1:34:05.680 --> 1:34:08.760
 and have made their career developing one aspect,

1:34:08.760 --> 1:34:12.880
 like a cognitive architecture or a deep learning approach,

1:34:12.880 --> 1:34:14.760
 it can be hard once you're old

1:34:14.760 --> 1:34:17.280
 and have made your career doing one thing,

1:34:17.280 --> 1:34:19.640
 it can be hard to mentally shift gears.

1:34:19.640 --> 1:34:23.640
 I mean, I try quite hard to remain flexible minded.

1:34:23.640 --> 1:34:26.480
 Have you been successful somewhat in changing,

1:34:26.480 --> 1:34:29.640
 maybe, have you changed your mind on some aspects

1:34:29.640 --> 1:34:32.920
 of what it takes to build an AGI, like technical things?

1:34:32.920 --> 1:34:36.040
 The hard part is that the world doesn't want you to.

1:34:36.040 --> 1:34:37.360
 The world or your own brain?

1:34:37.360 --> 1:34:39.560
 The world, well, that one point

1:34:39.560 --> 1:34:41.040
 is that your brain doesn't want to.

1:34:41.040 --> 1:34:43.480
 The other part is that the world doesn't want you to.

1:34:43.480 --> 1:34:46.520
 Like the people who have followed your ideas

1:34:46.520 --> 1:34:49.280
 get mad at you if you change your mind.

1:34:49.280 --> 1:34:54.280
 And the media wants to pigeonhole you as an avatar

1:34:54.560 --> 1:34:57.160
 of a certain idea.

1:34:57.160 --> 1:35:01.480
 But yeah, I've changed my mind on a bunch of things.

1:35:01.480 --> 1:35:03.800
 I mean, when I started my career,

1:35:03.800 --> 1:35:05.240
 I really thought quantum computing

1:35:05.240 --> 1:35:07.920
 would be necessary for AGI.

1:35:07.920 --> 1:35:10.800
 And I doubt it's necessary now,

1:35:10.800 --> 1:35:14.680
 although I think it will be a super major enhancement.

1:35:14.680 --> 1:35:19.360
 But I mean, I'm now in the middle of embarking

1:35:19.360 --> 1:35:23.400
 on the complete rethink and rewrite from scratch

1:35:23.400 --> 1:35:28.400
 of our OpenCog AGI system together with Alexey Potapov

1:35:28.480 --> 1:35:29.840
 and his team in St. Petersburg,

1:35:29.840 --> 1:35:31.600
 who's working with me in SingularityNet.

1:35:31.600 --> 1:35:35.680
 So now we're trying to like go back to basics,

1:35:35.680 --> 1:35:37.800
 take everything we learned from working

1:35:37.800 --> 1:35:39.600
 with the current OpenCog system,

1:35:39.600 --> 1:35:41.880
 take everything everybody else has learned

1:35:41.880 --> 1:35:45.680
 from working with their proto AGI systems

1:35:45.680 --> 1:35:50.000
 and design the best framework for the next stage.

1:35:50.000 --> 1:35:53.320
 And I do think there's a lot to be learned

1:35:53.320 --> 1:35:56.800
 from the recent successes with deep neural nets

1:35:56.800 --> 1:35:59.000
 and deep reinforcement systems.

1:35:59.000 --> 1:36:02.680
 I mean, people made these essentially trivial systems

1:36:02.680 --> 1:36:04.840
 work much better than I thought they would.

1:36:04.840 --> 1:36:07.080
 And there's a lot to be learned from that.

1:36:07.080 --> 1:36:10.720
 And I wanna incorporate that knowledge appropriately

1:36:10.720 --> 1:36:13.520
 in our OpenCog 2.0 system.

1:36:13.520 --> 1:36:18.520
 On the other hand, I also think current deep neural net

1:36:18.520 --> 1:36:22.240
 architectures as such will never get you anywhere near AGI.

1:36:22.240 --> 1:36:25.080
 So I think you wanna avoid the pathology

1:36:25.080 --> 1:36:28.360
 of throwing the baby out with the bathwater

1:36:28.360 --> 1:36:30.880
 and like saying, well, these things are garbage

1:36:30.880 --> 1:36:33.840
 because foolish journalists overblow them

1:36:33.840 --> 1:36:37.040
 as being the path to AGI

1:36:37.040 --> 1:36:41.600
 and a few researchers overblow them as well.

1:36:41.600 --> 1:36:45.440
 There's a lot of interesting stuff to be learned there

1:36:45.440 --> 1:36:48.000
 even though those are not the golden path.

1:36:48.000 --> 1:36:50.160
 So maybe this is a good chance to step back.

1:36:50.160 --> 1:36:52.920
 You mentioned OpenCog 2.0, but...

1:36:52.920 --> 1:36:56.040
 Go back to OpenCog 0.0, which exists now.

1:36:56.040 --> 1:36:57.440
 Alpha, yeah.

1:36:58.440 --> 1:37:01.920
 Yeah, maybe talk through the history of OpenCog

1:37:01.920 --> 1:37:03.920
 and your thinking about these ideas.

1:37:03.920 --> 1:37:08.920
 I would say OpenCog 2.0 is a term we're throwing around

1:37:10.120 --> 1:37:14.560
 sort of tongue in cheek because the existing OpenCog system

1:37:14.560 --> 1:37:17.200
 that we're working on now is not remotely close

1:37:17.200 --> 1:37:20.000
 to what we'd consider a 1.0, right?

1:37:20.000 --> 1:37:23.360
 I mean, it's an early...

1:37:23.360 --> 1:37:27.400
 It's been around, what, 13 years or something,

1:37:27.400 --> 1:37:29.800
 but it's still an early stage research system, right?

1:37:29.800 --> 1:37:34.800
 And actually, we are going back to the beginning

1:37:37.360 --> 1:37:40.680
 in terms of theory and implementation

1:37:40.680 --> 1:37:42.840
 because we feel like that's the right thing to do,

1:37:42.840 --> 1:37:45.560
 but I'm sure what we end up with is gonna have

1:37:45.560 --> 1:37:48.560
 a huge amount in common with the current system.

1:37:48.560 --> 1:37:51.640
 I mean, we all still like the general approach.

1:37:51.640 --> 1:37:54.400
 So first of all, what is OpenCog?

1:37:54.400 --> 1:37:59.400
 Sure, OpenCog is an open source software project

1:37:59.800 --> 1:38:04.400
 that I launched together with several others in 2008

1:38:04.400 --> 1:38:08.280
 and probably the first code written toward that

1:38:08.280 --> 1:38:11.160
 was written in 2001 or two or something

1:38:11.160 --> 1:38:15.320
 that was developed as a proprietary code base

1:38:15.320 --> 1:38:18.280
 within my AI company, Novamente LLC.

1:38:18.280 --> 1:38:22.000
 Then we decided to open source it in 2008,

1:38:22.000 --> 1:38:23.840
 cleaned up the code throughout some things

1:38:23.840 --> 1:38:26.920
 and added some new things and...

1:38:26.920 --> 1:38:28.080
 What language is it written in?

1:38:28.080 --> 1:38:29.440
 It's C++.

1:38:29.440 --> 1:38:31.400
 Primarily, there's a bunch of scheme as well,

1:38:31.400 --> 1:38:33.040
 but most of it's C++.

1:38:33.040 --> 1:38:36.520
 And it's separate from something we'll also talk about,

1:38:36.520 --> 1:38:37.480
 the SingularityNet.

1:38:37.480 --> 1:38:41.360
 So it was born as a non networked thing.

1:38:41.360 --> 1:38:42.400
 Correct, correct.

1:38:42.400 --> 1:38:47.000
 Well, there are many levels of networks involved here.

1:38:47.000 --> 1:38:52.000
 No connectivity to the internet, or no, at birth.

1:38:52.000 --> 1:38:57.000
 Yeah, I mean, SingularityNet is a separate project

1:38:57.240 --> 1:38:59.440
 and a separate body of code.

1:38:59.440 --> 1:39:02.600
 And you can use SingularityNet as part of the infrastructure

1:39:02.600 --> 1:39:04.480
 for a distributed OpenCog system,

1:39:04.480 --> 1:39:07.520
 but there are different layers.

1:39:07.520 --> 1:39:08.360
 Yeah, got it.

1:39:08.360 --> 1:39:13.360
 So OpenCog on the one hand as a software framework

1:39:14.840 --> 1:39:17.000
 could be used to implement a variety

1:39:17.000 --> 1:39:21.840
 of different AI architectures and algorithms,

1:39:21.840 --> 1:39:26.440
 but in practice, there's been a group of developers

1:39:26.440 --> 1:39:29.440
 which I've been leading together with Linus Vepstas,

1:39:29.440 --> 1:39:31.680
 Neil Geisweiler, and a few others,

1:39:31.680 --> 1:39:35.080
 which have been using the OpenCog platform

1:39:35.080 --> 1:39:39.440
 and infrastructure to implement certain ideas

1:39:39.440 --> 1:39:41.280
 about how to make an AGI.

1:39:41.280 --> 1:39:43.480
 So there's been a little bit of ambiguity

1:39:43.480 --> 1:39:46.120
 about OpenCog, the software platform

1:39:46.120 --> 1:39:49.360
 versus OpenCog, the AGI design,

1:39:49.360 --> 1:39:52.160
 because in theory, you could use that software to do,

1:39:52.160 --> 1:39:53.440
 you could use it to make a neural net.

1:39:53.440 --> 1:39:55.880
 You could use it to make a lot of different AGI.

1:39:55.880 --> 1:39:58.640
 What kind of stuff does the software platform provide,

1:39:58.640 --> 1:40:00.760
 like in terms of utilities, tools, like what?

1:40:00.760 --> 1:40:03.840
 Yeah, let me first tell about OpenCog

1:40:03.840 --> 1:40:05.520
 as a software platform,

1:40:05.520 --> 1:40:08.680
 and then I'll tell you the specific AGI R&D

1:40:08.680 --> 1:40:10.760
 we've been building on top of it.

1:40:12.240 --> 1:40:16.200
 So the core component of OpenCog as a software platform

1:40:16.200 --> 1:40:17.920
 is what we call the atom space,

1:40:17.920 --> 1:40:21.240
 which is a weighted labeled hypergraph.

1:40:21.240 --> 1:40:22.880
 ATOM, atom space.

1:40:22.880 --> 1:40:25.880
 Atom space, yeah, yeah, not atom, like Adam and Eve,

1:40:25.880 --> 1:40:28.080
 although that would be cool too.

1:40:28.080 --> 1:40:32.120
 Yeah, so you have a hypergraph, which is like,

1:40:32.120 --> 1:40:35.360
 so a graph in this sense is a bunch of nodes

1:40:35.360 --> 1:40:37.120
 with links between them.

1:40:37.120 --> 1:40:40.960
 A hypergraph is like a graph,

1:40:40.960 --> 1:40:43.960
 but links can go between more than two nodes.

1:40:43.960 --> 1:40:45.520
 So you have a link between three nodes.

1:40:45.520 --> 1:40:49.560
 And in fact, OpenCog's atom space

1:40:49.560 --> 1:40:51.760
 would properly be called a metagraph

1:40:51.760 --> 1:40:54.080
 because you can have links pointing to links,

1:40:54.080 --> 1:40:56.840
 or you could have links pointing to whole subgraphs, right?

1:40:56.840 --> 1:41:00.920
 So it's an extended hypergraph or a metagraph.

1:41:00.920 --> 1:41:02.280
 Is metagraph a technical term?

1:41:02.280 --> 1:41:03.640
 It is now a technical term.

1:41:03.640 --> 1:41:04.480
 Interesting.

1:41:04.480 --> 1:41:06.360
 But I don't think it was yet a technical term

1:41:06.360 --> 1:41:10.080
 when we started calling this a generalized hypergraph.

1:41:10.080 --> 1:41:13.400
 But in any case, it's a weighted labeled

1:41:13.400 --> 1:41:16.920
 generalized hypergraph or weighted labeled metagraph.

1:41:16.920 --> 1:41:19.200
 The weights and labels mean that the nodes and links

1:41:19.200 --> 1:41:22.360
 can have numbers and symbols attached to them.

1:41:22.360 --> 1:41:24.920
 So they can have types on them.

1:41:24.920 --> 1:41:27.440
 They can have numbers on them that represent,

1:41:27.440 --> 1:41:30.120
 say, a truth value or an importance value

1:41:30.120 --> 1:41:32.000
 for a certain purpose.

1:41:32.000 --> 1:41:33.240
 And of course, like with all things,

1:41:33.240 --> 1:41:35.080
 you can reduce that to a hypergraph,

1:41:35.080 --> 1:41:35.920
 and then the hypergraph can be reduced to a graph.

1:41:35.920 --> 1:41:37.680
 You can reduce hypergraph to a graph,

1:41:37.680 --> 1:41:39.880
 and you could reduce a graph to an adjacency matrix.

1:41:39.880 --> 1:41:42.720
 So, I mean, there's always multiple representations.

1:41:42.720 --> 1:41:44.000
 But there's a layer of representation

1:41:44.000 --> 1:41:45.120
 that seems to work well here.

1:41:45.120 --> 1:41:45.960
 Got it.

1:41:45.960 --> 1:41:46.800
 Right, right, right.

1:41:46.800 --> 1:41:51.800
 And so similarly, you could have a link to a whole graph

1:41:52.080 --> 1:41:53.440
 because a whole graph could represent,

1:41:53.440 --> 1:41:54.920
 say, a body of information.

1:41:54.920 --> 1:41:58.640
 And I could say, I reject this body of information.

1:41:58.640 --> 1:42:00.320
 Then one way to do that is make that link

1:42:00.320 --> 1:42:02.000
 go to that whole subgraph representing

1:42:02.000 --> 1:42:04.040
 the body of information, right?

1:42:04.040 --> 1:42:07.200
 I mean, there are many alternate representations,

1:42:07.200 --> 1:42:10.720
 but that's, anyway, what we have in OpenCOG,

1:42:10.720 --> 1:42:13.160
 we have an atom space, which is this weighted, labeled,

1:42:13.160 --> 1:42:15.080
 generalized hypergraph.

1:42:15.080 --> 1:42:17.840
 Knowledge store, it lives in RAM.

1:42:17.840 --> 1:42:20.120
 There's also a way to back it up to disk.

1:42:20.120 --> 1:42:22.320
 There are ways to spread it among

1:42:22.320 --> 1:42:24.120
 multiple different machines.

1:42:24.120 --> 1:42:27.960
 Then there are various utilities for dealing with that.

1:42:27.960 --> 1:42:29.800
 So there's a pattern matcher,

1:42:29.800 --> 1:42:33.880
 which lets you specify a sort of abstract pattern

1:42:33.880 --> 1:42:36.200
 and then search through a whole atom space

1:42:36.200 --> 1:42:39.800
 with labeled hypergraph to see what subhypergraphs

1:42:39.800 --> 1:42:42.880
 may match that pattern, for an example.

1:42:42.880 --> 1:42:45.920
 So that's, then there's something called

1:42:45.920 --> 1:42:48.760
 the COG server in OpenCOG,

1:42:48.760 --> 1:42:52.560
 which lets you run a bunch of different agents

1:42:52.560 --> 1:42:55.880
 or processes in a scheduler.

1:42:55.880 --> 1:42:59.160
 And each of these agents, basically it reads stuff

1:42:59.160 --> 1:43:01.880
 from the atom space and it writes stuff to the atom space.

1:43:01.880 --> 1:43:05.640
 So this is sort of the basic operational model.

1:43:05.640 --> 1:43:07.760
 That's the software framework.

1:43:07.760 --> 1:43:10.360
 And of course that's, there's a lot there

1:43:10.360 --> 1:43:13.200
 just from a scalable software engineering standpoint.

1:43:13.200 --> 1:43:15.080
 So you could use this, I don't know if you've,

1:43:15.080 --> 1:43:18.000
 have you looked into the Stephen Wolfram's physics project

1:43:18.000 --> 1:43:20.160
 recently with the hypergraphs and stuff?

1:43:20.160 --> 1:43:22.840
 Could you theoretically use like the software framework

1:43:22.840 --> 1:43:23.800
 to play with it? You certainly could,

1:43:23.800 --> 1:43:26.160
 although Wolfram would rather die

1:43:26.160 --> 1:43:29.080
 than use anything but Mathematica for his work.

1:43:29.080 --> 1:43:32.120
 Well that's, yeah, but there's a big community of people

1:43:32.120 --> 1:43:36.080
 who are, you know, would love integration.

1:43:36.080 --> 1:43:38.400
 Like you said, the young minds love the idea

1:43:38.400 --> 1:43:40.440
 of integrating, of connecting things.

1:43:40.440 --> 1:43:41.280
 Yeah, that's right.

1:43:41.280 --> 1:43:42.840
 And I would add on that note,

1:43:42.840 --> 1:43:46.600
 the idea of using hypergraph type models in physics

1:43:46.600 --> 1:43:47.680
 is not very new.

1:43:47.680 --> 1:43:49.120
 Like if you look at...

1:43:49.120 --> 1:43:50.360
 The Russians did it first.

1:43:50.360 --> 1:43:52.200
 Well, I'm sure they did.

1:43:52.200 --> 1:43:55.880
 And a guy named Ben Dribis, who's a mathematician,

1:43:55.880 --> 1:43:58.200
 a professor in Louisiana or somewhere,

1:43:58.200 --> 1:44:01.960
 had a beautiful book on quantum sets and hypergraphs

1:44:01.960 --> 1:44:05.520
 and algebraic topology for discrete models of physics.

1:44:05.520 --> 1:44:09.080
 And carried it much farther than Wolfram has,

1:44:09.080 --> 1:44:10.920
 but he's not rich and famous,

1:44:10.920 --> 1:44:13.280
 so it didn't get in the headlines.

1:44:13.280 --> 1:44:15.280
 But yeah, Wolfram aside, yeah,

1:44:15.280 --> 1:44:17.120
 certainly that's a good way to put it.

1:44:17.120 --> 1:44:19.280
 The whole OpenCog framework,

1:44:19.280 --> 1:44:22.200
 you could use it to model biological networks

1:44:22.200 --> 1:44:24.200
 and simulate biology processes.

1:44:24.200 --> 1:44:26.480
 You could use it to model physics

1:44:26.480 --> 1:44:30.160
 on discrete graph models of physics.

1:44:30.160 --> 1:44:35.160
 So you could use it to do, say, biologically realistic

1:44:36.840 --> 1:44:39.280
 neural networks, for example.

1:44:39.280 --> 1:44:42.360
 And that's a framework.

1:44:42.360 --> 1:44:44.240
 What do agents and processes do?

1:44:44.240 --> 1:44:45.880
 Do they grow the graph?

1:44:45.880 --> 1:44:48.200
 What kind of computations, just to get a sense,

1:44:48.200 --> 1:44:49.040
 are they supposed to do?

1:44:49.040 --> 1:44:51.200
 So in theory, they could do anything they want to do.

1:44:51.200 --> 1:44:53.320
 They're just C++ processes.

1:44:53.320 --> 1:44:56.880
 On the other hand, the computation framework

1:44:56.880 --> 1:44:59.160
 is sort of designed for agents

1:44:59.160 --> 1:45:02.000
 where most of their processing time

1:45:02.000 --> 1:45:05.400
 is taken up with reads and writes to the atom space.

1:45:05.400 --> 1:45:09.000
 And so that's a very different processing model

1:45:09.000 --> 1:45:12.440
 than, say, the matrix multiplication based model

1:45:12.440 --> 1:45:15.080
 as underlies most deep learning systems, right?

1:45:15.080 --> 1:45:19.560
 So you could create an agent

1:45:19.560 --> 1:45:22.720
 that just factored numbers for a billion years.

1:45:22.720 --> 1:45:25.000
 It would run within the OpenCog platform,

1:45:25.000 --> 1:45:26.600
 but it would be pointless, right?

1:45:26.600 --> 1:45:28.880
 I mean, the point of doing OpenCog

1:45:28.880 --> 1:45:30.520
 is because you want to make agents

1:45:30.520 --> 1:45:33.160
 that are cooperating via reading and writing

1:45:33.160 --> 1:45:36.400
 into this weighted labeled hypergraph, right?

1:45:36.400 --> 1:45:41.400
 And that has both cognitive architecture importance

1:45:41.560 --> 1:45:43.400
 because then this hypergraph is being used

1:45:43.400 --> 1:45:46.040
 as a sort of shared memory

1:45:46.040 --> 1:45:48.240
 among different cognitive processes,

1:45:48.240 --> 1:45:51.000
 but it also has software and hardware

1:45:51.000 --> 1:45:52.840
 implementation implications

1:45:52.840 --> 1:45:54.840
 because current GPU architectures

1:45:54.840 --> 1:45:57.120
 are not so useful for OpenCog,

1:45:57.120 --> 1:46:01.200
 whereas a graph chip would be incredibly useful, right?

1:46:01.200 --> 1:46:03.640
 And I think Graphcore has those now,

1:46:03.640 --> 1:46:05.240
 but they're not ideally suited for this.

1:46:05.240 --> 1:46:10.240
 But I think in the next, let's say, three to five years,

1:46:10.640 --> 1:46:12.000
 we're gonna see new chips

1:46:12.000 --> 1:46:14.680
 where like a graph is put on the chip

1:46:14.680 --> 1:46:19.320
 and the back and forth between multiple processes

1:46:19.320 --> 1:46:23.600
 acting SIMD and MIMD on that graph is gonna be fast.

1:46:23.600 --> 1:46:26.480
 And then that may do for OpenCog type architectures

1:46:26.480 --> 1:46:29.840
 what GPUs did for deep neural architecture.

1:46:29.840 --> 1:46:31.320
 It's a small tangent.

1:46:31.320 --> 1:46:34.600
 Can you comment on thoughts about neuromorphic computing?

1:46:34.600 --> 1:46:36.400
 So like hardware implementations

1:46:36.400 --> 1:46:39.360
 of all these different kind of, are you interested?

1:46:39.360 --> 1:46:41.000
 Are you excited by that possibility?

1:46:41.000 --> 1:46:42.680
 I'm excited by graph processors

1:46:42.680 --> 1:46:46.440
 because I think they can massively speed up OpenCog,

1:46:46.440 --> 1:46:50.680
 which is a class of architectures that I'm working on.

1:46:50.680 --> 1:46:56.680
 I think if, you know, in principle, neuromorphic computing

1:46:57.240 --> 1:46:58.760
 should be amazing.

1:46:58.760 --> 1:47:00.480
 I haven't yet been fully sold

1:47:00.480 --> 1:47:03.320
 on any of the systems that are out.

1:47:03.320 --> 1:47:06.400
 They're like, memristors should be amazing too, right?

1:47:06.400 --> 1:47:09.400
 So a lot of these things have obvious potential,

1:47:09.400 --> 1:47:11.360
 but I haven't yet put my hands on a system

1:47:11.360 --> 1:47:13.280
 that seemed to manifest that.

1:47:13.280 --> 1:47:14.880
 Mark's system should be amazing,

1:47:14.880 --> 1:47:17.880
 but the current systems have not been great.

1:47:17.880 --> 1:47:19.640
 Yeah, I mean, look, for example,

1:47:19.640 --> 1:47:23.960
 if you wanted to make a biologically realistic

1:47:23.960 --> 1:47:25.680
 hardware neural network,

1:47:25.680 --> 1:47:30.680
 like making a circuit in hardware

1:47:31.520 --> 1:47:34.360
 that emulated like the Hodgkin–Huxley equation

1:47:34.360 --> 1:47:35.640
 or the Izhekevich equation,

1:47:35.640 --> 1:47:38.240
 like differential equations

1:47:38.240 --> 1:47:40.680
 for a biologically realistic neuron

1:47:40.680 --> 1:47:43.800
 and putting that in hardware on the chip,

1:47:43.800 --> 1:47:46.360
 that would seem that it would make more feasible

1:47:46.360 --> 1:47:50.320
 to make a large scale, truly biologically realistic

1:47:50.320 --> 1:47:51.160
 neural network.

1:47:51.160 --> 1:47:54.320
 Now, what's been done so far is not like that.

1:47:54.320 --> 1:47:57.120
 So I guess personally, as a researcher,

1:47:57.120 --> 1:48:02.120
 I mean, I've done a bunch of work in computational neuroscience

1:48:02.480 --> 1:48:05.600
 where I did some work with IARPA in DC,

1:48:05.600 --> 1:48:08.240
 Intelligence Advanced Research Project Agency.

1:48:08.240 --> 1:48:10.880
 We were looking at how do you make

1:48:10.880 --> 1:48:13.000
 a biologically realistic simulation

1:48:13.000 --> 1:48:15.720
 of seven different parts of the brain

1:48:15.720 --> 1:48:17.080
 cooperating with each other,

1:48:17.080 --> 1:48:20.440
 using like realistic nonlinear dynamical models of neurons,

1:48:20.440 --> 1:48:21.920
 and how do you get that to simulate

1:48:21.920 --> 1:48:24.800
 what's going on in the mind of a geo intelligence analyst

1:48:24.800 --> 1:48:27.160
 while they're trying to find terrorists on a map, right?

1:48:27.160 --> 1:48:29.880
 So if you want to do something like that,

1:48:29.880 --> 1:48:34.080
 having neuromorphic hardware that really let you simulate

1:48:34.080 --> 1:48:38.720
 like a realistic model of the neuron would be amazing.

1:48:38.720 --> 1:48:42.280
 But that's sort of with my computational neuroscience

1:48:42.280 --> 1:48:43.120
 hat on, right?

1:48:43.120 --> 1:48:47.160
 With an AGI hat on, I'm just more interested

1:48:47.160 --> 1:48:50.200
 in these hypergraph knowledge representation

1:48:50.200 --> 1:48:54.480
 based architectures, which would benefit more

1:48:54.480 --> 1:48:57.720
 from various types of graph processors

1:48:57.720 --> 1:49:00.480
 because the main processing bottleneck

1:49:00.480 --> 1:49:02.000
 is reading writing to RAM.

1:49:02.000 --> 1:49:03.960
 It's reading writing to the graph in RAM.

1:49:03.960 --> 1:49:06.120
 The main processing bottleneck for this kind of

1:49:06.120 --> 1:49:09.840
 proto AGI architecture is not multiplying matrices.

1:49:09.840 --> 1:49:13.280
 And for that reason, GPUs, which are really good

1:49:13.280 --> 1:49:17.520
 at multiplying matrices, don't apply as well.

1:49:17.520 --> 1:49:20.240
 There are frameworks like Gunrock and others

1:49:20.240 --> 1:49:22.160
 that try to boil down graph processing

1:49:22.160 --> 1:49:24.640
 to matrix operations, and they're cool,

1:49:24.640 --> 1:49:26.160
 but you're still putting a square peg

1:49:26.160 --> 1:49:28.800
 into a round hole in a certain way.

1:49:28.800 --> 1:49:32.760
 The same is true, I mean, current quantum machine learning,

1:49:32.760 --> 1:49:34.240
 which is very cool.

1:49:34.240 --> 1:49:37.320
 It's also all about how to get matrix and vector operations

1:49:37.320 --> 1:49:41.280
 in quantum mechanics, and I see why that's natural to do.

1:49:41.280 --> 1:49:44.240
 I mean, quantum mechanics is all unitary matrices

1:49:44.240 --> 1:49:45.800
 and vectors, right?

1:49:45.800 --> 1:49:48.040
 On the other hand, you could also try

1:49:48.040 --> 1:49:50.760
 to make graph centric quantum computers,

1:49:50.760 --> 1:49:54.400
 which I think is where things will go.

1:49:54.400 --> 1:49:57.080
 And then we can have, then we can make,

1:49:57.080 --> 1:50:00.120
 like take the open cog implementation layer,

1:50:00.120 --> 1:50:04.000
 implement it in a collapsed state inside a quantum computer.

1:50:04.000 --> 1:50:06.480
 But that may be the singularity squared, right?

1:50:06.480 --> 1:50:11.480
 I'm not sure we need that to get to human level.

1:50:12.360 --> 1:50:14.680
 That's already beyond the first singularity.

1:50:14.680 --> 1:50:17.640
 But can we just go back to open cog?

1:50:17.640 --> 1:50:20.040
 Yeah, and the hypergraph and open cog.

1:50:20.040 --> 1:50:21.640
 That's the software framework, right?

1:50:21.640 --> 1:50:25.440
 So the next thing is our cognitive architecture

1:50:25.440 --> 1:50:27.960
 tells us particular algorithms to put there.

1:50:27.960 --> 1:50:28.800
 Got it.

1:50:28.800 --> 1:50:33.720
 Can we backtrack on the kind of, is this graph designed,

1:50:33.720 --> 1:50:37.680
 is it in general supposed to be sparse

1:50:37.680 --> 1:50:40.640
 and the operations constantly grow and change the graph?

1:50:40.640 --> 1:50:42.320
 Yeah, the graph is sparse.

1:50:42.320 --> 1:50:45.040
 But is it constantly adding links and so on?

1:50:45.040 --> 1:50:47.200
 It is a self modifying hypergraph.

1:50:47.200 --> 1:50:49.800
 So it's not, so the write and read operations

1:50:49.800 --> 1:50:53.040
 you're referring to, this isn't just a fixed graph

1:50:53.040 --> 1:50:55.840
 to which you change the way, it's a constantly growing graph.

1:50:55.840 --> 1:50:58.000
 Yeah, that's true.

1:50:58.000 --> 1:51:03.000
 So it is different model than,

1:51:03.000 --> 1:51:04.680
 say current deep neural nets

1:51:04.680 --> 1:51:06.840
 and have a fixed neural architecture

1:51:06.840 --> 1:51:08.600
 and you're updating the weights.

1:51:08.600 --> 1:51:10.880
 Although there have been like cascade correlational

1:51:10.880 --> 1:51:13.920
 neural net architectures that grow new nodes and links,

1:51:13.920 --> 1:51:16.640
 but the most common neural architectures now

1:51:16.640 --> 1:51:17.960
 have a fixed neural architecture,

1:51:17.960 --> 1:51:19.080
 you're updating the weights.

1:51:19.080 --> 1:51:22.520
 And then open cog, you can update the weights

1:51:22.520 --> 1:51:24.760
 and that certainly happens a lot,

1:51:24.760 --> 1:51:28.200
 but adding new nodes, adding new links,

1:51:28.200 --> 1:51:30.720
 removing nodes and links is an equally critical part

1:51:30.720 --> 1:51:32.160
 of the system's operations.

1:51:32.160 --> 1:51:33.000
 Got it.

1:51:33.000 --> 1:51:37.040
 So now when you start to add these cognitive algorithms

1:51:37.040 --> 1:51:39.840
 on top of this open cog architecture,

1:51:39.840 --> 1:51:41.280
 what does that look like?

1:51:41.280 --> 1:51:44.800
 Yeah, so within this framework then,

1:51:44.800 --> 1:51:48.040
 creating a cognitive architecture is basically two things.

1:51:48.040 --> 1:51:52.080
 It's choosing what type system you wanna put

1:51:52.080 --> 1:51:53.800
 on the nodes and links in the hypergraph,

1:51:53.800 --> 1:51:56.120
 what types of nodes and links you want.

1:51:56.120 --> 1:52:01.000
 And then it's choosing what collection of agents,

1:52:01.000 --> 1:52:04.640
 what collection of AI algorithms or processes

1:52:04.640 --> 1:52:08.040
 are gonna run to operate on this hypergraph.

1:52:08.040 --> 1:52:10.520
 And of course those two decisions

1:52:10.520 --> 1:52:13.920
 are closely connected to each other.

1:52:13.920 --> 1:52:17.480
 So in terms of the type system,

1:52:17.480 --> 1:52:19.920
 there are some links that are more neural net like,

1:52:19.920 --> 1:52:22.360
 they're just like have weights to get updated

1:52:22.360 --> 1:52:26.000
 by heavy and learning and activation spreads along them.

1:52:26.000 --> 1:52:29.080
 There are other links that are more logic like

1:52:29.080 --> 1:52:30.520
 and nodes that are more logic like.

1:52:30.520 --> 1:52:32.240
 So you could have a variable node

1:52:32.240 --> 1:52:34.240
 and you can have a node representing a universal

1:52:34.240 --> 1:52:37.680
 or existential quantifier as in predicate logic

1:52:37.680 --> 1:52:39.160
 or term logic.

1:52:39.160 --> 1:52:42.080
 So you can have logic like nodes and links,

1:52:42.080 --> 1:52:44.440
 or you can have neural like nodes and links.

1:52:44.440 --> 1:52:47.400
 You can also have procedure like nodes and links

1:52:47.400 --> 1:52:51.960
 as in say a combinatorial logic or Lambda calculus

1:52:51.960 --> 1:52:53.680
 representing programs.

1:52:53.680 --> 1:52:56.520
 So you can have nodes and links representing

1:52:56.520 --> 1:52:58.640
 many different types of semantics,

1:52:58.640 --> 1:53:00.840
 which means you could make a horrible ugly mess

1:53:00.840 --> 1:53:02.800
 or you could make a system

1:53:02.800 --> 1:53:04.280
 where these different types of knowledge

1:53:04.280 --> 1:53:06.840
 all interpenetrate and synergize

1:53:06.840 --> 1:53:08.960
 with each other beautifully, right?

1:53:08.960 --> 1:53:12.800
 So the hypergraph can contain programs.

1:53:12.800 --> 1:53:14.440
 Yeah, it can contain programs,

1:53:14.440 --> 1:53:17.960
 although in the current version,

1:53:17.960 --> 1:53:19.760
 it is a very inefficient way

1:53:19.760 --> 1:53:21.960
 to guide the execution of programs,

1:53:21.960 --> 1:53:25.000
 which is one thing that we are aiming to resolve

1:53:25.000 --> 1:53:27.520
 with our rewrite of the system now.

1:53:27.520 --> 1:53:32.520
 So what to you is the most beautiful aspect of OpenCog?

1:53:32.720 --> 1:53:34.600
 Just to you personally,

1:53:34.600 --> 1:53:38.080
 some aspect that captivates your imagination

1:53:38.080 --> 1:53:40.960
 from beauty or power?

1:53:42.000 --> 1:53:47.000
 What fascinates me is finding a common representation

1:53:48.320 --> 1:53:53.320
 that underlies abstract, declarative knowledge

1:53:53.320 --> 1:53:57.320
 and sensory knowledge and movement knowledge

1:53:57.320 --> 1:54:00.760
 and procedural knowledge and episodic knowledge,

1:54:00.760 --> 1:54:03.960
 finding the right level of representation

1:54:03.960 --> 1:54:06.560
 where all these types of knowledge are stored

1:54:06.560 --> 1:54:10.560
 in a sort of universal and interconvertible

1:54:10.560 --> 1:54:13.440
 yet practically manipulable way, right?

1:54:13.440 --> 1:54:16.840
 So to me, that's the core,

1:54:16.840 --> 1:54:18.640
 because once you've done that,

1:54:18.640 --> 1:54:20.800
 then the different learning algorithms

1:54:20.800 --> 1:54:23.640
 can help each other out. Like what you want is,

1:54:23.640 --> 1:54:25.120
 if you have a logic engine

1:54:25.120 --> 1:54:26.840
 that helps with declarative knowledge

1:54:26.840 --> 1:54:28.040
 and you have a deep neural net

1:54:28.040 --> 1:54:29.960
 that gathers perceptual knowledge,

1:54:29.960 --> 1:54:32.400
 and you have, say, an evolutionary learning system

1:54:32.400 --> 1:54:34.120
 that learns procedures,

1:54:34.120 --> 1:54:36.600
 you want these to not only interact

1:54:36.600 --> 1:54:38.880
 on the level of sharing results

1:54:38.880 --> 1:54:41.120
 and passing inputs and outputs to each other,

1:54:41.120 --> 1:54:43.680
 you want the logic engine, when it gets stuck,

1:54:43.680 --> 1:54:46.240
 to be able to share its intermediate state

1:54:46.240 --> 1:54:49.360
 with the neural net and with the evolutionary system

1:54:49.360 --> 1:54:52.240
 and with the evolutionary learning algorithm

1:54:52.240 --> 1:54:55.440
 so that they can help each other out of bottlenecks

1:54:55.440 --> 1:54:58.320
 and help each other solve combinatorial explosions

1:54:58.320 --> 1:55:02.040
 by intervening inside each other's cognitive processes.

1:55:02.040 --> 1:55:03.520
 But that can only be done

1:55:03.520 --> 1:55:05.960
 if the intermediate state of a logic engine,

1:55:05.960 --> 1:55:07.400
 the evolutionary learning engine,

1:55:07.400 --> 1:55:11.120
 and a deep neural net are represented in the same form.

1:55:11.120 --> 1:55:13.120
 And that's what we figured out how to do

1:55:13.120 --> 1:55:14.800
 by putting the right type system

1:55:14.800 --> 1:55:17.040
 on top of this weighted labeled hypergraph.

1:55:17.040 --> 1:55:19.680
 So is there, can you maybe elaborate

1:55:19.680 --> 1:55:21.880
 on what are the different characteristics

1:55:21.880 --> 1:55:26.520
 of a type system that can coexist

1:55:26.520 --> 1:55:28.760
 amongst all these different kinds of knowledge

1:55:28.760 --> 1:55:30.080
 that needs to be represented?

1:55:30.080 --> 1:55:33.320
 And is, I mean, like, is it hierarchical?

1:55:34.280 --> 1:55:36.720
 Just any kind of insights you can give

1:55:36.720 --> 1:55:37.840
 on that kind of type system?

1:55:37.840 --> 1:55:41.680
 Yeah, yeah, so this gets very nitty gritty

1:55:41.680 --> 1:55:44.000
 and mathematical, of course,

1:55:44.000 --> 1:55:47.200
 but one key part is switching

1:55:47.200 --> 1:55:50.440
 from predicate logic to term logic.

1:55:50.440 --> 1:55:51.640
 What is predicate logic?

1:55:51.640 --> 1:55:53.200
 What is term logic?

1:55:53.200 --> 1:55:56.080
 So term logic was invented by Aristotle,

1:55:56.080 --> 1:56:01.080
 or at least that's the oldest recollection we have of it.

1:56:01.320 --> 1:56:05.280
 But term logic breaks down basic logic

1:56:05.280 --> 1:56:07.480
 into basically simple links between nodes,

1:56:07.480 --> 1:56:12.480
 like an inheritance link between node A and node B.

1:56:12.480 --> 1:56:16.280
 So in term logic, the basic deduction operation

1:56:16.280 --> 1:56:21.080
 is A implies B, B implies C, therefore A implies C.

1:56:21.080 --> 1:56:22.600
 Whereas in predicate logic,

1:56:22.600 --> 1:56:24.520
 the basic operation is modus ponens,

1:56:24.520 --> 1:56:27.680
 like A implies B, therefore B.

1:56:27.680 --> 1:56:31.440
 So it's a slightly different way of breaking down logic,

1:56:31.440 --> 1:56:35.320
 but by breaking down logic into term logic,

1:56:35.320 --> 1:56:37.440
 you get a nice way of breaking logic down

1:56:37.440 --> 1:56:40.120
 into nodes and links.

1:56:40.120 --> 1:56:42.960
 So your concepts can become nodes,

1:56:42.960 --> 1:56:45.200
 the logical relations become links.

1:56:45.200 --> 1:56:46.640
 And so then inference is like,

1:56:46.640 --> 1:56:48.720
 so if this link is A implies B,

1:56:48.720 --> 1:56:50.840
 this link is B implies C,

1:56:50.840 --> 1:56:53.360
 then deduction builds a link A implies C.

1:56:53.360 --> 1:56:54.920
 And your probabilistic algorithm

1:56:54.920 --> 1:56:57.440
 can assign a certain weight there.

1:56:57.440 --> 1:57:00.040
 Now, you may also have like a Hebbian neural link

1:57:00.040 --> 1:57:03.600
 from A to C, which is the degree to which thinking,

1:57:03.600 --> 1:57:06.640
 the degree to which A being the focus of attention

1:57:06.640 --> 1:57:09.080
 should make B the focus of attention, right?

1:57:09.080 --> 1:57:10.880
 So you could have then a neural link

1:57:10.880 --> 1:57:13.720
 and you could have a symbolic,

1:57:13.720 --> 1:57:17.000
 like logical inheritance link in your term logic.

1:57:17.000 --> 1:57:19.520
 And they have separate meaning,

1:57:19.520 --> 1:57:22.960
 but they could be used to guide each other as well.

1:57:22.960 --> 1:57:26.720
 Like if there's a large amount of neural weight

1:57:26.720 --> 1:57:28.400
 on the link between A and B,

1:57:28.400 --> 1:57:30.440
 that may direct your logic engine to think about,

1:57:30.440 --> 1:57:31.320
 well, what is the relation?

1:57:31.320 --> 1:57:32.160
 Are they similar?

1:57:32.160 --> 1:57:33.880
 Is there an inheritance relation?

1:57:33.880 --> 1:57:37.400
 Are they similar in some context?

1:57:37.400 --> 1:57:39.920
 On the other hand, if there's a logical relation

1:57:39.920 --> 1:57:43.360
 between A and B, that may direct your neural component

1:57:43.360 --> 1:57:45.520
 to think, well, when I'm thinking about A,

1:57:45.520 --> 1:57:48.240
 should I be directing some attention to B also?

1:57:48.240 --> 1:57:50.160
 Because there's a logical relation.

1:57:50.160 --> 1:57:53.000
 So in terms of logic,

1:57:53.000 --> 1:57:54.320
 there's a lot of thought that went into

1:57:54.320 --> 1:57:58.280
 how do you break down logic relations,

1:57:58.280 --> 1:58:02.320
 including basic sort of propositional logic relations

1:58:02.320 --> 1:58:04.160
 as Aristotelian term logic deals with,

1:58:04.160 --> 1:58:07.080
 and then quantifier logic relations also.

1:58:07.080 --> 1:58:10.920
 How do you break those down elegantly into a hypergraph?

1:58:10.920 --> 1:58:13.480
 Because you, I mean, you can boil logic expression

1:58:13.480 --> 1:58:14.840
 into a graph in many different ways.

1:58:14.840 --> 1:58:16.680
 Many of them are very ugly, right?

1:58:16.680 --> 1:58:19.200
 We tried to find elegant ways

1:58:19.200 --> 1:58:22.600
 of sort of hierarchically breaking down

1:58:22.600 --> 1:58:26.880
 complex logic expression into nodes and links.

1:58:26.880 --> 1:58:31.400
 So that if you have say different nodes representing,

1:58:31.400 --> 1:58:34.200
 Ben, AI, Lex, interview or whatever,

1:58:34.200 --> 1:58:36.800
 the logic relations between those things

1:58:36.800 --> 1:58:40.480
 are compact in the node and link representation.

1:58:40.480 --> 1:58:42.080
 So that when you have a neural net acting

1:58:42.080 --> 1:58:43.960
 on the same nodes and links,

1:58:43.960 --> 1:58:45.760
 the neural net and the logic engine

1:58:45.760 --> 1:58:48.240
 can sort of interoperate with each other.

1:58:48.240 --> 1:58:49.920
 And also interpretable by humans.

1:58:49.920 --> 1:58:51.400
 Is that an important?

1:58:51.400 --> 1:58:52.240
 That's tough.

1:58:52.240 --> 1:58:54.600
 Yeah, in simple cases, it's interpretable by humans.

1:58:54.600 --> 1:58:59.600
 But honestly, I would say logic systems

1:58:59.600 --> 1:59:04.600
 I would say logic systems give more potential

1:59:05.440 --> 1:59:09.800
 for transparency and comprehensibility

1:59:09.800 --> 1:59:11.640
 than neural net systems,

1:59:11.640 --> 1:59:12.840
 but you still have to work at it.

1:59:12.840 --> 1:59:16.680
 Because I mean, if I show you a predicate logic proposition

1:59:16.680 --> 1:59:20.080
 with like 500 nested universal and existential quantifiers

1:59:20.080 --> 1:59:23.680
 and 217 variables, that's no more comprehensible

1:59:23.680 --> 1:59:26.560
 than the weight metrics of a neural network, right?

1:59:26.560 --> 1:59:28.560
 So I'd say the logic expressions

1:59:28.560 --> 1:59:30.920
 that AI learns from its experience

1:59:30.920 --> 1:59:33.440
 are mostly totally opaque to human beings

1:59:33.440 --> 1:59:36.200
 and maybe even harder to understand than neural net.

1:59:36.200 --> 1:59:37.440
 Because I mean, when you have multiple

1:59:37.440 --> 1:59:38.960
 nested quantifier bindings,

1:59:38.960 --> 1:59:41.520
 it's a very high level of abstraction.

1:59:41.520 --> 1:59:42.680
 There is a difference though,

1:59:42.680 --> 1:59:46.880
 in that within logic, it's a little more straightforward

1:59:46.880 --> 1:59:49.120
 to pose the problem of like normalize this

1:59:49.120 --> 1:59:51.080
 and boil this down to a certain form.

1:59:51.080 --> 1:59:52.720
 I mean, you can do that in neural nets too.

1:59:52.720 --> 1:59:55.680
 Like you can distill a neural net to a simpler form,

1:59:55.680 --> 1:59:57.280
 but that's more often done to make a neural net

1:59:57.280 --> 1:59:59.720
 that'll run on an embedded device or something.

1:59:59.720 --> 2:00:03.440
 It's harder to distill a net to a comprehensible form

2:00:03.440 --> 2:00:05.640
 than it is to simplify a logic expression

2:00:05.640 --> 2:00:08.600
 to a comprehensible form, but it doesn't come for free.

2:00:08.600 --> 2:00:13.040
 Like what's in the AI's mind is incomprehensible

2:00:13.040 --> 2:00:15.720
 to a human unless you do some special work

2:00:15.720 --> 2:00:16.880
 to make it comprehensible.

2:00:16.880 --> 2:00:20.400
 So on the procedural side, there's some different

2:00:20.400 --> 2:00:23.000
 and sort of interesting voodoo there.

2:00:23.000 --> 2:00:25.800
 I mean, if you're familiar in computer science,

2:00:25.800 --> 2:00:27.800
 there's something called the Curry Howard correspondence,

2:00:27.800 --> 2:00:30.920
 which is a one to one mapping between proofs and programs.

2:00:30.920 --> 2:00:33.520
 So every program can be mapped into a proof.

2:00:33.520 --> 2:00:35.960
 Every proof can be mapped into a program.

2:00:35.960 --> 2:00:37.800
 You can model this using category theory

2:00:37.800 --> 2:00:40.960
 and a bunch of nice math,

2:00:40.960 --> 2:00:43.280
 but we wanna make that practical, right?

2:00:43.280 --> 2:00:46.520
 So that if you have an executable program

2:00:46.520 --> 2:00:49.960
 that like moves the robot's arm or figures out

2:00:49.960 --> 2:00:51.840
 in what order to say things in a dialogue,

2:00:51.840 --> 2:00:55.840
 that's a procedure represented in OpenCog's hypergraph.

2:00:55.840 --> 2:01:00.120
 But if you wanna reason on how to improve that procedure,

2:01:00.120 --> 2:01:03.080
 you need to map that procedure into logic

2:01:03.080 --> 2:01:05.520
 using Curry Howard isomorphism.

2:01:05.520 --> 2:01:09.320
 So then the logic engine can reason

2:01:09.320 --> 2:01:11.120
 about how to improve that procedure

2:01:11.120 --> 2:01:14.080
 and then map that back into the procedural representation

2:01:14.080 --> 2:01:16.160
 that is efficient for execution.

2:01:16.160 --> 2:01:18.800
 So again, that comes down to not just

2:01:18.800 --> 2:01:21.440
 can you make your procedure into a bunch of nodes and links?

2:01:21.440 --> 2:01:23.280
 Cause I mean, that can be done trivially.

2:01:23.280 --> 2:01:26.440
 A C++ compiler has nodes and links inside it.

2:01:26.440 --> 2:01:27.960
 Can you boil down your procedure

2:01:27.960 --> 2:01:29.840
 into a bunch of nodes and links

2:01:29.840 --> 2:01:32.560
 in a way that's like hierarchically decomposed

2:01:32.560 --> 2:01:33.680
 and simple enough?

2:01:33.680 --> 2:01:34.520
 It can reason about.

2:01:34.520 --> 2:01:37.040
 Yeah, yeah, that given the resource constraints at hand,

2:01:37.040 --> 2:01:40.920
 you can map it back and forth to your term logic,

2:01:40.920 --> 2:01:42.080
 like fast enough

2:01:42.080 --> 2:01:45.200
 and without having a bloated logic expression, right?

2:01:45.200 --> 2:01:47.000
 So there's just a lot of,

2:01:48.320 --> 2:01:50.360
 there's a lot of nitty gritty particulars there,

2:01:50.360 --> 2:01:54.520
 but by the same token, if you ask a chip designer,

2:01:54.520 --> 2:01:58.560
 like how do you make the Intel I7 chip so good?

2:01:58.560 --> 2:02:02.560
 There's a long list of technical answers there,

2:02:02.560 --> 2:02:04.800
 which will take a while to go through, right?

2:02:04.800 --> 2:02:06.640
 And this has been decades of work.

2:02:06.640 --> 2:02:10.880
 I mean, the first AI system of this nature I tried to build

2:02:10.880 --> 2:02:13.440
 was called WebMind in the mid 1990s.

2:02:13.440 --> 2:02:15.600
 And we had a big graph,

2:02:15.600 --> 2:02:18.880
 a big graph operating in RAM implemented with Java 1.1,

2:02:18.880 --> 2:02:21.800
 which was a terrible, terrible implementation idea.

2:02:21.800 --> 2:02:25.960
 And then each node had its own processing.

2:02:25.960 --> 2:02:27.440
 So like that there,

2:02:27.440 --> 2:02:29.560
 the core loop looped through all nodes in the network

2:02:29.560 --> 2:02:32.920
 and let each node enact what its little thing was doing.

2:02:32.920 --> 2:02:35.880
 And we had logic and neural nets in there,

2:02:35.880 --> 2:02:38.400
 but an evolutionary learning,

2:02:38.400 --> 2:02:40.760
 but we hadn't done enough of the math

2:02:40.760 --> 2:02:43.400
 to get them to operate together very cleanly.

2:02:43.400 --> 2:02:46.240
 So it was really, it was quite a horrible mess.

2:02:46.240 --> 2:02:49.400
 So as well as shifting an implementation

2:02:49.400 --> 2:02:51.840
 where the graph is its own object

2:02:51.840 --> 2:02:54.720
 and the agents are separately scheduled,

2:02:54.720 --> 2:02:56.800
 we've also done a lot of work

2:02:56.800 --> 2:02:58.400
 on how do you represent programs?

2:02:58.400 --> 2:03:00.800
 How do you represent procedures?

2:03:00.800 --> 2:03:03.640
 You know, how do you represent genotypes for evolution

2:03:03.640 --> 2:03:06.640
 in a way that the interoperability

2:03:06.640 --> 2:03:09.000
 between the different types of learning

2:03:09.000 --> 2:03:11.720
 associated with these different types of knowledge

2:03:11.720 --> 2:03:13.040
 actually works?

2:03:13.040 --> 2:03:14.960
 And that's been quite difficult.

2:03:14.960 --> 2:03:18.600
 It's taken decades and it's totally off to the side

2:03:18.600 --> 2:03:23.080
 of what the commercial mainstream of the AI field is doing,

2:03:23.080 --> 2:03:27.640
 which isn't thinking about representation at all really.

2:03:27.640 --> 2:03:30.800
 Although you could see like in the DNC,

2:03:30.800 --> 2:03:32.320
 they had to think a little bit about

2:03:32.320 --> 2:03:33.880
 how do you make representation of a map

2:03:33.880 --> 2:03:36.680
 in this memory matrix work together

2:03:36.680 --> 2:03:38.160
 with the representation needed

2:03:38.160 --> 2:03:40.240
 for say visual pattern recognition

2:03:40.240 --> 2:03:42.120
 in the hierarchical neural network.

2:03:42.120 --> 2:03:45.120
 But I would say we have taken that direction

2:03:45.120 --> 2:03:47.920
 of taking the types of knowledge you need

2:03:47.920 --> 2:03:49.120
 for different types of learning,

2:03:49.120 --> 2:03:52.040
 like declarative, procedural, attentional,

2:03:52.040 --> 2:03:55.520
 and how do you make these types of knowledge represent

2:03:55.520 --> 2:03:58.160
 in a way that allows cross learning

2:03:58.160 --> 2:04:00.200
 across these different types of memory.

2:04:00.200 --> 2:04:03.920
 We've been prototyping and experimenting with this

2:04:03.920 --> 2:04:07.560
 within OpenCog and before that WebMind

2:04:07.560 --> 2:04:10.640
 since the mid 1990s.

2:04:10.640 --> 2:04:13.840
 Now, disappointingly to all of us,

2:04:13.840 --> 2:04:18.400
 this has not yet been cashed out in an AGI system, right?

2:04:18.400 --> 2:04:20.640
 I mean, we've used this system

2:04:20.640 --> 2:04:22.440
 within our consulting business.

2:04:22.440 --> 2:04:24.320
 So we've built natural language processing

2:04:24.320 --> 2:04:27.760
 and robot control and financial analysis.

2:04:27.760 --> 2:04:31.160
 We've built a bunch of sort of vertical market specific

2:04:31.160 --> 2:04:33.600
 proprietary AI projects.

2:04:33.600 --> 2:04:36.720
 They use OpenCog on the backend,

2:04:36.720 --> 2:04:39.560
 but we haven't, that's not the AGI goal, right?

2:04:39.560 --> 2:04:42.680
 It's interesting, but it's not the AGI goal.

2:04:42.680 --> 2:04:47.680
 So now what we're looking at with our rebuild of the system.

2:04:48.520 --> 2:04:49.360
 2.0.

2:04:49.360 --> 2:04:51.400
 Yeah, we're also calling it True AGI.

2:04:51.400 --> 2:04:54.800
 So we're not quite sure what the name is yet.

2:04:54.800 --> 2:04:57.480
 We made a website for trueagi.io,

2:04:57.480 --> 2:04:59.840
 but we haven't put anything on there yet.

2:04:59.840 --> 2:05:02.160
 We may come up with an even better name.

2:05:02.160 --> 2:05:04.960
 It's kind of like the real AI starting point

2:05:04.960 --> 2:05:05.800
 for your AGI book.

2:05:05.800 --> 2:05:06.920
 Yeah, but I like True better

2:05:06.920 --> 2:05:09.760
 because True has like, you can be true hearted, right?

2:05:09.760 --> 2:05:11.040
 You can be true to your girlfriend.

2:05:11.040 --> 2:05:15.720
 So True has a number and it also has logic in it, right?

2:05:15.720 --> 2:05:18.280
 Because logic is a key part of the system.

2:05:18.280 --> 2:05:22.400
 So yeah, with the True AGI system,

2:05:22.400 --> 2:05:25.400
 we're sticking with the same basic architecture,

2:05:25.400 --> 2:05:29.640
 but we're trying to build on what we've learned.

2:05:29.640 --> 2:05:32.360
 And one thing we've learned is that,

2:05:32.360 --> 2:05:36.920
 we need type checking among dependent types

2:05:36.920 --> 2:05:38.040
 to be much faster

2:05:38.040 --> 2:05:41.120
 and among probabilistic dependent types to be much faster.

2:05:41.120 --> 2:05:43.600
 So as it is now,

2:05:43.600 --> 2:05:47.120
 you can have complex types on the nodes and links.

2:05:47.120 --> 2:05:48.360
 But if you wanna put,

2:05:48.360 --> 2:05:51.280
 like if you want types to be first class citizens,

2:05:51.280 --> 2:05:53.800
 so that you can have the types can be variables

2:05:53.800 --> 2:05:55.680
 and then you do type checking

2:05:55.680 --> 2:05:58.040
 among complex higher order types.

2:05:58.040 --> 2:06:00.960
 You can do that in the system now, but it's very slow.

2:06:00.960 --> 2:06:02.560
 This is stuff like it's done

2:06:02.560 --> 2:06:05.360
 in cutting edge program languages like Agda or something,

2:06:05.360 --> 2:06:07.400
 these obscure research languages.

2:06:07.400 --> 2:06:08.600
 On the other hand,

2:06:08.600 --> 2:06:11.240
 we've been doing a lot tying together deep neural nets

2:06:11.240 --> 2:06:12.360
 with symbolic learning.

2:06:12.360 --> 2:06:15.200
 So we did a project for Cisco, for example,

2:06:15.200 --> 2:06:17.440
 which was on, this was street scene analysis,

2:06:17.440 --> 2:06:18.600
 but they had deep neural models

2:06:18.600 --> 2:06:21.000
 for a bunch of cameras watching street scenes,

2:06:21.000 --> 2:06:23.400
 but they trained a different model for each camera

2:06:23.400 --> 2:06:24.840
 because they couldn't get the transfer learning

2:06:24.840 --> 2:06:27.040
 to work between camera A and camera B.

2:06:27.040 --> 2:06:29.040
 So we took what came out of all the deep neural models

2:06:29.040 --> 2:06:30.400
 for the different cameras,

2:06:30.400 --> 2:06:33.440
 we fed it into an open called symbolic representation.

2:06:33.440 --> 2:06:36.280
 Then we did some pattern mining and some reasoning

2:06:36.280 --> 2:06:38.120
 on what came out of all the different cameras

2:06:38.120 --> 2:06:39.480
 within the symbolic graph.

2:06:39.480 --> 2:06:42.040
 And that worked well for that application.

2:06:42.040 --> 2:06:45.880
 I mean, Hugo Latapie from Cisco gave a talk touching on that

2:06:45.880 --> 2:06:48.760
 at last year's AGI conference, it was in Shenzhen.

2:06:48.760 --> 2:06:51.000
 On the other hand, we learned from there,

2:06:51.000 --> 2:06:53.280
 it was kind of clunky to get the deep neural models

2:06:53.280 --> 2:06:55.640
 to work well with the symbolic system

2:06:55.640 --> 2:06:58.560
 because we were using torch.

2:06:58.560 --> 2:07:03.560
 And torch keeps a sort of state computation graph,

2:07:03.560 --> 2:07:05.280
 but you needed like real time access

2:07:05.280 --> 2:07:07.640
 to that computation graph within our hypergraph.

2:07:07.640 --> 2:07:10.640
 And we certainly did it,

2:07:10.640 --> 2:07:13.080
 Alexey Polopov who leads our St. Petersburg team

2:07:13.080 --> 2:07:16.480
 wrote a great paper on cognitive modules in OpenCog

2:07:16.480 --> 2:07:17.720
 explaining sort of how do you deal

2:07:17.720 --> 2:07:19.960
 with the torch compute graph inside OpenCog.

2:07:19.960 --> 2:07:22.840
 But in the end we realized like,

2:07:22.840 --> 2:07:25.400
 that just hadn't been one of our design thoughts

2:07:25.400 --> 2:07:27.240
 when we built OpenCog, right?

2:07:27.240 --> 2:07:30.680
 So between wanting really fast dependent type checking

2:07:30.680 --> 2:07:33.640
 and wanting much more efficient interoperation

2:07:33.640 --> 2:07:35.160
 between the computation graphs

2:07:35.160 --> 2:07:37.720
 of deep neural net frameworks and OpenCog's hypergraph

2:07:37.720 --> 2:07:40.000
 and adding on top of that,

2:07:40.000 --> 2:07:42.480
 wanting to more effectively run an OpenCog hypergraph

2:07:42.480 --> 2:07:45.200
 distributed across RAM in 10,000 machines,

2:07:45.200 --> 2:07:47.280
 which is we're doing dozens of machines now,

2:07:47.280 --> 2:07:50.720
 but it's just not, we didn't architect it

2:07:50.720 --> 2:07:53.080
 with that sort of modern scalability in mind.

2:07:53.080 --> 2:07:56.280
 So these performance requirements are what have driven us

2:07:56.280 --> 2:08:00.520
 to want to rearchitect the base,

2:08:00.520 --> 2:08:05.320
 but the core AGI paradigm doesn't really change.

2:08:05.320 --> 2:08:07.760
 Like the mathematics is the same.

2:08:07.760 --> 2:08:11.440
 It's just, we can't scale to the level that we want

2:08:11.440 --> 2:08:13.880
 in terms of distributed processing

2:08:13.880 --> 2:08:16.280
 or speed of various kinds of processing

2:08:16.280 --> 2:08:19.160
 with the current infrastructure

2:08:19.160 --> 2:08:22.880
 that was built in the phase 2001 to 2008,

2:08:22.880 --> 2:08:26.120
 which is hardly shocking.

2:08:26.120 --> 2:08:27.880
 Well, I mean, the three things you mentioned

2:08:27.880 --> 2:08:28.720
 are really interesting.

2:08:28.720 --> 2:08:32.320
 So what do you think about in terms of interoperability

2:08:32.320 --> 2:08:36.320
 communicating with computational graph of neural networks?

2:08:36.320 --> 2:08:38.480
 What do you think about the representations

2:08:38.480 --> 2:08:40.680
 that neural networks form?

2:08:40.680 --> 2:08:42.920
 They're bad, but there's many ways

2:08:42.920 --> 2:08:44.360
 that you could deal with that.

2:08:44.360 --> 2:08:46.880
 So I've been wrestling with this a lot

2:08:46.880 --> 2:08:49.920
 in some work on supervised grammar induction,

2:08:49.920 --> 2:08:52.120
 and I have a simple paper on that.

2:08:52.120 --> 2:08:55.400
 They'll give it the next AGI conference,

2:08:55.400 --> 2:08:58.200
 online portion of which is next week, actually.

2:08:58.200 --> 2:09:00.400
 What is grammar induction?

2:09:00.400 --> 2:09:02.560
 So this isn't AGI either,

2:09:02.560 --> 2:09:05.200
 but it's sort of on the verge

2:09:05.200 --> 2:09:08.280
 between narrow AI and AGI or something.

2:09:08.280 --> 2:09:11.320
 Unsupervised grammar induction is the problem.

2:09:11.320 --> 2:09:15.400
 Throw your AI system, a huge body of text,

2:09:15.400 --> 2:09:18.160
 and have it learn the grammar of the language

2:09:18.160 --> 2:09:19.400
 that produced that text.

2:09:20.280 --> 2:09:22.600
 So you're not giving it labeled examples.

2:09:22.600 --> 2:09:24.440
 So you're not giving it like a thousand sentences

2:09:24.440 --> 2:09:27.120
 where the parses were marked up by graduate students.

2:09:27.120 --> 2:09:30.280
 So it's just got to infer the grammar from the text.

2:09:30.280 --> 2:09:33.440
 It's like the Rosetta Stone, but worse, right?

2:09:33.440 --> 2:09:35.320
 Because you only have the one language,

2:09:35.320 --> 2:09:37.160
 and you have to figure out what is the grammar.

2:09:37.160 --> 2:09:41.440
 So that's not really AGI because,

2:09:41.440 --> 2:09:44.360
 I mean, the way a human learns language is not that, right?

2:09:44.360 --> 2:09:47.720
 I mean, we learn from language that's used in context.

2:09:47.720 --> 2:09:49.320
 So it's a social embodied thing.

2:09:49.320 --> 2:09:53.520
 We see how a given sentence is grounded in observation.

2:09:53.520 --> 2:09:55.200
 There's an interactive element, I guess.

2:09:55.200 --> 2:09:56.520
 Yeah, yeah, yeah.

2:09:56.520 --> 2:10:00.360
 On the other hand, so I'm more interested in that.

2:10:00.360 --> 2:10:02.960
 I'm more interested in making an AGI system learn language

2:10:02.960 --> 2:10:05.560
 from its social and embodied experience.

2:10:05.560 --> 2:10:08.240
 On the other hand, that's also more of a pain to do,

2:10:08.240 --> 2:10:10.640
 and that would lead us into Hanson Robotics

2:10:10.640 --> 2:10:12.080
 and their robotics work I've known much.

2:10:12.080 --> 2:10:14.600
 We'll talk about it in a few minutes.

2:10:14.600 --> 2:10:17.120
 But just as an intellectual exercise,

2:10:17.120 --> 2:10:18.840
 as a learning exercise,

2:10:18.840 --> 2:10:22.480
 trying to learn grammar from a corpus

2:10:22.480 --> 2:10:24.560
 is very, very interesting, right?

2:10:24.560 --> 2:10:27.520
 And that's been a field in AI for a long time.

2:10:27.520 --> 2:10:29.200
 No one can do it very well.

2:10:29.200 --> 2:10:32.080
 So we've been looking at transformer neural networks

2:10:32.080 --> 2:10:35.760
 and tree transformers, which are amazing.

2:10:35.760 --> 2:10:39.080
 These came out of Google Brain, actually.

2:10:39.080 --> 2:10:41.920
 And actually on that team was Lucas Kaiser,

2:10:41.920 --> 2:10:44.080
 who used to work for me in the one,

2:10:44.080 --> 2:10:46.960
 the period 2005 through eight or something.

2:10:46.960 --> 2:10:50.200
 So it's been fun to see my former

2:10:50.200 --> 2:10:52.760
 sort of AGI employees disperse and do

2:10:52.760 --> 2:10:54.080
 all these amazing things.

2:10:54.080 --> 2:10:56.080
 Way too many sucked into Google, actually.

2:10:56.080 --> 2:10:57.640
 Well, yeah, anyway.

2:10:57.640 --> 2:10:58.960
 We'll talk about that too.

2:10:58.960 --> 2:11:00.640
 Lucas Kaiser and a bunch of these guys,

2:11:00.640 --> 2:11:03.200
 they create transformer networks,

2:11:03.200 --> 2:11:05.480
 that classic paper like attention is all you need

2:11:05.480 --> 2:11:08.160
 and all these things following on from that.

2:11:08.160 --> 2:11:10.160
 So we're looking at transformer networks.

2:11:10.160 --> 2:11:13.520
 And like, these are able to,

2:11:13.520 --> 2:11:16.480
 I mean, this is what underlies GPT2 and GPT3 and so on,

2:11:16.480 --> 2:11:18.120
 which are very, very cool

2:11:18.120 --> 2:11:20.320
 and have absolutely no cognitive understanding

2:11:20.320 --> 2:11:21.680
 of any of the texts they're looking at.

2:11:21.680 --> 2:11:24.960
 Like they're very intelligent idiots, right?

2:11:24.960 --> 2:11:28.080
 So sorry to take, but this small, I'll bring this back,

2:11:28.080 --> 2:11:31.760
 but do you think GPT3 understands language?

2:11:31.760 --> 2:11:34.080
 No, no, it understands nothing.

2:11:34.080 --> 2:11:35.320
 It's a complete idiot.

2:11:35.320 --> 2:11:36.720
 But it's a brilliant idiot.

2:11:36.720 --> 2:11:40.520
 You don't think GPT20 will understand language?

2:11:40.520 --> 2:11:42.240
 No, no, no.

2:11:42.240 --> 2:11:45.160
 So size is not gonna buy you understanding.

2:11:45.160 --> 2:11:48.840
 And any more than a faster car is gonna get you to Mars.

2:11:48.840 --> 2:11:50.920
 It's a completely different kind of thing.

2:11:50.920 --> 2:11:54.280
 I mean, these networks are very cool.

2:11:54.280 --> 2:11:55.520
 And as an entrepreneur,

2:11:55.520 --> 2:11:57.760
 I can see many highly valuable uses for them.

2:11:57.760 --> 2:12:01.080
 And as an artist, I love them, right?

2:12:01.080 --> 2:12:05.240
 So I mean, we're using our own neural model,

2:12:05.240 --> 2:12:06.560
 which is along those lines

2:12:06.560 --> 2:12:09.000
 to control the Philip K. Dick robot now.

2:12:09.000 --> 2:12:12.200
 And it's amazing to like train a neural model

2:12:12.200 --> 2:12:14.000
 on the robot Philip K. Dick

2:12:14.000 --> 2:12:15.840
 and see it come up with like crazed,

2:12:15.840 --> 2:12:18.400
 stoned philosopher pronouncements,

2:12:18.400 --> 2:12:21.440
 very much like what Philip K. Dick might've said, right?

2:12:21.440 --> 2:12:24.840
 Like these models are super cool.

2:12:24.840 --> 2:12:27.720
 And I'm working with Hanson Robotics now

2:12:27.720 --> 2:12:30.600
 on using a similar, but more sophisticated one for Sophia,

2:12:30.600 --> 2:12:34.080
 which we haven't launched yet.

2:12:34.080 --> 2:12:36.080
 But so I think it's cool.

2:12:36.080 --> 2:12:39.480
 But no, these are recognizing a large number

2:12:39.480 --> 2:12:42.200
 of shallow patterns.

2:12:42.200 --> 2:12:44.840
 They're not forming an abstract representation.

2:12:44.840 --> 2:12:47.120
 And that's the point I was coming to

2:12:47.120 --> 2:12:50.680
 when we're looking at grammar induction,

2:12:50.680 --> 2:12:53.520
 we tried to mine patterns out of the structure

2:12:53.520 --> 2:12:55.880
 of the transformer network.

2:12:55.880 --> 2:12:59.600
 And you can, but the patterns aren't what you want.

2:12:59.600 --> 2:13:00.600
 They're nasty.

2:13:00.600 --> 2:13:03.200
 So I mean, if you do supervised learning,

2:13:03.200 --> 2:13:04.560
 if you look at sentences where you know

2:13:04.560 --> 2:13:06.520
 the correct parts of a sentence,

2:13:06.520 --> 2:13:09.120
 you can learn a matrix that maps

2:13:09.120 --> 2:13:12.240
 between the internal representation of the transformer

2:13:12.240 --> 2:13:14.120
 and the parse of the sentence.

2:13:14.120 --> 2:13:16.120
 And so then you can actually train something

2:13:16.120 --> 2:13:18.440
 that will output the sentence parse

2:13:18.440 --> 2:13:20.680
 from the transformer network's internal state.

2:13:20.680 --> 2:13:24.720
 And we did this, I think Christopher Manning,

2:13:24.720 --> 2:13:27.120
 some others have not done this also.

2:13:28.080 --> 2:13:30.600
 But I mean, what you get is that the representation

2:13:30.600 --> 2:13:33.200
 is hardly ugly and is scattered all over the network

2:13:33.200 --> 2:13:34.920
 and doesn't look like the rules of grammar

2:13:34.920 --> 2:13:37.240
 that you know are the right rules of grammar, right?

2:13:37.240 --> 2:13:38.240
 It's kind of ugly.

2:13:38.240 --> 2:13:41.440
 So what we're actually doing is we're using

2:13:41.440 --> 2:13:44.280
 a symbolic grammar learning algorithm,

2:13:44.280 --> 2:13:46.760
 but we're using the transformer neural network

2:13:46.760 --> 2:13:48.880
 as a sentence probability oracle.

2:13:48.880 --> 2:13:52.120
 So like if you have a rule of grammar

2:13:52.120 --> 2:13:54.800
 and you aren't sure if it's a correct rule of grammar or not,

2:13:54.800 --> 2:13:56.440
 you can generate a bunch of sentences

2:13:56.440 --> 2:13:58.040
 using that rule of grammar

2:13:58.040 --> 2:14:00.880
 and a bunch of sentences violating that rule of grammar.

2:14:00.880 --> 2:14:04.480
 And you can see the transformer model

2:14:04.480 --> 2:14:06.720
 doesn't think the sentences obeying the rule of grammar

2:14:06.720 --> 2:14:08.280
 are more probable than the sentences

2:14:08.280 --> 2:14:10.080
 disobeying the rule of grammar.

2:14:10.080 --> 2:14:11.840
 So in that way, you can use the neural model

2:14:11.840 --> 2:14:13.840
 as a sense probability oracle

2:14:13.840 --> 2:14:18.840
 to guide a symbolic grammar learning process.

2:14:19.960 --> 2:14:24.000
 And that seems to work better than trying to milk

2:14:24.000 --> 2:14:25.840
 the grammar out of the neural network

2:14:25.840 --> 2:14:26.760
 that doesn't have it in there.

2:14:26.760 --> 2:14:29.480
 So I think the thing is these neural nets

2:14:29.480 --> 2:14:32.880
 are not getting a semantically meaningful representation

2:14:32.880 --> 2:14:35.360
 internally by and large.

2:14:35.360 --> 2:14:38.120
 So one line of research is to try to get them to do that.

2:14:38.120 --> 2:14:40.000
 And InfoGAN was trying to do that.

2:14:40.000 --> 2:14:43.040
 So like if you look back like two years ago,

2:14:43.040 --> 2:14:45.280
 there was all these papers on like at Edward,

2:14:45.280 --> 2:14:47.400
 this probabilistic programming neural net framework

2:14:47.400 --> 2:14:49.640
 that Google had, which came out of InfoGAN.

2:14:49.640 --> 2:14:53.720
 So the idea there was like you could train

2:14:53.720 --> 2:14:55.600
 an InfoGAN neural net model,

2:14:55.600 --> 2:14:57.200
 which is a generative associative network

2:14:57.200 --> 2:14:59.200
 to recognize and generate faces.

2:14:59.200 --> 2:15:02.160
 And the model would automatically learn a variable

2:15:02.160 --> 2:15:04.400
 for how long the nose is and automatically learn a variable

2:15:04.400 --> 2:15:05.760
 for how wide the eyes are

2:15:05.760 --> 2:15:08.040
 or how big the lips are or something, right?

2:15:08.040 --> 2:15:11.040
 So it automatically learned these variables,

2:15:11.040 --> 2:15:12.480
 which have a semantic meaning.

2:15:12.480 --> 2:15:15.320
 So that was a rare case where a neural net

2:15:15.320 --> 2:15:18.080
 trained with a fairly standard GAN method

2:15:18.080 --> 2:15:20.880
 was able to actually learn the semantic representation.

2:15:20.880 --> 2:15:23.240
 So for many years, many of us tried to take that

2:15:23.240 --> 2:15:27.200
 the next step and get a GAN type neural network

2:15:27.200 --> 2:15:31.680
 that would have not just a list of semantic latent variables,

2:15:31.680 --> 2:15:33.960
 but would have say a Bayes net of semantic latent variables

2:15:33.960 --> 2:15:35.440
 with dependencies between them.

2:15:35.440 --> 2:15:38.840
 The whole programming framework Edward was made for that.

2:15:38.840 --> 2:15:40.720
 I mean, no one got it to work, right?

2:15:40.720 --> 2:15:41.560
 And it could be.

2:15:41.560 --> 2:15:42.960
 Do you think it's possible?

2:15:42.960 --> 2:15:43.800
 Yeah, do you think?

2:15:43.800 --> 2:15:44.760
 I don't know.

2:15:44.760 --> 2:15:47.280
 It might be that back propagation just won't work for it

2:15:47.280 --> 2:15:49.720
 because the gradients are too screwed up.

2:15:49.720 --> 2:15:52.000
 Maybe you could get it to work using CMAES

2:15:52.000 --> 2:15:54.840
 or some like floating point evolutionary algorithm.

2:15:54.840 --> 2:15:57.000
 We tried, we didn't get it to work.

2:15:57.000 --> 2:16:01.360
 Eventually we just paused that rather than gave it up.

2:16:01.360 --> 2:16:04.000
 We paused that and said, well, okay, let's try

2:16:04.000 --> 2:16:08.640
 more innovative ways to learn implicit,

2:16:08.640 --> 2:16:11.000
 to learn what are the representations implicit

2:16:11.000 --> 2:16:13.640
 in that network without trying to make it grow

2:16:13.640 --> 2:16:14.720
 inside that network.

2:16:14.720 --> 2:16:19.720
 And I described how we're doing that in language.

2:16:19.720 --> 2:16:21.440
 You can do similar things in vision, right?

2:16:21.440 --> 2:16:22.280
 So what?

2:16:22.280 --> 2:16:23.360
 Use it as an oracle.

2:16:23.360 --> 2:16:24.200
 Yeah, yeah, yeah.

2:16:24.200 --> 2:16:26.240
 So you can, that's one way is that you use

2:16:26.240 --> 2:16:29.120
 a structure learning algorithm, which is symbolic.

2:16:29.120 --> 2:16:32.480
 And then you use the deep neural net as an oracle

2:16:32.480 --> 2:16:34.240
 to guide the structure learning algorithm.

2:16:34.240 --> 2:16:37.880
 The other way to do it is like Infogam was trying to do

2:16:37.880 --> 2:16:40.040
 and try to tweak the neural network

2:16:40.040 --> 2:16:43.760
 to have the symbolic representation inside it.

2:16:43.760 --> 2:16:46.440
 I tend to think what the brain is doing

2:16:46.440 --> 2:16:51.440
 is more like using the deep neural net type thing

2:16:51.680 --> 2:16:52.520
 as an oracle.

2:16:52.520 --> 2:16:56.680
 I think the visual cortex or the cerebellum

2:16:56.680 --> 2:17:00.280
 are probably learning a non semantically meaningful

2:17:00.280 --> 2:17:02.400
 opaque tangled representation.

2:17:02.400 --> 2:17:04.600
 And then when they interface with the more cognitive parts

2:17:04.600 --> 2:17:08.080
 of the cortex, the cortex is sort of using those

2:17:08.080 --> 2:17:10.720
 as an oracle and learning the abstract representation.

2:17:10.720 --> 2:17:13.200
 So if you do sports, say take for example,

2:17:13.200 --> 2:17:15.240
 serving in tennis, right?

2:17:15.240 --> 2:17:17.680
 I mean, my tennis serve is okay, not great,

2:17:17.680 --> 2:17:19.760
 but I learned it by trial and error, right?

2:17:19.760 --> 2:17:22.120
 And I mean, I learned music by trial and error too.

2:17:22.120 --> 2:17:25.960
 I just sit down and play, but then if you're an athlete,

2:17:25.960 --> 2:17:27.080
 which I'm not a good athlete,

2:17:27.080 --> 2:17:30.360
 I mean, then you'll watch videos of yourself serving

2:17:30.360 --> 2:17:32.760
 and your coach will help you think about what you're doing

2:17:32.760 --> 2:17:35.040
 and you'll then form a declarative representation,

2:17:35.040 --> 2:17:37.160
 but your cerebellum maybe didn't have

2:17:37.160 --> 2:17:38.640
 a declarative representation.

2:17:38.640 --> 2:17:43.560
 Same way with music, like I will hear something in my head,

2:17:43.560 --> 2:17:46.960
 I'll sit down and play the thing like I heard it.

2:17:46.960 --> 2:17:51.000
 And then I will try to study what my fingers did

2:17:51.000 --> 2:17:52.760
 to see like, what did you just play?

2:17:52.760 --> 2:17:55.600
 Like how did you do that, right?

2:17:55.600 --> 2:17:57.720
 Because if you're composing,

2:17:57.720 --> 2:17:59.720
 you may wanna see how you did it

2:17:59.720 --> 2:18:02.680
 and then declaratively morph that in some way

2:18:02.680 --> 2:18:05.240
 that your fingers wouldn't think of, right?

2:18:05.240 --> 2:18:10.240
 But the physiological movement may come out of some opaque,

2:18:10.280 --> 2:18:14.440
 like cerebellar reinforcement learned thing, right?

2:18:14.440 --> 2:18:17.680
 And so that's, I think trying to milk the structure

2:18:17.680 --> 2:18:19.320
 of a neural net by treating it as an oracle,

2:18:19.320 --> 2:18:23.960
 maybe more like how your declarative mind post processes

2:18:23.960 --> 2:18:27.760
 what your visual or motor cortex.

2:18:27.760 --> 2:18:29.400
 I mean, in vision, it's the same way,

2:18:29.400 --> 2:18:33.520
 like you can recognize beautiful art

2:18:34.800 --> 2:18:36.760
 much better than you can say why

2:18:36.760 --> 2:18:38.520
 you think that piece of art is beautiful.

2:18:38.520 --> 2:18:40.520
 But if you're trained as an art critic,

2:18:40.520 --> 2:18:41.680
 you do learn to say why.

2:18:41.680 --> 2:18:44.040
 And some of it's bullshit, but some of it isn't, right?

2:18:44.040 --> 2:18:46.840
 Some of it is learning to map sensory knowledge

2:18:46.840 --> 2:18:51.120
 into declarative and linguistic knowledge,

2:18:51.120 --> 2:18:56.040
 yet without necessarily making the sensory system itself

2:18:56.040 --> 2:19:00.640
 use a transparent and an easily communicable representation.

2:19:00.640 --> 2:19:02.960
 Yeah, that's fascinating to think of neural networks

2:19:02.960 --> 2:19:07.960
 as like dumb question answers that you can just milk

2:19:08.200 --> 2:19:10.920
 to build up a knowledge base.

2:19:10.920 --> 2:19:12.680
 And then it can be multiple networks, I suppose,

2:19:12.680 --> 2:19:13.600
 from different.

2:19:13.600 --> 2:19:18.160
 Yeah, yeah, so I think if a group like DeepMind or OpenAI

2:19:18.160 --> 2:19:21.520
 were to build AGI, and I think DeepMind is like

2:19:21.520 --> 2:19:24.720
 a thousand times more likely from what I could tell,

2:19:25.920 --> 2:19:30.040
 because they've hired a lot of people with broad minds

2:19:30.040 --> 2:19:34.360
 and many different approaches and angles on AGI,

2:19:34.360 --> 2:19:36.640
 whereas OpenAI is also awesome,

2:19:36.640 --> 2:19:39.040
 but I see them as more of like a pure

2:19:39.040 --> 2:19:41.160
 deep reinforcement learning shop.

2:19:41.160 --> 2:19:42.000
 Yeah, this time, I got you.

2:19:42.000 --> 2:19:43.880
 So far. Yeah, there's a lot of,

2:19:43.880 --> 2:19:48.600
 you're right, I mean, there's so much interdisciplinary

2:19:48.600 --> 2:19:50.280
 work at DeepMind, like neuroscience.

2:19:50.280 --> 2:19:52.240
 And you put that together with Google Brain,

2:19:52.240 --> 2:19:54.760
 which granted they're not working that closely together now,

2:19:54.760 --> 2:19:58.840
 but my oldest son Zarathustra is doing his PhD

2:19:58.840 --> 2:20:01.640
 in machine learning applied to automated theorem proving

2:20:01.640 --> 2:20:03.840
 in Prague under Josef Urban.

2:20:03.840 --> 2:20:08.400
 So the first paper, DeepMath, which applied deep neural nets

2:20:08.400 --> 2:20:10.680
 to guide theorem proving was out of Google Brain.

2:20:10.680 --> 2:20:14.960
 I mean, by now, the automated theorem proving community

2:20:14.960 --> 2:20:18.360
 is going way, way, way beyond anything Google was doing,

2:20:18.360 --> 2:20:21.120
 but still, yeah, but anyway,

2:20:21.120 --> 2:20:23.760
 if that community was gonna make an AGI,

2:20:23.760 --> 2:20:27.160
 probably one way they would do it was,

2:20:27.160 --> 2:20:30.680
 take 25 different neural modules,

2:20:30.680 --> 2:20:32.040
 architected in different ways,

2:20:32.040 --> 2:20:33.800
 maybe resembling different parts of the brain,

2:20:33.800 --> 2:20:36.280
 like a basal ganglia model, cerebellum model,

2:20:36.280 --> 2:20:40.440
 a thalamus module, a few hippocampus models,

2:20:40.440 --> 2:20:41.480
 number of different models,

2:20:41.480 --> 2:20:43.680
 representing parts of the cortex, right?

2:20:43.680 --> 2:20:47.920
 Take all of these and then wire them together

2:20:47.920 --> 2:20:52.520
 to co train and learn them together like that.

2:20:52.520 --> 2:20:57.240
 That would be an approach to creating an AGI.

2:20:57.240 --> 2:20:59.640
 One could implement something like that efficiently

2:20:59.640 --> 2:21:03.800
 on top of our true AGI, like OpenCog 2.0 system,

2:21:03.800 --> 2:21:06.640
 once it exists, although obviously Google

2:21:06.640 --> 2:21:10.240
 has their own highly efficient implementation architecture.

2:21:10.240 --> 2:21:13.280
 So I think that's a decent way to build AGI.

2:21:13.280 --> 2:21:15.680
 I was very interested in that in the mid 90s,

2:21:15.680 --> 2:21:19.440
 but I mean, the knowledge about how the brain works

2:21:19.440 --> 2:21:21.520
 sort of pissed me off, like it wasn't there yet.

2:21:21.520 --> 2:21:23.080
 Like, you know, in the hippocampus,

2:21:23.080 --> 2:21:24.760
 you have these concept neurons,

2:21:24.760 --> 2:21:26.720
 like the so called grandmother neuron,

2:21:26.720 --> 2:21:28.520
 which everyone laughed at it, it's actually there.

2:21:28.520 --> 2:21:31.080
 Like I have some Lex Friedman neurons

2:21:31.080 --> 2:21:33.280
 that fire differentially when I see you

2:21:33.280 --> 2:21:35.360
 and not when I see any other person, right?

2:21:35.360 --> 2:21:38.880
 So how do these Lex Friedman neurons,

2:21:38.880 --> 2:21:41.400
 how do they coordinate with the distributed representation

2:21:41.400 --> 2:21:44.520
 of Lex Friedman I have in my cortex, right?

2:21:44.520 --> 2:21:47.680
 There's some back and forth between cortex and hippocampus

2:21:47.680 --> 2:21:50.120
 that lets these discrete symbolic representations

2:21:50.120 --> 2:21:53.200
 in hippocampus correlate and cooperate

2:21:53.200 --> 2:21:55.680
 with the distributed representations in cortex.

2:21:55.680 --> 2:21:57.400
 This probably has to do with how the brain

2:21:57.400 --> 2:22:00.240
 does its version of abstraction and quantifier logic, right?

2:22:00.240 --> 2:22:02.640
 Like you can have a single neuron in the hippocampus

2:22:02.640 --> 2:22:05.880
 that activates a whole distributed activation pattern

2:22:05.880 --> 2:22:09.080
 in cortex, well, this may be how the brain does

2:22:09.080 --> 2:22:11.120
 like symbolization and abstraction

2:22:11.120 --> 2:22:14.280
 as in functional programming or something,

2:22:14.280 --> 2:22:15.360
 but we can't measure it.

2:22:15.360 --> 2:22:17.560
 Like we don't have enough electrodes stuck

2:22:17.560 --> 2:22:20.960
 between the cortex and the hippocampus

2:22:20.960 --> 2:22:23.080
 in any known experiment to measure it.

2:22:23.080 --> 2:22:26.360
 So I got frustrated with that direction,

2:22:26.360 --> 2:22:27.560
 not because it's impossible.

2:22:27.560 --> 2:22:29.720
 Because we just don't understand enough yet.

2:22:29.720 --> 2:22:31.760
 Of course, it's a valid research direction.

2:22:31.760 --> 2:22:33.720
 You can try to understand more and more.

2:22:33.720 --> 2:22:34.960
 And we are measuring more and more

2:22:34.960 --> 2:22:38.120
 about what happens in the brain now than ever before.

2:22:38.120 --> 2:22:40.560
 So it's quite interesting.

2:22:40.560 --> 2:22:43.400
 On the other hand, I sort of got more

2:22:43.400 --> 2:22:46.520
 of an engineering mindset about AGI.

2:22:46.520 --> 2:22:47.920
 I'm like, well, okay,

2:22:47.920 --> 2:22:50.200
 we don't know how the brain works that well.

2:22:50.200 --> 2:22:52.360
 We don't know how birds fly that well yet either.

2:22:52.360 --> 2:22:54.080
 We have no idea how a hummingbird flies

2:22:54.080 --> 2:22:56.280
 in terms of the aerodynamics of it.

2:22:56.280 --> 2:22:59.280
 On the other hand, we know basic principles

2:22:59.280 --> 2:23:01.760
 of like flapping and pushing the air down.

2:23:01.760 --> 2:23:03.520
 And we know the basic principles

2:23:03.520 --> 2:23:05.720
 of how the different parts of the brain work.

2:23:05.720 --> 2:23:07.480
 So let's take those basic principles

2:23:07.480 --> 2:23:11.480
 and engineer something that embodies those basic principles,

2:23:11.480 --> 2:23:14.040
 but is well designed for the hardware

2:23:14.040 --> 2:23:18.080
 that we have on hand right now.

2:23:18.080 --> 2:23:20.200
 So do you think we can create AGI

2:23:20.200 --> 2:23:22.440
 before we understand how the brain works?

2:23:22.440 --> 2:23:25.120
 I think that's probably what will happen.

2:23:25.120 --> 2:23:28.560
 And maybe the AGI will help us do better brain imaging

2:23:28.560 --> 2:23:30.880
 that will then let us build artificial humans,

2:23:30.880 --> 2:23:33.400
 which is very, very interesting to us

2:23:33.400 --> 2:23:34.960
 because we are humans, right?

2:23:34.960 --> 2:23:38.840
 I mean, building artificial humans is super worthwhile.

2:23:38.840 --> 2:23:42.760
 I just think it's probably not the shortest path to AGI.

2:23:42.760 --> 2:23:45.680
 So it's fascinating idea that we would build AGI

2:23:45.680 --> 2:23:47.320
 to help us understand ourselves.

2:23:50.040 --> 2:23:54.600
 A lot of people ask me if the young people

2:23:54.600 --> 2:23:56.440
 interested in doing artificial intelligence,

2:23:56.440 --> 2:24:01.440
 they look at sort of doing graduate level, even undergrads,

2:24:01.440 --> 2:24:04.520
 but graduate level research and they see

2:24:04.520 --> 2:24:06.840
 whether the artificial intelligence community stands now,

2:24:06.840 --> 2:24:09.920
 it's not really AGI type research for the most part.

2:24:09.920 --> 2:24:12.080
 So the natural question they ask is

2:24:12.080 --> 2:24:13.640
 what advice would you give?

2:24:13.640 --> 2:24:17.320
 I mean, maybe I could ask if people were interested

2:24:17.320 --> 2:24:22.320
 in working on OpenCog or in some kind of direct

2:24:22.520 --> 2:24:25.160
 or indirect connection to OpenCog or AGI research,

2:24:25.160 --> 2:24:26.560
 what would you recommend?

2:24:28.040 --> 2:24:30.960
 OpenCog, first of all, is open source project.

2:24:30.960 --> 2:24:35.360
 There's a Google group discussion list.

2:24:35.360 --> 2:24:36.760
 There's a GitHub repository.

2:24:36.760 --> 2:24:39.800
 So if anyone's interested in lending a hand

2:24:39.800 --> 2:24:42.600
 with that aspect of AGI,

2:24:42.600 --> 2:24:46.000
 introduce yourself on the OpenCog email list.

2:24:46.000 --> 2:24:47.920
 And there's a Slack as well.

2:24:47.920 --> 2:24:52.920
 I mean, we're certainly interested to have inputs

2:24:53.080 --> 2:24:57.520
 into our redesign process for a new version of OpenCog,

2:24:57.520 --> 2:25:01.160
 but also we're doing a lot of very interesting research.

2:25:01.160 --> 2:25:04.080
 I mean, we're working on data analysis

2:25:04.080 --> 2:25:05.600
 for COVID clinical trials.

2:25:05.600 --> 2:25:06.960
 We're working with Hanson Robotics.

2:25:06.960 --> 2:25:08.000
 We're doing a lot of cool things

2:25:08.000 --> 2:25:10.720
 with the current version of OpenCog now.

2:25:10.720 --> 2:25:14.720
 So there's certainly opportunity to jump into OpenCog

2:25:14.720 --> 2:25:18.760
 or various other open source AGI oriented projects.

2:25:18.760 --> 2:25:20.280
 So would you say there's like masters

2:25:20.280 --> 2:25:22.080
 and PhD theses in there?

2:25:22.080 --> 2:25:23.960
 Plenty, yeah, plenty, of course.

2:25:23.960 --> 2:25:26.920
 I mean, the challenge is to find a supervisor

2:25:26.920 --> 2:25:29.720
 who wants to foster that sort of research,

2:25:29.720 --> 2:25:32.840
 but it's way easier than it was when I got my PhD, right?

2:25:32.840 --> 2:25:33.680
 It's okay, great.

2:25:33.680 --> 2:25:36.360
 We talked about OpenCog, which is kind of one,

2:25:36.360 --> 2:25:38.000
 the software framework,

2:25:38.000 --> 2:25:43.000
 but also the actual attempt to build an AGI system.

2:25:44.160 --> 2:25:48.600
 And then there is this exciting idea of SingularityNet.

2:25:48.600 --> 2:25:53.160
 So maybe can you say first what is SingularityNet?

2:25:53.160 --> 2:25:54.280
 Sure, sure.

2:25:54.280 --> 2:25:59.040
 SingularityNet is a platform

2:25:59.040 --> 2:26:04.040
 for realizing a decentralized network

2:26:05.880 --> 2:26:08.280
 of artificial intelligences.

2:26:08.280 --> 2:26:13.280
 So Marvin Minsky, the AI pioneer who I knew a little bit,

2:26:14.440 --> 2:26:16.560
 he had the idea of a society of minds,

2:26:16.560 --> 2:26:18.360
 like you should achieve an AI

2:26:18.360 --> 2:26:21.040
 not by writing one algorithm or one program,

2:26:21.040 --> 2:26:24.000
 but you should put a bunch of different AIs out there

2:26:24.000 --> 2:26:27.760
 and the different AIs will interact with each other,

2:26:27.760 --> 2:26:29.480
 each playing their own role.

2:26:29.480 --> 2:26:32.560
 And then the totality of the society of AIs

2:26:32.560 --> 2:26:34.240
 would be the thing

2:26:34.240 --> 2:26:36.560
 that displayed the human level intelligence.

2:26:36.560 --> 2:26:39.000
 And I had, when he was alive,

2:26:39.000 --> 2:26:43.000
 I had many debates with Marvin about this idea.

2:26:43.000 --> 2:26:48.000
 And I think he really thought the mind

2:26:49.080 --> 2:26:51.200
 was more like a society than I do.

2:26:51.200 --> 2:26:54.080
 Like I think you could have a mind

2:26:54.080 --> 2:26:56.720
 that was as disorganized as a human society,

2:26:56.720 --> 2:26:57.880
 but I think a human like mind

2:26:57.880 --> 2:27:00.080
 has a bit more central control than that actually.

2:27:00.080 --> 2:27:02.840
 Like, I mean, we have this thalamus

2:27:02.840 --> 2:27:04.760
 and the medulla and limbic system.

2:27:04.760 --> 2:27:07.960
 We have a sort of top down control system

2:27:07.960 --> 2:27:10.840
 that guides much of what we do,

2:27:10.840 --> 2:27:12.760
 more so than a society does.

2:27:12.760 --> 2:27:16.880
 So I think he stretched that metaphor a little too far,

2:27:16.880 --> 2:27:20.840
 but I also think there's something interesting there.

2:27:20.840 --> 2:27:24.040
 And so in the 90s,

2:27:24.040 --> 2:27:27.960
 when I started my first sort of nonacademic AI project,

2:27:27.960 --> 2:27:30.960
 WebMind, which was an AI startup in New York

2:27:30.960 --> 2:27:34.640
 in the Silicon Alley area in the late 90s,

2:27:34.640 --> 2:27:36.280
 what I was aiming to do there

2:27:36.280 --> 2:27:38.880
 was make a distributed society of AIs,

2:27:40.000 --> 2:27:41.360
 the different parts of which would live

2:27:41.360 --> 2:27:43.640
 on different computers all around the world.

2:27:43.640 --> 2:27:45.240
 And each one would do its own thinking

2:27:45.240 --> 2:27:47.080
 about the data local to it,

2:27:47.080 --> 2:27:48.960
 but they would all share information with each other

2:27:48.960 --> 2:27:51.320
 and outsource work with each other and cooperate.

2:27:51.320 --> 2:27:54.040
 And the intelligence would be in the whole collective.

2:27:54.040 --> 2:27:57.680
 And I organized a conference together with Francis Heiligen

2:27:57.680 --> 2:28:00.600
 at Free University of Brussels in 2001,

2:28:00.600 --> 2:28:02.920
 which was the Global Brain Zero Conference.

2:28:02.920 --> 2:28:04.680
 And we're planning the next version,

2:28:04.680 --> 2:28:06.920
 the Global Brain One Conference

2:28:06.920 --> 2:28:10.120
 at the Free University of Brussels for next year, 2021.

2:28:10.120 --> 2:28:12.000
 So 20 years after.

2:28:12.000 --> 2:28:14.560
 And then maybe we can have the next one 10 years after that,

2:28:14.560 --> 2:28:19.320
 like exponentially faster until the singularity comes, right?

2:28:19.320 --> 2:28:20.680
 The timing is right, yeah.

2:28:20.680 --> 2:28:22.160
 Yeah, yeah, exactly.

2:28:22.160 --> 2:28:25.000
 So yeah, the idea with the Global Brain

2:28:25.000 --> 2:28:28.120
 was maybe the AI won't just be in a program

2:28:28.120 --> 2:28:29.560
 on one guy's computer,

2:28:29.560 --> 2:28:32.960
 but the AI will be in the internet as a whole

2:28:32.960 --> 2:28:35.080
 with the cooperation of different AI modules

2:28:35.080 --> 2:28:37.040
 living in different places.

2:28:37.040 --> 2:28:39.280
 So one of the issues you face

2:28:39.280 --> 2:28:41.160
 when architecting a system like that

2:28:41.160 --> 2:28:44.760
 is, you know, how is the whole thing controlled?

2:28:44.760 --> 2:28:47.200
 Do you have like a centralized control unit

2:28:47.200 --> 2:28:48.640
 that pulls the puppet strings

2:28:48.640 --> 2:28:50.720
 of all the different modules there?

2:28:50.720 --> 2:28:55.480
 Or do you have a fundamentally decentralized network

2:28:55.480 --> 2:28:59.320
 where the society of AIs is controlled

2:28:59.320 --> 2:29:01.040
 in some democratic and self organized way,

2:29:01.040 --> 2:29:04.760
 but all the AIs in that society, right?

2:29:04.760 --> 2:29:08.680
 And Francis and I had different view of many things,

2:29:08.680 --> 2:29:13.680
 but we both wanted to make like a global society

2:29:13.680 --> 2:29:18.680
 of AI minds with a decentralized organizational mode.

2:29:19.840 --> 2:29:24.840
 Now, the main difference was he wanted the individual AIs

2:29:25.400 --> 2:29:27.440
 to be all incredibly simple

2:29:27.440 --> 2:29:30.360
 and all the intelligence to be on the collective level.

2:29:30.360 --> 2:29:32.960
 Whereas I thought that was cool,

2:29:32.960 --> 2:29:35.880
 but I thought a more practical way to do it might be

2:29:35.880 --> 2:29:39.480
 if some of the agents in the society of minds

2:29:39.480 --> 2:29:41.520
 were fairly generally intelligent on their own.

2:29:41.520 --> 2:29:44.480
 So like you could have a bunch of open cogs out there

2:29:44.480 --> 2:29:47.120
 and a bunch of simpler learning systems.

2:29:47.120 --> 2:29:49.840
 And then these are all cooperating, coordinating together

2:29:49.840 --> 2:29:51.760
 sort of like in the brain.

2:29:51.760 --> 2:29:55.320
 Okay, the brain as a whole is the general intelligence,

2:29:55.320 --> 2:29:56.640
 but some parts of the cortex,

2:29:56.640 --> 2:29:58.560
 you could say have a fair bit of general intelligence

2:29:58.560 --> 2:29:59.720
 on their own,

2:29:59.720 --> 2:30:02.120
 whereas say parts of the cerebellum or limbic system

2:30:02.120 --> 2:30:04.520
 have very little general intelligence on their own.

2:30:04.520 --> 2:30:07.240
 And they're contributing to general intelligence

2:30:07.240 --> 2:30:10.880
 by way of their connectivity to other modules.

2:30:10.880 --> 2:30:13.680
 Do you see instantiations of the same kind of,

2:30:13.680 --> 2:30:15.400
 maybe different versions of open cog,

2:30:15.400 --> 2:30:17.320
 but also just the same version of open cog

2:30:17.320 --> 2:30:21.320
 and maybe many instantiations of it as being all parts of it?

2:30:21.320 --> 2:30:23.040
 That's what David and Hans and I want to do

2:30:23.040 --> 2:30:25.320
 with many Sophia and other robots.

2:30:25.320 --> 2:30:29.200
 Each one has its own individual mind living on the server,

2:30:29.200 --> 2:30:32.080
 but there's also a collective intelligence infusing them

2:30:32.080 --> 2:30:35.440
 and a part of the mind living on the edge in each robot.

2:30:35.440 --> 2:30:38.520
 So the thing is at that time,

2:30:38.520 --> 2:30:41.840
 as well as WebMind being implemented in Java 1.1

2:30:41.840 --> 2:30:44.760
 as like a massive distributed system,

2:30:46.920 --> 2:30:48.160
 blockchain wasn't there yet.

2:30:48.160 --> 2:30:51.880
 So had them do this decentralized control.

2:30:51.880 --> 2:30:52.880
 We sort of knew it.

2:30:52.880 --> 2:30:54.360
 We knew about distributed systems.

2:30:54.360 --> 2:30:55.760
 We knew about encryption.

2:30:55.760 --> 2:30:58.080
 So I mean, we had the key principles

2:30:58.080 --> 2:31:00.080
 of what underlies blockchain now,

2:31:00.080 --> 2:31:01.760
 but I mean, we didn't put it together

2:31:01.760 --> 2:31:02.880
 in the way that it's been done now.

2:31:02.880 --> 2:31:05.360
 So when Vitalik Buterin and colleagues

2:31:05.360 --> 2:31:07.200
 came out with Ethereum blockchain,

2:31:08.120 --> 2:31:11.000
 many, many years later, like 2013 or something,

2:31:11.840 --> 2:31:13.920
 then I was like, well, this is interesting.

2:31:13.920 --> 2:31:17.000
 Like this is solidity scripting language.

2:31:17.000 --> 2:31:18.520
 It's kind of dorky in a way.

2:31:18.520 --> 2:31:21.440
 And I don't see why you need to turn complete language

2:31:21.440 --> 2:31:22.440
 for this purpose.

2:31:22.440 --> 2:31:24.320
 But on the other hand,

2:31:24.320 --> 2:31:27.160
 this is like the first time I could sit down

2:31:27.160 --> 2:31:29.920
 and start to like script infrastructure

2:31:29.920 --> 2:31:32.440
 for decentralized control of the AIs

2:31:32.440 --> 2:31:35.240
 in this society of minds in a tractable way.

2:31:35.240 --> 2:31:37.200
 Like you can hack the Bitcoin code base,

2:31:37.200 --> 2:31:38.520
 but it's really annoying.

2:31:38.520 --> 2:31:41.720
 Whereas solidity is Ethereum scripting language

2:31:41.720 --> 2:31:44.440
 is just nicer and easier to use.

2:31:44.440 --> 2:31:45.880
 I'm very annoyed with it by this point.

2:31:45.880 --> 2:31:49.000
 But like Java, I mean, these languages are amazing

2:31:49.000 --> 2:31:50.920
 when they first come out.

2:31:50.920 --> 2:31:52.480
 So then I came up with the idea

2:31:52.480 --> 2:31:53.840
 that turned into SingularityNet.

2:31:53.840 --> 2:31:58.200
 Okay, let's make a decentralized agent system

2:31:58.200 --> 2:32:00.480
 where a bunch of different AIs,

2:32:00.480 --> 2:32:02.680
 wrapped up in say different Docker containers

2:32:02.680 --> 2:32:04.320
 or LXC containers,

2:32:04.320 --> 2:32:07.440
 different AIs can each of them have their own identity

2:32:07.440 --> 2:32:08.760
 on the blockchain.

2:32:08.760 --> 2:32:11.800
 And the coordination of this community of AIs

2:32:11.800 --> 2:32:14.680
 has no central controller, no dictator, right?

2:32:14.680 --> 2:32:17.160
 And there's no central repository of information.

2:32:17.160 --> 2:32:19.400
 The coordination of the society of minds

2:32:19.400 --> 2:32:22.680
 is done entirely by the decentralized network

2:32:22.680 --> 2:32:25.840
 in a decentralized way by the algorithms, right?

2:32:25.840 --> 2:32:29.200
 Because the model of Bitcoin is in math we trust, right?

2:32:29.200 --> 2:32:30.800
 And so that's what you need.

2:32:30.800 --> 2:32:33.880
 You need the society of minds to trust only in math,

2:32:33.880 --> 2:32:37.720
 not trust only in one centralized server.

2:32:37.720 --> 2:32:40.640
 So the AI systems themselves are outside of the blockchain,

2:32:40.640 --> 2:32:41.800
 but then the communication between them.

2:32:41.800 --> 2:32:43.960
 At the moment, yeah, yeah.

2:32:43.960 --> 2:32:46.880
 I would have loved to put the AI's operations on chain

2:32:46.880 --> 2:32:50.480
 in some sense, but in Ethereum, it's just too slow.

2:32:50.480 --> 2:32:52.680
 You can't do it.

2:32:52.680 --> 2:32:56.120
 Somehow it's the basic communication between AI systems.

2:32:56.120 --> 2:32:58.360
 That's the distribution.

2:32:58.360 --> 2:33:02.520
 Basically an AI is just some software in singularity.

2:33:02.520 --> 2:33:05.920
 An AI is just some software process living in a container.

2:33:05.920 --> 2:33:09.040
 And there's a proxy that lives in that container

2:33:09.040 --> 2:33:10.840
 along with the AI that handles the interaction

2:33:10.840 --> 2:33:13.120
 with the rest of singularity net.

2:33:13.120 --> 2:33:15.880
 And then when one AI wants to contribute

2:33:15.880 --> 2:33:16.920
 with another one in the network,

2:33:16.920 --> 2:33:18.600
 they set up a number of channels.

2:33:18.600 --> 2:33:22.600
 And the setup of those channels uses the Ethereum blockchain.

2:33:22.600 --> 2:33:24.480
 Once the channels are set up,

2:33:24.480 --> 2:33:26.160
 then data flows along those channels

2:33:26.160 --> 2:33:29.240
 without having to be on the blockchain.

2:33:29.240 --> 2:33:31.080
 All that goes on the blockchain is the fact

2:33:31.080 --> 2:33:33.160
 that some data went along that channel.

2:33:33.160 --> 2:33:34.240
 So you can do...

2:33:34.240 --> 2:33:37.040
 So there's not a shared knowledge.

2:33:38.720 --> 2:33:43.160
 Well, the identity of each agent is on the blockchain,

2:33:43.160 --> 2:33:44.800
 on the Ethereum blockchain.

2:33:44.800 --> 2:33:48.000
 If one agent rates the reputation of another agent,

2:33:48.000 --> 2:33:49.560
 that goes on the blockchain.

2:33:49.560 --> 2:33:52.880
 And agents can publish what APIs they will fulfill

2:33:52.880 --> 2:33:54.520
 on the blockchain.

2:33:54.520 --> 2:33:58.040
 But the actual data for AI and the results for AI

2:33:58.040 --> 2:33:58.880
 is not on the blockchain.

2:33:58.880 --> 2:33:59.720
 Do you think it could be?

2:33:59.720 --> 2:34:02.320
 Do you think it should be?

2:34:02.320 --> 2:34:04.120
 In some cases, it should be.

2:34:04.120 --> 2:34:05.880
 In some cases, maybe it shouldn't be.

2:34:05.880 --> 2:34:09.320
 But I mean, I think that...

2:34:09.320 --> 2:34:10.160
 So I'll give you an example.

2:34:10.160 --> 2:34:11.640
 Using Ethereum, you can't do it.

2:34:11.640 --> 2:34:16.640
 Using now, there's more modern and faster blockchains

2:34:16.640 --> 2:34:21.640
 where you could start to do that in some cases.

2:34:21.920 --> 2:34:23.360
 Two years ago, that was less so.

2:34:23.360 --> 2:34:25.640
 It's a very rapidly evolving ecosystem.

2:34:25.640 --> 2:34:28.920
 So like one example, maybe you can comment on

2:34:28.920 --> 2:34:31.840
 something I worked a lot on is autonomous vehicles.

2:34:31.840 --> 2:34:35.680
 You can see each individual vehicle as an AI system.

2:34:35.680 --> 2:34:39.600
 And you can see vehicles from Tesla, for example,

2:34:39.600 --> 2:34:44.600
 and then Ford and GM and all these as also like larger...

2:34:44.600 --> 2:34:47.000
 I mean, they all are running the same kind of system

2:34:47.000 --> 2:34:49.280
 on each sets of vehicles.

2:34:49.280 --> 2:34:52.360
 So it's individual AI systems and individual vehicles,

2:34:52.360 --> 2:34:53.800
 but it's all different.

2:34:53.800 --> 2:34:57.520
 The station is the same AI system within the same company.

2:34:57.520 --> 2:35:02.360
 So you can envision a situation where all of those AI systems

2:35:02.360 --> 2:35:05.400
 are put on SingularityNet, right?

2:35:05.400 --> 2:35:10.160
 And how do you see that happening?

2:35:10.160 --> 2:35:11.520
 And what would be the benefit?

2:35:11.520 --> 2:35:13.000
 And could they share data?

2:35:13.000 --> 2:35:16.440
 I guess one of the biggest things is that the power there's

2:35:16.440 --> 2:35:20.440
 in a decentralized control, but the benefit would have been,

2:35:20.440 --> 2:35:24.080
 is really nice if they can somehow share the knowledge

2:35:24.080 --> 2:35:26.280
 in an open way if they choose to.

2:35:26.280 --> 2:35:29.920
 Yeah, yeah, yeah, those are all quite good points.

2:35:29.920 --> 2:35:34.920
 So I think the benefit from being on the decentralized network

2:35:37.760 --> 2:35:41.320
 as we envision it is that we want the AIs in the network

2:35:41.320 --> 2:35:43.800
 to be outsourcing work to each other

2:35:43.800 --> 2:35:47.440
 and making API calls to each other frequently.

2:35:47.440 --> 2:35:51.880
 So the real benefit would be if that AI wanted to outsource

2:35:51.880 --> 2:35:54.920
 some cognitive processing or data processing

2:35:54.920 --> 2:35:56.720
 or data pre processing, whatever,

2:35:56.720 --> 2:35:59.320
 to some other AIs in the network,

2:35:59.320 --> 2:36:01.600
 which specialize in something different.

2:36:01.600 --> 2:36:06.120
 And this really requires a different way of thinking

2:36:06.120 --> 2:36:07.960
 about AI software development, right?

2:36:07.960 --> 2:36:10.320
 So just like object oriented programming

2:36:10.320 --> 2:36:12.720
 was different than imperative programming.

2:36:12.720 --> 2:36:16.720
 And now object oriented programmers all use these

2:36:16.720 --> 2:36:20.680
 frameworks to do things rather than just libraries even.

2:36:20.680 --> 2:36:23.120
 You know, shifting to agent based programming

2:36:23.120 --> 2:36:26.600
 where AI agent is asking other like live real time

2:36:26.600 --> 2:36:29.960
 evolving agents for feedback and what they're doing.

2:36:29.960 --> 2:36:31.480
 That's a different way of thinking.

2:36:31.480 --> 2:36:32.960
 I mean, it's not a new one.

2:36:32.960 --> 2:36:35.320
 There was loads of papers on agent based programming

2:36:35.320 --> 2:36:37.120
 in the 80s and onward.

2:36:37.120 --> 2:36:41.520
 But if you're willing to shift to an agent based model

2:36:41.520 --> 2:36:45.920
 of development, then you can put less and less in your AI

2:36:45.920 --> 2:36:48.600
 and rely more and more on interactive calls

2:36:48.600 --> 2:36:51.440
 to other AIs running in the network.

2:36:51.440 --> 2:36:54.560
 And of course, that's not fully manifested yet

2:36:54.560 --> 2:36:57.640
 because although we've rolled out a nice working version

2:36:57.640 --> 2:36:59.760
 of SingularityNet platform,

2:36:59.760 --> 2:37:03.760
 there's only 50 to 100 AIs running in there now.

2:37:03.760 --> 2:37:05.880
 There's not tens of thousands of AIs.

2:37:05.880 --> 2:37:08.240
 So we don't have the critical mass

2:37:08.240 --> 2:37:11.120
 for the whole society of mind to be doing

2:37:11.120 --> 2:37:11.960
 what we want to do.

2:37:11.960 --> 2:37:13.400
 Yeah, the magic really happens

2:37:13.400 --> 2:37:15.320
 when there's just a huge number of agents.

2:37:15.320 --> 2:37:16.680
 Yeah, yeah, exactly.

2:37:16.680 --> 2:37:19.600
 In terms of data, we're partnering closely

2:37:19.600 --> 2:37:23.520
 with another blockchain project called Ocean Protocol.

2:37:23.520 --> 2:37:27.240
 And Ocean Protocol, that's the project of Trent McConnachie

2:37:27.240 --> 2:37:28.720
 who developed BigchainDB,

2:37:28.720 --> 2:37:30.800
 which is a blockchain based database.

2:37:30.800 --> 2:37:35.440
 So Ocean Protocol is basically blockchain based big data

2:37:35.440 --> 2:37:39.440
 and aims at making it efficient for different AI processes

2:37:39.440 --> 2:37:41.240
 or statistical processes or whatever

2:37:41.240 --> 2:37:44.080
 to share large data sets.

2:37:44.080 --> 2:37:46.600
 Or if one process can send a clone of itself

2:37:46.600 --> 2:37:48.200
 to work on the other guy's data set

2:37:48.200 --> 2:37:50.600
 and send results back and so forth.

2:37:50.600 --> 2:37:55.560
 So by getting Ocean and you have data lake,

2:37:55.560 --> 2:37:56.920
 so this is the data ocean, right?

2:37:56.920 --> 2:37:59.760
 So again, by getting Ocean and SingularityNet

2:37:59.760 --> 2:38:03.760
 to interoperate, we're aiming to take into account

2:38:03.760 --> 2:38:05.840
 the big data aspect also.

2:38:05.840 --> 2:38:08.240
 But it's quite challenging

2:38:08.240 --> 2:38:10.120
 because to build this whole decentralized

2:38:10.120 --> 2:38:12.400
 blockchain based infrastructure,

2:38:12.400 --> 2:38:14.960
 I mean, your competitors are like Google, Microsoft,

2:38:14.960 --> 2:38:17.960
 Alibaba and Amazon, which have so much money

2:38:17.960 --> 2:38:20.560
 to put behind their centralized infrastructures,

2:38:20.560 --> 2:38:23.360
 plus they're solving simpler algorithmic problems

2:38:23.360 --> 2:38:27.360
 because making it centralized in some ways is easier, right?

2:38:27.360 --> 2:38:32.360
 So they're very major computer science challenges.

2:38:32.360 --> 2:38:35.760
 And I think what you saw with the whole ICO boom

2:38:35.760 --> 2:38:37.880
 in the blockchain and cryptocurrency world

2:38:37.880 --> 2:38:42.040
 is a lot of young hackers who were hacking Bitcoin

2:38:42.040 --> 2:38:43.840
 or Ethereum, and they see, well,

2:38:43.840 --> 2:38:46.800
 why don't we make this decentralized on blockchain?

2:38:46.800 --> 2:38:48.720
 Then after they raised some money through an ICO,

2:38:48.720 --> 2:38:49.880
 they realize how hard it is.

2:38:49.880 --> 2:38:52.040
 And it's like, actually we're wrestling

2:38:52.040 --> 2:38:54.680
 with incredibly hard computer science

2:38:54.680 --> 2:38:58.720
 and software engineering and distributed systems problems,

2:38:58.720 --> 2:39:02.560
 which can be solved, but they're just very difficult

2:39:02.560 --> 2:39:03.400
 to solve.

2:39:03.400 --> 2:39:05.800
 And in some cases, the individuals who started

2:39:05.800 --> 2:39:08.760
 those projects were not well equipped

2:39:08.760 --> 2:39:12.320
 to actually solve the problems that they wanted to solve.

2:39:12.320 --> 2:39:14.560
 So you think, would you say that's the main bottleneck?

2:39:14.560 --> 2:39:17.640
 If you look at the future of currency,

2:39:19.560 --> 2:39:21.040
 the question is, well...

2:39:21.040 --> 2:39:23.800
 Currency, the main bottleneck is politics.

2:39:23.800 --> 2:39:26.440
 It's governments and the bands of armed thugs

2:39:26.440 --> 2:39:29.840
 that will shoot you if you bypass their currency restriction.

2:39:29.840 --> 2:39:30.680
 That's right.

2:39:30.680 --> 2:39:33.760
 So like your sense is that versus the technical challenges,

2:39:33.760 --> 2:39:34.840
 because you kind of just suggested

2:39:34.840 --> 2:39:36.560
 the technical challenges are quite high as well.

2:39:36.560 --> 2:39:39.000
 I mean, for making a distributed money,

2:39:39.000 --> 2:39:41.280
 you could do that on Algorand right now.

2:39:41.280 --> 2:39:43.880
 I mean, so that while Ethereum is too slow,

2:39:44.760 --> 2:39:47.240
 there's Algorand and there's a few other more modern,

2:39:47.240 --> 2:39:49.360
 more scalable blockchains that would work fine

2:39:49.360 --> 2:39:53.640
 for a decentralized global currency.

2:39:53.640 --> 2:39:56.480
 So I think there were technical bottlenecks

2:39:56.480 --> 2:39:57.920
 to that two years ago.

2:39:57.920 --> 2:40:00.760
 And maybe Ethereum 2.0 will be as fast as Algorand.

2:40:00.760 --> 2:40:04.160
 I don't know, that's not fully written yet, right?

2:40:04.160 --> 2:40:07.520
 So I think the obstacle to currency

2:40:07.520 --> 2:40:09.400
 being put on the blockchain is that...

2:40:09.400 --> 2:40:10.240
 Is the other stuff you mentioned.

2:40:10.240 --> 2:40:11.760
 I mean, currency will be on the blockchain.

2:40:11.760 --> 2:40:13.840
 It'll just be on the blockchain in a way

2:40:13.840 --> 2:40:16.520
 that enforces centralized control

2:40:16.520 --> 2:40:18.320
 and government hedge money rather than otherwise.

2:40:18.320 --> 2:40:20.920
 Like the ERNB will probably be the first global,

2:40:20.920 --> 2:40:22.200
 the first currency on the blockchain.

2:40:22.200 --> 2:40:23.360
 The EURUBIL maybe next.

2:40:23.360 --> 2:40:24.200
 There are any...

2:40:24.200 --> 2:40:25.040
 EURUBIL?

2:40:25.040 --> 2:40:25.860
 Yeah, yeah, yeah.

2:40:25.860 --> 2:40:26.700
 I mean, the point is...

2:40:26.700 --> 2:40:27.540
 Oh, that's hilarious.

2:40:27.540 --> 2:40:30.720
 Digital currency, you know, makes total sense,

2:40:30.720 --> 2:40:32.160
 but they would rather do it in the way

2:40:32.160 --> 2:40:34.720
 that Putin and Xi Jinping have access

2:40:34.720 --> 2:40:37.840
 to the global keys for everything, right?

2:40:37.840 --> 2:40:42.040
 So, and then the analogy to that in terms of SingularityNet,

2:40:42.040 --> 2:40:43.600
 I mean, there's Echoes.

2:40:43.600 --> 2:40:47.200
 I think you've mentioned before that Linux gives you hope.

2:40:47.200 --> 2:40:49.960
 AI is not as heavily regulated as money, right?

2:40:49.960 --> 2:40:51.000
 Not yet, right?

2:40:51.000 --> 2:40:52.000
 Not yet.

2:40:52.000 --> 2:40:54.240
 Oh, that's a lot slipperier than money too, right?

2:40:54.240 --> 2:40:58.280
 I mean, money is easier to regulate

2:40:58.280 --> 2:41:00.800
 because it's kind of easier to define,

2:41:00.800 --> 2:41:04.120
 whereas AI is, it's almost everywhere inside everything.

2:41:04.120 --> 2:41:06.440
 Where's the boundary between AI and software, right?

2:41:06.440 --> 2:41:09.200
 I mean, if you're gonna regulate AI,

2:41:09.200 --> 2:41:11.720
 there's no IQ test for every hardware device

2:41:11.720 --> 2:41:12.800
 that has a learning algorithm.

2:41:12.800 --> 2:41:15.720
 You're gonna be putting like hegemonic regulation

2:41:15.720 --> 2:41:16.760
 on all software.

2:41:16.760 --> 2:41:18.880
 And I don't rule out that that can happen.

2:41:18.880 --> 2:41:21.060
 And the adaptive software.

2:41:21.060 --> 2:41:23.360
 Yeah, but how do you tell if a software is adaptive

2:41:23.360 --> 2:41:26.100
 and what, every software is gonna be adaptive, I mean.

2:41:26.100 --> 2:41:28.800
 Or maybe they, maybe the, you know,

2:41:28.800 --> 2:41:31.120
 maybe we're living in the golden age of open source

2:41:31.120 --> 2:41:33.360
 that will not always be open.

2:41:33.360 --> 2:41:35.640
 Maybe it'll become centralized control

2:41:35.640 --> 2:41:37.020
 of software by governments.

2:41:37.020 --> 2:41:38.840
 It is entirely possible.

2:41:38.840 --> 2:41:42.200
 And part of what I think we're doing

2:41:42.200 --> 2:41:45.220
 with things like SingularityNet protocol

2:41:45.220 --> 2:41:50.220
 is creating a tool set that can be used

2:41:50.220 --> 2:41:52.740
 to counteract that sort of thing.

2:41:52.740 --> 2:41:55.620
 Say a similar thing about mesh networking, right?

2:41:55.620 --> 2:41:59.060
 Plays a minor role now, the ability to access internet

2:41:59.060 --> 2:42:01.000
 like directly phone to phone.

2:42:01.000 --> 2:42:03.740
 On the other hand, if your government starts trying

2:42:03.740 --> 2:42:06.060
 to control your use of the internet,

2:42:06.060 --> 2:42:09.220
 suddenly having mesh networking there

2:42:09.220 --> 2:42:10.800
 can be very convenient, right?

2:42:10.800 --> 2:42:15.360
 And so right now, something like a decentralized

2:42:15.360 --> 2:42:20.300
 blockchain based AGI framework or narrow AI framework,

2:42:20.300 --> 2:42:22.660
 it's cool, it's nice to have.

2:42:22.660 --> 2:42:25.140
 On the other hand, if governments start trying

2:42:25.140 --> 2:42:28.740
 to tap down on my AI interoperating

2:42:28.740 --> 2:42:31.460
 with someone's AI in Russia or somewhere, right?

2:42:31.460 --> 2:42:35.500
 Then suddenly having a decentralized protocol

2:42:35.500 --> 2:42:37.940
 that nobody owns or controls

2:42:37.940 --> 2:42:41.180
 becomes an extremely valuable part of the tool set.

2:42:41.180 --> 2:42:43.780
 And, you know, we've put that out there now.

2:42:43.780 --> 2:42:46.980
 It's not perfect, but it operates.

2:42:46.980 --> 2:42:51.100
 And, you know, it's pretty blockchain agnostic.

2:42:51.100 --> 2:42:53.420
 So we're talking to Algorand about making part

2:42:53.420 --> 2:42:56.220
 of SingularityNet run on Algorand.

2:42:56.220 --> 2:43:00.060
 My good friend Tufi Saliba has a cool blockchain project

2:43:00.060 --> 2:43:02.220
 called Toda, which is a blockchain

2:43:02.220 --> 2:43:03.540
 without a distributed ledger.

2:43:03.540 --> 2:43:05.180
 It's like a whole other architecture.

2:43:05.180 --> 2:43:08.300
 So there's a lot of more advanced things you can do

2:43:08.300 --> 2:43:09.820
 in the blockchain world.

2:43:09.820 --> 2:43:13.500
 SingularityNet could be ported to a whole bunch of,

2:43:13.500 --> 2:43:14.980
 it could be made multi chain important

2:43:14.980 --> 2:43:17.100
 to a whole bunch of different blockchains.

2:43:17.100 --> 2:43:21.540
 And there's a lot of potential and a lot of importance

2:43:21.540 --> 2:43:23.620
 to putting this kind of tool set out there.

2:43:23.620 --> 2:43:26.660
 If you compare to OpenCog, what you could see is

2:43:26.660 --> 2:43:31.660
 OpenCog allows tight integration of a few AI algorithms

2:43:32.220 --> 2:43:36.860
 that share the same knowledge store in real time, in RAM.

2:43:36.860 --> 2:43:40.900
 SingularityNet allows loose integration

2:43:40.900 --> 2:43:42.660
 of multiple different AIs.

2:43:42.660 --> 2:43:45.620
 They can share knowledge, but they're mostly not gonna

2:43:45.620 --> 2:43:49.980
 be sharing knowledge in RAM on the same machine.

2:43:49.980 --> 2:43:53.060
 And I think what we're gonna have is a network

2:43:53.060 --> 2:43:54.500
 of network of networks, right?

2:43:54.500 --> 2:43:57.260
 Like, I mean, you have the knowledge graph

2:43:57.260 --> 2:44:00.900
 inside the OpenCog system,

2:44:00.900 --> 2:44:03.220
 and then you have a network of machines

2:44:03.220 --> 2:44:05.900
 inside a distributed OpenCog mind,

2:44:05.900 --> 2:44:10.260
 but then that OpenCog will interface with other AIs

2:44:10.260 --> 2:44:14.420
 doing deep neural nets or custom biology data analysis

2:44:14.420 --> 2:44:17.620
 or whatever they're doing in SingularityNet,

2:44:17.620 --> 2:44:21.020
 which is a looser integration of different AIs,

2:44:21.020 --> 2:44:24.060
 some of which may be their own networks, right?

2:44:24.060 --> 2:44:27.900
 And I think at a very loose analogy,

2:44:27.900 --> 2:44:29.380
 you could see that in the human body.

2:44:29.380 --> 2:44:33.820
 Like the brain has regions like cortex or hippocampus,

2:44:33.820 --> 2:44:36.820
 which tightly interconnects like cortical columns

2:44:36.820 --> 2:44:39.140
 within the cortex, for example.

2:44:39.140 --> 2:44:40.860
 Then there's looser connection

2:44:40.860 --> 2:44:42.700
 within the different lobes of the brain,

2:44:42.700 --> 2:44:45.020
 and then the brain interconnects with the endocrine system

2:44:45.020 --> 2:44:48.260
 and different parts of the body even more loosely.

2:44:48.260 --> 2:44:50.780
 Then your body interacts even more loosely

2:44:50.780 --> 2:44:53.300
 with the other people that you talk to.

2:44:53.300 --> 2:44:56.460
 So you often have networks within networks within networks

2:44:56.460 --> 2:44:59.340
 with progressively looser coupling

2:44:59.340 --> 2:45:02.740
 as you get higher up in that hierarchy.

2:45:02.740 --> 2:45:03.860
 I mean, you have that in biology,

2:45:03.860 --> 2:45:08.180
 you have that in the internet as a just networking medium.

2:45:08.180 --> 2:45:10.940
 And I think that's what we're gonna have

2:45:10.940 --> 2:45:15.940
 in the network of software processes leading to AGI.

2:45:15.940 --> 2:45:17.780
 That's a beautiful way to see the world.

2:45:17.780 --> 2:45:21.900
 Again, the same similar question is with OpenCog.

2:45:21.900 --> 2:45:24.620
 If somebody wanted to build an AI system

2:45:24.620 --> 2:45:27.020
 and plug into the SingularityNet,

2:45:27.020 --> 2:45:28.620
 what would you recommend?

2:45:28.620 --> 2:45:30.180
 Yeah, so that's much easier.

2:45:30.180 --> 2:45:33.860
 I mean, OpenCog is still a research system.

2:45:33.860 --> 2:45:36.660
 So it takes some expertise to, and sometimes,

2:45:36.660 --> 2:45:40.220
 we have tutorials, but it's somewhat cognitively

2:45:40.220 --> 2:45:44.340
 labor intensive to get up to speed on OpenCog.

2:45:44.340 --> 2:45:46.620
 And I mean, what's one of the things we hope to change

2:45:46.620 --> 2:45:49.900
 with the true AGI OpenCog 2.0 version

2:45:49.900 --> 2:45:52.740
 is just make the learning curve more similar

2:45:52.740 --> 2:45:54.620
 to TensorFlow or Torch or something.

2:45:54.620 --> 2:45:57.340
 Right now, OpenCog is amazingly powerful,

2:45:57.340 --> 2:46:00.620
 but not simple to deal with.

2:46:00.620 --> 2:46:03.700
 On the other hand, SingularityNet,

2:46:03.700 --> 2:46:08.260
 as an open platform was developed a little more

2:46:08.260 --> 2:46:10.580
 with usability in mind over the blockchain,

2:46:10.580 --> 2:46:11.660
 it's still kind of a pain.

2:46:11.660 --> 2:46:14.940
 So I mean, if you're a command line guy,

2:46:14.940 --> 2:46:16.180
 there's a command line interface.

2:46:16.180 --> 2:46:20.060
 It's quite easy to take any AI that has an API

2:46:20.060 --> 2:46:23.540
 and lives in a Docker container and put it online anywhere.

2:46:23.540 --> 2:46:25.740
 And then it joins the global SingularityNet.

2:46:25.740 --> 2:46:28.980
 And anyone who puts a request for services

2:46:28.980 --> 2:46:30.180
 out into the SingularityNet,

2:46:30.180 --> 2:46:32.340
 the peer to peer discovery mechanism will find

2:46:32.340 --> 2:46:35.740
 your AI and if it does what was asked,

2:46:35.740 --> 2:46:38.980
 it can then start a conversation with your AI

2:46:38.980 --> 2:46:42.180
 about whether it wants to ask your AI to do something for it,

2:46:42.180 --> 2:46:43.580
 how much it would cost and so on.

2:46:43.580 --> 2:46:46.860
 So that's fairly simple.

2:46:46.860 --> 2:46:50.380
 If you wrote an AI and want it listed

2:46:50.380 --> 2:46:53.020
 on like official SingularityNet marketplace,

2:46:53.020 --> 2:46:55.140
 which is on our website,

2:46:55.140 --> 2:46:57.820
 then we have a publisher portal

2:46:57.820 --> 2:47:00.220
 and then there's a KYC process to go through

2:47:00.220 --> 2:47:02.420
 because then we have some legal liability

2:47:02.420 --> 2:47:04.700
 for what goes on that website.

2:47:04.700 --> 2:47:07.340
 So in a way that's been an education too.

2:47:07.340 --> 2:47:08.420
 There's sort of two layers.

2:47:08.420 --> 2:47:11.700
 Like there's the open decentralized protocol.

2:47:11.700 --> 2:47:12.980
 And there's the market.

2:47:12.980 --> 2:47:15.540
 Yeah, anyone can use the open decentralized protocol.

2:47:15.540 --> 2:47:17.980
 So say some developers from Iran

2:47:17.980 --> 2:47:19.460
 and there's brilliant AI guys

2:47:19.460 --> 2:47:21.780
 in University of Isfahan in Tehran,

2:47:21.780 --> 2:47:24.660
 they can put their stuff on SingularityNet protocol

2:47:24.660 --> 2:47:27.100
 and just like they can put something on the internet, right?

2:47:27.100 --> 2:47:28.460
 I don't control it.

2:47:28.460 --> 2:47:29.740
 But if we're gonna list something

2:47:29.740 --> 2:47:32.020
 on the SingularityNet marketplace

2:47:32.020 --> 2:47:34.300
 and put a little picture and a link to it,

2:47:34.300 --> 2:47:38.860
 then if I put some Iranian AI geniuses code on there,

2:47:38.860 --> 2:47:41.500
 then Donald Trump can send a bunch of jackbooted thugs

2:47:41.500 --> 2:47:45.300
 to my house to arrest me for doing business with Iran, right?

2:47:45.300 --> 2:47:48.980
 So, I mean, we already see in some ways

2:47:48.980 --> 2:47:51.100
 the value of having a decentralized protocol

2:47:51.100 --> 2:47:53.740
 because what I hope is that someone in Iran

2:47:53.740 --> 2:47:57.340
 will put online an Iranian SingularityNet marketplace, right?

2:47:57.340 --> 2:47:59.700
 Which you can pay in the cryptographic token,

2:47:59.700 --> 2:48:01.540
 which is not owned by any country.

2:48:01.540 --> 2:48:04.620
 And then if you're in like Congo or somewhere

2:48:04.620 --> 2:48:06.780
 that doesn't have any problem with Iran,

2:48:06.780 --> 2:48:09.220
 you can subcontract AI services

2:48:09.220 --> 2:48:11.980
 that you find on that marketplace, right?

2:48:11.980 --> 2:48:16.060
 Even though US citizens can't by US law.

2:48:16.060 --> 2:48:20.140
 So right now, that's kind of a minor point.

2:48:20.140 --> 2:48:24.020
 As you alluded, if regulations go in the wrong direction,

2:48:24.020 --> 2:48:25.540
 it could become more of a major point.

2:48:25.540 --> 2:48:28.060
 But I think it also is the case

2:48:28.060 --> 2:48:31.860
 that having these workarounds to regulations in place

2:48:31.860 --> 2:48:35.180
 is a defense mechanism against those regulations

2:48:35.180 --> 2:48:36.660
 being put into place.

2:48:36.660 --> 2:48:39.220
 And you can see that in the music industry, right?

2:48:39.220 --> 2:48:43.020
 I mean, Napster just happened and BitTorrent just happened.

2:48:43.020 --> 2:48:45.980
 And now most people in my kid's generation,

2:48:45.980 --> 2:48:48.500
 they're baffled by the idea of paying for music, right?

2:48:48.500 --> 2:48:51.380
 I mean, my dad pays for music.

2:48:51.380 --> 2:48:55.700
 I mean, but that because these decentralized mechanisms

2:48:55.700 --> 2:48:58.940
 happened and then the regulations followed, right?

2:48:58.940 --> 2:49:01.220
 And the regulations would be very different

2:49:01.220 --> 2:49:04.380
 if they'd been put into place before there was Napster

2:49:04.380 --> 2:49:05.500
 and BitTorrent and so forth.

2:49:05.500 --> 2:49:08.620
 So in the same way, we gotta put AI out there

2:49:08.620 --> 2:49:11.060
 in a decentralized vein and big data out there

2:49:11.060 --> 2:49:13.780
 in a decentralized vein now,

2:49:13.780 --> 2:49:16.300
 so that the most advanced AI in the world

2:49:16.300 --> 2:49:18.300
 is fundamentally decentralized.

2:49:18.300 --> 2:49:20.940
 And if that's the case, that's just the reality

2:49:20.940 --> 2:49:23.740
 the regulators have to deal with.

2:49:23.740 --> 2:49:25.460
 And then as in the music case,

2:49:25.460 --> 2:49:27.460
 they're gonna come up with regulations

2:49:27.460 --> 2:49:32.060
 that sort of work with the decentralized reality.

2:49:32.860 --> 2:49:34.020
 Beautiful.

2:49:34.020 --> 2:49:37.980
 You are the chief scientist of Hanson Robotics.

2:49:37.980 --> 2:49:40.500
 You're still involved with Hanson Robotics,

2:49:40.500 --> 2:49:42.740
 doing a lot of really interesting stuff there.

2:49:42.740 --> 2:49:44.500
 This is for people who don't know the company

2:49:44.500 --> 2:49:47.380
 that created Sophia the Robot.

2:49:47.380 --> 2:49:51.460
 Can you tell me who Sophia is?

2:49:51.460 --> 2:49:54.140
 I'd rather start by telling you who David Hanson is.

2:49:54.140 --> 2:49:58.780
 Because David is the brilliant mind behind the Sophia Robot.

2:49:58.780 --> 2:50:01.980
 And he remains, so far, he remains more interesting

2:50:01.980 --> 2:50:05.900
 than his creation, although she may be improving

2:50:05.900 --> 2:50:07.380
 faster than he is, actually.

2:50:07.380 --> 2:50:08.780
 I mean, he's a...

2:50:08.780 --> 2:50:13.780
 So yeah, I met David maybe 2007 or something

2:50:15.300 --> 2:50:18.420
 at some futurist conference we were both speaking at.

2:50:18.420 --> 2:50:22.860
 And I could see we had a great deal in common.

2:50:22.860 --> 2:50:25.020
 I mean, we were both kind of crazy,

2:50:25.020 --> 2:50:30.020
 but we both had a passion for AGI and the singularity.

2:50:31.540 --> 2:50:33.580
 And we were both huge fans of the work

2:50:33.580 --> 2:50:36.900
 of Philip K. Dick, the science fiction writer.

2:50:36.900 --> 2:50:40.780
 And I wanted to create benevolent AGI

2:50:40.780 --> 2:50:44.820
 that would create massively better life

2:50:44.820 --> 2:50:47.580
 for all humans and all sentient beings,

2:50:47.580 --> 2:50:50.060
 including animals, plants, and superhuman beings.

2:50:50.060 --> 2:50:53.780
 And David, he wanted exactly the same thing,

2:50:53.780 --> 2:50:56.380
 but he had a different idea of how to do it.

2:50:56.380 --> 2:50:59.420
 He wanted to get computational compassion.

2:50:59.420 --> 2:51:03.940
 Like he wanted to get machines that would love people

2:51:03.940 --> 2:51:05.820
 and empathize with people.

2:51:05.820 --> 2:51:08.220
 And he thought the way to do that was to make a machine

2:51:08.220 --> 2:51:12.220
 that could look people eye to eye, face to face,

2:51:12.220 --> 2:51:15.700
 look at people and make people love the machine,

2:51:15.700 --> 2:51:17.540
 and the machine loves the people back.

2:51:17.540 --> 2:51:21.500
 So I thought that was very different way of looking at it

2:51:21.500 --> 2:51:22.940
 because I'm very math oriented.

2:51:22.940 --> 2:51:24.740
 And I'm just thinking like,

2:51:24.740 --> 2:51:28.100
 what is the abstract cognitive algorithm

2:51:28.100 --> 2:51:29.420
 that will let the system, you know,

2:51:29.420 --> 2:51:32.580
 internalize the complex patterns of human values,

2:51:32.580 --> 2:51:33.420
 blah, blah, blah.

2:51:33.420 --> 2:51:35.980
 Whereas he's like, look you in the face and the eye

2:51:35.980 --> 2:51:37.380
 and love you, right?

2:51:37.380 --> 2:51:41.340
 So we hit it off quite well.

2:51:41.340 --> 2:51:44.460
 And we talked to each other off and on.

2:51:44.460 --> 2:51:49.380
 Then I moved to Hong Kong in 2011.

2:51:49.380 --> 2:51:53.380
 So I've been living all over the place.

2:51:53.380 --> 2:51:56.780
 I've been in Australia and New Zealand in my academic career.

2:51:56.780 --> 2:51:59.380
 Then in Las Vegas for a while.

2:51:59.380 --> 2:52:00.860
 Was in New York in the late 90s

2:52:00.860 --> 2:52:03.660
 starting my entrepreneurial career.

2:52:03.660 --> 2:52:05.020
 Was in DC for nine years

2:52:05.020 --> 2:52:07.940
 doing a bunch of US government consulting stuff.

2:52:07.940 --> 2:52:12.060
 Then moved to Hong Kong in 2011,

2:52:12.060 --> 2:52:13.900
 mostly because I met a Chinese girl

2:52:13.900 --> 2:52:16.060
 who I fell in love with and we got married.

2:52:16.060 --> 2:52:17.380
 She's actually not from Hong Kong.

2:52:17.380 --> 2:52:18.380
 She's from mainland China,

2:52:18.380 --> 2:52:21.340
 but we converged together in Hong Kong.

2:52:21.340 --> 2:52:24.180
 Still married now, I have a two year old baby.

2:52:24.180 --> 2:52:26.820
 So went to Hong Kong to see about a girl, I guess.

2:52:26.820 --> 2:52:29.060
 Yeah, pretty much, yeah.

2:52:29.060 --> 2:52:31.060
 And on the other hand,

2:52:31.060 --> 2:52:33.100
 I started doing some cool research there

2:52:33.100 --> 2:52:36.540
 with Gino Yu at Hong Kong Polytechnic University.

2:52:36.540 --> 2:52:38.300
 I got involved with a project called IDEA

2:52:38.300 --> 2:52:41.220
 using machine learning for stock and futures prediction,

2:52:41.220 --> 2:52:43.140
 which was quite interesting.

2:52:43.140 --> 2:52:45.100
 And I also got to know something

2:52:45.100 --> 2:52:47.420
 about the consumer electronics

2:52:47.420 --> 2:52:50.220
 and hardware manufacturer ecosystem in Shenzhen

2:52:50.220 --> 2:52:51.060
 across the border,

2:52:51.060 --> 2:52:53.260
 which is like the only place in the world

2:52:53.260 --> 2:52:56.500
 that makes sense to make complex consumer electronics

2:52:56.500 --> 2:52:57.860
 at large scale and low cost.

2:52:57.860 --> 2:53:00.900
 It's just, it's astounding the hardware ecosystem

2:53:00.900 --> 2:53:03.220
 that you have in South China.

2:53:03.220 --> 2:53:07.220
 Like US people here cannot imagine what it's like.

2:53:07.220 --> 2:53:12.060
 So David was starting to explore that also.

2:53:12.060 --> 2:53:13.860
 I invited him to Hong Kong to give a talk

2:53:13.860 --> 2:53:15.660
 at Hong Kong PolyU,

2:53:15.660 --> 2:53:19.220
 and I introduced him in Hong Kong to some investors

2:53:19.220 --> 2:53:21.580
 who were interested in his robots.

2:53:21.580 --> 2:53:23.540
 And he didn't have Sophia then,

2:53:23.540 --> 2:53:25.140
 he had a robot of Philip K. Dick,

2:53:25.140 --> 2:53:26.980
 our favorite science fiction writer.

2:53:26.980 --> 2:53:28.180
 He had a robot Einstein,

2:53:28.180 --> 2:53:29.540
 he had some little toy robots

2:53:29.540 --> 2:53:31.940
 that looked like his son Zeno.

2:53:31.940 --> 2:53:35.620
 So through the investors I connected him to,

2:53:35.620 --> 2:53:37.500
 he managed to get some funding

2:53:37.500 --> 2:53:40.660
 to basically port Hanson Robotics to Hong Kong.

2:53:40.660 --> 2:53:42.660
 And when he first moved to Hong Kong,

2:53:42.660 --> 2:53:45.300
 I was working on AGI research

2:53:45.300 --> 2:53:49.340
 and also on this machine learning trading project.

2:53:49.340 --> 2:53:50.940
 So I didn't get that tightly involved

2:53:50.940 --> 2:53:52.980
 with Hanson Robotics.

2:53:52.980 --> 2:53:56.540
 But as I hung out with David more and more,

2:53:56.540 --> 2:53:59.180
 as we were both there in the same place,

2:53:59.180 --> 2:54:00.220
 I started to get,

2:54:01.260 --> 2:54:03.380
 I started to think about what you could do

2:54:04.620 --> 2:54:08.500
 to make his robots smarter than they were.

2:54:08.500 --> 2:54:10.340
 And so we started working together

2:54:10.340 --> 2:54:12.780
 and for a few years I was chief scientist

2:54:12.780 --> 2:54:15.740
 and head of software at Hanson Robotics.

2:54:15.740 --> 2:54:19.420
 Then when I got deeply into the blockchain side of things,

2:54:19.420 --> 2:54:24.340
 I stepped back from that and cofounded Singularity Net.

2:54:24.340 --> 2:54:26.340
 David Hanson was also one of the cofounders

2:54:26.340 --> 2:54:27.780
 of Singularity Net.

2:54:27.780 --> 2:54:30.060
 So part of our goal there had been

2:54:30.060 --> 2:54:33.940
 to make the blockchain based like cloud mind platform

2:54:33.940 --> 2:54:37.020
 for Sophia and the other Hanson robots.

2:54:37.020 --> 2:54:41.780
 Sophia would be just one of the robots in Singularity Net.

2:54:41.780 --> 2:54:43.300
 Yeah, yeah, yeah, exactly.

2:54:43.300 --> 2:54:47.380
 Sophia, many copies of the Sophia robot

2:54:47.380 --> 2:54:51.500
 would be among the user interfaces

2:54:51.500 --> 2:54:54.420
 to the globally distributed Singularity Net cloud mind.

2:54:54.420 --> 2:54:57.140
 And I mean, David and I talked about that

2:54:57.140 --> 2:55:01.540
 for quite a while before cofounding Singularity Net.

2:55:01.540 --> 2:55:04.380
 By the way, in his vision and your vision,

2:55:04.380 --> 2:55:09.380
 was Sophia tightly coupled to a particular AI system

2:55:09.580 --> 2:55:11.660
 or was the idea that you can plug,

2:55:11.660 --> 2:55:14.140
 you could just keep plugging in different AI systems

2:55:14.140 --> 2:55:15.100
 within the head of it?

2:55:15.100 --> 2:55:20.100
 David's view was always that Sophia would be a platform,

2:55:22.940 --> 2:55:26.820
 much like say the Pepper robot is a platform from SoftBank.

2:55:26.820 --> 2:55:31.660
 Should be a platform with a set of nicely designed APIs

2:55:31.660 --> 2:55:33.540
 that anyone can use to experiment

2:55:33.540 --> 2:55:38.540
 with their different AI algorithms on that platform.

2:55:38.620 --> 2:55:41.580
 And Singularity Net, of course, fits right into that, right?

2:55:41.580 --> 2:55:44.060
 Because Singularity Net, it's an API marketplace.

2:55:44.060 --> 2:55:46.220
 So anyone can put their AI on there.

2:55:46.220 --> 2:55:49.020
 OpenCog is a little bit different.

2:55:49.020 --> 2:55:52.140
 I mean, David likes it, but I'd say it's my thing.

2:55:52.140 --> 2:55:52.980
 It's not his.

2:55:52.980 --> 2:55:55.100
 Like David has a little more passion

2:55:55.100 --> 2:55:58.700
 for biologically based approaches to AI than I do,

2:55:58.700 --> 2:56:00.140
 which makes sense.

2:56:00.140 --> 2:56:02.860
 I mean, he's really into human physiology and biology.

2:56:02.860 --> 2:56:05.140
 He's a character sculptor, right?

2:56:05.140 --> 2:56:07.860
 So yeah, he's interested in,

2:56:07.860 --> 2:56:09.700
 but he also worked a lot with rule based

2:56:09.700 --> 2:56:11.420
 and logic based AI systems too.

2:56:11.420 --> 2:56:14.860
 So yeah, he's interested in not just Sophia,

2:56:14.860 --> 2:56:17.780
 but all the Hanson robots as a powerful social

2:56:17.780 --> 2:56:21.220
 and emotional robotics platform.

2:56:21.220 --> 2:56:26.220
 And what I saw in Sophia was a way to get AI algorithms

2:56:26.220 --> 2:56:31.220
 was a way to get AI algorithms out there

2:56:32.140 --> 2:56:34.660
 in front of a whole lot of different people

2:56:34.660 --> 2:56:36.300
 in an emotionally compelling way.

2:56:36.300 --> 2:56:39.820
 And part of my thought was really kind of abstract

2:56:39.820 --> 2:56:41.740
 connected to AGI ethics.

2:56:41.740 --> 2:56:46.740
 And many people are concerned AGI is gonna enslave everybody

2:56:46.940 --> 2:56:50.060
 or turn everybody into computronium

2:56:50.060 --> 2:56:54.740
 to make extra hard drives for their cognitive engine

2:56:54.740 --> 2:56:55.580
 or whatever.

2:56:55.580 --> 2:57:00.580
 And emotionally I'm not driven to that sort of paranoia.

2:57:01.660 --> 2:57:04.100
 I'm really just an optimist by nature,

2:57:04.100 --> 2:57:09.100
 but intellectually I have to assign a non zero probability

2:57:09.220 --> 2:57:12.140
 to those sorts of nasty outcomes.

2:57:12.140 --> 2:57:14.900
 Cause if you're making something 10 times as smart as you,

2:57:14.900 --> 2:57:16.300
 how can you know what it's gonna do?

2:57:16.300 --> 2:57:19.780
 There's an irreducible uncertainty there

2:57:19.780 --> 2:57:22.780
 just as my dog can't predict what I'm gonna do tomorrow.

2:57:22.780 --> 2:57:26.420
 So it seemed to me that based on our current state

2:57:26.420 --> 2:57:31.420
 of knowledge, the best way to bias the AGI as we create

2:57:32.500 --> 2:57:37.500
 toward benevolence would be to infuse them with love

2:57:38.820 --> 2:57:41.620
 and compassion the way that we do our own children.

2:57:41.620 --> 2:57:45.820
 So you want to interact with AIs in the context

2:57:45.820 --> 2:57:49.900
 of doing compassionate, loving and beneficial things.

2:57:49.900 --> 2:57:52.140
 And in that way, as your children will learn

2:57:52.140 --> 2:57:53.740
 by doing compassionate, beneficial,

2:57:53.740 --> 2:57:55.940
 loving things alongside you.

2:57:55.940 --> 2:57:58.660
 And that way the AI will learn in practice

2:57:58.660 --> 2:58:02.340
 what it means to be compassionate, beneficial and loving.

2:58:02.340 --> 2:58:06.380
 It will get a sort of ingrained intuitive sense of this,

2:58:06.380 --> 2:58:09.260
 which it can then abstract in its own way

2:58:09.260 --> 2:58:11.180
 as it gets more and more intelligent.

2:58:11.180 --> 2:58:12.780
 Now, David saw this the same way.

2:58:12.780 --> 2:58:15.540
 That's why he came up with the name Sophia,

2:58:15.540 --> 2:58:18.140
 which means wisdom.

2:58:18.140 --> 2:58:22.780
 So it seemed to me making these beautiful, loving robots

2:58:22.780 --> 2:58:26.060
 to be rolled out for beneficial applications

2:58:26.060 --> 2:58:31.060
 would be the perfect way to roll out early stage AGI systems

2:58:31.260 --> 2:58:33.940
 so they can learn from people

2:58:33.940 --> 2:58:35.420
 and not just learn factual knowledge,

2:58:35.420 --> 2:58:38.580
 but learn human values and ethics from people

2:58:38.580 --> 2:58:41.540
 while being their home service robots,

2:58:41.540 --> 2:58:44.100
 their education assistants, their nursing robots.

2:58:44.100 --> 2:58:46.060
 So that was the grand vision.

2:58:46.060 --> 2:58:48.620
 Now, if you've ever worked with robots,

2:58:48.620 --> 2:58:50.420
 the reality is quite different, right?

2:58:50.420 --> 2:58:53.220
 Like the first principle is the robot is always broken.

2:58:53.220 --> 2:58:57.660
 I mean, I worked with robots in the 90s a bunch

2:58:57.660 --> 2:58:59.540
 when you had to solder them together yourself

2:58:59.540 --> 2:59:02.580
 and I'd put neural nets during reinforcement learning

2:59:02.580 --> 2:59:05.940
 on like overturned solid ball type robots

2:59:05.940 --> 2:59:09.300
 and in the 90s when I was a professor.

2:59:09.300 --> 2:59:12.020
 Things of course advanced a lot, but...

2:59:12.020 --> 2:59:13.180
 But the principle still holds.

2:59:13.180 --> 2:59:16.500
 The principle that the robot's always broken still holds.

2:59:16.500 --> 2:59:21.020
 Yeah, so faced with the reality of making Sophia do stuff,

2:59:21.020 --> 2:59:26.020
 many of my robo AGI aspirations were temporarily cast aside.

2:59:26.620 --> 2:59:30.660
 And I mean, there's just a practical problem

2:59:30.660 --> 2:59:33.700
 of making this robot interact in a meaningful way

2:59:33.700 --> 2:59:36.700
 because like, you put nice computer vision on there,

2:59:36.700 --> 2:59:38.140
 but there's always glare.

2:59:38.140 --> 2:59:41.420
 And then, or you have a dialogue system,

2:59:41.420 --> 2:59:43.740
 but at the time I was there,

2:59:43.740 --> 2:59:46.580
 like no speech to text algorithm could deal

2:59:46.580 --> 2:59:49.780
 with Hong Kongese people's English accents.

2:59:49.780 --> 2:59:51.620
 So the speech to text was always bad.

2:59:51.620 --> 2:59:53.620
 So the robot always sounded stupid

2:59:53.620 --> 2:59:55.620
 because it wasn't getting the right text, right?

2:59:55.620 --> 2:59:58.020
 So I started to view that really

2:59:58.020 --> 3:00:02.820
 as what in software engineering you call a walking skeleton,

3:00:02.820 --> 3:00:05.420
 which is maybe the wrong metaphor to use for Sophia

3:00:05.420 --> 3:00:06.980
 or maybe the right one.

3:00:06.980 --> 3:00:08.420
 I mean, where the walking skeleton is

3:00:08.420 --> 3:00:10.620
 in software development is

3:00:10.620 --> 3:00:14.020
 if you're building a complex system, how do you get started?

3:00:14.020 --> 3:00:16.140
 But one way is to first build part one well,

3:00:16.140 --> 3:00:18.340
 then build part two well, then build part three well

3:00:18.340 --> 3:00:19.260
 and so on.

3:00:19.260 --> 3:00:22.060
 And the other way is you make like a simple version

3:00:22.060 --> 3:00:24.820
 of the whole system and put something in the place

3:00:24.820 --> 3:00:27.300
 of every part the whole system will need

3:00:27.300 --> 3:00:29.660
 so that you have a whole system that does something.

3:00:29.660 --> 3:00:31.900
 And then you work on improving each part

3:00:31.900 --> 3:00:34.340
 in the context of that whole integrated system.

3:00:34.340 --> 3:00:38.140
 So that's what we did on a software level in Sophia.

3:00:38.140 --> 3:00:41.580
 We made like a walking skeleton software system

3:00:41.580 --> 3:00:43.100
 where so there's something that sees,

3:00:43.100 --> 3:00:46.220
 there's something that hears, there's something that moves,

3:00:46.220 --> 3:00:48.180
 there's something that remembers,

3:00:48.180 --> 3:00:49.980
 there's something that learns.

3:00:49.980 --> 3:00:52.460
 You put a simple version of each thing in there

3:00:52.460 --> 3:00:54.420
 and you connect them all together

3:00:54.420 --> 3:00:56.660
 so that the system will do its thing.

3:00:56.660 --> 3:00:59.660
 So there's a lot of AI in there.

3:00:59.660 --> 3:01:01.380
 There's not any AGI in there.

3:01:01.380 --> 3:01:04.660
 I mean, there's computer vision to recognize people's faces,

3:01:04.660 --> 3:01:07.660
 recognize when someone comes in the room and leaves,

3:01:07.660 --> 3:01:10.740
 trying to recognize whether two people are together or not.

3:01:10.740 --> 3:01:13.300
 I mean, the dialogue system,

3:01:13.300 --> 3:01:18.300
 it's a mix of like hand coded rules with deep neural nets

3:01:18.780 --> 3:01:21.580
 that come up with their own responses.

3:01:21.580 --> 3:01:25.660
 And there's some attempt to have a narrative structure

3:01:25.660 --> 3:01:28.420
 and sort of try to pull the conversation

3:01:28.420 --> 3:01:30.780
 into something with a beginning, middle and end

3:01:30.780 --> 3:01:32.180
 and this sort of story arc.

3:01:32.180 --> 3:01:33.500
 So it's...

3:01:33.500 --> 3:01:37.620
 I mean, like if you look at the Lobner Prize and the systems

3:01:37.620 --> 3:01:39.060
 that beat the Turing Test currently,

3:01:39.060 --> 3:01:40.540
 they're heavily rule based

3:01:40.540 --> 3:01:43.900
 because like you had said, narrative structure

3:01:43.900 --> 3:01:45.700
 to create compelling conversations,

3:01:45.700 --> 3:01:48.420
 you currently, neural networks cannot do that well,

3:01:48.420 --> 3:01:50.660
 even with Google MENA.

3:01:50.660 --> 3:01:53.060
 When you actually look at full scale conversations,

3:01:53.060 --> 3:01:53.900
 it's just not...

3:01:53.900 --> 3:01:54.740
 Yeah, this is the thing.

3:01:54.740 --> 3:01:57.900
 So we've been, I've actually been running an experiment

3:01:57.900 --> 3:02:01.420
 the last couple of weeks taking Sophia's chat bot

3:02:01.420 --> 3:02:03.740
 and then Facebook's Transformer chat bot,

3:02:03.740 --> 3:02:05.260
 which they opened the model.

3:02:05.260 --> 3:02:06.780
 We've had them chatting to each other

3:02:06.780 --> 3:02:08.860
 for a number of weeks on the server just...

3:02:08.860 --> 3:02:10.020
 That's funny.

3:02:10.020 --> 3:02:13.260
 We're generating training data of what Sophia says

3:02:13.260 --> 3:02:15.500
 in a wide variety of conversations.

3:02:15.500 --> 3:02:20.260
 But we can see, compared to Sophia's current chat bot,

3:02:20.260 --> 3:02:23.460
 the Facebook deep neural chat bot comes up

3:02:23.460 --> 3:02:27.300
 with a wider variety of fluent sounding sentences.

3:02:27.300 --> 3:02:30.100
 On the other hand, it rambles like mad.

3:02:30.100 --> 3:02:33.900
 The Sophia chat bot, it's a little more repetitive

3:02:33.900 --> 3:02:36.620
 in the sentence structures it uses.

3:02:36.620 --> 3:02:39.820
 On the other hand, it's able to keep like a conversation arc

3:02:39.820 --> 3:02:42.460
 over a much longer, longer period, right?

3:02:42.460 --> 3:02:43.300
 So there...

3:02:43.300 --> 3:02:46.620
 Now, you can probably surmount that using Reformer

3:02:46.620 --> 3:02:51.140
 and like using various other deep neural architectures

3:02:51.140 --> 3:02:53.980
 to improve the way these Transformer models are trained.

3:02:53.980 --> 3:02:58.300
 But in the end, neither one of them really understands

3:02:58.300 --> 3:02:59.140
 what's going on.

3:02:59.140 --> 3:03:02.660
 I mean, that's the challenge I had with Sophia

3:03:02.660 --> 3:03:07.660
 is if I were doing a robotics project aimed at AGI,

3:03:08.340 --> 3:03:10.100
 I would wanna make like a robo toddler

3:03:10.100 --> 3:03:11.940
 that was just learning about what it was seeing.

3:03:11.940 --> 3:03:13.220
 Because then the language is grounded

3:03:13.220 --> 3:03:14.940
 in the experience of the robot.

3:03:14.940 --> 3:03:17.740
 But what Sophia needs to do to be Sophia

3:03:17.740 --> 3:03:21.420
 is talk about sports or the weather or robotics

3:03:21.420 --> 3:03:24.100
 or the conference she's talking at.

3:03:24.100 --> 3:03:26.380
 She needs to be fluent talking about

3:03:26.380 --> 3:03:28.420
 any damn thing in the world.

3:03:28.420 --> 3:03:32.500
 And she doesn't have grounding for all those things.

3:03:32.500 --> 3:03:35.700
 So there's this, just like, I mean, Google Mina

3:03:35.700 --> 3:03:37.460
 and Facebook's chat, but I don't have grounding

3:03:37.460 --> 3:03:40.140
 for what they're talking about either.

3:03:40.140 --> 3:03:45.060
 So in a way, the need to speak fluently about things

3:03:45.060 --> 3:03:47.940
 where there's no nonlinguistic grounding

3:03:47.940 --> 3:03:52.940
 pushes what you can do for Sophia in the short term

3:03:53.660 --> 3:03:56.340
 a bit away from AGI.

3:03:56.340 --> 3:04:00.900
 I mean, it pushes you towards IBM Watson situation

3:04:00.900 --> 3:04:02.740
 where you basically have to do heuristic

3:04:02.740 --> 3:04:05.380
 and hard code stuff and rule based stuff.

3:04:05.380 --> 3:04:07.860
 I have to ask you about this, okay.

3:04:07.860 --> 3:04:12.860
 So because in part Sophia is like an art creation

3:04:18.860 --> 3:04:20.100
 because it's beautiful.

3:04:21.260 --> 3:04:24.780
 She's beautiful because she inspires

3:04:24.780 --> 3:04:29.540
 through our human nature of anthropomorphize things.

3:04:29.540 --> 3:04:32.620
 We immediately see an intelligent being there.

3:04:32.620 --> 3:04:34.100
 Because David is a great sculptor.

3:04:34.100 --> 3:04:35.500
 He is a great sculptor, that's right.

3:04:35.500 --> 3:04:40.500
 So in fact, if Sophia just had nothing inside her head,

3:04:40.820 --> 3:04:43.260
 said nothing, if she just sat there,

3:04:43.260 --> 3:04:45.940
 we already prescribed some intelligence to her.

3:04:45.940 --> 3:04:47.780
 There's a long selfie line in front of her

3:04:47.780 --> 3:04:48.740
 after every talk.

3:04:48.740 --> 3:04:49.940
 That's right.

3:04:49.940 --> 3:04:53.820
 So it captivated the imagination of many people.

3:04:53.820 --> 3:04:54.860
 I wasn't gonna say the world,

3:04:54.860 --> 3:04:56.540
 but yeah, I mean a lot of people.

3:04:58.180 --> 3:05:00.180
 Billions of people, which is amazing.

3:05:00.180 --> 3:05:01.940
 It's amazing, right.

3:05:01.940 --> 3:05:06.940
 Now, of course, many people have prescribed

3:05:08.260 --> 3:05:11.060
 essentially AGI type of capabilities to Sophia

3:05:11.060 --> 3:05:12.380
 when they see her.

3:05:12.380 --> 3:05:17.380
 And of course, friendly French folk like Yann LeCun

3:05:19.860 --> 3:05:22.820
 immediately see that of the people from the AI community

3:05:22.820 --> 3:05:25.900
 and get really frustrated because...

3:05:25.900 --> 3:05:27.060
 It's understandable.

3:05:27.060 --> 3:05:31.700
 So what, and then they criticize people like you

3:05:31.700 --> 3:05:36.100
 who sit back and don't say anything about,

3:05:36.100 --> 3:05:39.980
 like basically allow the imagination of the world,

3:05:39.980 --> 3:05:42.420
 allow the world to continue being captivated.

3:05:43.860 --> 3:05:48.860
 So what's your sense of that kind of annoyance

3:05:49.140 --> 3:05:51.220
 that the AI community has?

3:05:51.220 --> 3:05:55.380
 I think there's several parts to my reaction there.

3:05:55.380 --> 3:05:59.820
 First of all, if I weren't involved with Hanson and Box

3:05:59.820 --> 3:06:03.420
 and didn't know David Hanson personally,

3:06:03.420 --> 3:06:06.420
 I probably would have been very annoyed initially

3:06:06.420 --> 3:06:07.980
 at Sophia as well.

3:06:07.980 --> 3:06:09.460
 I mean, I can understand the reaction.

3:06:09.460 --> 3:06:11.820
 I would have been like, wait,

3:06:11.820 --> 3:06:16.260
 all these stupid people out there think this is an AGI,

3:06:16.260 --> 3:06:19.980
 but it's not an AGI, but they're tricking people

3:06:19.980 --> 3:06:23.060
 that this very cool robot is an AGI.

3:06:23.060 --> 3:06:28.060
 And now those of us trying to raise funding to build AGI,

3:06:28.180 --> 3:06:31.180
 people will think it's already there and it already works.

3:06:31.180 --> 3:06:36.180
 So on the other hand, I think,

3:06:36.740 --> 3:06:38.340
 even if I weren't directly involved with it,

3:06:38.340 --> 3:06:41.660
 once I dug a little deeper into David and the robot

3:06:41.660 --> 3:06:43.460
 and the intentions behind it,

3:06:43.460 --> 3:06:47.020
 I think I would have stopped being pissed off.

3:06:47.020 --> 3:06:51.380
 Whereas folks like Yann LeCun have remained pissed off

3:06:51.380 --> 3:06:54.460
 after their initial reaction.

3:06:54.460 --> 3:06:56.100
 That's his thing, that's his thing.

3:06:56.100 --> 3:07:01.100
 I think that in particular struck me as somewhat ironic

3:07:01.940 --> 3:07:05.620
 because Yann LeCun is working for Facebook,

3:07:05.620 --> 3:07:09.020
 which is using machine learning to program the brains

3:07:09.020 --> 3:07:13.340
 of the people in the world toward vapid consumerism

3:07:13.340 --> 3:07:14.860
 and political extremism.

3:07:14.860 --> 3:07:19.660
 So if your ethics allows you to use machine learning

3:07:19.660 --> 3:07:23.460
 in such a blatantly destructive way,

3:07:23.460 --> 3:07:26.220
 why would your ethics not allow you to use machine learning

3:07:26.220 --> 3:07:29.780
 to make a lovable theatrical robot

3:07:29.780 --> 3:07:32.100
 that draws some foolish people

3:07:32.100 --> 3:07:34.420
 into its theatrical illusion?

3:07:34.420 --> 3:07:38.780
 Like if the pushback had come from Yoshua Bengio,

3:07:38.780 --> 3:07:40.900
 I would have felt much more humbled by it

3:07:40.900 --> 3:07:45.460
 because he's not using AI for blatant evil, right?

3:07:45.460 --> 3:07:48.540
 On the other hand, he also is a super nice guy

3:07:48.540 --> 3:07:50.860
 and doesn't bother to go out there

3:07:50.860 --> 3:07:54.420
 trashing other people's work for no good reason, right?

3:07:54.420 --> 3:07:55.940
 Shots fired, but I get you.

3:07:55.940 --> 3:07:58.020
 I mean, that's...

3:07:58.020 --> 3:08:01.100
 I mean, if you're gonna ask, I'm gonna answer.

3:08:01.100 --> 3:08:02.060
 No, for sure.

3:08:02.060 --> 3:08:03.300
 I think we'll go back and forth.

3:08:03.300 --> 3:08:04.500
 I'll talk to Yann again.

3:08:04.500 --> 3:08:06.060
 I would add on this though.

3:08:06.060 --> 3:08:11.060
 I mean, David Hansen is an artist

3:08:11.540 --> 3:08:14.180
 and he often speaks off the cuff.

3:08:14.180 --> 3:08:16.300
 And I have not agreed with everything

3:08:16.300 --> 3:08:19.300
 that David has said or done regarding Sophia.

3:08:19.300 --> 3:08:22.740
 And David also has not agreed with everything

3:08:22.740 --> 3:08:24.740
 David has said or done about Sophia.

3:08:24.740 --> 3:08:25.780
 That's an important point.

3:08:25.780 --> 3:08:30.140
 I mean, David is an artistic wild man

3:08:30.140 --> 3:08:33.340
 and that's part of his charm.

3:08:33.340 --> 3:08:34.740
 That's part of his genius.

3:08:34.740 --> 3:08:39.380
 So certainly there have been conversations

3:08:39.380 --> 3:08:42.260
 within Hansen Robotics and between me and David

3:08:42.260 --> 3:08:45.700
 where I was like, let's be more open

3:08:45.700 --> 3:08:48.180
 about how this thing is working.

3:08:48.180 --> 3:08:52.060
 And I did have some influence in nudging Hansen Robotics

3:08:52.060 --> 3:08:56.740
 to be more open about how Sophia was working.

3:08:56.740 --> 3:09:00.740
 And David wasn't especially opposed to this.

3:09:00.740 --> 3:09:02.460
 And he was actually quite right about it.

3:09:02.460 --> 3:09:04.940
 What he said was, you can tell people exactly

3:09:04.940 --> 3:09:08.020
 how it's working and they won't care.

3:09:08.020 --> 3:09:09.580
 They want to be drawn into the illusion.

3:09:09.580 --> 3:09:12.580
 And he was 100% correct.

3:09:12.580 --> 3:09:14.620
 I'll tell you what, this wasn't Sophia.

3:09:14.620 --> 3:09:15.740
 This was Philip K. Dick.

3:09:15.740 --> 3:09:18.780
 But we did some interactions between humans

3:09:18.780 --> 3:09:23.780
 and Philip K. Dick robot in Austin, Texas a few years back.

3:09:23.820 --> 3:09:26.700
 And in this case, the Philip K. Dick was just teleoperated

3:09:26.700 --> 3:09:28.540
 by another human in the other room.

3:09:28.540 --> 3:09:31.260
 So during the conversations, we didn't tell people

3:09:31.260 --> 3:09:32.860
 the robot was teleoperated.

3:09:32.860 --> 3:09:35.020
 We just said, here, have a conversation with Phil Dick.

3:09:35.020 --> 3:09:37.100
 We're gonna film you, right?

3:09:37.100 --> 3:09:39.740
 And they had a great conversation with Philip K. Dick

3:09:39.740 --> 3:09:42.900
 teleoperated by my friend, Stefan Bugaj.

3:09:42.900 --> 3:09:45.860
 After the conversation, we brought the people

3:09:45.860 --> 3:09:47.980
 in the back room to see Stefan

3:09:47.980 --> 3:09:52.980
 who was controlling the Philip K. Dick robot,

3:09:53.540 --> 3:09:54.820
 but they didn't believe it.

3:09:54.820 --> 3:09:56.500
 These people were like, well, yeah,

3:09:56.500 --> 3:09:58.780
 but I know I was talking to Phil.

3:09:58.780 --> 3:10:00.780
 Maybe Stefan was typing,

3:10:00.780 --> 3:10:03.820
 but the spirit of Phil was animating his mind

3:10:03.820 --> 3:10:05.100
 while he was typing.

3:10:05.100 --> 3:10:07.660
 So like, even though they knew it was a human in the loop,

3:10:07.660 --> 3:10:09.420
 even seeing the guy there,

3:10:09.420 --> 3:10:12.860
 they still believed that was Phil they were talking to.

3:10:12.860 --> 3:10:16.700
 A small part of me believes that they were right, actually.

3:10:16.700 --> 3:10:17.900
 Because our understanding...

3:10:17.900 --> 3:10:19.460
 Well, we don't understand the universe.

3:10:19.460 --> 3:10:20.300
 That's the thing.

3:10:20.300 --> 3:10:22.460
 I mean, there is a cosmic mind field

3:10:22.460 --> 3:10:24.300
 that we're all embedded in

3:10:24.300 --> 3:10:28.260
 that yields many strange synchronicities in the world,

3:10:28.260 --> 3:10:31.540
 which is a topic we don't have time to go into too much here.

3:10:31.540 --> 3:10:35.020
 Yeah, I mean, there's something to this

3:10:35.020 --> 3:10:39.740
 where our imagination about Sophia

3:10:39.740 --> 3:10:43.260
 and people like Yann LeCun being frustrated about it

3:10:43.260 --> 3:10:45.860
 is all part of this beautiful dance

3:10:45.860 --> 3:10:47.420
 of creating artificial intelligence

3:10:47.420 --> 3:10:48.900
 that's almost essential.

3:10:48.900 --> 3:10:50.420
 You see with Boston Dynamics,

3:10:50.420 --> 3:10:53.340
 whom I'm a huge fan of as well,

3:10:53.340 --> 3:10:54.260
 you know, the kind of...

3:10:54.260 --> 3:10:58.380
 I mean, these robots are very far from intelligent.

3:10:58.380 --> 3:11:01.940
 I played with their last one, actually.

3:11:01.940 --> 3:11:02.780
 With a spot mini.

3:11:02.780 --> 3:11:03.620
 Yeah, very cool.

3:11:03.620 --> 3:11:07.180
 I mean, it reacts quite in a fluid and flexible way.

3:11:07.180 --> 3:11:10.500
 But we immediately ascribe the kind of intelligence.

3:11:10.500 --> 3:11:12.500
 We immediately ascribe AGI to them.

3:11:12.500 --> 3:11:14.820
 Yeah, yeah, if you kick it and it falls down and goes out,

3:11:14.820 --> 3:11:15.660
 you feel bad, right?

3:11:15.660 --> 3:11:17.300
 You can't help it.

3:11:17.300 --> 3:11:21.820
 And I mean, that's part of...

3:11:21.820 --> 3:11:23.180
 That's gonna be part of our journey

3:11:23.180 --> 3:11:24.540
 in creating intelligent systems

3:11:24.540 --> 3:11:25.660
 more and more and more and more.

3:11:25.660 --> 3:11:29.460
 Like, as Sophia starts out with a walking skeleton,

3:11:29.460 --> 3:11:31.980
 as you add more and more intelligence,

3:11:31.980 --> 3:11:34.500
 I mean, we're gonna have to deal with this kind of idea.

3:11:34.500 --> 3:11:35.340
 Absolutely.

3:11:35.340 --> 3:11:37.660
 And about Sophia, I would say,

3:11:37.660 --> 3:11:39.900
 I mean, first of all, I have nothing against Yann LeCun.

3:11:39.900 --> 3:11:40.860
 No, no, this is fun.

3:11:40.860 --> 3:11:41.700
 This is all for fun.

3:11:41.700 --> 3:11:42.540
 He's a nice guy.

3:11:42.540 --> 3:11:45.820
 If he wants to play the media banter game,

3:11:45.820 --> 3:11:48.020
 I'm happy to play him.

3:11:48.020 --> 3:11:50.860
 He's a good researcher and a good human being.

3:11:50.860 --> 3:11:53.580
 I'd happily work with the guy.

3:11:53.580 --> 3:11:56.220
 The other thing I was gonna say is,

3:11:56.220 --> 3:12:00.340
 I have been explicit about how Sophia works

3:12:00.340 --> 3:12:04.580
 and I've posted online and what, H Plus Magazine,

3:12:04.580 --> 3:12:06.420
 an online webzine.

3:12:06.420 --> 3:12:09.780
 I mean, I posted a moderately detailed article

3:12:09.780 --> 3:12:12.860
 explaining like, there are three software systems

3:12:12.860 --> 3:12:14.380
 we've used inside Sophia.

3:12:14.380 --> 3:12:16.660
 There's a timeline editor,

3:12:16.660 --> 3:12:18.820
 which is like a rule based authoring system

3:12:18.820 --> 3:12:21.140
 where she's really just being an outlet

3:12:21.140 --> 3:12:22.660
 for what a human scripted.

3:12:22.660 --> 3:12:23.660
 There's a chat bot,

3:12:23.660 --> 3:12:26.420
 which has some rule based and some neural aspects.

3:12:26.420 --> 3:12:29.420
 And then sometimes we've used OpenCog behind Sophia,

3:12:29.420 --> 3:12:31.900
 where there's more learning and reasoning.

3:12:31.900 --> 3:12:34.980
 And the funny thing is,

3:12:34.980 --> 3:12:37.700
 I can't always tell which system is operating here, right?

3:12:37.700 --> 3:12:41.700
 I mean, whether she's really learning or thinking,

3:12:41.700 --> 3:12:44.660
 or just appears to be over a half hour, I could tell,

3:12:44.660 --> 3:12:47.460
 but over like three or four minutes of interaction,

3:12:47.460 --> 3:12:48.940
 I could tell.

3:12:48.940 --> 3:12:49.900
 So even having three systems

3:12:49.900 --> 3:12:51.500
 that's already sufficiently complex

3:12:51.500 --> 3:12:53.020
 where you can't really tell right away.

3:12:53.020 --> 3:12:56.980
 Yeah, the thing is, even if you get up on stage

3:12:56.980 --> 3:12:59.540
 and tell people how Sophia is working,

3:12:59.540 --> 3:13:00.900
 and then they talk to her,

3:13:01.780 --> 3:13:06.100
 they still attribute more agency and consciousness to her

3:13:06.100 --> 3:13:08.900
 than is really there.

3:13:08.900 --> 3:13:13.820
 So I think there's a couple of levels of ethical issue there.

3:13:13.820 --> 3:13:18.340
 One issue is, should you be transparent

3:13:18.340 --> 3:13:21.540
 about how Sophia is working?

3:13:21.540 --> 3:13:22.860
 And I think you should,

3:13:22.860 --> 3:13:26.140
 and I think we have been.

3:13:26.140 --> 3:13:29.100
 I mean, there's articles online,

3:13:29.100 --> 3:13:32.780
 there's some TV special that goes through me

3:13:32.780 --> 3:13:35.380
 explaining the three subsystems behind Sophia.

3:13:35.380 --> 3:13:38.420
 So the way Sophia works

3:13:38.420 --> 3:13:41.420
 is out there much more clearly

3:13:41.420 --> 3:13:43.340
 than how Facebook's AI works or something, right?

3:13:43.340 --> 3:13:45.900
 I mean, we've been fairly explicit about it.

3:13:45.900 --> 3:13:50.500
 The other is, given that telling people how it works

3:13:50.500 --> 3:13:52.380
 doesn't cause them to not attribute

3:13:52.380 --> 3:13:55.060
 too much intelligence agency to it anyway,

3:13:55.060 --> 3:13:58.260
 then should you keep fooling them

3:13:58.260 --> 3:14:01.100
 when they want to be fooled?

3:14:01.100 --> 3:14:03.620
 And I mean, the whole media industry

3:14:03.620 --> 3:14:06.700
 is based on fooling people the way they want to be fooled.

3:14:06.700 --> 3:14:11.700
 And we are fooling people 100% toward a good end.

3:14:11.700 --> 3:14:16.700
 I mean, we are playing on people's sense of empathy

3:14:18.020 --> 3:14:20.540
 and compassion so that we can give them

3:14:20.540 --> 3:14:23.620
 a good user experience with helpful robots.

3:14:23.620 --> 3:14:27.820
 And so that we can fill the AI's mind

3:14:27.820 --> 3:14:29.420
 with love and compassion.

3:14:29.420 --> 3:14:34.100
 So I've been talking a lot with Hanson Robotics lately

3:14:34.100 --> 3:14:37.580
 about collaborations in the area of medical robotics.

3:14:37.580 --> 3:14:41.500
 And we haven't quite pulled the trigger on a project

3:14:41.500 --> 3:14:44.700
 in that domain yet, but we may well do so quite soon.

3:14:44.700 --> 3:14:48.220
 So we've been talking a lot about robots

3:14:48.220 --> 3:14:51.340
 can help with elder care, robots can help with kids.

3:14:51.340 --> 3:14:54.180
 David's done a lot of things with autism therapy

3:14:54.180 --> 3:14:56.540
 and robots before.

3:14:56.540 --> 3:14:58.660
 In the COVID era, having a robot

3:14:58.660 --> 3:15:00.620
 that can be a nursing assistant in various senses

3:15:00.620 --> 3:15:02.340
 can be quite valuable.

3:15:02.340 --> 3:15:04.180
 The robots don't spread infection

3:15:04.180 --> 3:15:06.300
 and they can also deliver more attention

3:15:06.300 --> 3:15:07.940
 than human nurses can give, right?

3:15:07.940 --> 3:15:11.180
 So if you have a robot that's helping a patient

3:15:11.180 --> 3:15:15.700
 with COVID, if that patient attributes more understanding

3:15:15.700 --> 3:15:19.060
 and compassion and agency to that robot than it really has

3:15:19.060 --> 3:15:22.940
 because it looks like a human, I mean, is that really bad?

3:15:22.940 --> 3:15:25.660
 I mean, we can tell them it doesn't fully understand you

3:15:25.660 --> 3:15:27.700
 and they don't care because they're lying there

3:15:27.700 --> 3:15:29.340
 with a fever and they're sick,

3:15:29.340 --> 3:15:31.020
 but they'll react better to that robot

3:15:31.020 --> 3:15:33.500
 with its loving, warm facial expression

3:15:33.500 --> 3:15:35.420
 than they would to a pepper robot

3:15:35.420 --> 3:15:38.100
 or a metallic looking robot.

3:15:38.100 --> 3:15:41.340
 So it's really, it's about how you use it, right?

3:15:41.340 --> 3:15:45.100
 If you made a human looking like door to door sales robot

3:15:45.100 --> 3:15:47.140
 that used its human looking appearance

3:15:47.140 --> 3:15:49.940
 to scam people out of their money,

3:15:49.940 --> 3:15:53.900
 then you're using that connection in a bad way,

3:15:53.900 --> 3:15:57.060
 but you could also use it in a good way.

3:15:57.060 --> 3:16:01.740
 But then that's the same problem with every technology.

3:16:01.740 --> 3:16:02.980
 Beautifully put.

3:16:02.980 --> 3:16:07.900
 So like you said, we're living in the era

3:16:07.900 --> 3:16:10.900
 of the COVID, this is 2020,

3:16:10.900 --> 3:16:14.740
 one of the craziest years in recent history.

3:16:14.740 --> 3:16:19.740
 So if we zoom out and look at this pandemic,

3:16:21.420 --> 3:16:23.180
 the coronavirus pandemic,

3:16:24.380 --> 3:16:29.380
 maybe let me ask you this kind of thing in viruses in general,

3:16:29.820 --> 3:16:32.620
 when you look at viruses,

3:16:32.620 --> 3:16:35.900
 do you see them as a kind of intelligence system?

3:16:35.900 --> 3:16:38.700
 I think the concept of intelligence is not that natural

3:16:38.700 --> 3:16:39.740
 of a concept in the end.

3:16:39.740 --> 3:16:43.700
 I mean, I think human minds and bodies

3:16:43.700 --> 3:16:48.700
 are a kind of complex self organizing adaptive system.

3:16:49.380 --> 3:16:51.900
 And viruses certainly are that, right?

3:16:51.900 --> 3:16:54.980
 They're a very complex self organizing adaptive system.

3:16:54.980 --> 3:16:58.380
 If you wanna look at intelligence as Marcus Hutter defines it

3:16:58.380 --> 3:17:02.300
 as sort of optimizing computable reward functions

3:17:02.300 --> 3:17:04.740
 over computable environments,

3:17:04.740 --> 3:17:06.700
 for sure viruses are doing that, right?

3:17:06.700 --> 3:17:11.700
 And I mean, in doing so they're causing some harm to us.

3:17:13.820 --> 3:17:17.780
 So the human immune system is a very complex

3:17:17.780 --> 3:17:19.340
 of organizing adaptive system,

3:17:19.340 --> 3:17:21.100
 which has a lot of intelligence to it.

3:17:21.100 --> 3:17:23.980
 And viruses are also adapting

3:17:23.980 --> 3:17:27.660
 and dividing into new mutant strains and so forth.

3:17:27.660 --> 3:17:31.660
 And ultimately the solution is gonna be nanotechnology,

3:17:31.660 --> 3:17:32.500
 right?

3:17:32.500 --> 3:17:35.940
 The solution is gonna be making little nanobots that.

3:17:35.940 --> 3:17:38.060
 Fight the viruses or.

3:17:38.060 --> 3:17:40.660
 Well, people will use them to make nastier viruses,

3:17:40.660 --> 3:17:42.020
 but hopefully we can also use them

3:17:42.020 --> 3:17:46.220
 to just detect combat and kill the viruses.

3:17:46.220 --> 3:17:48.820
 But I think now we're stuck

3:17:48.820 --> 3:17:53.820
 with the biological mechanisms to combat these viruses.

3:17:54.980 --> 3:17:59.500
 And yeah, we've been AGI is not yet mature enough

3:17:59.500 --> 3:18:01.580
 to use against COVID,

3:18:01.580 --> 3:18:03.980
 but we've been using machine learning

3:18:03.980 --> 3:18:07.020
 and also some machine reasoning in open cog

3:18:07.020 --> 3:18:10.420
 to help some doctors to do personalized medicine

3:18:10.420 --> 3:18:11.260
 against COVID.

3:18:11.260 --> 3:18:14.140
 So the problem there is given the person's genomics

3:18:14.140 --> 3:18:16.460
 and given their clinical medical indicators,

3:18:16.460 --> 3:18:20.220
 how do you figure out which combination of antivirals

3:18:20.220 --> 3:18:24.260
 is gonna be most effective against COVID for that person?

3:18:24.260 --> 3:18:26.420
 And so that's something

3:18:26.420 --> 3:18:28.500
 where machine learning is interesting,

3:18:28.500 --> 3:18:30.380
 but also we're finding the abstraction

3:18:30.380 --> 3:18:33.860
 to get an open cog with machine reasoning is interesting

3:18:33.860 --> 3:18:36.660
 because it can help with transfer learning

3:18:36.660 --> 3:18:40.380
 when you have not that many different cases to study

3:18:40.380 --> 3:18:43.900
 and qualitative differences between different strains

3:18:43.900 --> 3:18:47.180
 of a virus or people of different ages who may have COVID.

3:18:47.180 --> 3:18:50.700
 So there's a lot of different disparate data to work with

3:18:50.700 --> 3:18:53.740
 and it's small data sets and somehow integrating them.

3:18:53.740 --> 3:18:55.500
 This is one of the shameful things

3:18:55.500 --> 3:18:57.300
 that's very hard to get that data.

3:18:57.300 --> 3:19:00.340
 So, I mean, we're working with a couple of groups

3:19:00.340 --> 3:19:04.780
 doing clinical trials and they're sharing data with us

3:19:04.780 --> 3:19:06.860
 like under non disclosure,

3:19:06.860 --> 3:19:10.660
 but what should be the case is like every COVID

3:19:10.660 --> 3:19:14.420
 clinical trial should be putting data online somewhere

3:19:14.420 --> 3:19:17.820
 like suitably encrypted to protect patient privacy

3:19:17.820 --> 3:19:20.980
 so that anyone with the right AI algorithms

3:19:20.980 --> 3:19:22.300
 should be able to help analyze it

3:19:22.300 --> 3:19:24.500
 and any biologists should be able to analyze it by hand

3:19:24.500 --> 3:19:25.860
 to understand what they can, right?

3:19:25.860 --> 3:19:30.060
 Instead that data is like siloed inside whatever hospital

3:19:30.060 --> 3:19:31.740
 is running the clinical trial,

3:19:31.740 --> 3:19:35.060
 which is completely asinine and ridiculous.

3:19:35.060 --> 3:19:37.820
 So why the world works that way?

3:19:37.820 --> 3:19:39.140
 I mean, we could all analyze why,

3:19:39.140 --> 3:19:40.700
 but it's insane that it does.

3:19:40.700 --> 3:19:44.060
 You look at this hydrochloroquine, right?

3:19:44.060 --> 3:19:45.700
 All these clinical trials were done

3:19:45.700 --> 3:19:47.700
 were reported by Surgisphere,

3:19:47.700 --> 3:19:50.220
 some little company no one ever heard of

3:19:50.220 --> 3:19:53.220
 and everyone paid attention to this.

3:19:53.220 --> 3:19:55.540
 So they were doing more clinical trials based on that

3:19:55.540 --> 3:19:57.460
 then they stopped doing clinical trials based on that

3:19:57.460 --> 3:19:58.460
 then they started again

3:19:58.460 --> 3:20:01.420
 and why isn't that data just out there

3:20:01.420 --> 3:20:05.060
 so everyone can analyze it and see what's going on, right?

3:20:05.060 --> 3:20:10.060
 Do you have hope that data will be out there eventually

3:20:10.580 --> 3:20:11.860
 for future pandemics?

3:20:11.860 --> 3:20:13.620
 I mean, do you have hope that our society

3:20:13.620 --> 3:20:15.420
 will move in the direction of?

3:20:15.420 --> 3:20:16.860
 It's not in the immediate future

3:20:16.860 --> 3:20:21.580
 because the US and China frictions are getting very high.

3:20:21.580 --> 3:20:24.380
 So it's hard to see US and China

3:20:24.380 --> 3:20:26.660
 as moving in the direction of openly sharing data

3:20:26.660 --> 3:20:27.580
 with each other, right?

3:20:27.580 --> 3:20:30.780
 It's not, there's some sharing of data,

3:20:30.780 --> 3:20:32.940
 but different groups are keeping their data private

3:20:32.940 --> 3:20:34.660
 till they've milked the best results from it

3:20:34.660 --> 3:20:36.220
 and then they share it, right?

3:20:36.220 --> 3:20:39.140
 So yeah, we're working with some data

3:20:39.140 --> 3:20:41.380
 that we've managed to get our hands on,

3:20:41.380 --> 3:20:43.140
 something we're doing to do good for the world

3:20:43.140 --> 3:20:44.620
 and it's a very cool playground

3:20:44.620 --> 3:20:47.860
 for like putting deep neural nets and open cog together.

3:20:47.860 --> 3:20:49.900
 So we have like a bioadden space

3:20:49.900 --> 3:20:51.860
 full of all sorts of knowledge

3:20:51.860 --> 3:20:53.620
 from many different biology experiments

3:20:53.620 --> 3:20:54.700
 about human longevity

3:20:54.700 --> 3:20:57.660
 and from biology knowledge bases online.

3:20:57.660 --> 3:21:00.780
 And we can do like graph to vector type embeddings

3:21:00.780 --> 3:21:03.060
 where we take nodes from the hypergraph,

3:21:03.060 --> 3:21:04.580
 embed them into vectors,

3:21:04.580 --> 3:21:06.180
 which can then feed into neural nets

3:21:06.180 --> 3:21:07.900
 for different types of analysis.

3:21:07.900 --> 3:21:09.980
 And we were doing this

3:21:09.980 --> 3:21:13.180
 in the context of a project called Rejuve

3:21:13.180 --> 3:21:15.540
 that we spun off from SingularityNet

3:21:15.540 --> 3:21:18.580
 to do longevity analytics,

3:21:18.580 --> 3:21:21.220
 like understand why people live to 105 years or over

3:21:21.220 --> 3:21:22.300
 and other people don't.

3:21:22.300 --> 3:21:25.740
 And then we had this spin off Singularity Studio

3:21:25.740 --> 3:21:28.900
 where we're working with some healthcare companies

3:21:28.900 --> 3:21:31.060
 on data analytics.

3:21:31.060 --> 3:21:33.100
 But so there's bioadden space

3:21:33.100 --> 3:21:35.420
 that we built for these more commercial

3:21:35.420 --> 3:21:38.140
 and longevity data analysis purposes.

3:21:38.140 --> 3:21:41.220
 We're repurposing and feeding COVID data

3:21:41.220 --> 3:21:44.380
 into the same bioadden space

3:21:44.380 --> 3:21:47.540
 and playing around with like graph embeddings

3:21:47.540 --> 3:21:51.180
 from that graph into neural nets for bioinformatics.

3:21:51.180 --> 3:21:54.740
 So it's both being a cool testing ground,

3:21:54.740 --> 3:21:57.260
 some of our bio AI learning and reasoning.

3:21:57.260 --> 3:21:59.980
 And it seems we're able to discover things

3:21:59.980 --> 3:22:01.900
 that people weren't seeing otherwise.

3:22:01.900 --> 3:22:03.820
 Cause the thing in this case is

3:22:03.820 --> 3:22:05.820
 for each combination of antivirals,

3:22:05.820 --> 3:22:07.060
 you may have only a few patients

3:22:07.060 --> 3:22:08.900
 who've tried that combination.

3:22:08.900 --> 3:22:09.980
 And those few patients

3:22:09.980 --> 3:22:11.700
 may have their particular characteristics.

3:22:11.700 --> 3:22:13.380
 Like this combination of three

3:22:13.380 --> 3:22:16.260
 was tried only on people age 80 or over.

3:22:16.260 --> 3:22:18.140
 This other combination of three,

3:22:18.140 --> 3:22:20.500
 which has an overlap with the first combination

3:22:20.500 --> 3:22:22.060
 was tried more on young people.

3:22:22.060 --> 3:22:25.500
 So how do you combine those different pieces of data?

3:22:25.500 --> 3:22:28.620
 It's a very dodgy transfer learning problem,

3:22:28.620 --> 3:22:29.580
 which is the kind of thing

3:22:29.580 --> 3:22:31.660
 that the probabilistic reasoning algorithms

3:22:31.660 --> 3:22:34.140
 we have inside OpenCog are better at

3:22:34.140 --> 3:22:35.220
 than deep neural networks.

3:22:35.220 --> 3:22:38.260
 On the other hand, you have gene expression data

3:22:38.260 --> 3:22:39.740
 where you have 25,000 genes

3:22:39.740 --> 3:22:41.340
 and the expression level of each gene

3:22:41.340 --> 3:22:43.620
 in the peripheral blood of each person.

3:22:43.620 --> 3:22:44.980
 So that sort of data,

3:22:44.980 --> 3:22:48.220
 either deep neural nets or tools like XGBoost or CatBoost,

3:22:48.220 --> 3:22:50.900
 these decision forest trees are better at dealing

3:22:50.900 --> 3:22:52.100
 with than OpenCog.

3:22:52.100 --> 3:22:53.940
 Cause it's just these huge,

3:22:53.940 --> 3:22:55.860
 huge messy floating point vectors

3:22:55.860 --> 3:22:59.180
 that are annoying for a logic engine to deal with,

3:22:59.180 --> 3:23:02.540
 but are perfect for a decision forest or a neural net.

3:23:02.540 --> 3:23:07.540
 So it's a great playground for like hybrid AI methodology.

3:23:07.820 --> 3:23:11.100
 And we can have SingularityNet have OpenCog in one agent

3:23:11.100 --> 3:23:12.780
 and XGBoost in a different agent

3:23:12.780 --> 3:23:14.540
 and they talk to each other.

3:23:14.540 --> 3:23:18.060
 But at the same time, it's highly practical, right?

3:23:18.060 --> 3:23:20.580
 Cause we're working with, for example,

3:23:20.580 --> 3:23:24.620
 some physicians on this project,

3:23:24.620 --> 3:23:27.500
 physicians in the group called Nth Opinion

3:23:27.500 --> 3:23:30.180
 based out of Vancouver in Seattle,

3:23:30.180 --> 3:23:32.980
 who are, these guys are working every day

3:23:32.980 --> 3:23:36.540
 like in the hospital with patients dying of COVID.

3:23:36.540 --> 3:23:41.100
 So it's quite cool to see like neural symbolic AI,

3:23:41.100 --> 3:23:43.340
 like where the rubber hits the road,

3:23:43.340 --> 3:23:45.460
 trying to save people's lives.

3:23:45.460 --> 3:23:48.540
 I've been doing bio AI since 2001,

3:23:48.540 --> 3:23:51.220
 but mostly human longevity research

3:23:51.220 --> 3:23:53.100
 and fly longevity research,

3:23:53.100 --> 3:23:57.220
 try to understand why some organisms really live a long time.

3:23:57.220 --> 3:24:00.380
 This is the first time like race against the clock

3:24:00.380 --> 3:24:04.660
 and try to use the AI to figure out stuff that,

3:24:04.660 --> 3:24:09.620
 like if we take two months longer to solve the AI problem,

3:24:09.620 --> 3:24:10.740
 some more people will die

3:24:10.740 --> 3:24:12.220
 because we don't know what combination

3:24:12.220 --> 3:24:14.140
 of antivirals to give them.

3:24:14.140 --> 3:24:16.660
 At the societal level, at the biological level,

3:24:16.660 --> 3:24:21.260
 at any level, are you hopeful about us

3:24:21.260 --> 3:24:24.940
 as a human species getting out of this pandemic?

3:24:24.940 --> 3:24:26.700
 What are your thoughts on it in general?

3:24:26.700 --> 3:24:28.980
 The pandemic will be gone in a year or two

3:24:28.980 --> 3:24:30.500
 once there's a vaccine for it.

3:24:30.500 --> 3:24:32.980
 So, I mean, that's...

3:24:32.980 --> 3:24:35.580
 A lot of pain and suffering can happen in that time.

3:24:35.580 --> 3:24:38.580
 So that could be irreversible.

3:24:38.580 --> 3:24:43.180
 I think if you spend much time in Sub Saharan Africa,

3:24:43.180 --> 3:24:45.220
 you can see there's a lot of pain and suffering

3:24:45.220 --> 3:24:47.620
 happening all the time.

3:24:47.620 --> 3:24:49.660
 Like you walk through the streets

3:24:49.660 --> 3:24:53.340
 of any large city in Sub Saharan Africa,

3:24:53.340 --> 3:24:56.860
 and there are loads, I mean, tens of thousands,

3:24:56.860 --> 3:24:59.300
 probably hundreds of thousands of people

3:24:59.300 --> 3:25:01.540
 lying by the side of the road,

3:25:01.540 --> 3:25:06.060
 dying mainly of curable diseases without food or water

3:25:06.060 --> 3:25:07.940
 and either ostracized by their families

3:25:07.940 --> 3:25:09.140
 or they left their family house

3:25:09.140 --> 3:25:11.220
 because they didn't want to infect their family, right?

3:25:11.220 --> 3:25:14.420
 I mean, there's tremendous human suffering

3:25:14.420 --> 3:25:17.220
 on the planet all the time,

3:25:17.220 --> 3:25:21.780
 which most folks in the developed world pay no attention to.

3:25:21.780 --> 3:25:25.100
 And COVID is not remotely the worst.

3:25:25.100 --> 3:25:27.940
 How many people are dying of malaria all the time?

3:25:27.940 --> 3:25:30.460
 I mean, so COVID is bad.

3:25:30.460 --> 3:25:33.180
 It is by no mean the worst thing happening.

3:25:33.180 --> 3:25:36.100
 And setting aside diseases,

3:25:36.100 --> 3:25:38.340
 I mean, there are many places in the world

3:25:38.340 --> 3:25:41.180
 where you're at risk of having like your teenage son

3:25:41.180 --> 3:25:44.220
 kidnapped by armed militias and forced to get killed

3:25:44.220 --> 3:25:46.980
 in someone else's war, fighting tribe against tribe.

3:25:46.980 --> 3:25:50.500
 I mean, so humanity has a lot of problems

3:25:50.500 --> 3:25:53.740
 which we don't need to have given the state of advancement

3:25:53.740 --> 3:25:56.060
 of our technology right now.

3:25:56.060 --> 3:25:59.860
 And I think COVID is one of the easier problems to solve

3:25:59.860 --> 3:26:02.380
 in the sense that there are many brilliant people

3:26:02.380 --> 3:26:03.580
 working on vaccines.

3:26:03.580 --> 3:26:06.020
 We have the technology to create vaccines

3:26:06.020 --> 3:26:08.580
 and we're gonna create new vaccines.

3:26:08.580 --> 3:26:09.500
 We should be more worried

3:26:09.500 --> 3:26:12.940
 that we haven't managed to defeat malaria after so long.

3:26:12.940 --> 3:26:14.700
 And after the Gates Foundation and others

3:26:14.700 --> 3:26:18.460
 putting so much money into it.

3:26:18.460 --> 3:26:23.220
 I mean, I think clearly the whole global medical system,

3:26:23.220 --> 3:26:25.020
 the global health system

3:26:25.020 --> 3:26:28.260
 and the global political and socioeconomic system

3:26:28.260 --> 3:26:33.260
 are incredibly unethical and unequal and badly designed.

3:26:33.260 --> 3:26:38.260
 And I mean, I don't know how to solve that directly.

3:26:39.460 --> 3:26:42.300
 I think what we can do indirectly to solve it

3:26:42.300 --> 3:26:46.020
 is to make systems that operate in parallel

3:26:46.020 --> 3:26:49.180
 and off to the side of the governments

3:26:49.180 --> 3:26:52.020
 that are nominally controlling the world

3:26:52.020 --> 3:26:54.940
 with their armies and militias.

3:26:54.940 --> 3:26:58.500
 And to the extent that you can make compassionate

3:26:58.500 --> 3:27:01.900
 peer to peer decentralized frameworks

3:27:01.900 --> 3:27:03.580
 for doing things,

3:27:03.580 --> 3:27:06.580
 these are things that can start out unregulated.

3:27:06.580 --> 3:27:07.860
 And then if they get traction

3:27:07.860 --> 3:27:09.820
 before the regulators come in,

3:27:09.820 --> 3:27:12.220
 then they've influenced the way the world works, right?

3:27:12.220 --> 3:27:16.740
 SingularityNet aims to do this with AI.

3:27:16.740 --> 3:27:20.260
 REJUVE, which is a spinoff from SingularityNet.

3:27:20.260 --> 3:27:22.100
 You can see REJUVE.io.

3:27:22.100 --> 3:27:23.180
 How do you spell that?

3:27:23.180 --> 3:27:26.660
 R E J U V E, REJUVE.io.

3:27:26.660 --> 3:27:28.540
 That aims to do the same thing for medicine.

3:27:28.540 --> 3:27:31.140
 So it's like peer to peer sharing of information

3:27:31.140 --> 3:27:33.660
 peer to peer sharing of medical data.

3:27:33.660 --> 3:27:36.740
 So you can share medical data into a secure data wallet.

3:27:36.740 --> 3:27:39.500
 You can get advice about your health and longevity

3:27:39.500 --> 3:27:43.140
 through apps that REJUVE.io will launch

3:27:43.140 --> 3:27:44.660
 within the next couple of months.

3:27:44.660 --> 3:27:48.020
 And then SingularityNet AI can analyze all this data,

3:27:48.020 --> 3:27:50.100
 but then the benefits from that analysis

3:27:50.100 --> 3:27:52.780
 are spread among all the members of the network.

3:27:52.780 --> 3:27:54.700
 But I mean, of course,

3:27:54.700 --> 3:27:56.580
 I'm gonna hawk my particular projects,

3:27:56.580 --> 3:28:00.180
 but I mean, whether or not SingularityNet and REJUVE.io

3:28:00.180 --> 3:28:04.460
 are the answer, I think it's key to create

3:28:04.460 --> 3:28:09.180
 decentralized mechanisms for everything.

3:28:09.180 --> 3:28:13.300
 I mean, for AI, for human health, for politics,

3:28:13.300 --> 3:28:17.740
 for jobs and employment, for sharing social information.

3:28:17.740 --> 3:28:21.660
 And to the extent decentralized peer to peer methods

3:28:21.660 --> 3:28:25.500
 designed with universal compassion at the core

3:28:25.500 --> 3:28:29.780
 can gain traction, then these will just decrease the role

3:28:29.780 --> 3:28:31.260
 that government has.

3:28:31.260 --> 3:28:34.860
 And I think that's much more likely to do good

3:28:34.860 --> 3:28:37.860
 than trying to like explicitly reform

3:28:37.860 --> 3:28:39.180
 the global government system.

3:28:39.180 --> 3:28:41.740
 I mean, I'm happy other people are trying to explicitly

3:28:41.740 --> 3:28:43.900
 reform the global government system.

3:28:43.900 --> 3:28:47.180
 On the other hand, you look at how much good the internet

3:28:47.180 --> 3:28:50.660
 or Google did or mobile phones did,

3:28:50.660 --> 3:28:54.060
 even you're making something that's decentralized

3:28:54.060 --> 3:28:56.620
 and throwing it out everywhere and it takes hold,

3:28:56.620 --> 3:28:59.220
 then government has to adapt.

3:28:59.220 --> 3:29:01.740
 And I mean, that's what we need to do with AI

3:29:01.740 --> 3:29:02.580
 and with health.

3:29:02.580 --> 3:29:07.100
 And in that light, I mean, the centralization

3:29:07.100 --> 3:29:11.820
 of healthcare and of AI is certainly not ideal, right?

3:29:11.820 --> 3:29:15.980
 Like most AI PhDs are being sucked in by a half dozen

3:29:15.980 --> 3:29:17.220
 to a dozen big companies.

3:29:17.220 --> 3:29:20.820
 Most AI processing power is being bought

3:29:20.820 --> 3:29:23.660
 by a few big companies for their own proprietary good.

3:29:23.660 --> 3:29:26.860
 And most medical research is within a few

3:29:26.860 --> 3:29:29.420
 pharmaceutical companies and clinical trials

3:29:29.420 --> 3:29:31.740
 run by pharmaceutical companies will stay solid

3:29:31.740 --> 3:29:34.060
 within those pharmaceutical companies.

3:29:34.060 --> 3:29:37.220
 You know, these large centralized entities,

3:29:37.220 --> 3:29:40.460
 which are intelligences in themselves, these corporations,

3:29:40.460 --> 3:29:43.100
 but they're mostly malevolent psychopathic

3:29:43.100 --> 3:29:45.780
 and sociopathic intelligences,

3:29:45.780 --> 3:29:47.580
 not saying the people involved are,

3:29:47.580 --> 3:29:50.540
 but the corporations as self organizing entities

3:29:50.540 --> 3:29:53.260
 on their own, which are concerned with maximizing

3:29:53.260 --> 3:29:57.100
 shareholder value as a sole objective function.

3:29:57.100 --> 3:29:59.820
 I mean, AI and medicine are being sucked

3:29:59.820 --> 3:30:04.100
 into these pathological corporate organizations

3:30:04.100 --> 3:30:07.740
 with government cooperation and Google cooperating

3:30:07.740 --> 3:30:10.220
 with British and US government on this

3:30:10.220 --> 3:30:12.540
 as one among many, many different examples.

3:30:12.540 --> 3:30:15.940
 23andMe providing you the nice service of sequencing

3:30:15.940 --> 3:30:18.900
 your genome and then licensing the genome

3:30:18.900 --> 3:30:21.380
 to GlaxoSmithKline on an exclusive basis, right?

3:30:21.380 --> 3:30:23.460
 Now you can take your own DNA

3:30:23.460 --> 3:30:24.860
 and do whatever you want with it.

3:30:24.860 --> 3:30:28.100
 But the pooled collection of 23andMe sequence DNA

3:30:28.100 --> 3:30:30.820
 is just to GlaxoSmithKline.

3:30:30.820 --> 3:30:32.500
 Someone else could reach out to everyone

3:30:32.500 --> 3:30:36.300
 who had worked with 23andMe to sequence their DNA

3:30:36.300 --> 3:30:39.380
 and say, give us your DNA for our open

3:30:39.380 --> 3:30:41.700
 and decentralized repository that we'll make available

3:30:41.700 --> 3:30:43.700
 to everyone, but nobody's doing that

3:30:43.700 --> 3:30:45.700
 cause it's a pain to get organized.

3:30:45.700 --> 3:30:48.860
 And the customer list is proprietary to 23andMe, right?

3:30:48.860 --> 3:30:53.860
 So, yeah, I mean, this I think is a greater risk

3:30:54.340 --> 3:30:57.500
 to humanity from AI than rogue AGI

3:30:57.500 --> 3:31:01.100
 is turning the universe into paperclips or computronium.

3:31:01.100 --> 3:31:05.060
 Cause what you have here is mostly good hearted

3:31:05.060 --> 3:31:09.860
 and nice people who are sucked into a mode of organization

3:31:09.860 --> 3:31:12.580
 of large corporations, which has evolved

3:31:12.580 --> 3:31:14.180
 just for no individual's fault

3:31:14.180 --> 3:31:16.780
 just because that's the way society has evolved.

3:31:16.780 --> 3:31:18.900
 It's not altruistic, it's self interested

3:31:18.900 --> 3:31:20.540
 and become psychopathic like you said.

3:31:20.540 --> 3:31:21.380
 The human.

3:31:21.380 --> 3:31:23.700
 The corporation is psychopathic even if the people are not.

3:31:23.700 --> 3:31:26.660
 And that's really the disturbing thing about it

3:31:26.660 --> 3:31:30.500
 because the corporations can do things

3:31:30.500 --> 3:31:32.380
 that are quite bad for society

3:31:32.380 --> 3:31:35.580
 even if nobody has a bad intention.

3:31:35.580 --> 3:31:36.420
 Right.

3:31:36.420 --> 3:31:37.260
 And then.

3:31:37.260 --> 3:31:38.100
 No individual member of that corporation

3:31:38.100 --> 3:31:38.940
 has a bad intention.

3:31:38.940 --> 3:31:41.540
 No, some probably do, but it's not necessary

3:31:41.540 --> 3:31:43.180
 that they do for the corporation.

3:31:43.180 --> 3:31:47.060
 Like, I mean, Google, I know a lot of people in Google

3:31:47.060 --> 3:31:49.780
 and there are, with very few exceptions,

3:31:49.780 --> 3:31:51.300
 they're all very nice people

3:31:51.300 --> 3:31:53.980
 who genuinely want what's good for the world.

3:31:53.980 --> 3:31:56.940
 And Facebook, I know fewer people

3:31:56.940 --> 3:31:59.020
 but it's probably mostly true.

3:31:59.020 --> 3:32:01.460
 It's probably like fine young geeks

3:32:01.460 --> 3:32:03.940
 who wanna build cool technology.

3:32:03.940 --> 3:32:05.880
 I actually tend to believe that even the leaders,

3:32:05.880 --> 3:32:08.860
 even Mark Zuckerberg, one of the most disliked people

3:32:08.860 --> 3:32:11.940
 in tech is also wants to do good for the world.

3:32:11.940 --> 3:32:13.900
 I think about Jamie Dimon.

3:32:13.900 --> 3:32:14.740
 Who's Jamie Dimon?

3:32:14.740 --> 3:32:16.260
 Oh, the heads of the great banks

3:32:16.260 --> 3:32:17.620
 may have a different psychology.

3:32:17.620 --> 3:32:18.500
 Oh boy, yeah.

3:32:18.500 --> 3:32:22.820
 Well, I tend to be naive about these things

3:32:22.820 --> 3:32:27.340
 and see the best in, I tend to agree with you

3:32:27.340 --> 3:32:30.580
 that I think the individuals wanna do good by the world

3:32:30.580 --> 3:32:32.100
 but the mechanism of the company

3:32:32.100 --> 3:32:34.820
 can sometimes be its own intelligence system.

3:32:34.820 --> 3:32:38.500
 I mean, there's a, my cousin Mario Goetzler

3:32:38.500 --> 3:32:41.740
 has worked for Microsoft since 1985 or something

3:32:41.740 --> 3:32:44.160
 and I can see for him,

3:32:45.380 --> 3:32:48.980
 I mean, as well as just working on cool projects,

3:32:48.980 --> 3:32:51.340
 you're coding stuff that gets used

3:32:51.340 --> 3:32:54.560
 by like billions and billions of people.

3:32:54.560 --> 3:32:57.660
 And do you think if I improve this feature

3:32:57.660 --> 3:33:00.260
 that's making billions of people's lives easier, right?

3:33:00.260 --> 3:33:03.100
 So of course that's cool.

3:33:03.100 --> 3:33:05.520
 And the engineers are not in charge

3:33:05.520 --> 3:33:06.860
 of running the company anyway.

3:33:06.860 --> 3:33:10.120
 And of course, even if you're Mark Zuckerberg or Larry Page,

3:33:10.120 --> 3:33:13.560
 I mean, you still have a fiduciary responsibility.

3:33:13.560 --> 3:33:16.340
 And I mean, you're responsible to the shareholders,

3:33:16.340 --> 3:33:18.860
 your employees who you want to keep paying them

3:33:18.860 --> 3:33:19.700
 and so forth.

3:33:19.700 --> 3:33:22.900
 So yeah, you're enmeshed in this system.

3:33:22.900 --> 3:33:26.740
 And when I worked in DC,

3:33:26.740 --> 3:33:29.380
 I worked a bunch with INSCOM, US Army Intelligence

3:33:29.380 --> 3:33:31.900
 and I was heavily politically opposed

3:33:31.900 --> 3:33:34.740
 to what the US Army was doing in Iraq at that time,

3:33:34.740 --> 3:33:36.540
 like torturing people in Abu Ghraib

3:33:36.540 --> 3:33:39.860
 but everyone I knew in US Army and INSCOM,

3:33:39.860 --> 3:33:42.620
 when I hung out with them, was very nice person.

3:33:42.620 --> 3:33:43.520
 They were friendly to me.

3:33:43.520 --> 3:33:46.140
 They were nice to my kids and my dogs, right?

3:33:46.140 --> 3:33:48.380
 And they really believed that the US

3:33:48.380 --> 3:33:49.660
 was fighting the forces of evil.

3:33:49.660 --> 3:33:51.420
 And if you ask me about Abu Ghraib, they're like,

3:33:51.420 --> 3:33:54.460
 well, but these Arabs will chop us into pieces.

3:33:54.460 --> 3:33:56.300
 So how can you say we're wrong

3:33:56.300 --> 3:33:58.380
 to waterboard them a bit, right?

3:33:58.380 --> 3:34:00.340
 Like that's much less than what they would do to us.

3:34:00.340 --> 3:34:02.940
 It's just in their worldview,

3:34:02.940 --> 3:34:05.340
 what they were doing was really genuinely

3:34:05.340 --> 3:34:06.820
 for the good of humanity.

3:34:06.820 --> 3:34:09.020
 Like none of them woke up in the morning

3:34:09.020 --> 3:34:12.260
 and said like, I want to do harm to good people

3:34:12.260 --> 3:34:14.540
 because I'm just a nasty guy, right?

3:34:14.540 --> 3:34:18.220
 So yeah, most people on the planet,

3:34:18.220 --> 3:34:21.780
 setting aside a few genuine psychopaths and sociopaths,

3:34:21.780 --> 3:34:25.460
 I mean, most people on the planet have a heavy dose

3:34:25.460 --> 3:34:27.540
 of benevolence and wanting to do good

3:34:27.540 --> 3:34:32.160
 and also a heavy capability to convince themselves

3:34:32.160 --> 3:34:33.420
 whatever they feel like doing

3:34:33.420 --> 3:34:37.020
 or whatever is best for them is for the good of humankind.

3:34:37.020 --> 3:34:40.420
 So the more we can decentralize control.

3:34:40.420 --> 3:34:44.940
 Decentralization, you know, the democracy is horrible,

3:34:44.940 --> 3:34:47.320
 but this is like Winston Churchill said,

3:34:47.320 --> 3:34:49.380
 you know, it's the worst possible system of government

3:34:49.380 --> 3:34:50.700
 except for all the others, right?

3:34:50.700 --> 3:34:53.940
 I mean, I think the whole mess of humanity

3:34:53.940 --> 3:34:56.940
 has many, many very bad aspects to it,

3:34:56.940 --> 3:35:00.340
 but so far the track record of elite groups

3:35:00.340 --> 3:35:02.540
 who know what's better for all of humanity

3:35:02.540 --> 3:35:04.540
 is much worse than the track record

3:35:04.540 --> 3:35:08.040
 of the whole teaming democratic participatory

3:35:08.040 --> 3:35:09.540
 mess of humanity, right?

3:35:09.540 --> 3:35:13.420
 I mean, none of them is perfect by any means.

3:35:13.420 --> 3:35:16.660
 The issue with a small elite group that knows what's best

3:35:16.660 --> 3:35:20.340
 is even if it starts out as truly benevolent

3:35:20.340 --> 3:35:22.440
 and doing good things in accordance

3:35:22.440 --> 3:35:24.960
 with its initial good intentions,

3:35:24.960 --> 3:35:26.580
 you find out you need more resources,

3:35:26.580 --> 3:35:29.380
 you need a bigger organization, you pull in more people,

3:35:29.380 --> 3:35:32.940
 internal politics arises, difference of opinions arise

3:35:32.940 --> 3:35:37.940
 and bribery happens, like some opponent organization

3:35:38.140 --> 3:35:40.020
 takes a second in command now to make some,

3:35:40.020 --> 3:35:42.620
 the first in command of some other organization.

3:35:42.620 --> 3:35:45.580
 And I mean, that's, there's a lot of history

3:35:45.580 --> 3:35:47.380
 of what happens with elite groups

3:35:47.380 --> 3:35:50.100
 thinking they know what's best for the human race.

3:35:50.100 --> 3:35:53.060
 So yeah, if I have to choose,

3:35:53.060 --> 3:35:55.460
 I'm gonna reluctantly put my faith

3:35:55.460 --> 3:35:58.940
 in the vast democratic decentralized mass.

3:35:58.940 --> 3:36:02.900
 And I think corporations have a track record

3:36:02.900 --> 3:36:05.340
 of being ethically worse

3:36:05.340 --> 3:36:07.460
 than their constituent human parts.

3:36:07.460 --> 3:36:12.460
 And democratic governments have a more mixed track record,

3:36:13.540 --> 3:36:14.700
 but there are at least.

3:36:14.700 --> 3:36:15.860
 That's the best we got.

3:36:15.860 --> 3:36:18.500
 Yeah, I mean, you can, there's Iceland,

3:36:18.500 --> 3:36:19.660
 very nice country, right?

3:36:19.660 --> 3:36:23.340
 I've been very democratic for 800 plus years,

3:36:23.340 --> 3:36:26.860
 very, very benevolent, beneficial government.

3:36:26.860 --> 3:36:28.820
 And I think, yeah, there are track records

3:36:28.820 --> 3:36:31.860
 of democratic modes of organization.

3:36:31.860 --> 3:36:36.020
 Linux, for example, some of the people in charge of Linux

3:36:36.020 --> 3:36:38.580
 are overtly complete assholes, right?

3:36:38.580 --> 3:36:41.700
 And trying to reform themselves in many cases,

3:36:41.700 --> 3:36:45.980
 in other cases not, but the organization as a whole,

3:36:45.980 --> 3:36:49.700
 I think it's done a good job overall.

3:36:49.700 --> 3:36:53.980
 It's been very welcoming in the third world, for example,

3:36:53.980 --> 3:36:56.700
 and it's allowed advanced technology to roll out

3:36:56.700 --> 3:36:59.940
 on all sorts of different embedded devices and platforms

3:36:59.940 --> 3:37:02.100
 in places where people couldn't afford to pay

3:37:02.100 --> 3:37:03.820
 for proprietary software.

3:37:03.820 --> 3:37:08.820
 So I'd say the internet, Linux, and many democratic nations

3:37:09.140 --> 3:37:11.380
 are examples of how sort of an open,

3:37:11.380 --> 3:37:14.060
 decentralized democratic methodology

3:37:14.060 --> 3:37:16.580
 can be ethically better than the sum of the parts

3:37:16.580 --> 3:37:17.420
 rather than worse.

3:37:17.420 --> 3:37:21.420
 And corporations, that has happened only for a brief period

3:37:21.420 --> 3:37:24.580
 and then it goes sour, right?

3:37:24.580 --> 3:37:26.980
 I mean, I'd say a similar thing about universities.

3:37:26.980 --> 3:37:30.900
 Like university is a horrible way to organize research

3:37:30.900 --> 3:37:33.660
 and get things done, yet it's better than anything else

3:37:33.660 --> 3:37:34.500
 we've come up with, right?

3:37:34.500 --> 3:37:36.940
 A company can be much better,

3:37:36.940 --> 3:37:38.300
 but for a brief period of time,

3:37:38.300 --> 3:37:42.660
 and then it stops being so good, right?

3:37:42.660 --> 3:37:47.340
 So then I think if you believe that AGI

3:37:47.340 --> 3:37:50.700
 is gonna emerge sort of incrementally

3:37:50.700 --> 3:37:53.620
 out of AIs doing practical stuff in the world,

3:37:53.620 --> 3:37:57.060
 like controlling humanoid robots or driving cars

3:37:57.060 --> 3:38:01.260
 or diagnosing diseases or operating killer drones

3:38:01.260 --> 3:38:04.580
 or spying on people and reporting under the government,

3:38:04.580 --> 3:38:09.580
 then what kind of organization creates more and more

3:38:09.620 --> 3:38:12.500
 advanced narrow AI verging toward AGI

3:38:12.500 --> 3:38:14.620
 may be quite important because it will guide

3:38:14.620 --> 3:38:18.620
 like what's in the mind of the early stage AGI

3:38:18.620 --> 3:38:21.780
 as it first gains the ability to rewrite its own code base

3:38:21.780 --> 3:38:24.740
 and project itself toward super intelligence.

3:38:24.740 --> 3:38:29.740
 And if you believe that AI may move toward AGI

3:38:31.180 --> 3:38:33.300
 out of this sort of synergetic activity

3:38:33.300 --> 3:38:35.780
 of many agents cooperating together

3:38:35.780 --> 3:38:37.860
 rather than just have one person's project,

3:38:37.860 --> 3:38:42.580
 then who owns and controls that platform for AI cooperation

3:38:42.580 --> 3:38:47.260
 becomes also very, very important, right?

3:38:47.260 --> 3:38:49.380
 And is that platform AWS?

3:38:49.380 --> 3:38:50.580
 Is it Google Cloud?

3:38:50.580 --> 3:38:53.420
 Is it Alibaba or is it something more like the internet

3:38:53.420 --> 3:38:56.740
 or Singularity Net, which is open and decentralized?

3:38:56.740 --> 3:39:01.100
 So if all of my weird machinations come to pass, right?

3:39:01.100 --> 3:39:03.740
 I mean, we have the Hanson robots

3:39:03.740 --> 3:39:06.140
 being a beautiful user interface,

3:39:06.140 --> 3:39:09.100
 gathering information on human values

3:39:09.100 --> 3:39:12.060
 and being loving and compassionate to people in medical,

3:39:12.060 --> 3:39:14.620
 home service, robot office applications,

3:39:14.620 --> 3:39:16.900
 you have Singularity Net in the backend

3:39:16.900 --> 3:39:19.460
 networking together many different AIs

3:39:19.460 --> 3:39:21.500
 toward cooperative intelligence,

3:39:21.500 --> 3:39:24.020
 fueling the robots among many other things.

3:39:24.020 --> 3:39:27.340
 You have OpenCog 2.0 and true AGI

3:39:27.340 --> 3:39:29.420
 as one of the sources of AI

3:39:29.420 --> 3:39:31.700
 inside this decentralized network,

3:39:31.700 --> 3:39:34.140
 powering the robot and medical AIs

3:39:34.140 --> 3:39:36.300
 helping us live a long time

3:39:36.300 --> 3:39:39.740
 and cure diseases among other things.

3:39:39.740 --> 3:39:42.380
 And this whole thing is operating

3:39:42.380 --> 3:39:46.060
 in a democratic and decentralized way, right?

3:39:46.060 --> 3:39:50.420
 And I think if anyone can pull something like this off,

3:39:50.420 --> 3:39:53.900
 whether using the specific technologies I've mentioned

3:39:53.900 --> 3:39:55.780
 or something else, I mean,

3:39:55.780 --> 3:39:58.380
 then I think we have a higher odds

3:39:58.380 --> 3:40:02.740
 of moving toward a beneficial technological singularity

3:40:02.740 --> 3:40:06.220
 rather than one in which the first super AGI

3:40:06.220 --> 3:40:07.620
 is indifferent to humans

3:40:07.620 --> 3:40:11.900
 and just considers us an inefficient use of molecules.

3:40:11.900 --> 3:40:15.540
 That was a beautifully articulated vision for the world.

3:40:15.540 --> 3:40:16.700
 So thank you for that.

3:40:16.700 --> 3:40:20.460
 Well, let's talk a little bit about life and death.

3:40:21.860 --> 3:40:26.860
 I'm pro life and anti death for most people.

3:40:27.100 --> 3:40:29.460
 There's few exceptions that I won't mention here.

3:40:30.860 --> 3:40:32.340
 I'm glad just like your dad,

3:40:32.340 --> 3:40:34.660
 you're taking a stand against death.

3:40:36.420 --> 3:40:39.940
 You have, by the way, you have a bunch of awesome music

3:40:39.940 --> 3:40:41.780
 where you play piano online.

3:40:41.780 --> 3:40:45.380
 One of the songs that I believe you've written

3:40:45.380 --> 3:40:49.140
 the lyrics go, by the way, I like the way it sounds,

3:40:49.140 --> 3:40:51.460
 people should listen to it, it's awesome.

3:40:51.460 --> 3:40:54.980
 I considered, I probably will cover it, it's a good song.

3:40:54.980 --> 3:40:58.660
 Tell me why do you think it is a good thing

3:40:58.660 --> 3:41:01.980
 that we all get old and die is one of the songs.

3:41:01.980 --> 3:41:03.180
 I love the way it sounds,

3:41:03.180 --> 3:41:05.660
 but let me ask you about death first.

3:41:06.780 --> 3:41:08.300
 Do you think there's an element to death

3:41:08.300 --> 3:41:12.260
 that's essential to give our life meaning?

3:41:12.260 --> 3:41:14.020
 Like the fact that this thing ends.

3:41:14.020 --> 3:41:19.020
 Well, let me say I'm pleased and a little embarrassed

3:41:19.220 --> 3:41:21.540
 you've been listening to that music I put online.

3:41:21.540 --> 3:41:22.380
 That's awesome.

3:41:22.380 --> 3:41:25.540
 One of my regrets in life recently is I would love

3:41:25.540 --> 3:41:28.460
 to get time to really produce music well.

3:41:28.460 --> 3:41:31.100
 Like I haven't touched my sequencer software

3:41:31.100 --> 3:41:32.620
 in like five years.

3:41:32.620 --> 3:41:37.220
 I would love to like rehearse and produce and edit.

3:41:37.220 --> 3:41:39.580
 But with a two year old baby

3:41:39.580 --> 3:41:42.260
 and trying to create the singularity, there's no time.

3:41:42.260 --> 3:41:44.740
 So I just made the decision to,

3:41:45.660 --> 3:41:47.740
 when I'm playing random shit in an off moment.

3:41:47.740 --> 3:41:48.580
 Just record it.

3:41:48.580 --> 3:41:51.820
 Just record it, put it out there, like whatever.

3:41:51.820 --> 3:41:54.460
 Maybe if I'm unfortunate enough to die,

3:41:54.460 --> 3:41:56.260
 maybe that can be input to the AGI

3:41:56.260 --> 3:41:58.980
 when it tries to make an accurate mind upload of me, right?

3:41:58.980 --> 3:42:00.100
 Death is bad.

3:42:01.100 --> 3:42:02.700
 I mean, that's very simple.

3:42:02.700 --> 3:42:04.300
 It's baffling we should have to say that.

3:42:04.300 --> 3:42:08.740
 I mean, of course people can make meaning out of death.

3:42:08.740 --> 3:42:10.940
 And if someone is tortured,

3:42:10.940 --> 3:42:13.220
 maybe they can make beautiful meaning out of that torture

3:42:13.220 --> 3:42:14.540
 and write a beautiful poem

3:42:14.540 --> 3:42:16.980
 about what it was like to be tortured, right?

3:42:16.980 --> 3:42:19.100
 I mean, we're very creative.

3:42:19.100 --> 3:42:22.420
 We can milk beauty and positivity

3:42:22.420 --> 3:42:25.300
 out of even the most horrible and shitty things.

3:42:25.300 --> 3:42:27.860
 But just because if I was tortured,

3:42:27.860 --> 3:42:28.940
 I could write a good song

3:42:28.940 --> 3:42:30.780
 about what it was like to be tortured,

3:42:30.780 --> 3:42:31.980
 doesn't make torture good.

3:42:31.980 --> 3:42:35.660
 And just because people are able to derive meaning

3:42:35.660 --> 3:42:37.500
 and value from death,

3:42:37.500 --> 3:42:39.620
 doesn't mean they wouldn't derive even better meaning

3:42:39.620 --> 3:42:42.580
 and value from ongoing life without death,

3:42:42.580 --> 3:42:43.420
 which I very...

3:42:43.420 --> 3:42:44.260
 Indefinite.

3:42:44.260 --> 3:42:45.100
 Yeah, yeah.

3:42:45.100 --> 3:42:47.740
 So if you could live forever, would you live forever?

3:42:47.740 --> 3:42:48.620
 Forever.

3:42:50.460 --> 3:42:52.820
 My goal with longevity research

3:42:52.820 --> 3:42:57.460
 is to abolish the plague of involuntary death.

3:42:57.460 --> 3:43:00.340
 I don't think people should die unless they choose to die.

3:43:01.340 --> 3:43:05.700
 If I had to choose forced immortality

3:43:05.700 --> 3:43:09.180
 versus dying, I would choose forced immortality.

3:43:09.180 --> 3:43:11.860
 On the other hand, if I chose...

3:43:11.860 --> 3:43:13.500
 If I had the choice of immortality

3:43:13.500 --> 3:43:15.620
 with the choice of suicide whenever I felt like it,

3:43:15.620 --> 3:43:17.220
 of course I would take that instead.

3:43:17.220 --> 3:43:18.860
 And that's the more realistic choice.

3:43:18.860 --> 3:43:20.180
 I mean, there's no reason

3:43:20.180 --> 3:43:21.660
 you should have forced immortality.

3:43:21.660 --> 3:43:25.780
 You should be able to live until you get sick of living,

3:43:25.780 --> 3:43:26.620
 right?

3:43:26.620 --> 3:43:27.460
 I mean, that's...

3:43:27.460 --> 3:43:29.780
 And that will seem insanely obvious

3:43:29.780 --> 3:43:31.380
 to everyone 50 years from now.

3:43:31.380 --> 3:43:33.180
 And they will be so...

3:43:33.180 --> 3:43:35.980
 I mean, people who thought death gives meaning to life,

3:43:35.980 --> 3:43:37.660
 so we should all die,

3:43:37.660 --> 3:43:39.380
 they will look at that 50 years from now

3:43:39.380 --> 3:43:43.340
 the way we now look at the Anabaptists in the year 1000

3:43:43.340 --> 3:43:45.180
 who gave away all their positions,

3:43:45.180 --> 3:43:47.700
 went on top of the mountain for Jesus

3:43:47.700 --> 3:43:50.220
 to come and bring them to the ascension.

3:43:50.220 --> 3:43:55.220
 I mean, it's ridiculous that people think death is good

3:43:55.740 --> 3:44:00.180
 because you gain more wisdom as you approach dying.

3:44:00.180 --> 3:44:01.940
 I mean, of course it's true.

3:44:01.940 --> 3:44:03.460
 I mean, I'm 53.

3:44:03.460 --> 3:44:08.220
 And the fact that I might have only a few more decades left,

3:44:08.220 --> 3:44:11.460
 it does make me reflect on things differently.

3:44:11.460 --> 3:44:15.700
 It does give me a deeper understanding of many things.

3:44:15.700 --> 3:44:18.100
 But I mean, so what?

3:44:18.100 --> 3:44:19.500
 You could get a deep understanding

3:44:19.500 --> 3:44:20.900
 in a lot of different ways.

3:44:20.900 --> 3:44:22.460
 Pain is the same way.

3:44:22.460 --> 3:44:24.260
 We're gonna abolish pain.

3:44:24.260 --> 3:44:27.460
 And that's even more amazing than abolishing death, right?

3:44:27.460 --> 3:44:30.420
 I mean, once we get a little better at neuroscience,

3:44:30.420 --> 3:44:32.660
 we'll be able to go in and adjust the brain

3:44:32.660 --> 3:44:34.740
 so that pain doesn't hurt anymore, right?

3:44:34.740 --> 3:44:37.100
 And that, you know, people will say that's bad

3:44:37.100 --> 3:44:39.420
 because there's so much beauty

3:44:39.420 --> 3:44:41.100
 in overcoming pain and suffering.

3:44:41.100 --> 3:44:42.340
 Oh, sure.

3:44:42.340 --> 3:44:45.220
 And there's beauty in overcoming torture too.

3:44:45.220 --> 3:44:46.860
 And some people like to cut themselves,

3:44:46.860 --> 3:44:48.100
 but not many, right?

3:44:48.100 --> 3:44:48.940
 I mean.

3:44:48.940 --> 3:44:49.780
 That's an interesting.

3:44:49.780 --> 3:44:52.260
 So, but to push, I mean, to push back again,

3:44:52.260 --> 3:44:53.300
 this is the Russian side of me.

3:44:53.300 --> 3:44:55.020
 I do romanticize suffering.

3:44:55.020 --> 3:44:56.380
 It's not obvious.

3:44:56.380 --> 3:44:59.460
 I mean, the way you put it, it seems very logical.

3:44:59.460 --> 3:45:02.820
 It's almost absurd to romanticize suffering or pain

3:45:02.820 --> 3:45:07.740
 or death, but to me, a world without suffering,

3:45:07.740 --> 3:45:10.620
 without pain, without death, it's not obvious.

3:45:10.620 --> 3:45:13.500
 Well, then you can stay in the people's zoo,

3:45:13.500 --> 3:45:15.460
 people torturing each other.

3:45:15.460 --> 3:45:18.140
 No, but what I'm saying is I don't,

3:45:18.140 --> 3:45:20.220
 well, that's, I guess what I'm trying to say,

3:45:20.220 --> 3:45:22.820
 I don't know if I was presented with that choice,

3:45:22.820 --> 3:45:25.420
 what I would choose because it, to me.

3:45:25.420 --> 3:45:30.100
 This is a subtler, it's a subtler matter.

3:45:30.100 --> 3:45:33.980
 And I've posed it in this conversation

3:45:33.980 --> 3:45:37.100
 in an unnecessarily extreme way.

3:45:37.100 --> 3:45:41.060
 So I think, I think the way you should think about it

3:45:41.060 --> 3:45:44.700
 is what if there's a little dial on the side of your head

3:45:44.700 --> 3:45:48.180
 and you could turn how much pain hurt,

3:45:48.180 --> 3:45:50.660
 turn it down to zero, turn it up to 11,

3:45:50.660 --> 3:45:52.220
 like in spinal tap, if it wants,

3:45:52.220 --> 3:45:53.980
 maybe through an actual spinal tap, right?

3:45:53.980 --> 3:45:58.940
 So, I mean, would you opt to have that dial there or not?

3:45:58.940 --> 3:45:59.780
 That's the question.

3:45:59.780 --> 3:46:02.300
 The question isn't whether you would turn the pain down

3:46:02.300 --> 3:46:05.220
 to zero all the time.

3:46:05.220 --> 3:46:07.180
 Would you opt to have the dial or not?

3:46:07.180 --> 3:46:10.000
 My guess is that in some dark moment of your life,

3:46:10.000 --> 3:46:12.180
 you would choose to have the dial implanted

3:46:12.180 --> 3:46:13.340
 and then it would be there.

3:46:13.340 --> 3:46:17.180
 Just to confess a small thing, don't ask me why,

3:46:17.180 --> 3:46:20.760
 but I'm doing this physical challenge currently

3:46:20.760 --> 3:46:24.420
 where I'm doing 680 pushups and pull ups a day.

3:46:25.860 --> 3:46:29.180
 And my shoulder is currently, as we sit here,

3:46:29.180 --> 3:46:30.700
 in a lot of pain.

3:46:30.700 --> 3:46:35.700
 And I don't know, I would certainly right now,

3:46:35.860 --> 3:46:38.880
 if you gave me a dial, I would turn that sucker to zero

3:46:38.880 --> 3:46:40.540
 as quickly as possible.

3:46:40.540 --> 3:46:45.540
 But I think the whole point of this journey is,

3:46:46.740 --> 3:46:47.580
 I don't know.

3:46:47.580 --> 3:46:49.540
 Well, because you're a twisted human being.

3:46:49.540 --> 3:46:53.580
 I'm a twisted, so the question is am I somehow twisted

3:46:53.580 --> 3:46:57.440
 because I created some kind of narrative for myself

3:46:57.440 --> 3:47:00.820
 so that I can deal with the injustice

3:47:00.820 --> 3:47:02.420
 and the suffering in the world?

3:47:03.700 --> 3:47:06.340
 Or is this actually going to be a source of happiness

3:47:06.340 --> 3:47:07.180
 for me?

3:47:07.180 --> 3:47:10.820
 Well, this is to an extent is a research question

3:47:10.820 --> 3:47:12.300
 that humanity will undertake, right?

3:47:12.300 --> 3:47:17.300
 So I mean, human beings do have a particular biological

3:47:17.300 --> 3:47:22.300
 makeup, which sort of implies a certain probability

3:47:22.860 --> 3:47:25.880
 distribution over motivational systems, right?

3:47:25.880 --> 3:47:30.400
 So I mean, we, and that is there, that is there.

3:47:30.400 --> 3:47:35.400
 Now the question is how flexibly can that morph

3:47:36.540 --> 3:47:38.980
 as society and technology change, right?

3:47:38.980 --> 3:47:43.740
 So if we're given that dial and we're given a society

3:47:43.740 --> 3:47:47.540
 in which say we don't have to work for a living

3:47:47.540 --> 3:47:50.700
 and in which there's an ambient decentralized

3:47:50.700 --> 3:47:52.460
 benevolent AI network that will warn us

3:47:52.460 --> 3:47:54.660
 when we're about to hurt ourself,

3:47:54.660 --> 3:47:57.060
 if we're in a different context,

3:47:57.060 --> 3:48:02.060
 can we consistently with being genuinely and fully human,

3:48:02.880 --> 3:48:05.880
 can we consistently get into a state of consciousness

3:48:05.880 --> 3:48:09.220
 where we just want to keep the pain dial turned

3:48:09.220 --> 3:48:12.420
 all the way down and yet we're leading very rewarding

3:48:12.420 --> 3:48:13.860
 and fulfilling lives, right?

3:48:13.860 --> 3:48:17.660
 Now, I suspect the answer is yes, we can do that,

3:48:17.660 --> 3:48:21.580
 but I don't know that, I don't know that for certain.

3:48:21.580 --> 3:48:25.960
 Yeah, now I'm more confident that we could create

3:48:25.960 --> 3:48:30.960
 a nonhuman AGI system, which just didn't need an analog

3:48:31.220 --> 3:48:33.100
 of feeling pain.

3:48:33.100 --> 3:48:37.380
 And I think that AGI system will be fundamentally healthier

3:48:37.380 --> 3:48:39.740
 and more benevolent than human beings.

3:48:39.740 --> 3:48:42.340
 So I think it might or might not be true

3:48:42.340 --> 3:48:45.220
 that humans need a certain element of suffering

3:48:45.220 --> 3:48:49.460
 to be satisfied humans, consistent with the human physiology.

3:48:49.460 --> 3:48:53.220
 If it is true, that's one of the things that makes us fucked

3:48:53.220 --> 3:48:58.220
 and disqualified to be the super AGI, right?

3:48:58.380 --> 3:49:03.380
 I mean, the nature of the human motivational system

3:49:03.620 --> 3:49:08.620
 is that we seem to gravitate towards situations

3:49:08.620 --> 3:49:12.740
 where the best thing in the large scale

3:49:12.740 --> 3:49:15.860
 is not the best thing in the small scale

3:49:15.860 --> 3:49:18.100
 according to our subjective value system.

3:49:18.100 --> 3:49:20.740
 So we gravitate towards subjective value judgments

3:49:20.740 --> 3:49:22.940
 where to gratify ourselves in the large,

3:49:22.940 --> 3:49:25.620
 we have to ungratify ourselves in the small.

3:49:25.620 --> 3:49:29.340
 And we do that in, you see that in music,

3:49:29.340 --> 3:49:31.740
 there's a theory of music which says

3:49:31.740 --> 3:49:33.780
 the key to musical aesthetics

3:49:33.780 --> 3:49:36.860
 is the surprising fulfillment of expectations.

3:49:36.860 --> 3:49:38.900
 Like you want something that will fulfill

3:49:38.900 --> 3:49:41.820
 the expectations are listed in the prior part of the music,

3:49:41.820 --> 3:49:44.820
 but in a way with a bit of a twist that surprises you.

3:49:44.820 --> 3:49:48.140
 And I mean, that's true not only in outdoor music

3:49:48.140 --> 3:49:53.140
 like my own or that of Zappa or Steve Vai or Buckethead

3:49:53.300 --> 3:49:55.460
 or Christoph Pendergast or something,

3:49:55.460 --> 3:49:57.980
 it's even there in Mozart or something.

3:49:57.980 --> 3:49:59.980
 It's not there in elevator music too much,

3:49:59.980 --> 3:50:02.940
 but that's why it's boring, right?

3:50:02.940 --> 3:50:07.540
 But wrapped up in there is we want to hurt a little bit

3:50:07.540 --> 3:50:11.300
 so that we can feel the pain go away.

3:50:11.300 --> 3:50:15.700
 Like we wanna be a little confused by what's coming next.

3:50:15.700 --> 3:50:18.380
 So then when the thing that comes next actually makes sense,

3:50:18.380 --> 3:50:19.940
 it's so satisfying, right?

3:50:19.940 --> 3:50:22.300
 That's the surprising fulfillment of expectations,

3:50:22.300 --> 3:50:23.140
 is that what you said?

3:50:23.140 --> 3:50:23.960
 Yeah, yeah, yeah.

3:50:23.960 --> 3:50:24.800
 So beautifully put.

3:50:24.800 --> 3:50:26.820
 We've been skirting around a little bit,

3:50:26.820 --> 3:50:29.380
 but if I were to ask you the most ridiculous big question

3:50:29.380 --> 3:50:32.740
 of what is the meaning of life,

3:50:32.740 --> 3:50:34.380
 what would your answer be?

3:50:37.340 --> 3:50:40.080
 Three values, joy, growth, and choice.

3:50:43.580 --> 3:50:46.420
 I think you need joy.

3:50:46.420 --> 3:50:48.060
 I mean, that's the basis of everything.

3:50:48.060 --> 3:50:49.700
 If you want the number one value.

3:50:49.700 --> 3:50:54.700
 On the other hand, I'm unsatisfied with a static joy

3:50:54.860 --> 3:50:58.100
 that doesn't progress perhaps because of some

3:50:58.100 --> 3:51:00.140
 elemental element of human perversity,

3:51:00.140 --> 3:51:02.220
 but the idea of something that grows

3:51:02.220 --> 3:51:04.860
 and becomes more and more and better and better

3:51:04.860 --> 3:51:06.780
 in some sense appeals to me.

3:51:06.780 --> 3:51:10.580
 But I also sort of like the idea of individuality

3:51:10.580 --> 3:51:14.500
 that as a distinct system, I have some agency.

3:51:14.500 --> 3:51:18.820
 So there's some nexus of causality within this system

3:51:18.820 --> 3:51:22.420
 rather than the causality being wholly evenly distributed

3:51:22.420 --> 3:51:23.920
 over the joyous growing mass.

3:51:23.920 --> 3:51:27.080
 So you start with joy, growth, and choice

3:51:27.080 --> 3:51:28.860
 as three basic values.

3:51:28.860 --> 3:51:31.940
 Those three things could continue indefinitely.

3:51:31.940 --> 3:51:35.180
 That's something that can last forever.

3:51:35.180 --> 3:51:38.740
 Is there some aspect of something you called,

3:51:38.740 --> 3:51:43.740
 which I like, super longevity that you find exciting?

3:51:44.980 --> 3:51:48.340
 Is there research wise, is there ideas in that space that?

3:51:48.340 --> 3:51:53.240
 I mean, I think, yeah, in terms of the meaning of life,

3:51:53.240 --> 3:51:58.020
 this really ties into that because for us as humans,

3:51:58.020 --> 3:52:02.260
 probably the way to get the most joy, growth, and choice

3:52:02.260 --> 3:52:06.180
 is transhumanism and to go beyond the human form

3:52:06.180 --> 3:52:08.420
 that we have right now, right?

3:52:08.420 --> 3:52:10.980
 I mean, I think human body is great

3:52:10.980 --> 3:52:15.140
 and by no means do any of us maximize the potential

3:52:15.140 --> 3:52:18.560
 for joy, growth, and choice imminent in our human bodies.

3:52:18.560 --> 3:52:21.780
 On the other hand, it's clear that other configurations

3:52:21.780 --> 3:52:25.260
 of matter could manifest even greater amounts

3:52:25.260 --> 3:52:29.620
 of joy, growth, and choice than humans do,

3:52:29.620 --> 3:52:33.140
 maybe even finding ways to go beyond the realm of matter

3:52:33.140 --> 3:52:34.940
 as we understand it right now.

3:52:34.940 --> 3:52:38.100
 So I think in a practical sense,

3:52:38.100 --> 3:52:40.740
 much of the meaning I see in human life

3:52:40.740 --> 3:52:42.880
 is to create something better than humans

3:52:42.880 --> 3:52:45.460
 and go beyond human life.

3:52:45.460 --> 3:52:47.980
 But certainly that's not all of it for me

3:52:47.980 --> 3:52:49.220
 in a practical sense, right?

3:52:49.220 --> 3:52:51.740
 Like I have four kids and a granddaughter

3:52:51.740 --> 3:52:55.060
 and many friends and parents and family

3:52:55.060 --> 3:52:59.740
 and just enjoying everyday human social existence.

3:52:59.740 --> 3:53:00.900
 But we can do even better.

3:53:00.900 --> 3:53:01.740
 Yeah, yeah.

3:53:01.740 --> 3:53:03.860
 And I mean, I love, I've always,

3:53:03.860 --> 3:53:05.700
 when I could live near nature,

3:53:05.700 --> 3:53:08.740
 I spend a bunch of time out in nature in the forest

3:53:08.740 --> 3:53:10.940
 and on the water every day and so forth.

3:53:10.940 --> 3:53:15.040
 So, I mean, enjoying the pleasant moment is part of it,

3:53:15.040 --> 3:53:20.040
 but the growth and choice aspect are severely limited

3:53:20.780 --> 3:53:22.420
 by our human biology.

3:53:22.420 --> 3:53:25.980
 In particular, dying seems to inhibit your potential

3:53:25.980 --> 3:53:29.520
 for personal growth considerably as far as we know.

3:53:29.520 --> 3:53:32.980
 I mean, there's some element of life after death perhaps,

3:53:32.980 --> 3:53:34.980
 but even if there is,

3:53:34.980 --> 3:53:39.300
 why not also continue going in this biological realm, right?

3:53:39.300 --> 3:53:41.880
 In super longevity, I mean,

3:53:43.300 --> 3:53:45.580
 you know, we haven't yet cured aging.

3:53:45.580 --> 3:53:48.020
 We haven't yet cured death.

3:53:48.020 --> 3:53:51.860
 Certainly there's very interesting progress all around.

3:53:51.860 --> 3:53:56.860
 I mean, CRISPR and gene editing can be an incredible tool.

3:53:57.220 --> 3:54:00.120
 And I mean, right now,

3:54:00.120 --> 3:54:03.180
 stem cells could potentially prolong life a lot.

3:54:03.180 --> 3:54:05.980
 Like if you got stem cell injections

3:54:05.980 --> 3:54:09.140
 of just stem cells for every tissue of your body

3:54:09.140 --> 3:54:11.360
 injected into every tissue,

3:54:11.360 --> 3:54:15.360
 and you can just have replacement of your old cells

3:54:15.360 --> 3:54:17.340
 with new cells produced by those stem cells,

3:54:17.340 --> 3:54:21.240
 I mean, that could be highly impactful at prolonging life.

3:54:21.240 --> 3:54:23.260
 Now we just need slightly better technology

3:54:23.260 --> 3:54:25.420
 for having them grow, right?

3:54:25.420 --> 3:54:28.840
 So using machine learning to guide procedures

3:54:28.840 --> 3:54:32.700
 for stem cell differentiation and trans differentiation,

3:54:32.700 --> 3:54:33.740
 it's kind of nitty gritty,

3:54:33.740 --> 3:54:36.680
 but I mean, that's quite interesting.

3:54:36.680 --> 3:54:41.060
 So I think there's a lot of different things being done

3:54:41.060 --> 3:54:44.740
 to help with prolongation of human life,

3:54:44.740 --> 3:54:47.560
 but we could do a lot better.

3:54:47.560 --> 3:54:51.460
 So for example, the extracellular matrix,

3:54:51.460 --> 3:54:52.620
 which is the bunch of proteins

3:54:52.620 --> 3:54:54.300
 in between the cells in your body,

3:54:54.300 --> 3:54:57.360
 they get stiffer and stiffer as you get older.

3:54:57.360 --> 3:55:01.300
 And the extracellular matrix transmits information

3:55:01.300 --> 3:55:03.540
 both electrically, mechanically,

3:55:03.540 --> 3:55:05.380
 and to some extent, biophotonically.

3:55:05.380 --> 3:55:07.280
 So there's all this transmission

3:55:07.280 --> 3:55:08.880
 through the parts of the body,

3:55:08.880 --> 3:55:11.860
 but the stiffer the extracellular matrix gets,

3:55:11.860 --> 3:55:13.520
 the less the transmission happens,

3:55:13.520 --> 3:55:15.660
 which makes your body get worse coordinated

3:55:15.660 --> 3:55:17.460
 between the different organs as you get older.

3:55:17.460 --> 3:55:19.460
 So my friend Christian Schaffmeister

3:55:19.460 --> 3:55:22.460
 at my alumnus organization,

3:55:22.460 --> 3:55:25.100
 my Alma mater, the Great Temple University,

3:55:25.100 --> 3:55:28.640
 Christian Schaffmeister has a potential solution to this,

3:55:28.640 --> 3:55:32.340
 where he has these novel molecules called spiral ligamers,

3:55:32.340 --> 3:55:34.440
 which are like polymers that are not organic.

3:55:34.440 --> 3:55:37.780
 They're specially designed polymers

3:55:37.780 --> 3:55:39.420
 so that you can algorithmically predict

3:55:39.420 --> 3:55:41.580
 exactly how they'll fold very simply.

3:55:41.580 --> 3:55:43.280
 So he designed the molecular scissors

3:55:43.280 --> 3:55:45.560
 that have spiral ligamers that you could eat

3:55:45.560 --> 3:55:49.220
 and would then cut through all the glucosamine

3:55:49.220 --> 3:55:50.620
 and other crosslink proteins

3:55:50.620 --> 3:55:52.760
 in your extracellular matrix, right?

3:55:52.760 --> 3:55:55.200
 But to make that technology really work

3:55:55.200 --> 3:55:56.860
 and be mature as several years of work,

3:55:56.860 --> 3:56:00.140
 as far as I know, no one's finding it at the moment.

3:56:00.140 --> 3:56:02.380
 So there's so many different ways

3:56:02.380 --> 3:56:05.080
 that technology could be used to prolong longevity.

3:56:05.080 --> 3:56:06.540
 What we really need,

3:56:06.540 --> 3:56:09.580
 we need an integrated database of all biological knowledge

3:56:09.580 --> 3:56:12.020
 about human beings and model organisms,

3:56:12.020 --> 3:56:14.480
 like hopefully a massively distributed

3:56:14.480 --> 3:56:15.980
 open cog bioatom space,

3:56:15.980 --> 3:56:18.260
 but it can exist in other forms too.

3:56:18.260 --> 3:56:20.860
 We need that data to be opened up

3:56:20.860 --> 3:56:23.300
 in a suitably privacy protecting way.

3:56:23.300 --> 3:56:26.100
 We need massive funding into machine learning,

3:56:26.100 --> 3:56:29.240
 AGI, proto AGI statistical research

3:56:29.240 --> 3:56:31.240
 aimed at solving biology,

3:56:31.240 --> 3:56:33.440
 both molecular biology and human biology

3:56:33.440 --> 3:56:36.700
 based on this massive data set, right?

3:56:36.700 --> 3:56:40.700
 And then we need regulators not to stop people

3:56:40.700 --> 3:56:43.820
 from trying radical therapies on themselves

3:56:43.820 --> 3:56:46.180
 if they so wish to,

3:56:46.180 --> 3:56:49.420
 as well as better cloud based platforms

3:56:49.420 --> 3:56:52.720
 for like automated experimentation on microorganisms,

3:56:52.720 --> 3:56:54.300
 flies and mice and so forth.

3:56:54.300 --> 3:56:55.820
 And we could do all this.

3:56:55.820 --> 3:56:58.900
 You look after the last financial crisis,

3:56:58.900 --> 3:57:01.300
 Obama, who I generally like pretty well,

3:57:01.300 --> 3:57:03.740
 but he gave $4 trillion to large banks

3:57:03.740 --> 3:57:05.400
 and insurance companies.

3:57:05.400 --> 3:57:07.480
 You know, now in this COVID crisis,

3:57:08.420 --> 3:57:10.780
 trillions are being spent to help everyday people

3:57:10.780 --> 3:57:12.240
 and small businesses.

3:57:12.240 --> 3:57:14.580
 In the end, we'll probably will find many more trillions

3:57:14.580 --> 3:57:17.220
 are being given to large banks and insurance companies.

3:57:17.220 --> 3:57:21.020
 Anyway, like could the world put $10 trillion

3:57:21.020 --> 3:57:25.560
 into making a massive holistic bio AI and bio simulation

3:57:25.560 --> 3:57:27.800
 and experimental biology infrastructure?

3:57:27.800 --> 3:57:30.600
 We could, we could put $10 trillion into that

3:57:30.600 --> 3:57:32.300
 without even screwing us up too badly.

3:57:32.300 --> 3:57:35.260
 Just as in the end COVID and the last financial crisis

3:57:35.260 --> 3:57:37.900
 won't screw up the world economy so badly.

3:57:37.900 --> 3:57:39.900
 We're not putting $10 trillion into that.

3:57:39.900 --> 3:57:43.140
 Instead, all this research is siloed inside

3:57:43.140 --> 3:57:46.820
 a few big companies and government agencies.

3:57:46.820 --> 3:57:51.140
 And most of the data that comes from our individual bodies

3:57:51.140 --> 3:57:54.340
 personally, that could feed this AI to solve aging

3:57:54.340 --> 3:57:56.820
 and death, most of that data is sitting

3:57:56.820 --> 3:58:00.160
 in some hospital's database doing nothing, right?

3:58:03.960 --> 3:58:07.160
 I got two more quick questions for you.

3:58:07.160 --> 3:58:09.820
 One, I know a lot of people are gonna ask me,

3:58:09.820 --> 3:58:11.740
 you are on the Joe Rogan podcast

3:58:11.740 --> 3:58:13.660
 wearing that same amazing hat.

3:58:14.860 --> 3:58:17.500
 Do you have a origin story for the hat?

3:58:17.500 --> 3:58:21.420
 Does the hat have its own story that you're able to share?

3:58:21.420 --> 3:58:23.180
 The hat story has not been told yet.

3:58:23.180 --> 3:58:24.220
 So we're gonna have to come back

3:58:24.220 --> 3:58:25.880
 and you can interview the hat.

3:58:27.880 --> 3:58:30.060
 We'll leave that for the hat's own interview.

3:58:30.060 --> 3:58:30.900
 All right.

3:58:30.900 --> 3:58:32.100
 It's too much to pack into.

3:58:32.100 --> 3:58:32.940
 Is there a book?

3:58:32.940 --> 3:58:34.320
 Is the hat gonna write a book?

3:58:34.320 --> 3:58:35.160
 Okay.

3:58:35.160 --> 3:58:38.340
 Well, it may transmit the information

3:58:38.340 --> 3:58:40.020
 through direct neural transmission.

3:58:40.020 --> 3:58:41.420
 Okay, so it's actually,

3:58:41.420 --> 3:58:44.780
 there might be some Neuralink competition there.

3:58:44.780 --> 3:58:46.900
 Beautiful, we'll leave it as a mystery.

3:58:46.900 --> 3:58:49.040
 Maybe one last question.

3:58:49.040 --> 3:58:52.740
 If you build an AGI system,

3:58:54.580 --> 3:58:58.540
 you're successful at building the AGI system

3:58:58.540 --> 3:59:00.420
 that could lead us to the singularity

3:59:00.420 --> 3:59:04.560
 and you get to talk to her and ask her one question,

3:59:04.560 --> 3:59:05.960
 what would that question be?

3:59:05.960 --> 3:59:08.140
 We're not allowed to ask,

3:59:08.140 --> 3:59:10.220
 what is the question I should be asking?

3:59:10.220 --> 3:59:12.220
 Yeah, that would be cheating,

3:59:12.220 --> 3:59:14.040
 but I guess that's a good question.

3:59:14.040 --> 3:59:15.700
 I'm thinking of a,

3:59:15.700 --> 3:59:18.600
 I wrote a story with Stefan Bugay once

3:59:18.600 --> 3:59:23.380
 where these AI developers,

3:59:23.380 --> 3:59:25.900
 they created a super smart AI

3:59:25.900 --> 3:59:30.900
 aimed at answering all the philosophical questions

3:59:31.220 --> 3:59:32.060
 that have been worrying them.

3:59:32.060 --> 3:59:34.260
 Like what is the meaning of life?

3:59:34.260 --> 3:59:35.700
 Is there free will?

3:59:35.700 --> 3:59:37.980
 What is consciousness and so forth?

3:59:37.980 --> 3:59:40.380
 So they got the super AGI built

3:59:40.380 --> 3:59:43.300
 and it turned a while.

3:59:43.300 --> 3:59:46.580
 It said, those are really stupid questions.

3:59:46.580 --> 3:59:50.220
 And then it puts off on a spaceship and left the earth.

3:59:51.420 --> 3:59:54.320
 So you'd be afraid of scaring it off.

3:59:55.540 --> 3:59:56.500
 That's it, yeah.

3:59:56.500 --> 4:00:01.500
 I mean, honestly, there is no one question

4:00:01.500 --> 4:00:06.500
 that rises among all the others, really.

4:00:08.540 --> 4:00:10.020
 I mean, what interests me more

4:00:10.020 --> 4:00:13.500
 is upgrading my own intelligence

4:00:13.500 --> 4:00:18.500
 so that I can absorb the whole world view of the super AGI.

4:00:19.380 --> 4:00:23.100
 But I mean, of course, if the answer could be like,

4:00:23.100 --> 4:00:27.500
 what is the chemical formula for the immortality pill?

4:00:27.500 --> 4:00:32.500
 Like then I would do that or emit a bit string,

4:00:33.340 --> 4:00:38.340
 which will be the code for a super AGI

4:00:38.740 --> 4:00:41.220
 on the Intel i7 processor.

4:00:41.220 --> 4:00:42.860
 So those would be good questions.

4:00:42.860 --> 4:00:46.260
 So if your own mind was expanded

4:00:46.260 --> 4:00:49.340
 to become super intelligent, like you're describing,

4:00:49.340 --> 4:00:53.500
 I mean, there's kind of a notion

4:00:53.500 --> 4:00:57.840
 that intelligence is a burden, that it's possible

4:00:57.840 --> 4:01:00.020
 that with greater and greater intelligence,

4:01:00.020 --> 4:01:03.020
 that other metric of joy that you mentioned

4:01:03.020 --> 4:01:04.740
 becomes more and more difficult.

4:01:04.740 --> 4:01:05.900
 What's your sense?

4:01:05.900 --> 4:01:07.100
 Pretty stupid idea.

4:01:08.260 --> 4:01:09.860
 So you think if you're super intelligent,

4:01:09.860 --> 4:01:11.460
 you can also be super joyful?

4:01:11.460 --> 4:01:15.460
 I think getting root access to your own brain

4:01:15.460 --> 4:01:19.220
 will enable new forms of joy that we don't have now.

4:01:19.220 --> 4:01:22.740
 And I think as I've said before,

4:01:22.740 --> 4:01:27.740
 what I aim at is really make multiple versions of myself.

4:01:27.820 --> 4:01:30.180
 So I would like to keep one version,

4:01:30.180 --> 4:01:33.580
 which is basically human like I am now,

4:01:33.580 --> 4:01:36.980
 but keep the dial to turn pain up and down

4:01:36.980 --> 4:01:38.580
 and get rid of death, right?

4:01:38.580 --> 4:01:43.580
 And make another version which fuses its mind

4:01:43.640 --> 4:01:46.600
 with superhuman AGI,

4:01:46.600 --> 4:01:50.060
 and then will become massively transhuman.

4:01:50.060 --> 4:01:52.800
 And whether it will send some messages back

4:01:52.800 --> 4:01:55.580
 to the human me or not will be interesting to find out.

4:01:55.580 --> 4:01:58.500
 The thing is, once you're a super AGI,

4:01:58.500 --> 4:02:01.540
 like one subjective second to a human

4:02:01.540 --> 4:02:03.620
 might be like a million subjective years

4:02:03.620 --> 4:02:04.980
 to that super AGI, right?

4:02:04.980 --> 4:02:07.580
 So it would be on a whole different basis.

4:02:07.580 --> 4:02:10.940
 I mean, at very least those two copies will be good to have,

4:02:10.940 --> 4:02:13.980
 but it could be interesting to put your mind

4:02:13.980 --> 4:02:16.860
 into a dolphin or a space amoeba

4:02:16.860 --> 4:02:18.520
 or all sorts of other things.

4:02:18.520 --> 4:02:21.060
 You can imagine one version that doubled its intelligence

4:02:21.060 --> 4:02:24.140
 every year and another version that just became

4:02:24.140 --> 4:02:26.140
 a super AGI as fast as possible, right?

4:02:26.140 --> 4:02:29.780
 So, I mean, now we're sort of constrained to think

4:02:29.780 --> 4:02:33.020
 one mind, one self, one body, right?

4:02:33.020 --> 4:02:36.260
 But I think we actually, we don't need to be that

4:02:36.260 --> 4:02:40.820
 constrained in thinking about future intelligence

4:02:40.820 --> 4:02:44.280
 after we've mastered AGI and nanotechnology

4:02:44.280 --> 4:02:47.820
 and longevity biology.

4:02:47.820 --> 4:02:49.540
 I mean, then each of our minds

4:02:49.540 --> 4:02:52.020
 is a certain pattern of organization, right?

4:02:52.020 --> 4:02:54.300
 And I know we haven't talked about consciousness,

4:02:54.300 --> 4:02:56.860
 but I sort of, I'm panpsychist.

4:02:56.860 --> 4:03:00.080
 I sort of view the universe as conscious.

4:03:00.080 --> 4:03:03.860
 And so, you know, a light bulb or a quark

4:03:03.860 --> 4:03:06.040
 or an ant or a worm or a monkey

4:03:06.040 --> 4:03:08.780
 have their own manifestations of consciousness.

4:03:08.780 --> 4:03:11.900
 And the human manifestation of consciousness,

4:03:11.900 --> 4:03:15.580
 it's partly tied to the particular meat

4:03:15.580 --> 4:03:19.380
 that we're manifested by, but it's largely tied

4:03:19.380 --> 4:03:22.360
 to the pattern of organization in the brain, right?

4:03:22.360 --> 4:03:25.040
 So, if you upload yourself into a computer

4:03:25.040 --> 4:03:28.640
 or a robot or whatever else it is,

4:03:28.640 --> 4:03:31.780
 some element of your human consciousness may not be there

4:03:31.780 --> 4:03:34.260
 because it's just tied to the biological embodiment.

4:03:34.260 --> 4:03:36.300
 But I think most of it will be there.

4:03:36.300 --> 4:03:40.020
 And these will be incarnations of your consciousness

4:03:40.020 --> 4:03:42.500
 in a slightly different flavor.

4:03:42.500 --> 4:03:45.600
 And, you know, creating these different versions

4:03:45.600 --> 4:03:48.500
 will be amazing, and each of them will discover

4:03:48.500 --> 4:03:52.020
 meanings of life that have some overlap,

4:03:52.020 --> 4:03:54.300
 but probably not total overlap

4:03:54.300 --> 4:03:59.260
 with the human Ben's meaning of life.

4:03:59.260 --> 4:04:02.940
 The thing is, to get to that future

4:04:02.940 --> 4:04:06.500
 where we can explore different varieties of joy,

4:04:06.500 --> 4:04:09.680
 different variations of human experience and values

4:04:09.680 --> 4:04:13.140
 and transhuman experiences and values to get to that future,

4:04:13.140 --> 4:04:16.780
 we need to navigate through a whole lot of human bullshit

4:04:16.780 --> 4:04:21.480
 of companies and governments and killer drones

4:04:21.480 --> 4:04:25.460
 and making and losing money and so forth, right?

4:04:25.460 --> 4:04:28.580
 And that's the challenge we're facing now

4:04:28.580 --> 4:04:30.740
 is if we do things right,

4:04:30.740 --> 4:04:33.580
 we can get to a benevolent singularity,

4:04:33.580 --> 4:04:36.320
 which is levels of joy, growth, and choice

4:04:36.320 --> 4:04:39.920
 that are literally unimaginable to human beings.

4:04:39.920 --> 4:04:41.720
 If we do things wrong,

4:04:41.720 --> 4:04:44.120
 we could either annihilate all life on the planet,

4:04:44.120 --> 4:04:47.060
 or we could lead to a scenario where, say,

4:04:47.060 --> 4:04:52.060
 all humans are annihilated and there's some super AGI

4:04:52.140 --> 4:04:55.460
 that goes on and does its own thing unrelated to us

4:04:55.460 --> 4:04:58.380
 except via our role in originating it.

4:04:58.380 --> 4:05:02.420
 And we may well be at a bifurcation point now, right?

4:05:02.420 --> 4:05:05.820
 Where what we do now has significant causal impact

4:05:05.820 --> 4:05:06.720
 on what comes about,

4:05:06.720 --> 4:05:09.040
 and yet most people on the planet

4:05:09.040 --> 4:05:11.540
 aren't thinking that way whatsoever,

4:05:11.540 --> 4:05:16.220
 they're thinking only about their own narrow aims

4:05:16.220 --> 4:05:17.780
 and aims and goals, right?

4:05:17.780 --> 4:05:20.880
 Now, of course, I'm thinking about my own narrow aims

4:05:20.880 --> 4:05:24.260
 and goals to some extent also,

4:05:24.260 --> 4:05:29.260
 but I'm trying to use as much of my energy and mind as I can

4:05:29.480 --> 4:05:33.200
 to push toward this more benevolent alternative,

4:05:33.200 --> 4:05:34.660
 which will be better for me,

4:05:34.660 --> 4:05:37.980
 but also for everybody else.

4:05:37.980 --> 4:05:42.540
 And it's weird that so few people understand

4:05:42.540 --> 4:05:43.380
 what's going on.

4:05:43.380 --> 4:05:44.780
 I know you interviewed Elon Musk,

4:05:44.780 --> 4:05:47.380
 and he understands a lot of what's going on,

4:05:47.380 --> 4:05:49.620
 but he's much more paranoid than I am, right?

4:05:49.620 --> 4:05:52.040
 Because Elon gets that AGI

4:05:52.040 --> 4:05:54.260
 is gonna be way, way smarter than people,

4:05:54.260 --> 4:05:57.100
 and he gets that an AGI does not necessarily

4:05:57.100 --> 4:05:58.740
 have to give a shit about people

4:05:58.740 --> 4:06:01.660
 because we're a very elementary mode of organization

4:06:01.660 --> 4:06:04.700
 of matter compared to many AGI's.

4:06:04.700 --> 4:06:06.340
 But I don't think he has a clear vision

4:06:06.340 --> 4:06:10.140
 of how infusing early stage AGI's

4:06:10.140 --> 4:06:13.540
 with compassion and human warmth

4:06:13.540 --> 4:06:18.020
 can lead to an AGI that loves and helps people

4:06:18.020 --> 4:06:22.860
 rather than viewing us as a historical artifact

4:06:22.860 --> 4:06:26.200
 and a waste of mass energy.

4:06:26.200 --> 4:06:28.060
 But on the other hand,

4:06:28.060 --> 4:06:29.600
 while I have some disagreements with him,

4:06:29.600 --> 4:06:33.140
 like he understands way, way more of the story

4:06:33.140 --> 4:06:34.820
 than almost anyone else

4:06:34.820 --> 4:06:38.180
 in such a large scale corporate leadership position, right?

4:06:38.180 --> 4:06:40.740
 It's terrible how little understanding

4:06:40.740 --> 4:06:45.060
 of these fundamental issues exists out there now.

4:06:45.060 --> 4:06:47.220
 That may be different five or 10 years from now though,

4:06:47.220 --> 4:06:51.180
 because I can see understanding of AGI and longevity

4:06:51.180 --> 4:06:54.620
 and other such issues is certainly much stronger

4:06:54.620 --> 4:06:57.620
 and more prevalent now than 10 or 15 years ago, right?

4:06:57.620 --> 4:07:02.620
 So I mean, humanity as a whole can be slow learners

4:07:02.860 --> 4:07:05.460
 relative to what I would like,

4:07:05.460 --> 4:07:08.400
 but on a historical sense, on the other hand,

4:07:08.400 --> 4:07:11.220
 you could say the progress is astoundingly fast.

4:07:11.220 --> 4:07:15.640
 But Elon also said, I think on the Joe Rogan podcast,

4:07:15.640 --> 4:07:17.380
 that love is the answer.

4:07:17.380 --> 4:07:21.820
 So maybe in that way, you and him are both on the same page

4:07:21.820 --> 4:07:24.420
 of how we should proceed with AGI.

4:07:24.420 --> 4:07:27.300
 I think there's no better place to end it.

4:07:27.300 --> 4:07:30.860
 I hope we get to talk again about the hat

4:07:30.860 --> 4:07:32.020
 and about consciousness

4:07:32.020 --> 4:07:34.500
 and about a million topics we didn't cover.

4:07:34.500 --> 4:07:36.340
 Ben, it's a huge honor to talk to you.

4:07:36.340 --> 4:07:37.540
 Thank you for making it out.

4:07:37.540 --> 4:07:39.540
 Thank you for talking today.

4:07:39.540 --> 4:07:40.440
 Thanks for having me.

4:07:40.440 --> 4:07:44.380
 This was really, really good fun

4:07:44.380 --> 4:07:47.420
 and we dug deep into some very important things.

4:07:47.420 --> 4:07:48.740
 So thanks for doing this.

4:07:48.740 --> 4:07:49.820
 Thanks very much.

4:07:49.820 --> 4:07:51.200
 Awesome.

4:07:51.200 --> 4:07:53.860
 Thanks for listening to this conversation with Ben Gertzel

4:07:53.860 --> 4:07:55.860
 and thank you to our sponsors,

4:07:55.860 --> 4:07:59.380
 The Jordan Harbinger Show and Masterclass.

4:07:59.380 --> 4:08:01.080
 Please consider supporting the podcast

4:08:01.080 --> 4:08:04.580
 by going to jordanharbinger.com slash lex

4:08:04.580 --> 4:08:09.580
 and signing up to Masterclass at masterclass.com slash lex.

4:08:09.800 --> 4:08:12.280
 Click the links, buy the stuff.

4:08:12.280 --> 4:08:14.220
 It's the best way to support this podcast

4:08:14.220 --> 4:08:18.860
 and the journey I'm on in my research and startup.

4:08:18.860 --> 4:08:21.380
 If you enjoy this thing, subscribe on YouTube,

4:08:21.380 --> 4:08:23.720
 review it with five stars on a podcast,

4:08:23.720 --> 4:08:26.860
 support it on Patreon or connect with me on Twitter

4:08:26.860 --> 4:08:31.860
 at lexfriedman spelled without the E, just F R I D M A N.

4:08:32.400 --> 4:08:35.280
 I'm sure eventually you will figure it out.

4:08:35.280 --> 4:08:38.240
 And now let me leave you with some words from Ben Gertzel.

4:08:39.140 --> 4:08:42.540
 Our language for describing emotions is very crude.

4:08:42.540 --> 4:08:43.940
 That's what music is for.

4:08:43.940 --> 4:08:56.940
 Thank you for listening and hope to see you next time.

